{"paper":"arxiv:2305.18290", "question":"Given the following formula:\n\\begin{equation}\n\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\pi_{\\text{ref}}(y\\mid x)\\bigr],\n\\end{equation}\nprove that the optimal solution under the KL constraint for maximizing the reward is:\n\\begin{equation}\n    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n\\end{equation}\nwhere $Z(x) =\\sum_{y}\\pi_{\\text{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function.", "whole_label":"To derive the optimal solution under the KL constraint for maximizing the reward, we start by considering the optimization problem:\n\\begin{equation}\n\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi}\\bigl[r(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi(y|x)||\\pi_{\\text{ref}}(y|x)\\bigr].\n\\end{equation}\nThis can be rewritten as:\n\\begin{align}\n\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[r(x, y) - \\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right].\n\\end{align}\nBy converting the maximization problem into a minimization problem, we obtain:\n\\begin{align}\n\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\frac{1}{\\beta}r(x, y)\\right].\n\\end{align}\nIntroducing the partition function $Z(x) = \\sum_{y}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$, we can express the objective as:\n\\begin{align}\n\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)} - \\log Z(x)\\right].\n\\end{align}\nDefining the optimal policy as:\n\\begin{equation}\n\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n\\end{equation}\nwhich is a valid probability distribution, we can further simplify the objective to:\n\\begin{align}\n\\min_{\\pi}\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)||\\pi^*(y|x)) - \\log Z(x)\\right].\n\\end{align}\nSince $Z(x)$ does not depend on $\\pi$, the minimum is achieved when the KL divergence is minimized. According to Gibbs' inequality, the KL divergence is minimized to 0 when $\\pi(y|x) = \\pi^*(y|x)$. Therefore, the optimal solution is:\n\\begin{equation}\n\\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right).\n\\end{equation}\nThis completes the derivation, showing that the optimal policy under the given constraints is indeed the one specified."}
{"paper":"arxiv:2305.18290", "question":"Prove the following theorem: Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization $r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}$ for some model $\\pi(y\\mid x)$ and a given reference model $\\pi_{\\text{ref}}(y \\mid x)$.", "whole_label":"Consider any reward function $r(x,y)$, which induces an optimal model $\\pi_r(y|x)$ under the KL-constrained RL problem. The solution is given by the optimal policy equation. When we log-linearize both sides, we obtain:\n\\begin{equation*}\nr(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)\n\\end{equation*}\nwhere $Z(x) =\\sum_{y}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ (notice that $Z(x)$ also depends on the reward function $r$). Using the operator $r'(x, y) = f(r, \\pi_{\\text{ref}}, \\beta)(x, y) = r(x, y) - \\beta \\log Z(x)$, we see that this new reward function is within the equivalence class of $r$ and, we have:\n\\begin{equation*}\nr'(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_{\\text{ref}}(y|x)}\n\\end{equation*}\nwhich completes the proof."}
{"paper":"arxiv:2407.18242", "question":"Given $W = W_0 + \\Delta W \\approx W_0 + sBA$, where $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ are low-rank matrices, and $s$ is a scaling factor. The differential of $W$ is given by $\\dd W = \\frac{\\partial W}{\\partial A}^T \\dd A + \\frac{\\partial W}{\\partial B}^T \\dd B = -(sBg^A + sg^BA)$. Prove that the rank of $\\tilde{g} = sBg^A + sg^BA$ is at most $2r$.", "whole_label":"To prove that the rank of $\\tilde{g} = sBg^A + sg^BA$ is at most $2r$, we proceed with the following reasoning:\n\n1. Subadditivity of Matrix Rank: The rank of the sum of two matrices is less than or equal to the sum of their ranks. Therefore,\n   \\begin{equation}\n   rank(\\tilde{g}) = rank(sg^BA + sBg^A) \\le rank(g^BA) + rank(Bg^A).\n   \\end{equation}\n\n2. Rank of Product of Matrices: For any matrices $X$ and $Y$, the rank of their product $XY$ satisfies $rank(XY) \\le \\min(rank(X), rank(Y))$. Applying this property,\n   \\begin{align}\n   rank(g^BA) & \\le \\min(rank(g^B), rank(A)) \\le r, \\\\\n   rank(Bg^A) & \\le \\min(rank(B), rank(g^A)) \\le r.\n   \\end{align}\n\n3. Conclusion: Combining the above results, we conclude that\n   \\begin{align}\n   rank(\\tilde{g}) & \\le rank(g^BA) + rank(Bg^A) \\\\\n   & \\le \\min(rank(g^B), rank(A)) + \\min(rank(B), rank(g^A)) \\\\\n   & \\le 2r.\n   \\end{align}\n\nThus, the rank of $\\tilde{g}$ is indeed at most $2r$."}
{"paper":"arxiv:2407.18242", "question":"Prove the following theorem:\n\\begin{theorem}\nAssume matrices $B\\in\\mathbb{R}^{m\\times r}, A\\in\\mathbb{R}^{r\\times n}$ are both full rank. \nFor the objective $\\min_{g^A, g^B} \\|\\tilde{g} - g\\|^2_F$, the optimal solutions are given by:\n\\begin{align}\n    g^A &= \\frac{1}{s}(B^TB)^{-1}B^Tg + XA = \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} + XA, \\\\ \n    g^B &= \\frac{1}{s}[I - B(B^TB)^{-1}B^T]gA^T(AA^T)^{-1} - BX \\\\\n        &= \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} - BX. \n\\end{align}\nHere, $X\\in\\mathbb{R}^{r\\times r}$ represents an arbitrary matrix.\n\\end{theorem}", "whole_label":"To solve the optimization problem, we denote $L = \\|sBg^A + sg^BA - g\\|_F^2$ and derive the necessary conditions for optimality:\n\\begin{align}\n\\frac{\\partial L}{\\partial g^A} &= 2sB^T(sBg^A + sg^BA - g) = 0, \\\\\n\\frac{\\partial L}{\\partial g^B} &= 2(sBg^A + sg^BA - g)sA^T = 0.\n\\end{align}\nGiven that matrices $A$ and $B$ are full-rank, $AA^T$ and $B^TB$ are invertible. From the second condition, we derive:\n\\begin{equation}\ng^B = \\frac{1}{s}gA^T(AA^T)^{-1} - Bg^AA^T(AA^T)^{-1}.\n\\end{equation}\nSubstituting this into the first condition, we obtain a linear equation:\n\\begin{equation}\ng^A[I - A^T(AA^T)^{-1}A]=\\frac{1}{s}(B^TB)^{-1}B^Tg[I - A^T(AA^T)^{-1}A].\n\\end{equation}\nRecognizing that the matrix $P = I - A^T(AA^T)^{-1}A$ is a projection matrix, the solution to the linear equation is:\n\\begin{equation}\ng^A = \\frac{1}{s}(B^TB)^{-1}B^Tg + XA,\n\\end{equation}\nwhere $X\\in\\mathbb{R}^{r\\times r}$ is an arbitrary matrix. Substituting this solution back into the expression for $g^B$ yields:\n\\begin{equation}\ng^B = \\frac{1}{s}[I - B(B^TB)^{-1}B^T]gA^T(AA^T)^{-1} - BX.\n\\end{equation}\nTo express the solutions in terms of the gradients of standard LoRA, we use:\n\\begin{align}\ng^A_{lora} = sB^Tg, \\quad g^B_{lora} = sgA^T.\n\\end{align}\nThus, the optimal solutions can be rewritten as:\n\\begin{align}\ng^A &= \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} + XA, \\\\\ng^B &= \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} - BX.\n\\end{align}\nThis completes the proof, showing how the gradients can be adjusted and updated within the framework of LoRA optimization."}
{"paper":"arxiv:2407.18242", "question":"Prove the following theorem:\n\\begin{theorem}\nConsider the optimization problem,\n\\begin{equation}\n    \\min_X \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2,\n\\end{equation}\nwhere $g^A$ and $g^B$ are the optimal solutions.\nThe optimal $X$ can be determined by solving the Sylvester equation:\n\\begin{equation}\n    B^TBX + XAA^T = -\\frac{1}{s^2}(B^TB)^{-1}g^A_{lora}A^T,\n\\end{equation}\nwhich has a unique solution $X$ provided that $B^TB$ and $-AA^T$ do not have any shared eigenvalues.\n\\end{theorem}", "whole_label":"\\begin{proof}\nFor simplicity, we denote $L = \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2$.\nTo solve the optimization problem, we need to satisfy the following conditions:\n\\begin{equation}\n\\frac{\\partial L}{\\partial X} = 0.\n\\end{equation}\nGiven that $g^A_{lora} = sB^Tg$ and $g^B_{lora} = sgA^T$, we derive:\n\\begin{equation}\n\\begin{aligned}\n2(g^A - g^A_{lora})A^T &- 2B^T(g^B - g^B_{lora}) = 0, \\\\\n\\Rightarrow \\quad\ng^AA^T - B^Tg^B & = g^A_{lora}A^T - B^Tg^B_{lora}, \\\\\n\\Rightarrow \\quad\nB^TBX + XAA^T & = -\\frac{1}{s^2}(B^TB)^{-1}g^A_{lora}A^T,\n\\end{aligned}\n\\end{equation}\nwhich is a Sylvester equation. This equation has a unique solution for $X$ if and only if $B^TB$ and $-AA^T$ have no shared eigenvalues.\n\\end{proof}"}
{"paper":"arxiv:2405.20304", "question":"How to derive the robust objective in equation (9) from the robust objective in equation (8). Equation (8):\n\\begin{equation}\n\\label{eq: max-min objective}\n     \\min_\\pi \\rdpol(\\pi) =\\min_\\pi \\max_{\\alpha \\in \\Delta_K}\\sum_{g=1}^{K}\\alpha_g \\Big(-\\Ebb_{(x_g,y_w,y_l)\\sim \\train_g}\\Big[\\log\\Big( \\sigma (\\beta h_{\\pi}(x_g, y_w, y_l)\\Big)\\Big]\\Big),\n\\end{equation}\nwhere $\\Delta_K$ represents the $(K-1)$-dimensional simplex of probabilities.\nEquation (9):\n\\begin{equation}\\label{eq:robust-lin-obj}\n    \\min_{\\theta\\in\\Theta}\\max_{\\alpha \\in \\Delta_K} \\sum_{g=1}^{K}\\alpha_g \\Big(-\\Ebb_{(x_g,y_w,y_l)\\sim \\train_g}\\Big[\\log\\Big(\\sigma\\big(\\beta\\langle\\phi(x,y_w)-\\phi(x,y_l),\\theta\\rangle\\big)\\Big)\\Big]\\Big).\n\\end{equation}", "whole_label":"To derive the robust objective in equation (9) from equation (8), we follow these steps:\n1. Start with the general policy class robust objective in equation (8).\n2. Specialize the policy $\\pi$ to a log-linear policy class, leading to the expression of $h_{\\pi}(x_g, y_w, y_l)$ in terms of $\\theta$ and $\\phi$.\n3. Simplify the expression by assuming a uniform reference policy, which allows us to remove the reference policy terms.\n4. Substitute the log-linear policy form $\\pi_{\\theta}(y|x_g) = \\frac{\\exp{f_{\\theta}(x_g,y)}}{\\sum_{y\\in \\mathcal{Y}}\\exp{f_{\\theta}(x_g,y)}}$ into the equation.\n5. Further specialize $f_{\\theta}(x_g,y)$ to be a linear function in $\\theta$, i.e., $f_{\\theta}(x_g,y) = \\theta^T \\phi(x_g,y)$.\n6. Simplify the resulting expression to obtain the final form in equation (9), which is the robust objective for the log-linear policy class.\n\nThe key steps involve specializing the general policy to a log-linear form and simplifying under the assumption of a uniform reference policy, leading to the final robust objective that depends linearly on the parameters $\\theta$ through the feature map $\\phi$."}
{"paper":"arxiv:2405.20304", "question":"Given the equations:\n\n1. Equation (4):\n\\begin{equation}\nr(x, y) = \\beta \\log \\frac{\\pi^{*}(y\\mid x)}{\\refpolicy(y\\mid x)} + \\beta \\log Z(x),\n\\end{equation}\nwhere $Z(x)=\\sum_y\\refpolicy(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$.\n\n2. Equation (10):\n\\begin{equation}\n\\max_{\\pi} \\min_{g\\in\\mathcal{G}}\\Ebb_{x_g\\sim \\mathcal{P}_{x_g},y\\sim\\pi(\\cdot\\mid x_g)}\\Big[r(x_g,y)-\\beta  \\mathrm{KL}\\big[\\pi(y\\mid x_g)||\\refpolicy(y\\mid x_g)\\big]\\Big].\n\\end{equation}\n\nHow to prove that the optimal policy for equation (10) is the same as the solution to the non-robust KL-regularized reward maximization problem in equation (4)?", "whole_label":"To prove that the optimal policy for equation (10) aligns with the solution to the non-robust KL-regularized reward maximization problem in equation (4), we follow these steps:\n\n1. Recast the Objective: Start by recasting the robust KL-regularized reward maximization objective based on the max-min formulation.\n\n2. Derivation: Perform a series of transformations to express the objective in terms of the KL divergence between the policy $\\pi$ and the optimal policy $\\pi^{*}$, and the partition function $Z(x)$.\n\n3. Define Optimal Policy: Define the optimal policy $\\pi^{*}(y|x_g) = \\frac{1}{Z(x_g)}\\refpolicy(y|x_g)\\exp(\\frac{r(x_g,y)}{\\beta})$.\n\n4. Minimization: Show that the defined objective $roL(\\pi)$ is minimized by $\\pi^{*}$, by comparing it with any other policy $\\pi \\neq \\pi^{*}$ and using the non-negativity property of the KL divergence.\n\n5. Conclusion: Conclude that the optimal policy for the robust problem (equation (10)) indeed matches the form of the optimal policy for the non-robust problem (equation (4)), thus proving their equivalence.\n\nThis proof demonstrates that despite the robustness consideration in equation (10), the optimal policy retains the same form as in the non-robust case, highlighting the inherent connection between the two problems."}
{"paper":"arxiv:2405.20304", "question":"Please derive the following gradient update expression:\n\\begin{align}\n&\n\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l))\n=\\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big)\\label{eq: gradient-breakdown}\\\\\n  &=\\alpha_{g}^{(t)}\n  \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_l)-r_{\\theta^{(t-1)}}(x_g,y_w)\\big)\\times[\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g)-\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)].\\nonumber\n\\end{align}", "whole_label":"To derive the gradient update expression, we follow these steps:\n\\begin{align*}\n&\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l))\\\\\n&= \\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big)\\\\\n&\\stackrel{(i)}{=} \\alpha_{g}^{(t)} \\frac{\\sigma'(\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))}{\\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))} \\times [\\beta h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(ii)}{=} \\beta \\alpha_{g}^{(t)} \\Big(1-\\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(iii)}{=} \\beta \\alpha_{g}^{(t)} \\sigma\\big(-\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)\\big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(iv)}{=} \\beta \\alpha_{g}^{(t)} \\sigma\\Big(\\beta\\big( \\log(\\tfrac{\\pi_{\\theta^{(t-1)}}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)})-\\log(\\tfrac{\\pi_{\\theta^{(t-1)}}(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)})\\big)\\Big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(v)}{=} \\alpha_{g}^{(t)} \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_l)-r_{\\theta^{(t-1)}}(x_g,y_w)\\big) \\times [\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g)-\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)]\n\\end{align*}\nHere, (i) is obtained by applying the chain rule to the derivative of the logarithm of the sigmoid function. (ii) and (iii) utilize the property of the sigmoid function's derivative, where $\\sigma'(s) = \\sigma(s)(1-\\sigma(s)) = \\sigma(-s)$. (iv) substitutes the definition of $h_{\\pi}(x,y_w,y_l) = \\log(\\tfrac{\\pi(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}) - \\log(\\tfrac{\\pi(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)})$. Finally, (v) introduces the reward function $r_{\\theta}(x_g,y) = \\beta\\log(\\tfrac{\\pi_{\\theta}(y|x_g)}{\\pi_{\\text{ref}}(y|x_g)}) - \\beta \\log Z(x_g)$, leading to the final gradient update expression."}
{"paper":"arxiv:2405.16577", "question":"Prove the following theorem:\n\\begin{theorem}\nAssume i) the domain $\\Omega$ satisfies the uniform exterior sphere condition and the uniform cone condition;\nii) the velocity field $\\bm{v}_t(\\bm{x})$ is Lipschitz continuous in $\\bm{x}\\in\\Omega$ (uniformly on $t$).\nThen the solution to the reflected ODE exists and is unique on $t\\in[0,1]$.\n\\end{theorem}", "whole_label":"The reflected ODE can be viewed as a special case of reflected SDE where the diffusion coefficient is zero. Therefore, the theorem is indeed a corollary of a more general theorem with no diffusion term (i.e., diffusion coefficients are zeros). For a more rigorous derivation, under the assumptions of the domain, there exists a unique solution to the Skorokhod problem with normal reflection as long as the drift function is continuous. To prove the existence and uniqueness of the solution to the reflected ODE, one can first use Euler discretization to find a sequence of approximate solutions whose existence and uniqueness are guaranteed. We can then prove that this sequence of approximate solutions converges to the solution of the reflected ODE problem as the stepsize goes to zero. This proof follows the proof provided for reflected SDEs."}
{"paper":"arxiv:2409.05282", "question":"Prove the following theorem:\n\\begin{theorem}\nSuppose that i) $\\forall \\tau, \\bm{l}$ and $\\bm{\\psi}$, $q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau,\\bm{l}) \\in C^1(\\mathbb{R}^{|p|})$ as a function of CPDs $p$; ii) $G(\\phi)$ is $L_G$-Lipschitz continuous and with probability one, $\\hat{G}^{(h,t)}_R(\\phi)$ is $L_G$-Lipschitz continuous; ii) $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2=0$ and $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\bm{\\psi}^{(h,t)}-\\bm{\\psi}^\\ast||^2=0$ for some point $\\bm{\\phi}^\\ast$ and $\\bm{\\psi}^\\ast$.\nThen \n\\begin{enumerate}\n    \\item $\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})\\overset{\\mathrm{a.s.}}{\\longrightarrow} G(\\bm{\\phi}^{(h,t)})$ as $R,F\\to\\infty$;\n    \\item $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2\\overset{\\circ}{\\approx}O(1/F)$.\n\\end{enumerate}\n\\end{theorem}", "whole_label":"We first prove the strong consistency of the RWSVR estimator. By the strong law of large numbers, we have\n\\[\n\\lim_{R\\to\\infty}\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}, \\bm{\\psi}) = \\E_{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\frac{p(\\tau,\\bm{l},Y)}{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}=p(Y),\n\\]\n\\[\n\\lim_{R\\to\\infty}\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}, \\bm{\\psi}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)=\\E_{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\frac{p(\\tau,\\bm{l},Y)}{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)=p(Y)\\E_{p({\\tau,\\bm{l}|Y})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau).\n\\]\nTherefore,\n\\[\n\\lim_{R\\to\\infty}\\hat{G}^{(h,t)}_{R}(\\tilde{\\bm{\\phi}}) =\\lim_{R\\to\\infty} \\frac{\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)}{\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})}=\\E_{p({\\tau,\\bm{l}|Y})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)=G(\\tilde{\\bm{\\phi}}).\n\\]\n\nWe then estimate the order of the variance of the RWSVR estimator. We have the following estimate for the mean squared error (MSE) of $\\hat{G}^{(h,t)}_{F}(\\tilde{\\bm{\\phi}})$:\n\\begin{align*}\n\\mathbb{E}||\\hat{G}^{(h,t)}_{F}(\\tilde{\\bm{\\phi}})-G(\\tilde{\\bm{\\phi}})||^2&=\\E\\left(\\frac{\\frac{1}{F}\\sum_{i=1}^{F} w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)}{\\frac{1}{F}\\sum_{j=1}^{F} w^j(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})}-G(\\tilde{\\bm{\\phi}})\\right)^2\\\\\n&\\approx \\frac{1}{F}\\frac{\\E_{q_{\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)}}(\\tau,\\bm{l})}\\left(w(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})\\left(\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)-G(\\tilde{\\bm{\\phi}})\\right)\\right)^2}{\\E_{q_{\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)}}(\\tau,\\bm{l})}\\left(w(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})\\right)^2}\\\\\n&=:\\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F}\n\\end{align*}\nwhere the $\\approx$ refers to asymptotic equivalence as $F\\to\\infty$.\nAs the algorithm converges, i.e. $(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})\\to(\\bm{\\phi}^\\ast, \\bm{\\psi}^\\ast)$ for some $\\bm{\\phi}^\\ast, \\bm{\\psi}^\\ast$, the variance of $\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)})$ vanishes since it converges to 0 almost surely with $R$ fixed. Hence\n\\begin{align*}\n&\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\\\\n=& \\mathbb{E}||\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)}) + \\hat{G}^{(h,0)}_{F}(\\bm{\\phi}^{(h,0)})- G(\\bm{\\phi}^{(h,0)})+G(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,t)})||^2\\\\\n\\leq & 2\\mathbb{E}||\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)})||^2+2\\mathbb{E}||\\hat{G}^{(h,0)}_{F}(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,0)})||^2+2||G(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,t)})||^2\\\\\n\\overset{\\circ}{\\leq } & 4L_G^2 \\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^{(h,0)}||^2 + 2\\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F},\n\\end{align*}\nbecause $G$ is $L_G$-Lipschitz continuous and with probability one $\\hat{G}^{(h,t)}$ is $L_G$-Lipschitz continuous for all $h$ and $t$.\nUsing the fact\n$$\\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^{(h,0)}||^2\\leq \\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2+\\lim_{h\\to\\infty}\\mathbb{E}||\\bm{\\phi}^{(h,0)}-\\bm{\\phi}^\\ast||^2= 0$$\nbecause of the assumption $\\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2=0$,\nTherefore, we conclude that\n$$\\lim_{h\\to\\infty}\\sup_t \\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\overset{\\circ}{\\leq} 2\\lim_{h\\to\\infty}\\sup_t \\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi} = O(1/F)$$as the algorithm converges."}
{"paper":"arxiv:2404.12358", "question":"Prove the following lemma: \\begin{lemma} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the token MDP. \\end{lemma}", "whole_label":"Let $Q^*_r$ denote the optimal $Q$-function for reward $r$. We prove the statement directly, starting with the injective case. \\\\\n\nAssume there exists a reward function $r' \\ne r$ such that $Q^*_{r'} = Q^*_{r}$. Then, there must exist a state action pair such that $r'(\\bs_t,\\ba_t) \\ne r(\\bs_t,\\ba_t)$. In fact, proceeding backwards from a leaf node (terminal state), there must be a \\textit{first} state action pair $(\\bs_t,\\ba_t)$ where $r'(\\bs_t,\\ba_t) \\ne r(\\bs_t,\\ba_t)$. \\\\\n\nThe $Q$ functions at this location are \\\\\n\\begin{equation*}\nQ^*_{r'}(\\bs_t,\\ba_t) = r'(\\bs_t,\\ba_t) + V_{r'}^*(\\bs_{t+1}), \\quad Q^*_{r}(\\bs_t,\\ba_t) = r(\\bs_t,\\ba_t) + V_{r}^*(\\bs_{t+1})\n\\end{equation*} \\\\\n\nBy the fact that this was the \\textit{first} location where the reward functions differed starting from a leaf node, we must have that $V_{r'}^*(\\bs_{t+1}) = V_{r}^*(\\bs_{t+1})$. This is because the rewards in all possible future states from $s,a$ are equal by virtue of this being the location of the first difference and thus the dynamic programming solution up to this point is the same. Thus, we can see that $Q^*_{r'}(\\bs_t,\\ba_t) \\ne Q^*_{r}(\\bs_t,\\ba_t)$, completing this direction. Note that this proof does not hold in general MDPs, only the token MDP where it is impossible to return to the same state after taking any number of actions. \\\\\n\nThe surjective direction is easier. For all $Q^*$, we can compute a reward function $r(\\bs_t,\\ba_t) = Q^*(\\bs_t,\\ba_t) - V^*(\\bs_{t+1})$ under deterministic dynamics. Thus, we can see that the mapping is surjective."}
{"paper":"arxiv:2404.12358", "question":"Extend the following lemma to diffusion MDPs.\n\\begin{lemma} \\label{lemma:r_to_q} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the token MDP.\n\\end{lemma}", "whole_label":"\\begin{lemma} \\label{lemma:r_to_q_diffusion} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the diffusion MDP.\n\\end{lemma}\n\\begin{proof}\nSince the MDP still has deterministic dynamics, we have that Eq. \\ref{eq:policy}-\\ref{eq:critic} still hold. Now, given a reference policy $\\piref$, parameter $\\beta$ and a critic $Q$, we can trivially recover the unique reward function by inverting Eq. \\ref{eq:critic}. We will prove that given a reward function $r(\\bs_t, \\ba_t)$, we can recover a unique critic $Q$. We work inductively in the diffusion MDP starting with $t=T$, where we have $V^*(\\bs_T)=0$ for all terminal states. We then have that\n\\begin{align}\n&Q^*(\\bs_{t-1}, \\ba_{t-1}) = Q^*(\\bs_t=(\\vc, \\x_{T-t+1}, T-t+1), \\ba=\\x_{T-t})= \\nonumber\\\\\n&r(\\bs_t=(\\vc, \\x_{T-t+1}, T-t+1), \\ba=\\x_{T-t}) + \\beta \\log p_{ref}(\\x_{T-t}| \\vc, \\x_{T-t+1}, T-t+1) + \\nonumber\\\\\n&\\beta\\log\\int_{\\mathcal{A}} e^{Q^*(\\bs_t=(\\vc, \\x_{T-t}, T-t), \\x_{T-t-1})/\\beta}d\\x_{T-t-1} \\nonumber\n\\end{align}\n\nwhere $\\pi_{ref}$ is the reference backwards diffusion process. In this case even though the state space is deterministic, our approach to the proof of Lemma \\ref{lemma:r_to_q} still holds by using backwards induction on the diffusion step $t$. Notice, that from $V(\\bs_T=(\\vc, \\x_0, 0))=0$ we can uniquely determine the critic values for all states at time step $T-1$. Proceeding inductively backwards through time in the MDP/denoising process (forward in the diffusion process), we obtain the desired result.\n\\end{proof}"}
{"paper":"arxiv:2404.11999", "question":"Prove the following lemma: Given two policies $\\pi$ and $\\tilde{\\pi}$, if for any state $s_t=[x, y^{<t}]$, $\\mathbb{E}_{z\\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], z)\\right] \\ge 0$, then we can conclude: $\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] \\ge \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right]$.", "whole_label":"To prove the lemma, we start by defining the trajectory $\\tau:= (x, y^1, y^2, ...)$, where actions are sampled from $\\tilde{\\pi}$ to generate $\\tau$. The difference in expected values under $\\tilde{\\pi}$ and $\\pi$ can be expressed as:\n\n\\begin{align*}\n&\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] - \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} R_{t}-V_{\\pi}([x])\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} \\left(R_{t}+\\gamma V_{\\pi}([x, y^{<t+1}])-V_{\\pi}([x, y^{<t}])\\right)\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} A_{\\pi}([x, y^{<t}], y^t)\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} \\mathbb{E}_{y^t \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], y^t)\\right]\\right]\n\\end{align*}\n\nGiven that for any state $s_t=[x, y^{<t}]$, $\\mathbb{E}_{z \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], z)\\right]\\ge 0$, it follows that:\n\n\\begin{align*}\n\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] - \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right] \\ge 0\n\\end{align*}\n\nThis completes the proof, showing that the expected value under $\\tilde{\\pi}$ is at least as great as that under $\\pi$."}
{"paper":"arxiv:2404.11999", "question":"Prove the following lemma:\n\n\\begin{lemma}\n    The constrained problem has the closed-form solution:\n\\begin{equation}\n\\begin{aligned}\n    \\pi_{\\theta}^*(z|[x,y^{<t}])= \\\\\n     &\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)}{Z([x,y^{<t}];\\beta)},\n\\end{aligned}\n\\end{equation}\nwhere $Z([x,y^{<t}];\\beta) = \\mathbb{E}_{z\\sim \\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])}e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}$\nis the partition function.\n\\end{lemma}", "whole_label":"\\begin{proof}\n\\begin{align}\n&\\max_{\\pi_{\\theta}} \\ \\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x,y^{<t}])}A_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z)-\\beta D_{\\mathrm{KL}}\\left(\\pi_{\\theta}(\\cdot|[x,y^{<t}])\\|\\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])\\right)\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\left(\\left(Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)-V_{\\pi_{\\mathrm{ref}}}([x,y^{<t}])\\right)+\\beta\\log \\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x, y^{<t}])}{\\pi_{\\theta}(z|[x, y^{<t}])}\\right)\\right)\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\beta\\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{\\pi_{\\theta}(z|[x,y^{<t}])}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\beta\\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{Z([x, y^{<t}];\\beta)\\pi_{\\theta}(z|[x,y^{<t}])}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])+\\beta\\log Z([x, y^{<t}];\\beta)\\\\\n=&\\max_{\\pi_{\\theta}}\\  -\\beta D_{\\mathrm{KL}}\\left(\\pi_{\\theta}(z|[x, y^{<t}])\\bigg\\|\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{Z([x, y^{<t}];\\beta)}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])+\\beta\\log Z([x, y^{<t}];\\beta)\n\\end{align}\nwhere $Z([x, y^{<t}];\\beta)$ is the partition function:\n\\begin{equation}\nZ([x,y^{<t}];\\beta) = \\mathbb{E}_{z\\sim \\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])}\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)\n\\end{equation}\nBased on the property of KL divergence, we can derive the relationship between the optimal policy and the state-action function:\n\\begin{equation}\n\\pi_{\\theta}^*(z|[x,y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)}{Z([x,y^{<t}];\\beta)}\n\\end{equation}\n\\end{proof}"}
{"paper":"arxiv:2404.11999", "question":"Prove the following lemma:\n\\begin{lemma}\nGiven a reward function $r(\\mathrm{x}, \\mathrm{y})$, assuming a relationship between token-wise rewards and the reward function represented by $r({x}, {y}) = \\sum_{t=1}^T\\gamma^{t-1}R([{x},y^{<t}], y^t)$, we can establish the equivalence between the Bradley-Terry model and the Regret Preference Model in the task of text generation alignment, i.e.,\n\\begin{equation}\n    {\\small\n    \\begin{aligned}\n        P_{\\mathrm{BT}}&({y}_1 \\succ {y}_2 |{x})=\\\\\n        \\sigma&\\left(\\sum_{t=1}^{T_1}\\gamma^{t-1}A_{\\pi}([{x},y_1^{<t}], y_1^{t}) - \\sum_{t=1}^{T_2}\\gamma^{t-1}A_{\\pi}([{x},y_2^{<t}], y_2^{t})\\right),\n    \\end{aligned}}\\label{BT_eq_RPM}\n\\end{equation}\nwhere $\\sigma(x)=1/(1+exp(-x))$ is the logistic sigmoid function.\n\\end{lemma}", "whole_label":"\\begin{proof}\nAccording to the Bradley-Terry model, we have\n\\begin{equation}\n\\begin{aligned}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x})=\\frac{\\exp(r({x}, {y}_1))}{\\exp(r({x}, {y}_1))+\\exp(r({x}, {y}_2))},\n\\label{app_BT_model}\n\\end{aligned}\n\\end{equation}\nwhere $r({x}, {y})$ represents the overall reward of the pair $({x}, {y})$.\n\nBased on assumption that $r({x}, {y}) = \\sum_{t=1}^T\\gamma^{t-1}R([{x},y^{<t}], y^t)$, we can get:\n\\begin{align}\nr({x}, {y}) &= \\sum_{t=1}^{T}\\gamma^{t-1}R([{x},y^{<t}], y^t)\\\\\n&=\\sum_{t=1}^{T} \\gamma^{t-1}(R([{x},y^{<t}], y^t) + \\gamma V_{\\pi}([{x},y^{<t+1}]) - \\gamma V_{\\pi}([{x},y^{<t+1}]))\\\\\n&=V_{\\pi}([{x},y^{<1}]) + \\sum_{t=1}^{T}\\gamma^{t-1}\\left( R([{x},y^{<t}], y^t) + \\gamma V_{\\pi}([{x},y^{<t+1}]) - V_{\\pi}([{x},y^{<t}])\\right) - \\gamma^T V_{\\pi}([{x},y^{<T+1}])\\label{app_r}\n\\end{align}\nText generation is analogous to a deterministic contextual bandit, where the transition to the next state is certain given the current state and action, i.e., $p(s_{t+1}=[{x},y^{<t+1}]|s_{t}=[{x},y^{<t}], a_t=y^t)=1$, so we have:\n\\begin{align}\nQ_{\\pi}([{x},y^{<t}], y^t) &= R([{x},y^{<t}], y^t) + V_{\\pi}([{x},y^{<t+1}])\\label{app_Q}\\\\\nA_{\\pi}([{x},y^{<t}], y^t) &= Q_{\\pi}([{x},y^{<t}], y^t) - V_{\\pi}([{x},y^{<t}])\n\\label{app_A}\n\\end{align}\n\nNext, note that  $y^T=\\text{EOS}$ denotes the end of the text sequence. Therefore,\n\\begin{equation}\nV_{\\pi}([{x},y^{<T+1}])=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k} R([{x},y^{<T+1+k}], y^{T+1+k})\\bigg|s_{t}=[{x},y^{<T+1}]\\right]=0\\label{app_V}\n\\end{equation}\n\nSubstituting Eq.\\ref{app_r} to Eq.\\ref{app_V} into the Bradley-Terry model, we obtain\n\n\\begin{align}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x})&=\\frac{\\exp(r({x}, {y}_1))}{\\exp(r({x}, {y}_1))+\\exp(r({x}, {y}_2))}\\nonumber\\\\\n=&\\sigma\\left(\\left(V_{\\pi}([{x},y_1^{<1}]) + \\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y^t)\\right)\\right)-\\left(V_{\\pi}([{x},y_2^{<1}]) + \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\\right)\n\\end{align}\n\nAdditionally, note that $y^{<1}=[\\ ]$, so we can get\n$$\nV_{\\pi}([{x},y_1^{<1}]) = V_{\\pi}([{x},[\\ ]]) = V_{\\pi}([{x},y_2^{<1}])\n$$\nTherefore,\n\\begin{align}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x}) &= \\sigma\\left(\\left(V_{\\pi}([{x},y_1^{<1}]) + \\sum_{t=1}^{T_1}\\left( \\gamma^{t-1}A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)\\right)-\\left(V_{\\pi}([{x},y_2^{<1}]) + \\sum_{t=1}^{T_2}\\left( \\gamma^{t-1}A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\\right)\\\\\n&=\\sigma\\left(\\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)- \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\n\\end{align}\n\\end{proof}"}
{"paper":"arxiv:2404.11999", "question":"Prove the following theorem:\n\nIn the KL-constrainted advantage function maximization problem, the Bradley-Terry model expresses the human preference probability in terms of the optimal policy $\\pi_{\\theta}^*$ and reference policy $\\pi_{\\mathrm{ref}}$:\n\\begin{equation}\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x})=\\sigma(u^*({x}, {y}_1, {y}_2) - \\delta^*({x}, {y}_1, {y}_2)),\n\\end{equation}\nwhere, $u({x}, {y}_1, {y}_2)$ refers to the difference in rewards implicitly defined by the language model $\\pi_{\\theta}$ and the reference model $\\pi_{\\mathrm{ref}}$, represented as\n\\begin{equation}\nu({x}, {y}_1, {y}_2)=\\beta\\log\\frac{\\pi_{\\theta}({y}_1\\mid {x})}{\\pi_{\\mathrm{ref}}({y}_1\\mid {x})}-\\beta\\log\\frac{\\pi_{\\theta}({y}_2\\mid {x})}{\\pi_{\\mathrm{ref}}({y}_2\\mid {x})},\n\\end{equation}\nand $\\delta({x}, {y}_1, {y}_2)$ refers to the difference in sequential forward KL divergence between two pairs $({x}, {y}_1)$ and $({x}, {y}_2)$, weighted by $\\beta$, expressed as\n\\begin{equation}\n\\delta({x}, {y}_1, {y}_2) =\\beta D_{\\mathrm{SeqKL}}\\left({x},{y}_2;\\pi_{\\mathrm{ref}}\\| \\pi_{\\theta}\\right) - \\beta D_{\\mathrm{SeqKL}}\\left({x},{y}_1;\\pi_{\\mathrm{ref}}\\| \\pi_{\\theta}\\right).\n\\end{equation}", "whole_label":"The proof involves several steps to derive the Bradley-Terry model expression for human preference probability in terms of the optimal policy and reference policy. Here's a structured reasoning process:\n\n1. Optimal Policy Expression: Starting from the given optimal policy expression,\n\\begin{equation}\n\\pi_{\\theta}^*(z|[{x},y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[{x},y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],z)\\right)}{Z([{x},y^{<t}];\\beta)},\n\\end{equation}\nwhere $Z([{x},y^{<t}];\\beta)$ is the partition function.\n\n2. Rearranged Equation: Rearranging the above equation gives the Q-function in terms of the optimal and reference policies,\n\\begin{equation}\nQ_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],z) = \\beta \\log\\frac{\\pi_{\\theta}^*(z|[{x}, y^{<t}])}{\\pi_{\\mathrm{ref}}(z|[{x}, y^{<t}])} + \\beta \\log Z([{x}, y^{<t}];\\beta).\n\\end{equation}\n\n3. Advantage Function: The advantage function is derived from the Q-function and value function,\n\\begin{equation}\nA_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}], y^{t}) = Q_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],y^{t}) - V_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}]).\n\\end{equation}\n\n4. Preference Probability: The preference probability is expressed in terms of the advantage functions for two responses,\n\\begin{equation}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x}) = \\sigma\\left(\\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)- \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right).\n\\end{equation}\n\n5. Final Expression: Substituting the expressions for $u({x}, {y}_1, {y}_2)$ and $\\delta({x}, {y}_1, {y}_2)$ into the preference probability formula yields the final theorem,\n\\begin{equation}\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x})=\\sigma(u^*({x}, {y}_1, {y}_2) - \\delta^*({x}, {y}_1, {y}_2)).\n\\end{equation}\n\nThis proof systematically connects the optimal policy, advantage function, and preference probability to arrive at the desired theorem."}
{"paper":"arxiv:2402.01306", "question":"Prove the following theorem:\n\\begin{theorem}\n    Assuming the value function is logistic, for a reward function $r^*_a$ that maximizes the objective, there exists a reward function in its equivalence class (i.e., $r^*_b(x,y) = r^*_a(x,y) + h(x)$ for some $h(x)$) that induces the same optimal policy $\\pi^*$ and the same Bradley-Terry preference distribution but a different human value distribution.\n\\end{theorem}", "whole_label":"\\begin{proof}\nWe say $r^*_a$ and $r^*_b$ are in the same equivalence class if there exists some function $h(x)$ such that $r^*_b(x,y) = r^*_a(x,y) + h(x)$. Two functions in the same equivalence class induce the same optimal policy:\n\\begin{equation*}\n\\begin{split}\n\\pi^*_{r_a}(y|x) &= \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y)\\right) \\\\\n&= \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y)\\right) \\exp\\left( \\frac{1}{\\beta} h(x)\\right)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y) \\right) \\exp\\left(\\frac{1}{\\beta} h(x) \\right) \\\\\n&= \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} (r^*_a(x,y) +h(x))\\right)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} (r^*_a(x,y) + h(x))\\right) \\\\\n&= \\pi^*_{r_b}(y|x) \\\\\n\\end{split}\n\\end{equation*}\nFor a Bradley-Terry model of preferences, $p(y_w \\succ y_l|x)$ is unaffected by $h(x)$ since it is added to the reward of both $y_w$ and $y_l$.\nWe will now show that the two reward functions do not necessarily induce the same distribution of human values.\n\nA Taylor series expansion of the human value of $r^*_a(x,y)$ around 0 would be:\n$$\\sigma(0) + \\sigma'(0) (r^*_a(x,y) - z_0) + \\frac{\\sigma''(0)}{2} (r^*_a(x,y) - z_0)^2 + ...$$\n\nA Taylor series expansion of the value of $r^*_a(x,y) + h(x)$ around $h(x)$ would be:\n$$\\sigma(h(x)) + \\sigma'(h(x)) (r^*_a(x,y) - z_0) + \\frac{\\sigma''(h(x))}{2} (r^*_a(x,y) - z_0)^2 + ...$$\n\nSince $\\sigma$ is strictly monotonic, for these series to be equal, we must have $h(x) = 0$.\nIf this is not the case, then the values of $r^*_a(x,y)$ and $r^*_b(x,y)$ will be different.\nThus two arbitrary reward functions in the same equivalence class do not induce the same distribution of human values.\n\\end{proof}"}
{"paper":"arxiv:2402.01306", "question":"Prove the following theorem:\n\\begin{theorem}\nFor input $x$ with outputs $\\{y_a, y_b\\}$, let dataset $D$ comprise contradictory preferences $y_a \\succ y_b$ and $y_b \\succ y_a$ in proportion $p \\in (0.5, 1)$ and $(1 - p) \\in (0, 0.5)$ respectively. \nIf $p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1 - p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)$, then the optimal DPO policy is more likely to produce the minority-preferred $y_b$; the optimal KTO policy will strictly produce the majority-preferred $y_a$ for a loss-neutral value function ($\\lambda_D = \\lambda_U$). \n\\end{theorem}", "whole_label":"\\begin{proof}\nWhere $u = \\beta (r_\\theta(x, y_a) - r_\\theta(x, y_b))$, we can write the total DPO loss for $x$ as\n\\begin{equation*}\n\\mathcal{L}_{\\text{DPO}}(x) = p (- \\log \\sigma(u)) + (1 - p)(- \\log \\sigma(-u))\n\\end{equation*}\nTaking the derivative with respect to $u$ and setting to zero, we get\n\\begin{equation*}\n\\begin{split}\n0 &= -p \\frac{\\sigma(u) \\sigma(-u)}{\\sigma(u)} + (1 - p) \\frac{\\sigma(-u) \\sigma(u)}{\\sigma(-u)} = -p(1 - \\sigma(u)) + (1 - p) \\sigma(u) = -p + \\sigma(u) \\\\\n\\implies u &= \\sigma^{-1}(p) \\\\\n\\beta r^*_\\theta(x, y_a) &= \\sigma^{-1}(p) + \\beta r^*_\\theta(x, y_b) \\\\\n\\beta \\log \\frac{\\pi^*_\\theta(y_a|x)}{\\pi_\\text{ref}(y_a|x)} &= \\log \\frac{p}{1 - p} + \\beta \\log \\frac{\\pi^*_\\theta(y_b|x)}{\\pi_\\text{ref}(y_b|x)} \\\\\n\\pi^*_\\theta(y_a|x) &= \\left(\\frac{p}{1 - p}\\right)^{1/\\beta} \\cdot \\frac{ \\pi_\\text{ref}(y_a|x) } {\\pi_\\text{ref}(y_b|x)} \\cdot \\pi^*_\\theta(y_b|x)\n\\end{split}\n\\end{equation*}\nThus when $p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1 - p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)$, we have $\\pi^*_\\theta(y_a|x) < \\pi^*_\\theta(y_b|x)$, meaning the optimal DPO policy is more likely to produce the minority-preferred $y_b$.\n\nWhere $u_a = \\beta(r_\\theta(x, y_a) - \\mathbb{E}_{Q}[r_\\theta(x, y')])$ and $u_b = \\beta(r_\\theta(x, y_b) - \\mathbb{E}_{Q}[r_\\theta(x, y')])$, noting that $1 - \\sigma(-u) = \\sigma(u)$, we can write the total KTO loss for $x$ as\n\\begin{equation*}\n\\begin{split}\n\\mathcal{L}_{\\text{KTO}}(x) &= p \\lambda_D (1 - \\sigma(u_a)) + (1 - p) \\lambda_U \\sigma(u_a) + p \\lambda_U \\sigma(u_b) + (1 - p) \\lambda_D (1 - \\sigma(u_b)) \\\\\n&= p \\lambda_D + ((1 - p)\\lambda_U - p \\lambda_D) \\sigma(u_a) + (1 - p) \\lambda_D + (p \\lambda_U - (1 - p) \\lambda_D) \\sigma(u_b) \\\\\n&= \\lambda_D + ((1 - p)\\lambda_U - p \\lambda_D) \\sigma(u_a) + (p \\lambda_U - (1 - p) \\lambda_D) \\sigma(u_b) \\\\\n&= \\lambda_D + \\lambda_D ((1 - 2p) \\sigma(u_a) + (2p - 1) \\sigma(u_b)) \\quad \\text{(under loss neutrality)}\n\\end{split}\n\\end{equation*}\nGiven that $p > 0.5$ by assumption and $\\lambda_D > 0$ by definition, the KTO loss is decreasing in $u_a$ and increasing in $u_b$---and thus decreasing in $r_\\theta(x, y_a)$ and increasing in $r_\\theta(x, y_b)$ respectively.\nThe optimal KTO policy is thus $\\pi^*_\\theta(y|x) = \\mathbbm{1}[y = y_a]$.\n\\end{proof}"}
{"paper":"arxiv:2304.02835", "question":"The Hessian matrix $H_{\\theta_{0}}$ is defined by $H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})$. Given the loss $L_{0} = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i})$, how do we derive the Hessian definition from the second derivatives of the loss function $l(f_{G}(z_{i}), y_{i})$ evaluated at $\\theta_{0}$?", "whole_label":"To derive the Hessian matrix $H_{\\theta_{0}}$ from the second derivatives of the loss function $l(f_{G}(z_{i}), y_{i})$ evaluated at $\\theta_{0}$, we start by considering the definition of the Hessian matrix for the total loss $L_{0}$. The Hessian matrix is the square matrix of second-order partial derivatives of a scalar-valued function. For the total loss $L_{0}$, which is the sum of individual loss functions $l(f_{G}(z_{i}), y_{i})$ over the dataset $D_{0}$, the Hessian matrix at a point $\\theta_{0}$ is given by the sum of the Hessian matrices of each individual loss function evaluated at $\\theta_{0}$. Mathematically, this is expressed as:\n\n\\[ H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})\\bigg|_{\\theta = \\theta_{0}}. \\]\n\nThis equation shows that the Hessian matrix of the total loss $L_{0}$ at $\\theta_{0}$ is the sum of the second derivatives of each individual loss function with respect to the parameters $\\theta$, evaluated at $\\theta_{0}$. This derivation assumes that the loss function $l$ is twice differentiable with respect to $\\theta$ and that the operations of differentiation and summation can be interchanged, which is typically the case under standard regularity conditions."}
{"paper":"arxiv:2304.02835", "question":"These equalities express the stationary conditions on $\\theta_{\\epsilon}$ and $\\theta_{0}$. Specifically, we have $0 = \\nabla_{\\theta_{\\epsilon}} L_{0} + \\epsilon \\nabla_{\\theta_{\\epsilon}} L_{\\Delta G}$ and $0 = \\nabla_{\\theta_{0}} L_{0}$. How can we derive these gradient conditions from the definition $\\theta_{\\epsilon} = \\text{arg}\\,\\min\\,(L_{0} + \\epsilon L_{\\Delta G})$ and the original optimum $\\theta_{0}$ for $L_{0}$?", "whole_label":"To derive the gradient conditions, we start by considering the optimization problem for $\\theta_{\\epsilon}$ and $\\theta_{0}$. For $\\theta_{\\epsilon} = \\arg\\min_\\theta (L_0 + \\epsilon L_{\\Delta G})$, the first-order optimality condition requires that the gradient of the objective function with respect to $\\theta_{\\epsilon}$ must be zero. This gives us the equation $0 = \\nabla_{\\theta_{\\epsilon}} L_0 + \\epsilon \\nabla_{\\theta_{\\epsilon}} L_{\\Delta G}$. Similarly, for the original optimum $\\theta_{0} = \\arg\\min_\\theta L_0$, the first-order optimality condition is simply $0 = \\nabla_{\\theta_0} L_0$. These conditions ensure that both $\\theta_{\\epsilon}$ and $\\theta_{0}$ are at stationary points of their respective objective functions."}
{"paper":"arxiv:2304.02835", "question":"How can we prove Theorem 5 regarding the Neumann series for an invertible matrix H? The theorem states that if the spectral radius of $(I - H)$ is less than 1, then $H^{-1} = \\sum_{l=0}^{\\infty}(I - H)^{l}$ and $\\lim_{t\\to\\infty} H_{t}^{-1} = H^{-1}$. Which conditions on H (positive definiteness, spectral radius) ensure the convergence of the series?", "whole_label":"To prove Theorem 5 regarding the Neumann series for an invertible matrix H, we follow these steps:\n\n1. Neumann Series Expansion: Given that the spectral radius of $(I - H)$ is less than 1, we can express $H^{-1}$ as the infinite series $H^{-1} = \\sum_{l=0}^{\\infty}(I - H)^{l}$. This is because the condition on the spectral radius ensures the convergence of the series.\n\n2. Iterative Approximation: Consider the iterative process defined by $H_t^{-1} = (I - H) H_{t-1}^{-1} + I$. This process generates a sequence of approximations to $H^{-1}$.\n\n3. Eigenvalue Decomposition: Decompose $(I-H)$ as $T U T^{-1}$, where $T$ is a matrix whose columns are eigenvectors of $(I-H)$, and $U$ is a diagonal matrix of eigenvalues (i.e., $U = \\Lambda$). This allows us to express $(I - H)^n$ as $T U^n T^{-1}$.\n\n4. Convergence Analysis: Since the spectral radius of $(I - H)$ is less than 1, all eigenvalues of $(I - H)$ have magnitudes less than 1. Therefore, $\\lim_{n\\to\\infty} U^n = 0$, which implies $\\lim_{n\\to\\infty}(I - H)^n = 0$. Consequently, the iterative approximation $H_t^{-1}$ converges to $H^{-1}$ as $t$ approaches infinity.\n\nConditions for Convergence: The key condition ensuring the convergence of the Neumann series is that the spectral radius of $(I - H)$ must be less than 1. This condition is independent of whether H is positive definite or not. However, if H is positive definite, it can provide additional guarantees about the stability and rate of convergence of the series."}
{"paper":"arxiv:2309.04475", "question":"From the denoising diffusion probabilistic model, how do we obtain the reverse transition formula $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid \\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)?$$ In particular, how is it derived using the forward process $$q(L_t|L_0) = \\mathcal{N}\\bigl(L_t\\mid\\sqrt{\\bar{\\alpha}_t}L_0,\\,(1 - \\bar{\\alpha}_t)I\\bigr)$$ and the noise prediction term $\\hat{\\epsilon}_L(M_t, t)$?", "whole_label":"To derive the reverse transition formula $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid \\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$ from the denoising diffusion probabilistic model, we start by considering the forward process given by $$q(L_t|L_0) = \\mathcal{N}\\bigl(L_t\\mid\\sqrt{\\bar{\\alpha}_t}L_0,\\,(1 - \\bar{\\alpha}_t)I\\bigr).$$ The key idea is to use the noise prediction term $\\hat{\\epsilon}_L(M_t, t)$ to model the reverse process. The derivation involves the following steps:\n\n1. Bayes' Rule Application: The reverse process can be related to the forward process through Bayes' rule, which allows us to express $p(L_{t-1}\\mid L_t)$ in terms of $q(L_t\\mid L_{t-1})$ and the prior $q(L_{t-1})$.\n\n2. Noise Prediction Term: The noise prediction term $\\hat{\\epsilon}_L(M_t, t)$ is used to estimate the noise that was added during the forward process. This estimation helps in reversing the diffusion process by subtracting the predicted noise from $L_t$ to obtain $L_{t-1}$.\n\n3. Parameterization of Mean and Variance: The mean $\\mu(M_t)$ and variance $\\sigma^2(M_t)$ of the reverse transition are parameterized based on the noise prediction term. Specifically, the mean is adjusted to account for the predicted noise, and the variance is set to ensure that the reverse process gradually denoises the data.\n\n4. Derivation of Reverse Transition: Combining these elements, we arrive at the reverse transition formula, which is a Gaussian distribution centered around a mean that depends on the current state $L_t$ and the predicted noise, with a variance that ensures the process moves towards the original data distribution.\n\nThis derivation showcases how the forward process and noise prediction term are utilized to model the reverse process, enabling the denoising of data through a series of learned transitions."}
{"paper":"arxiv:2309.04475", "question":"Given the reverse transition $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$, derive the expressions for $$\\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t)$$ and $$\\sigma^2(M_t) = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$$. Also, explain how $\\hat{\\epsilon}_L(M_t, t)\\in\\mathbb{R}^{3\\times 3}$ is predicted by the denoising model $\\phi(L_t, F_t, A, t)$.", "whole_label":"To derive the expressions for $\\mu(M_t)$ and $\\sigma^2(M_t)$, we start from the standard DDPM framework where the forward process is defined by $$q(L_t\\mid L_0) = \\mathcal{N}(L_t\\mid\\sqrt{\\bar\\alpha_t}\\,L_0,\\,(1-\\bar\\alpha_t)I).$$ The reverse transition aims to approximate the true denoising process, leading to $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr).$$ The mean $\\mu(M_t)$ is derived by considering the noise prediction $\\hat\\epsilon_L(M_t,t)$, which is the output of the denoising model $\\phi(L_t, F_t, A, t)$. This model predicts the noise added at each step, allowing us to express the mean as $$\\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar\\alpha_t}}\\; -\\;\\frac{1-\\bar\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\;\\hat\\epsilon_L(M_t,t).$$ The variance $\\sigma^2(M_t)$ is derived from the ratio of the noise schedules, resulting in $$\\sigma^2(M_t) = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\;\\beta_t.$$ The noise prediction $\\hat\\epsilon_L(M_t,t)\\in\\mathbb{R}^{3\\times 3}$ is obtained by the denoising model $\\phi$, which takes as input the current noisy sample $L_t$, the feature $F_t$, the attribute $A$, and the time step $t$, and outputs the predicted noise in the sample."}
{"paper":"arxiv:2309.04475", "question":"How can we prove Proposition 1: 'The marginal distribution $$p(L_0)$$ by Eq. (3) is O(3)-invariant if $$\\hat{\\epsilon}_L(M_t, t)$$ is O(3)-equivariant, i.e. $$\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t),\\;\\forall Q^TQ=I$$'? Show how the O(3)-equivariance of the denoising term guarantees that $$p(L_0)$$ remains invariant under any orthogonal transformation $Q$.", "whole_label":"To prove Proposition 1, we start by understanding the implications of the O(3)-equivariance of the denoising term $$\\hat{\\epsilon}_L$$. The equivariance condition $$\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t)$$ for all orthogonal matrices $$Q$$ (i.e., $$Q^TQ = I$$) ensures that the denoising process transforms predictably under orthogonal transformations of the input. This property is crucial because it implies that the denoising operation commutes with the action of the orthogonal group O(3) on the input space.\n\nThe marginal distribution $$p(L_0)$$ is constructed through a process that involves the denoising term and a prior distribution, which is assumed to be O(3)-invariant (e.g., a standard normal distribution $$\\mathcal{N}(0, I)$$). The O(3)-invariance of the prior means that its probability density function remains unchanged under any orthogonal transformation $$Q$$ applied to its argument.\n\nGiven these two properties, the transition probability from $$L_t$$ to $$L_0$$, which involves the denoising term and the prior, will also be O(3)-invariant. This is because the transformation induced by $$Q$$ can be 'passed through' the denoising term due to its equivariance, and the prior's invariance ensures that the overall probability density remains unchanged. Therefore, the marginal distribution $$p(L_0)$$ inherits the O(3)-invariance from the denoising term and the prior, ensuring that it remains invariant under any orthogonal transformation $$Q$$."}
{"paper":"arxiv:2309.04475", "question":"Starting from the reverse diffusion transition $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$ and substituting $$\\mu(M_t), \\sigma^2(M_t)$$ for the lattice, how do we arrive at $$p(L_{t-1}\\mid L_t, F_t, A) = \\mathcal{N}\\bigl(L_{t-1}\\mid a_t(L_t - b_t\\hat{\\epsilon}_L(L_t, F_t, A, t)),\\,\\sigma_t^2 I\\bigr)?$$ In your explanation, clarify how $a_t$ and $b_t$ are defined in terms of $\\bar{\\alpha}_t$ and why $\\hat{\\epsilon}_L(M_t,t)$ simplifies to $\\hat{\\epsilon}_L(L_t,F_t,A,t).", "whole_label":"To derive the expression for $$p(L_{t-1}\\mid L_t, F_t, A)$$, we start with the given reverse diffusion transition and substitute the expressions for $$\\mu(M_t)$$ and $$\\sigma^2(M_t)$$. The key steps involve understanding the definitions of $$a_t$$ and $$b_t$$ in terms of $$\\bar{\\alpha}_t$$ and recognizing the simplification of $$\\hat{\\epsilon}_L(M_t,t)$$ to $$\\hat{\\epsilon}_L(L_t,F_t,A,t)$$. The parameters $$a_t$$ and $$b_t$$ are defined as $$a_t = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{\\sqrt{\\bar{\\alpha}_t}}$$ and $$b_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{\\sqrt{1 - \\bar{\\alpha}_{t-1}}}$$, respectively. The simplification occurs because $$\\hat{\\epsilon}_L$$ is a function that depends on the current state $$L_t$$, the features $$F_t$$, the adjacency matrix $$A$$, and the time step $$t$$, making $$M_t$$ redundant in this context. Thus, the expression simplifies to the given form, incorporating these definitions and simplifications directly into the reverse transition probability."}
{"paper":"arxiv:2309.04475", "question":"How can we prove Lemma 3: 'If the denoising term $$\\hat{\\epsilon}_F(L_t,F_t,A,t)$$ is periodic translation invariant, and $$p(F_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}_w(F_{t-1}\\mid F_t + u_t\\hat{\\epsilon}_F(L_t,F_t,A,t),\\,v_t^2 I)$$, then the transition is periodic translation equivariant'? Please clarify why $\\hat{\\epsilon}_F$ and the wrapped normal distribution $\\mathcal{N}_w$ must both respect the crystal s periodicity.", "whole_label":"To prove Lemma 3, we start by understanding the definitions and implications of periodic translation invariance and equivariance in the context of the given terms. The denoising term $\\hat{\\epsilon}_F(L_t, F_t, A, t)$ is periodic translation invariant, meaning its value does not change under periodic translations of its arguments within the crystal's lattice. This property ensures that the denoising process is consistent across equivalent positions in the crystal.\n\nThe transition probability is given by a wrapped normal distribution $\\mathcal{N}_w$, which is defined over a torus to respect the crystal's periodicity. This means that the distribution accounts for the periodic boundary conditions of the crystal, ensuring that transitions between states are physically meaningful within the crystal's framework.\n\nGiven these definitions, the proof proceeds as follows:\n1. Periodic Translation Invariance of $\\hat{\\epsilon}_F$: Since $\\hat{\\epsilon}_F$ is invariant under periodic translations, applying a periodic translation to $F_t$ and $L_t$ does not change the value of $\\hat{\\epsilon}_F(L_t, F_t, A, t)$. This implies that the mean of the distribution $F_t + u_t\\hat{\\epsilon}_F(L_t, F_t, A, t)$ is also invariant under the same translations.\n2. Wrapped Normal Distribution $\\mathcal{N}_w$: The use of $\\mathcal{N}_w$ ensures that the probability distribution respects the crystal's periodicity. This is crucial because it means that the transition probabilities are correctly computed modulo the crystal's lattice vectors, ensuring that the distribution 'wraps around' at the boundaries of the unit cell.\n3. Equivariance of the Transition: Combining the invariance of $\\hat{\\epsilon}_F$ and the periodicity-respecting property of $\\mathcal{N}_w$, the overall transition $p(F_{t-1}\\mid L_t, F_t, A)$ is equivariant under periodic translations. This means that translating the input $F_t$ by a lattice vector results in a corresponding translation of the output distribution $F_{t-1}$, preserving the crystal's symmetry.\n\nIn summary, both $\\hat{\\epsilon}_F$ and $\\mathcal{N}_w$ must respect the crystal's periodicity to ensure that the transition probabilities are physically meaningful and consistent with the crystal's symmetry. This leads to the periodic translation equivariance of the transition, as stated in Lemma 3."}
{"paper":"arxiv:2309.04475", "question":"Explain the sequence of equalities in the expression $$p\\bigl(w(F_{t-1}+t)\\bigr) = \\mathcal{N}_w(\\cdots) = p(F_{t-1}\\mid L_t,F_t,A)\\,?$$ Specifically, how does the periodic translation invariance of the wrapped normal distribution allow us to replace $w(F_t + t\\mathbf{1})$ with $F_t$ while maintaining the same distribution $p(F_{t-1}\\mid L_t,F_t,A$)?", "whole_label":"The sequence of equalities can be understood through the properties of the wrapped normal distribution and its invariance under periodic translations. The key steps are as follows:\n\n1. Initial Expression: The expression starts with $p\\bigl(w(F_{t-1}+t)\\bigr)$, which represents the probability distribution of the wrapped version of $F_{t-1}+t$.\n\n2. Wrapped Normal Distribution: This is then equated to $\\mathcal{N}_w\\bigl(w(F_{t-1}+t)\\mid w(F_t + t) + u_t\\hat{\\epsilon}_F(\\cdots), v_t^2I\\bigr)$, where $\\mathcal{N}_w$ denotes the wrapped normal distribution. The mean of this distribution is given by $w(F_t + t) + u_t\\hat{\\epsilon}_F(\\cdots)$, and the variance is $v_t^2I$.\n\n3. Invariance Under Shifts: The wrapped normal distribution is invariant under shifts by $t$. This means that adding $t$ to both $F_{t-1}$ and $F_t$ does not change the distribution. Therefore, $w(F_t + t)$ can be replaced by $F_t$ without altering the distribution.\n\n4. Final Equality: The final equality $p(F_{t-1}\\mid L_t,F_t,A)$ is achieved by recognizing that the transition probabilities remain unchanged under these operations, demonstrating the periodic translation invariance of the wrapped normal distribution.\n\nThis reasoning shows how the properties of the wrapped normal distribution allow for the substitution and ensure that the distribution $p(F_{t-1}\\mid L_t,F_t,A)$ remains consistent."}
{"paper":"arxiv:2309.04475", "question":"Given an orthogonal transformation $Q$ such that $Q^T Q = I$, derive the identity: $$(QL)^T (QL) = L^T L$$ by showing step by step how $Q^T Q = I$ implies $(QL)^T(QL)$ simplifies to $L^T L$.", "whole_label":"To derive the identity $(QL)^T (QL) = L^T L$ using the property of an orthogonal matrix $Q$ where $Q^T Q = I$, follow these steps:\n\n1. Substitute $QL$ into the expression: Start by considering the expression $(QL)^T (QL)$. This represents the inner product of $QL$ with itself.\n\n2. Apply the transpose of a product: Recall that the transpose of a product of two matrices is the product of their transposes in reverse order. Therefore, $(QL)^T = L^T Q^T$.\n\n3. Substitute back into the expression: Replace $(QL)^T$ with $L^T Q^T$ in the original expression to get $L^T Q^T Q L$.\n\n4. Use the orthogonality condition: Since $Q$ is orthogonal, it satisfies $Q^T Q = I$. Substitute $I$ for $Q^T Q$ in the expression to obtain $L^T I L$.\n\n5. Simplify using the identity matrix: The identity matrix $I$ acts as a multiplicative identity in matrix multiplication, so $L^T I L = L^T L$.\n\nThus, we have shown that $(QL)^T (QL) = L^T L$, demonstrating that the inner product is preserved under the orthogonal transformation $Q$."}
{"paper":"arxiv:2309.04475", "question":"How do we demonstrate the periodic-Fourier invariance expressed by the equation \\(\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]\\)? Using the substitution \\(\\sin(2\\pi m(w(f_{j,c} + t_c) - w(f_{i,c} + t_c))) = \\sin(2\\pi m(f_{j,c} - f_{i,c}))\\), explain how the wrapping function \\(w(\\cdot)\\) and the shift \\(t\\) cancel out in the sine argument, ensuring the Fourier transform remains the same for \\(f_j - f_i\\) and \\(w(f_j + t) - w(f_i + t)\\).", "whole_label":"To understand the periodic-Fourier invariance, let's break down the given equation step by step. The key lies in the properties of the wrapping function \\(w(\\cdot)\\) and how it interacts with the shift \\(t\\).\n\n1. Understanding the Wrapping Function: The wrapping function \\(w(\\cdot)\\) is periodic, meaning it maps any real number into a fixed interval, typically \\([0, T)\\) for some period \\(T\\). This property implies that \\(w(x + T) = w(x)\\) for any \\(x\\).\n\n2. Effect of the Shift \\(t\\): When we add a shift \\(t\\) inside the wrapping function, the periodicity ensures that \\(w(x + t) = w(w(x) + t)\\). This means that the shift \\(t\\) does not alter the fundamental behavior of \\(w(\\cdot)\\) due to its periodic nature.\n\n3. Substitution in the Sine Argument: The given substitution shows that \\(\\sin(2\\pi m(w(f_{j,c} + t_c) - w(f_{i,c} + t_c))) = \\sin(2\\pi m(f_{j,c} - f_{i,c}))\\). This equality holds because the differences inside the wrapping functions cancel out the shift \\(t_c\\), thanks to the periodicity of \\(w(\\cdot)\\). Essentially, the wrapping function and the shift \\(t\\) 'cancel out' in the context of the sine function's argument.\n\n4. Fourier Transform Invariance: Since the sine function's argument remains unchanged after the application of \\(w(\\cdot)\\) and the shift \\(t\\), the Fourier transform of the difference \\(w(f_j + t) - w(f_i + t)\\) is identical to that of \\(f_j - f_i\\). This demonstrates the periodic-Fourier invariance, showing that the Fourier transform is unaffected by the periodic wrapping and shifting operations.\n\nIn summary, the periodic nature of \\(w(\\cdot)\\) and the properties of the sine function ensure that the shift \\(t\\) and the wrapping operation do not alter the fundamental frequency components captured by the Fourier transform. This invariance is crucial for applications where periodic signals are analyzed, ensuring consistency in frequency domain representations regardless of shifts or wraps in the time domain."}
{"paper":"arxiv:2309.04475", "question":"Explain the derivation of the mean formula for the backward process in a diffusion model, specifically focusing on how the model estimates the noise in $A_t$ and uses this estimate to compute the backward mean of $p(A_{t-1}\\mid M_t)$. The formula in question is $$\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}}\\; -\\; \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\,\\hat{\\epsilon}_A(M_t,t),$$ where $\\hat{\\epsilon}_A(M_t,t)$ is the model's estimate of the noise in $A_t$.", "whole_label":"The derivation of the mean formula for the backward process in a diffusion model involves understanding how the model estimates and removes noise from $A_t$ to recover a cleaner version of the data. The key steps are as follows:\n\n1. Noise Addition in Forward Process: The forward process gradually adds noise to the data $A_0$ over $t$ steps, resulting in $A_t$ as $$A_t = \\sqrt{\\bar{\\alpha}_t} A_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_A,$$ where $\\bar{\\alpha}_t$ is a product of scaling factors up to step $t$, and $\\epsilon_A$ is the noise added at step $t$.\n\n2. Noise Estimation: The model learns to estimate the noise $\\epsilon_A$ added at each step, denoted as $\\hat{\\epsilon}_A(M_t, t)$. This estimation is crucial for reversing the diffusion process.\n\n3. Backward Mean Calculation: The backward mean $\\mu_A(M_t)$ is computed by adjusting $A_t$ to remove the estimated noise. The formula $$\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t)$$ effectively scales $A_t$ back by $\\sqrt{\\bar{\\alpha}_t}$ and subtracts the scaled estimated noise, yielding a denoised version of $A_t$.\n\nThis process is central to the diffusion model's ability to generate data by iteratively denoising from a noisy initial state, leveraging the estimated noise at each step to guide the denoising direction."}
{"paper":"arxiv:2303.10876", "question":"Based on Theorem 1, how can we prove Corollary 1, which states that the entire EqMotion network $F_{pred}(\\cdot)$ satisfies $\\hat{Y}R + t = F_{pred}(X_R + t)$ for any $R \\in SO(n)$ and $t \\in \\mathbb{R}^n$? Please show how the result of Theorem 1 implies this corollary.", "whole_label":"To prove Corollary 1, we start by recalling the result of Theorem 1, which establishes the equivariance of the network's operations under Euclidean transformations. Specifically, Theorem 1 shows that for any rotation matrix $R \\in SO(n)$ and translation vector $t \\in \\mathbb{R}^n$, the network's operations preserve the transformation properties of the input data. \n\nGiven this, we can derive Corollary 1 by considering the composition of all network operations. The network $F_{pred}(\\cdot)$ takes an input $X$ and applies a series of equivariant operations to produce an output $\\hat{Y}$. Due to the equivariance property established by Theorem 1, applying a rotation $R$ and translation $t$ to the input $X$ (resulting in $X_R + t$) and then passing it through the network yields an output that is equivalent to applying the same rotation and translation to the network's output for the original input $X$. Mathematically, this is expressed as:\n\n\\[ \\hat{Y}R + t = F_{pred}(X_R + t). \\]\n\nThis equation demonstrates that the entire network $F_{pred}(\\cdot)$ is equivariant under Euclidean transformations, thereby satisfying Corollary 1. The key insight is that the equivariance of individual operations, as guaranteed by Theorem 1, ensures the equivariance of the entire network when these operations are composed."}
{"paper":"arxiv:2303.10876", "question":"Based on the formula: $G_i^{(l+1)} = \\phi_a(h_i^{(l)})\\cdot(G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}$, demonstrate how the expression $G_i^{(l+1)} R + t$ under rotation $R$ and translation $t$ can be shown to equal $\\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}})R + \\overline{G_i^{(l)}}R + t$, thereby proving its equivariance.", "whole_label":"To establish the equivariance of the given formula under rotation $R$ and translation $t$, we start by applying the transformation to $G_i^{(l+1)}$:\n\n1. Apply the rotation and translation to $G_i^{(l+1)}$:\n   $$ G_i^{(l+1)} R + t = (\\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}) R + t $$\n\n2. Distribute the rotation $R$ and the translation $t$ across the terms:\n   $$ = \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) R + \\overline{G_i^{(l)}} R + t $$\n\n3. Recognize that $\\phi_a(h_i^{(l)})$ is a scalar and thus unaffected by the rotation $R$. The term $\\overline{G_i^{(l)}} R + t$ adjusts the coordinate frame to account for the rotation and translation.\n\nThis demonstrates that the formula maintains its structure under the transformation, confirming its equivariance with respect to rotation and translation."}
{"paper":"arxiv:2303.10876", "question":"Given the formulas $Q_i^{(l)} = W_Q^{(l)}(G_i^{(l)} - \\overline{G_i^{(l)}})$ and $K_i^{(l)} = W_K^{(l)}(G_i^{(l)} - \\overline{G_i^{(l)}})$, and knowing that $R R^T = I$, explain how we can derive the equality $Q_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} (K_i^{(l)})^T$. Show the reasoning behind why multiplying by $R$ and $R^T$ cancels out in the inner product of queries and keys.", "whole_label":"To understand why multiplying by $R$ and $R^T$ cancels out in the inner product of queries and keys, let's break down the derivation step by step. Starting with the expression $Q_i^{(l)} R (K_i^{(l)} R)^T$, we can expand it using the properties of matrix multiplication and transposition:\n\n1. First, apply the transpose to the product $(K_i^{(l)} R)^T$, which gives $R^T (K_i^{(l)})^T$ due to the property $(AB)^T = B^T A^T$.\n\n2. Now, the expression becomes $Q_i^{(l)} R R^T (K_i^{(l)})^T$.\n\n3. Given that $R R^T = I$, where $I$ is the identity matrix, we can substitute $I$ into the expression, resulting in $Q_i^{(l)} I (K_i^{(l)})^T$.\n\n4. Multiplying by the identity matrix $I$ leaves the original matrix unchanged, so the expression simplifies to $Q_i^{(l)} (K_i^{(l)})^T$.\n\nThis derivation shows that the attention score $Q_i^{(l)} (K_i^{(l)})^T$ remains unchanged when both the query and key are multiplied by $R$ and $R^T$, respectively, because the product $R R^T$ cancels out to the identity matrix. This property is crucial for maintaining the integrity of the attention mechanism when applying orthogonal transformations like $R$."}
{"paper":"A Connection Between Score Matching and Denoising Autoencoders", "question":"Step 1 (Preconditions)\n- This shows a key relationship between the Denoising Score Matching objective \\(J_{DSM_{q_\\sigma}}\\) and the DAE reconstruction error \\(J_{DAE_\\sigma}(\\theta)\\).\n- It depends on Formula 3 for \\(J_{DSM_{q_\\sigma}}(\\theta)\\), the gradient formula 4 for \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x})\\), and the specific form of \\(\\psi(\\tilde{x}; \\theta)\\).\n- \\(J_{DAE_\\sigma}(\\theta)\\) is the mean squared reconstruction error for the denoising autoencoder with noise level \\(\\sigma\\).\n\nStep 2 (Derived Question)\n\"Based on Formula 3: \\(J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\|\\psi(\\tilde{x}; \\theta) - \\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x})\\|^2\\right]\\), Formula 4: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x}) = \\frac{1}{\\sigma^2}(x - \\tilde{x})\\), and the definition of the DAE reconstruction cost \\(J_{DAE_\\sigma}(\\theta)\\), how do we derive \\(J_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta)\\)?\"", "whole_label":"Substituting the given equations in the expression for \\(J_{DSM_{q_\\sigma}}\\), we get, for this choice of Parzen kernel and density model, \n$$J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x,\\tilde{x})}\\Bigl[\\tfrac{1}{2}\\bigl\\|\\psi(\\tilde{x};\\theta) - \\tfrac{1}{\\sigma^2}(x-\\tilde{x})\\bigr\\|^2\\Bigr]\\;=\\;\\frac{1}{2\\sigma^4}\\,J_{DAE_\\sigma}(\\theta).$$\nThis shows that \\(J_{DSM_{q_\\sigma}} = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}\\)."}
{"paper":"A Connection Between Score Matching and Denoising Autoencoders", "question":"Step 1 (Preconditions): Convert \\(q(x|\\tilde{x})\\) into \\(q(x|\\tilde{x})\\nabla_x q(x|\\tilde{x})\\). Use the identity \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\). Also introduce \\(q(x, \\tilde{x}) = q(x|\\tilde{x}) q(\\tilde{x})\\). Step 2 (Derived Question): From the given identity, how do we use \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\) to obtain the formula for \\(S(\\theta)\\): \\(S(\\theta) = \\int_x \\int_{\\tilde{x}} q(x|\\tilde{x}) \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(\\tilde{x}) d\\tilde{x} dx\\)?", "whole_label":"Starting with the identity \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\), we can express the score function \\(S(\\theta)\\) as follows: \\[ S(\\theta) = \\int_x \\int_{\\tilde{x}} \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) d\\tilde{x} dx. \\] This can be rewritten in terms of expectation over the joint distribution \\(q_\\sigma(x, \\tilde{x})\\) as: \\[ S(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle\\right]. \\] This derivation shows how the given identity is used to transition from the gradient of the conditional probability to the expectation form of the score function, incorporating the joint distribution \\(q_\\sigma(x, \\tilde{x})\\)."}
{"paper":"arxiv:2309.04475", "question":"How can we demonstrate that the wrapped normal distribution $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ simplifies to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ through index shifting in the infinite sum? Provide the detailed reindexing process that leads to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$", "whole_label":"To show that the wrapped normal distribution $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ simplifies to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ by shifting indices in the infinite sum, follow these steps:\n\n1. Start with the definition of the wrapped normal distribution:\n   \\[\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + k' + k \\mid \\mu + k'', \\sigma^2).\\]\n\n2. Perform a change of variables by setting \\( m = k + k' - k'' \\). This reindexes the sum as:\n   \\[\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + k' + k \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m + k'' \\mid \\mu + k'', \\sigma^2).\\]\n\n3. Utilize the property of the normal distribution to simplify the expression inside the sum:\n   \\[\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m + k'' \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu, \\sigma^2).\\]\n\n4. Recognize that the resulting sum is the definition of the wrapped normal distribution for \\( x \\):\n   \\[\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\\]\n\nThus, through careful reindexing and simplification, we've shown that $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ indeed collapses to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$."}
{"paper":"arxiv:2309.04475", "question":"How can we prove the following lemma: 'Consider the generation Markov process defined by \\( p(x_0) = \\int p(x_{0:T-1}\\mid x_T)\\,dx_{1:T} \\). If the prior distribution \\( p(x_T) \\) is G-invariant and the Markov transitions \\( p(x_{t-1}\\mid x_t) \\) for \\( 0 < t \\le T \\) are G-equivariant, then the marginal distribution \\( p(x_0) \\) is also G-invariant.'? Please outline why the property of G-invariance propagates backward through the Markov chain.", "whole_label":"To prove that the marginal distribution \\( p(x_0) \\) is G-invariant under the given conditions, we can follow these steps:\n\n1. Understand G-invariance and G-equivariance: A distribution \\( p(x) \\) is G-invariant if for any group action \\( g \\in G \\), \\( p(gx) = p(x) \\). A Markov transition \\( p(x_{t-1}\\mid x_t) \\) is G-equivariant if \\( p(gx_{t-1}\\mid gx_t) = p(x_{t-1}\\mid x_t) \\) for all \\( g \\in G \\).\n\n2. Start with the prior distribution: Given that \\( p(x_T) \\) is G-invariant, we have \\( p(gx_T) = p(x_T) \\) for any \\( g \\in G \\).\n\n3. Propagate G-invariance through the Markov chain: For each transition \\( p(x_{t-1}\\mid x_t) \\), because it is G-equivariant, applying a group action \\( g \\) to \\( x_t \\) and \\( x_{t-1} \\) leaves the transition probability unchanged. This means that the G-invariance property is preserved at each step of the Markov chain.\n\n4. Integrate over the Markov chain: The marginal distribution \\( p(x_0) \\) is obtained by integrating over all possible paths \\( x_{1:T} \\). Since each component in the integrand (the prior and the transitions) preserves G-invariance, the resulting marginal distribution \\( p(x_0) \\) must also be G-invariant.\n\n5. Conclusion: Therefore, under the conditions that the prior distribution is G-invariant and the Markov transitions are G-equivariant, the marginal distribution \\( p(x_0) \\) inherits the G-invariance property, completing the proof."}
{"paper":"arxiv:2303.10876", "question":"Given that $R$ is an orthonormal rotation matrix and $t$ is a translation vector, demonstrate how the columnwise $\\ell_2$-distance $||G_i^{(0)} - G_j^{(0)}||_{2,col}$ remains invariant under Euclidean transformations by showing $||g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)||_2 = ||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$.", "whole_label":"To prove the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations, consider the transformation of the c-th column of matrices $G_i^{(0)}$ and $G_j^{(0)}$ by an orthonormal rotation matrix $R$ and a translation vector $t$. The calculation proceeds as follows:\n\n1. Apply the Euclidean Transformation: Each column is transformed as $g_{i,c}^{(0)} \\rightarrow g_{i,c}^{(0)} R + t$ and similarly for $g_{j,c}^{(0)}$.\n\n2. Compute the Difference: The difference between the transformed columns is $(g_{i,c}^{(0)} R + t) - (g_{j,c}^{(0)} R + t) = (g_{i,c}^{(0)} - g_{j,c}^{(0)}) R$.\n\n3. Calculate the Norm: The $\\ell_2$-norm of the difference is $||(g_{i,c}^{(0)} - g_{j,c}^{(0)}) R||_2$. Due to the orthonormality of $R$, this simplifies to $||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$, because multiplying by an orthonormal matrix preserves the norm.\n\nThus, we have shown that $||g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)||_2 = ||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$, proving the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations."}
{"paper":"arxiv:2407.10490", "question":"In our analysis of learning dynamics for neural network finetuning, we consider a model with logits \\(z = h_{\\theta}(x)\\) and a probability distribution defined by \\(\\pi = \\mathrm{Softmax}(z)\\). The model parameters are updated via gradient descent on a loss function \\(L(x,y)\\) with an update \\(\\Delta\\theta = -\\eta\\,\\nabla_{\\theta} L(x_u,y_u)\\), where \\((x_u,y_u)\\) is the updating example. For an observing example \\(x_o\\), the one-step change in the log-probability is given by \\(\\Delta\\log\\pi_t(y \\mid x_o)\\).\n\nProposition 1 states that\n\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = -\\eta\\, A_t(x_o)\\, K_t(x_o,x_u)\\, G_t(x_u,y_u) + O\\Big(\\eta^2\\,\\|\\nabla_{\\theta}z(x_u)\\|^2_{op}\\Big),\n\\]\n\nwhere\n\n- \\(A_t(x_o) = \\nabla_z \\log \\pi_{\\theta_t}(x_o) = I - 1\\,\\pi_{\\theta_t}^\\top(x_o)\\),\n- \\(K_t(x_o,x_u) = \\Big(\\nabla_{\\theta}z(x_o)\\Big|_{\\theta_t}\\Big)\\Big(\\nabla_{\\theta}z(x_u)\\Big|_{\\theta_t}\\Big)^\\top\\) is the empirical neural tangent kernel, and\n- \\(G_t(x_u,y_u) = \\nabla_z L(x_u,y_u)\\big|_{z_t}\\).\n\nCould you provide a complete and detailed proof of Proposition 1?", "whole_label":"Suppose we want to observe the model's prediction on an \"observing example\" \\(x_o\\). Starting from Equation (2), we first approximate \\(\\log \\pi_{t+1}(y \\mid x_o)\\) using first-order Taylor expansion (we use \\(\\pi_t\\) to represent \\(\\pi_{\\theta_t}\\) interchangeably for notation conciseness):\n\n\\[\n\\log \\pi_{t+1}(y \\mid x_o) = \\log \\pi_t(y \\mid x_o) + \\langle \\nabla \\log \\pi_t(y \\mid x_o), \\theta_{t+1} - \\theta_t \\rangle + O(\\|\\theta_{t+1} - \\theta_t\\|^2),\n\\]\n\nThen, assuming the model updates its parameters using SGD calculated by an \"updating example\" \\((x_u,y_u)\\), we can rearrange the terms in the above equation to get the following expression:\n\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = \\log \\pi_{t+1}(y \\mid x_o) - \\log \\pi_t(y \\mid x_o) = \\nabla_\\theta \\log \\pi_t(y \\mid x_o)\\big|_{\\theta_t} (\\theta_{t+1} - \\theta_t) + O(\\|\\theta_{t+1} - \\theta_t\\|^2),\n\\]\n\nwhere \\(d\\) is the number of parameters of the model. To evaluate the leading term, we plug in the definition of SGD and repeatedly use the chain rule:\n\n\\[\n\\begin{aligned}\n\\nabla_\\theta \\log \\pi_t(y \\mid x_o)\\big|_{\\theta_t} (\\theta_{t+1} - \\theta_t) & = \\Big( \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\Big) \\Big( -\\eta \\nabla_\\theta L(x_u)\\big|_{\\theta_t} \\Big)^\\top \\\\\n& = \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\Big( -\\eta \\nabla_z L(x_u)\\big|_{z_t} \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\Big) \\\\\n& = -\\eta\\, \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\Big[ \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\Big] \\; \\Big( \\nabla_z L(x_u)\\big|_{z_t} \\Big)^\\top \\\\\n& = -\\eta\\, A_t(x_o)\\, K_t(x_o,x_u)\\, G_t(x_u,y_u).\n\\end{aligned}\n\\]\n\nFor the higher-order term, using as above that\n\n\\[\n\\theta_{t+1} - \\theta_t = -\\eta \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\; G_t(x_u,y_u),\n\\]\n\nand noting that, since the residual term \\(G_t\\) is usually bounded (and the practical algorithms will also use gradient clipping to avoid too large gradients), we have that\n\n\\[\nO(\\|\\theta_{t+1} - \\theta_t\\|^2) = O\\Big(\\eta^2 \\; \\big\\|\\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top\\big\\|^2_{\\text{op}} \\; \\|G_t(x_u,y_u)\\|^2_{\\text{op}}\\Big) = O\\Big(\\eta^2 \\; \\|\\nabla_\\theta z(x_u)\\|^2_{\\text{op}}\\Big).\n\\]\n"}
{"paper":"arxiv:2407.10490", "question":"Consider a large language model (LLM) being fine-tuned using gradient descent with a small learning rate \\(\\eta\\). The model outputs logits \\(z = h_{\\theta}(\\chi)\\) given an input \\(\\chi\\) (which may represent a concatenation of a prompt and a response), and the output probability is computed via the softmax function as \\(\\pi_{\\theta}(y\\mid\\chi) = \\mathrm{Softmax}(z)\\). In this setting, the per-step change in the log-probability of an output token when performing an update on an updating sample \\((x_u, y_u)\\) (or more generally \\(\\chi_u\\)) is approximated by the decomposition\n\\[\n\\Delta \\log \\pi_t(y \\mid \\chi_o) = -\\eta\\, A_t(\\chi_o)\\, K_t(\\chi_o, \\chi_u)\\, G_t(\\chi_u) + O(\\eta^2),\n\\]\nwhere\n- \\(A_t(\\chi_o) = \\nabla_z \\log \\pi_{\\theta}(\\chi_o)\\) is the sensitivity matrix at an observation point \\(\\chi_o\\),\n- \\(K_t(\\chi_o, \\chi_u) = \\big(\\nabla_{\\theta} z(\\chi_o)\\big)\\big(\\nabla_{\\theta} z(\\chi_u)\\big)^\\top\\) is the empirical neural tangent kernel (eNTK), and\n- \\(G_t(\\chi_u) = \\nabla_z L(\\chi_u)\\big|_{z_t}\\) is the residual term corresponding to the gradient of the loss with respect to the logits, evaluated at \\(\\chi_u\\).\n\nFor supervised fine-tuning (SFT), the loss is defined as the negative log-likelihood (NLL) over the chosen response \\(y_u^+\\), i.e.,\n\\[\nL_{\\mathrm{SFT}}(\\chi_u) = -\\sum_{l=1}^{L} \\log \\pi_{\\theta}(y_l^+ \\mid \\chi_u),\n\\]\nwhile for Direct Preference Optimization (DPO) the loss involves comparing the chosen response \\(y_u^+\\) and a rejected response \\(y_u^-\\) (with a margin parameter \\(\\beta\\)) via terms such as\n\\[\nL_{\\mathrm{DPO}}(\\theta) = -\\mathbb{E}\\Big[\\log \\sigma\\Big(\\beta\\,\\Big(\\log \\frac{\\pi_{\\theta}(y_u^+\\mid \\chi_u^+)}{\\pi_{\\theta}(y_u^-\\mid \\chi_u^-)} - \\log \\frac{\\pi_{\\mathrm{ref}}(y_u^+\\mid \\chi_u^+)}{\\pi_{\\mathrm{ref}}(y_u^-\\mid \\chi_u^-)}\\Big)\\Big)\\Big],\n\\]\nwith the teacher forcing mechanism applied in both cases so that the target sequence is given.\n\nCould you derive the explicit expression for the residual term \\(G_t(\\chi_u)\\) for these LLM fine-tuning algorithms (SFT and DPO), including a clear statement of all symbols, definitions, and necessary assumptions?", "whole_label":"Here we derive the residual term, i.e., $G_t$ for different algorithms in LLM's finetuning. We first rewrite Equation (5) here:\n\n$$[\\Delta \\log \\pi_t(y | \\chi_o)]_m = -\\sum_{l=1}^{L} \\eta [A_t(\\chi_o)]_m [K_t(\\chi_o,\\chi_u)]_l [G_t(\\chi_u)]_l + O(\\eta^2),$$\n\nwhere $m \\in \\{1, \\dots, M\\}$, $l \\in \\{1, \\dots, L\\}$, and $G_t(\\chi_u) = \\nabla_{z}L(\\chi_u)|_{z_t}$ is a $V \\times L$ matrix. As the auto-regression nature of the SFT loss is already encoded in the causal mask used in $h_t$, as demonstrated in Figure 10a, the columns in $G_t(\\chi_u)$ are independent of each other, which can be separately calculated. Plus, the summation over $l$ can also be achieved by left-multiplying a length-$L$ all-one vector $\\mathbf{1}$. Specifically, the SFT loss for each $l$ is:\n\n$$[L_{\\mathrm{SFT}}(\\chi_u)]_l = - \\log \\pi(y_l = y_{u,l}^+ | \\chi_u) = -e^{\\top}_{y_{u,l}^+} \\log \\pi(y | \\chi_u) = -e^{\\top}_{y_{u,l}^+} \\log (\\mathrm{Softmax}(z_l)),$$\n\nwhere $y_{u,l}^+$ is the $l$-th dimension (token) of the target sequence $y_u^+$. The gradient of $L$ on $z$ can be then calculated as (denoting $L_l = [L_{\\mathrm{SFT}}(\\chi_u)]_l$, $\\pi_l = \\pi(y | \\chi_u)$ at step $l$, $k=y_{u,l}^+$):\n\n$$[G_t^{\\mathrm{SFT}}(\\chi_u)]_l = \\nabla_{z_l} L_l = (\\nabla_{\\pi_l} L_l) (\\nabla_{z_l}\\pi_l) = \\pi_l - e_{y_{u,l}^+} \\tag{10}$$\n\nwhere the result is a column vector (gradient w.r.t logits $z_l$ for the $l$-th token). The gradient $\\nabla_{z_l} L_l$ is derived below. Note that $\\pi_l$ represents the probability vector $\\pi(y_l | \\chi_u)$.\n\nTo calculate the equation above, we first recall the NLL loss of the $l$-th token is $[L_{\\mathrm{SFT}}]_l \\triangleq L_l = - \\log \\pi_l(y_l = y_{u,l}^+) = -e^{\\top}_{y_{u,l}^+} \\log \\pi_l$, where $\\pi_l = \\mathrm{Softmax}(z_l)$. Then, the gradient as a row vector is $\\nabla_{z_l}L_l = (\\nabla_{\\pi_l}L_l) (\\nabla_{z_l}\\pi_l)$. For each dimension $i$ of $\\pi_l$, we have $\\partial L_l / \\partial \\pi_{l,i} = 0$ if $i \\neq y_{u,l}^+$ and $\\partial L_l / \\partial \\pi_{l,i} = - 1/\\pi_{l,i}$ if $i = y_{u,l}^+$. Writing $\\nabla_{\\pi_l}L_l$ as a $1 \\times V$ row vector, we have $\\nabla_{\\pi_l}L_l = - \\frac{1}{\\pi_{l,k}} e_k^\\top$, where $k=y_{u,l}^+$. The Jacobian matrix $\\nabla_{z_l}\\pi_l$ (dimension $V \\times V$) is:\n\n$$\\nabla_{z_l}\\pi_l =\n\\begin{bmatrix} \\pi_{l,1}(1-\\pi_{l,1}) & -\\pi_{l,2}\\pi_{l,1} & \\dots & -\\pi_{l,V}\\pi_{l,1} \\\\ -\\pi_{l,1}\\pi_{l,2} & \\pi_{l,2}(1-\\pi_{l,2}) & \\dots & -\\pi_{l,V}\\pi_{l,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ -\\pi_{l,1}\\pi_{l,V} & -\\pi_{l,2}\\pi_{l,V} & \\dots & \\pi_{l,V}(1-\\pi_{l,V}) \\end{bmatrix}.$$\n\nCombining these, the gradient $\\nabla_{z_l}L_l$ (as a $1 \\times V$ row vector) is $\\nabla_{z_l}L_l = \\nabla_{\\pi_l}L_l \\nabla_{z_l}\\pi_l = - \\frac{1}{\\pi_{l,k}} e_k^\\top \\nabla_{z_l}\\pi_l$. This selects the $k$-th row of $\\nabla_{z_l}\\pi_l$ and multiplies by $-1/\\pi_{l,k}$.\nIn summary, we have:\n\n$$\\nabla_{z_l}L_l = - \\frac{1}{\\pi_{l,k}} [ -\\pi_{l,k}\\pi_{l,1}, -\\pi_{l,k}\\pi_{l,2}, \\dots, \\pi_{l,k}(1-\\pi_{l,k}), \\dots, -\\pi_{l,k}\\pi_{l,V} ] = [\\pi_{l,1}, \\pi_{l,2}, \\dots, \\pi_{l,k} - 1, \\dots, \\pi_{l,V}] = (\\pi_l - e_k)^\\top.$$\n\nAs $G_t^{\\mathrm{SFT}}(\\chi_u)$ is defined as a $V \\times L$ matrix where each column $l$ is the gradient $\\nabla_{z_l}L_l$ evaluated at $z_t$, the $l$-th column is $(\\pi_{l, t} - e_{y_{u,l}^+})$, where $\\pi_{l, t}$ is the probability vector at step $t$. By stacking the column vectors for different $l \\in \\{1, \\dots, L\\}$, we get:\n\n$$G_t^{\\mathrm{SFT}}(\\chi_u) = \\nabla_{z}L_{\\mathrm{SFT}}(\\chi_u)|_{z_t} = \\Pi_{\\theta_t}(y | \\chi_u) - Y_u^+ \\tag{11}$$ \nwhere $\\Pi_{\\theta_t}(y | \\chi_u)$ is the $V \\times L$ matrix whose $l$-th column is the predicted probability distribution $\\pi_{l,t} = \\mathrm{Softmax}(z_{l,t})$, and $Y_u^+$ is the $V \\times L$ matrix whose $l$-th column is the one-hot vector $e_{y_{u,l}^+}$ for the target token $y_{u,l}^+$."}
{"paper":"arxiv:2312.11456", "question":"Consider the following statement of Theorem 1:\n\nUnder Assumption 1, suppose that the reward function is parameterized as \\(r_{\\theta}(x,a)=\\langle \\theta, \\phi(x,a) \\rangle\\) with \\(\\|\\phi(x,a)\\|\\le 1\\) for all \\((x,a)\\) and \\(\\|\\theta\\|\\le B\\). Let the offline dataset \\(D_{off}\\) be given and define its associated covariance matrix as \\(\\Sigma_{off}=\\lambda I+\\sum_{(x,a_1,a_2)\\in D_{off}}(\\phi(x,a_1)-\\phi(x,a_2))(\\phi(x,a_1)-\\phi(x,a_2))^{\\top}\\). Assume that the performance measure of any policy \\(\\pi\\) is defined by\n\\[\nJ(\\pi)=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[ r^{*}(x,a)+\\eta\\log \\frac{\\pi_0(a|x)}{\\pi(a|x)} \\Big]\\Bigg],\n\\]\nwhere \\(\\pi_0\\) is the initial policy, \\(\\eta>0\\) is a constant, and \\(d_0\\) is the distribution over the states (or prompts). Also, let \\(\\nu\\) be defined as \\(\\nu=\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi_{ref})]\\) for a given reference policy \\(\\pi_{ref}\\), and let \\(\\Pi\\) be the considered policy class. Theorem 1 asserts that if \\(\\beta\\) is chosen appropriately as\n\\[\n\\beta=O\\Bigg(\\sqrt{\\frac{d+\\log(1/\\delta)}{\\gamma^2+\\lambda B^2}}\\Bigg),\n\\]\nthen with probability at least \\(1-\\delta\\), the policy \\(\\hat{\\pi}\\) output by Algorithm 1 satisfies for Option I\n\\[\nJ(\\pi)-J(\\hat{\\pi})\\le 2\\beta\\cdot \\Big\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\Big\\|_{\\Sigma_{off}^{-1}},\n\\]\nand for Option II\n\\[\nJ(\\pi)-J(\\hat{\\pi})\\le 2\\beta\\cdot \\mathbb{E}_{x\\sim d_0,\\,a\\sim \\pi(\\cdot|x)}\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big].\n\\]\n\nCould you provide a complete derivation and proof of Theorem 1?", "whole_label":"We start with Option I. If we set \\(\\hat{r}(x,a)=\\langle \\theta_{MLE},\\phi(x,a)\\rangle\\), and take the policy by\n\n\\[\n\\hat{\\pi}=\\arg\\max_{\\pi\\in \\Pi}\\Big[\\langle \\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]\\rangle-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\big[D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big]\\Big]\\]\n\nThen, we have\n\n\\[\n\\langle \\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]\\rangle+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x))-D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))\\Big]\\le 0. \\tag{20}\n\\]\n\nFor simplicity, we denote the LHS of Equation (20) as (($\\star$)). We plugging this into the estimation of \\(J(\\pi)-J(\\hat{\\pi})\\):\n\n\\[\n\\begin{aligned}\nJ(\\pi)-J(\\hat{\\pi})&=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[r^*(x,a)+\\eta\\log\\frac{\\pi_0(a|x)}{\\pi(a|x)}\\Big]\\\\\n&\\qquad\\qquad-\\mathbb{E}_{a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[r^*(x,a)+\\eta\\log\\frac{\\pi_0(a|x)}{\\hat{\\pi}(a|x)}\\Big]\\Bigg]\\\\\n&=(\\star)+\\langle \\theta^*-\\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]\\rangle+\\langle \\theta_{MLE}-\\theta^*,\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]\\rangle\\\\\n&\\qquad\\qquad-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}\\\\\n&\\le \\langle \\theta^*-\\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\rangle+\\langle \\theta_{MLE}-\\theta^*,\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\rangle\\\\\n&\\qquad\\qquad-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}\\\\\n&\\le 2\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}.\n\\end{aligned}\n\\]\n\nFor Option II, we use the point-wise pessimism:\n\n\\[\n\\hat{r}(x,a)=r_{MLE}(x,a)-\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}.\n\\]\n\nThen, we call Oracle 1 with \\(\\hat{r}\\) to get \\(\\hat{\\pi}\\). By Lemma 1, we have\n\n\\[\n\\begin{aligned}\nJ(\\pi)-J(\\hat{\\pi})&=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[r^*(x,a)-\\hat{r}(x,a)\\Big]+\\mathbb{E}_{a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[\\hat{r}(x,a)-r^*(x,a)\\Big]\\\\\n&\\qquad\\qquad-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big]\\Bigg]\\\\\n&=\\mathbb{E}_{x\\sim d_0,a\\sim \\pi(\\cdot|x)}\\Big[\\langle \\theta^*-\\theta_{MLE},\\phi(x,a)-\\nu\\rangle+\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}\\Big]\\\\\n&\\qquad+\\mathbb{E}_{x\\sim d_0,a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[\\langle \\theta_{MLE}-\\theta^*,\\phi(x,a)-\\nu\\rangle-\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}\\Big]\\\\\n&\\qquad-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big]\\\\\n&\\le 2\\beta\\cdot\\mathbb{E}_{x\\sim d_0,a\\sim \\pi(\\cdot|x)}\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big].\n\\end{aligned}\n\\]"}
{"paper":"arxiv:2312.11456", "question":"Consider the following setting: Let \\(\\pi_0\\) be the initial policy and \\(\\pi^*\\) the optimal policy, with the KL penalty coefficient \\(\\eta > 0\\). Under Assumption 1, the reward function is parameterized as \\(r_\\theta(x,a)=\\langle\\theta,\\phi(x,a)\\rangle\\) for all \\((x,a)\\) with \\(\\|\\phi(x,a)\\| \\le 1\\) and \\(\\|\\theta\\| \\le B\\), and \\(\\gamma = 1/(2+\\exp(-B)+\\exp(B))\\). Moreover, assume that we have an offline dataset \\(D_{\\mathrm{off}}\\) with covariance matrix \\(\\Sigma_{\\mathrm{off}}\\) and a reference policy \\(\\pi_{\\mathrm{ref}}\\). Let \\(T = \\min\\{n \\in \\mathbb{N}_+ : n \\ge d\\log(n)\\}\\) and \\(\\beta = O\\big(\\sqrt{d\\log(T/\\delta)}/\\gamma^2\\big)\\). Then, Theorem 2 asserts that with probability at least \\(1-3\\delta\\), there exists an iteration \\(t_0 \\in [T]\\) such that Algorithm 2 with Option I satisfies\n\\[\nJ(\\pi^*) - J(\\pi_{t_0}) \\lesssim \\sqrt{\\frac{d}{\\gamma^2 m}} + \\beta \\cdot \\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\mathrm{ref}})\\big]\\Big\\|_{\\Sigma_{\\mathrm{off}}^{-1}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{\\mathrm{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\,.\n\\]\nFurthermore, if Assumption 2 holds, a similar bound---with an adjusted second term---is obtained. Could you provide a detailed derivation and proof of Theorem 2 under these settings, clearly explaining the meaning of each symbol and the underlying assumptions?", "whole_label":"We recall the value decomposition\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})=\\mathbb{E}_{x\\sim d_0}\\left[\\langle \\theta^*-\\theta_{t_0},\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\rangle\\right]\n+\\mathbb{E}_{x\\sim d_0}\\left[\\langle \\theta_{t_0}-\\theta^*,\\phi(x,\\pi_{t_0})-\\phi(x,\\pi_{\\text{ref}})\\rangle\\right]\n-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\left[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\right].\n\\]\nFollowing the proof of batch online learning (Theorem 3), we can control the exploration error \\(P'_2\\) as in Equation (12) by fixing \\(\\pi^t_2=\\pi_{\\text{ref}}\\). We notice that since \\(\\pi_{\\text{ref}}\\) is directly available to the agent and is used to collect data, we do not need to add additional optimism to relate its uncertainty to the data. Therefore, we only need to handle the suboptimality source \\(P'_1\\), which satisfies\n\\[\nP'_1\\le \\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}+D_{1:t_0}}}.\n\\]\nIt follows that\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})\\le \\Big( \\sqrt{\\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1+2\\sqrt{\\frac{\\log(2/\\delta)}{2m}}}\\;\\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}}-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\Big)\n+\\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}+D_{1:t_0}}}.\n\\]\nBy choosing \\(T\\) satisfying \\(T\\ge d\\log(T)\\) and \\(\\lambda=\\Theta\\big(\\frac{d\\log(T/\\delta)}{m\\gamma^2}\\big)\\), we have\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})=\\widetilde{O}\\Big(\\sqrt{\\frac{d}{\\gamma^2 m}}+\\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}+D_{1:t_0}}}-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\Big).\n\\]\nThis concludes the proof."}
{"paper":"arxiv:2312.11456", "question":"Consider the following theorem (Theorem 3). Assume that the reward function is parameterized linearly as \\(r_\\theta(x,a)=\\langle \\theta, \\phi(x,a)\\rangle\\) with \\(\\|\\phi(x,a)\\|\\le 1\\) for all \\((x,a)\\), and that the true reward is given by \\(r^*(x,a)=\\langle \\theta^*, \\phi(x,a)\\rangle\\) where \\(\\|\\theta^*\\|\\le B\\) for some constant \\(B>0\\). Let the KL-regularized objective be defined as\n\\[\nJ(\\pi)=\\mathbb{E}_{x\\sim d_0}\\Big[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\big[r^*(x,a)\\big]-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\parallel \\pi_0(\\cdot|x))\\Big],\n\\]\nwith a given reference policy \\(\\pi_0\\) and a penalty coefficient \\(\\eta>0\\). Define the batch size as \\(m=\\frac{d}{\\gamma^2\\epsilon^2}\\), where \\(\\gamma=\\frac{1}{2+e^{-B}+e^{B}}\\) and \\(\\epsilon>0\\) is a desired accuracy level. Also, let \\(\\beta=O\\Big(\\sqrt{d}\\frac{\\log(T/\\delta)}{\\gamma^2m}\\Big)\\) and \\(\\lambda=\\Theta\\Big(\\frac{d\\log(T/\\delta)}{m\\gamma^2B^2}\\Big)\\) for a confidence parameter \\(\\delta\\). Suppose that Algorithm 2 with Option II is run for \\(T=\\min\\{n\\in\\mathbb{N}^+: n\\ge d\\log(n)\\}\\) iterations. Theorem 3 then asserts that with probability at least \\(1-3\\delta\\) there exists an iteration \\(t_0\\in[T]\\) such that\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0})\\lesssim\\epsilon-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel \\pi_1^{t_0}(\\cdot|x))\\Big],\n\\]\nand that the total number of collected samples is at most \\(mT=\\tilde{O}\\Big(\\frac{d^2}{\\gamma^2\\epsilon^2}\\Big)\\). Here, \\(\\pi^*\\) denotes the optimal policy and \\(\\pi_1^{t}\\) is the policy of the main agent at iteration \\(t\\) (as defined in Algorithm 2 with Option II), while \\(D_{KL}(\\cdot\\parallel\\cdot)\\) represents the Kullback-Leibler divergence. Could you provide a detailed derivation of Theorem 3 including all necessary intermediate steps and justifications", "whole_label":"Recall the definition of the covariance matrix:\n\n\\(\\Sigma_{t,m} = \\lambda I + \\frac{1}{m}\\sum_{i=1}^{t-1}\\sum_{j=1}^{m} (\\phi(x_{i,j}, a_{1\\,i,j}) - \\phi(x_{i,j}, a_{2\\,i,j}))(\\phi(x_{i,j}, a_{1\\,i,j}) - \\phi(x_{i,j}, a_{2\\,i,j}))^\\top\\).\\n\\nThen, by invoking Lemma 8 for \\(\\theta_t\\) with \\(\\Sigma_D = m\\Sigma_{t,m}\\) and \\(\\lambda' = m\\lambda\\), we have with probability at least \\(1-\\delta\\), for any \\(t \\in [T]\\),\n\\[\n\\|\\theta_t - \\theta^*\\|_{\\Sigma_{t,m}} = \\frac{1}{\\sqrt{m}} \\|\\theta_t - \\theta^*\\|_{\\Sigma_D} \\le C\\sqrt{\\frac{d+\\log(T/\\delta)}{\\gamma^2 m + \\lambda B^2}}. \\tag{10}\n\\]\n\nLet\n\\[\n\\tilde{\\Sigma}_t = \\lambda I + \\sum_{i=1}^{t-1} \\mathbb{E}_{x\\sim d_0,\\, a_1\\sim\\pi_1^i,\\, a_2\\sim\\pi_2^i}\\Big[(\\phi(x,a_1)-\\phi(x,a_2))(\\phi(x,a_1)-\\phi(x,a_2))^\\top\\Big].\n\\]\n\nNow, by the elliptical potential lemma (Lemma 9), we have\n\\[\n\\sum_{t=1}^{T} \\log\\Big(1+\\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^t)-\\phi(x,\\pi_2^t)\\|^2_{\\tilde{\\Sigma}_t^{-1}}\\Big) \\le \\log\\frac{\\det(\\tilde{\\Sigma}_T)}{\\det(\\lambda I)} \\le d\\log\\Big(1+\\frac{TL^2}{\\lambda d}\\Big) := \\gamma_T(\\lambda).\n\\]\n\nSince each term on the left-hand side is positive, we know that there exists at least a \\(t_0 \\in [T]\\) such that\n\\[\n\\log\\Big(1+\\psi_{t_0}^2\\Big) \\le \\frac{1}{T}\\gamma_T(\\lambda),\n\\]\nwhere we use the shorthand notation \\(\\psi_t = \\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^t)-\\phi(x,\\pi_2^t)\\|_{\\tilde{\\Sigma}_t^{-1}}\\). It is equivalent to\n\\[\n\\psi_{t_0}^2 \\le \\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1.\n\\]\n\nWe now consider the suboptimality at iteration \\(t_0\\):\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0}) = \\mathbb{E}_{x\\sim d_0}\\Big[\\langle\\theta_{t_0}-\\theta^*,\\, \\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\rangle\\Big] \\;\\; - \\;\\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\tag{11}\n\\]\nwhere the inequality uses the Cauchy-Schwarz inequality (Lemma 7). Then, since the samples \\(\\{x_{t,i}\\}_{i=1}^m\\) are i.i.d. and for any \\(x \\in X\\)\n\\[\n\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} \\le 2\\sqrt{\\lambda},\n\\]\nwe can use Chernoff's bound (Theorem 2.16 of Zhang (2023)) to obtain that with probability at least \\(1-\\delta/2\\),\n\\[\n\\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} \\le \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}}.\n\\]\nSimilarly, we also get with probability at least \\(1-\\delta/2\\),\n\\[\n\\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} \\le \\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}}.\n\\]\n\nWe take the two inequalities above back into Equation (11) to derive with probability at least \\(1-3\\delta\\),\n\\[\n\\begin{aligned}\nJ(\\pi^*)-J(\\pi_1^{t_0}) \\le & \\Bigg( \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\sqrt{3}\\;\\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi_2^{t_0})\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\sqrt{\\mathbb{E}_{x\\sim d_0}\\Big[\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}}^2\\Big] + 2\\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & C\\Big(\\sqrt{\\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1 + 2\\sqrt{\\frac{\\log(2/\\delta)}{2m}}\\Big) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big]\\Big), \\tag{12}\n\\end{aligned}\n\\]\nwhere the second inequality applies Lemma 10 with \\(\\lambda = \\Omega\\Big(\\frac{d\\log(T/\\delta)}{m}\\Big)\\), and the last inequality uses Equation (10). By choosing \\(T\\) satisfying that \\(T \\ge d\\log(T)\\) and \\(\\lambda = \\Theta\\Big(\\frac{d\\log(T/\\delta)}{m\\gamma^2}\\Big)\\), we have\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0}) = \\tilde{O}\\Big(\\sqrt{\\frac{d}{\\gamma^2 m}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big]\\Big),\n\\]\nwhich concludes the proof."}
{"paper":"arxiv:2312.11456", "question":"Given a preference dataset \\(D_{off}\\) consisting of tuples \\((x, a_w, a_l)\\) where for each prompt \\(x\\) the action \\(a_w\\) is preferred over \\(a_l\\), a reference policy \\(\\pi_0(a|x)\\) and a learned policy \\(\\pi_{\\theta}(a|x)\\), along with a positive scaling parameter \\(\\eta\\) and an uncertainty estimator \\(\\Gamma(x,a)\\) that quantifies the uncertainty of taking action \\(a\\) under context \\(x\\), Proposition 1 states that one can implement Option II of Algorithm 1 by minimizing the loss function\n\\[\nL_{D_{off}}(\\theta, \\pi_0) = \\sum_{(x, a_w, a_l) \\in D_{off}} \\log \\sigma \\Big( \\eta \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} + \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big)\\]\nwhere the term \\(\\Gamma(x,a_w) - \\Gamma(x,a_l)\\) serves as an adaptive margin based on the uncertainty difference between the two actions. Could you provide a complete derivation or proof of Proposition 1 that explains how this loss function is obtained?", "whole_label":"For notation simplicity, we denote the uncertainty bonus as \\(\\Gamma(x, a)\\). We first recall that in Algorithm 1, we optimize the following KL-regularized target:\n\n\\[\n\\hat{\\pi} = \\arg\\max_{\\pi} \\mathbb{E}_{x \\sim d_0, a \\sim \\pi(\\cdot|x)}\\Big[ r_{MLE}(x,a) - \\Gamma(x,a) - \\eta \\log \\frac{\\pi(a|x)}{\\pi_0(a|x)} \\Big]\n\\]  \\(21\\)\n\nwhere \\(r_{MLE}\\) is the MLE of the BT model on the offline preference dataset \\(D\\) obtained via\n\n\\[\nr_{MLE} = \\arg\\max_r \\sum_{(x,a_w,a_l) \\in D_{off}} \\log \\sigma \\Big( r(x,a_w) - r(x,a_l) \\Big)\n\\]  \\(22\\)\n\nAccording to Lemma 11, for any fixed \\(r\\), we have the following closed-form policy for Equation (21):\n\n\\[\n\\tilde{\\pi}_r(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\big( r(x,a) - \\Gamma(x,a) \\big) \\Big)\n\\]  \\(23\\)\n\nWe can solve the reward as\n\n\\[\nr(x,a) = \\Gamma(x,a) + \\eta \\log \\frac{\\tilde{\\pi}_r(a|x)}{\\pi_0(a|x)} + \\eta \\log Z(x)\n\\]  \\(24\\)\n\nWe can plug Equation (24) into Equation (22) to get\n\n\\[\n\\hat{\\pi} = \\arg\\max_{\\tilde{\\pi}_r} \\sum_{(x,a_w,a_l) \\in D_{off}} \\log \\sigma \\Big( \\eta \\log \\frac{\\pi_r(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_r(a_l|x)}{\\pi_0(a_l|x)} + \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big)\n\\]  \\(25\\)\n\nwhere the uncertainty serves as an adaptive margin.\n\nClearly, if \\(r\\) is the solution of Equation (22), then the \\(\\pi_r\\) is the solution of Equation (25). In contrast, if \\(\\pi\\) is optimal for the DPO target in Equation (25), then, the induced implicit reward \\(\\eta \\log \\frac{\\pi(y|x)}{\\pi_0(y|x)} - \\Gamma(x,a)\\) is optimal for Equation (22)."}
{"paper":"arxiv:2405.19107", "question":"Consider the following setting.\n\n  Let $x \\in \\mathcal{X}$ be a prompt sampled from a distribution $\\rho$, and let $y \\in \\mathcal{Y}$ be a completion sampled from an (unknown) behaviour policy $\\mu(\\cdot|x)$. A scalar reward $r(x,y) \\in \\mathbb{R}$ is observed for each pair $(x,y)$.\n\n  Let $\\pi(\\cdot|x)$ be a conditional policy to be learned, and let $\\pi_{\\mathrm{ref}}(\\cdot|x)$ be a fixed reference policy. Fix a temperature parameter $\\tau>0$.\n\n  For any pair $(\\pi,V)$ with $V:\\mathcal{X}\\to\\mathbb{R}$ define the loss\n\\[\nL(\\pi,V)=\\tfrac{1}{2}\\,\\mathbb{E}_{x\\sim\\rho,\\;y\\sim\\mu(\\cdot|x)}\\Bigl[\\,r(x,y)-V(x)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr]^2.\n\\]\n\n  For a fixed policy $\\pi$ define the minimising value function\n\\[\nV^{\\pi}(x)=\\operatorname*{argmin}_{V}L(\\pi,V)=\\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Bigl[r(x,y)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr].\n\\]\n\n  The optimal KL-regularised policy is\n\\[\n\\pi^{\\ast}(y\\mid x)=\\frac{\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\bigl(\\tfrac{1}{\\tau}\\,r(x,y)\\bigr)}{\\sum_{y'}\\pi_{\\mathrm{ref}}(y'\\mid x)\\exp\\bigl(\\tfrac{1}{\\tau}\\,r(x,y')\\bigr)}.\n\\]\n\nAssume that for every $x$ the support of $\\mu(\\cdot|x)$ coincides with that of $\\pi_{\\mathrm{ref}}(\\cdot|x)$.\n\n**Proposition 1.**\nLet $V:\\mathcal{X}\\to\\mathbb{R}$ be any candidate function (not necessarily equal to $V^{\\pi}$). Define\n\\[\\pi_{V}=\\operatorname*{argmin}_{\\pi}L(\\pi,V).\\]\nThen\n\n1. For all $x\\in\\mathcal{X}$ and $y\\in\\mathcal{Y}$,\n\\[\n\\pi_{V}(y\\mid x)\\;\\propto\\;\\pi_{\\mathrm{ref}}(y\\mid x)\\,\\exp\\Bigl\\{\\frac{1}{\\tau}\\Bigl(r(x,y)-\\frac{\\pi_{V}(y\\mid x)}{\\mu(y\\mid x)}\\bigl(V^{\\pi_{V}}(x)-V(x)\\bigr)\\Bigr)\\Bigr\\}.\n\\]\n\n2. Moreover, for every $x\\in\\mathcal{X}$ and $y\\in\\mathcal{Y}$,\n\\[\n\\left|\\log\\frac{\\pi_{V}(y\\mid x)}{\\pi^{\\ast}(y\\mid x)}\\right|\\le\\frac{2}{\\tau}\\,\\max_{y'}\\Bigl|\\bigl(V^{\\pi_{V}}(x)-V(x)\\bigr)\\bigl(1-\\tfrac{\\pi_{V}(y'\\mid x)}{\\mu(y'\\mid x)}\\bigr)\\Bigr|.\n\\]\n\n**Task** - Provide a complete and rigorous derivation or proof of Proposition 1 . Please include all intermediate steps and justifications.", "whole_label":"We proceed to prove this here. Consider the Lagrangian\n\n\\[G(\\pi)=L(\\pi,V)+\\sum_{x} \\rho(x)\\,\\lambda_{x}\\Big(\\sum_{y} \\pi(y|x)-1\\Big)\\]\n\nwhere \\(\\lambda_{x}\\) are the Lagrange multipliers corresponding to the constraint \\(\\sum_{y}\\pi(y|x)=1\\). Notice there is no need to impose such constraint outside of the support of \\(\\rho\\). The derivative of G(\\pi) with respect to a variable \\(\\pi(y|x)\\) is\n\n\\[\n\\partial_{\\pi(y|x)}G(\\pi)=\\mathbb{E}_{x'\\sim\\rho,\\,y'\\sim\\mu(\\cdot|x')}\\Big[(r(x',y')-\\tau\\log\\tfrac{\\pi(y'|x')}{\\pi_{\\mathrm{ref}}(y'|x')}-V(x'))\\,\\partial_{\\pi(y|x)}\\big(-\\tau\\log\\tfrac{\\pi(y'|x')}{\\pi_{\\mathrm{ref}}(y'|x')}\\big)\\Big]\n+\\partial_{\\pi(y|x)}\\Big(\\sum_{x'}\\rho(x')\\lambda_{x'}\\sum_{y'}\\pi(y'|x')\\Big)\n\\]\n\n\\[= -\\rho(x)\\Big[\\frac{\\mu(y|x)}{\\tau\\,\\pi(y|x)}\\bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\bigr)-\\lambda_{x}\\Big].\\]\n\nSetting the optimality conditions \\(\\partial_{\\pi(y|x)}G(\\pi)=0\\) for all x and y, we deduce that for any x in the support of \\(\\rho\\), we have, for any y,\n\n\\[\\lambda_{x}=\\tau\\,\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\sum_{y}\\pi(y|x)\\,\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\sum_{y}\\mu(y|x)\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\]\n\nThus\n\n\\[\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)=V^{\\pi}(x)-V(x),\\]\n\nor, equivalently,\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}=r(x,y)-V(x)-\\frac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\tag{11}\\]\n\nFrom this we deduce that the optimal policy satisfies\n\n\\[\\pi(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\,\\exp\\Bigl\\{\\tfrac{1}{\\tau}\\bigl[r(x,y)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr)-V(x)\\bigr]\\Bigr\\}\\tag{12}\\]\n\n\\[\\propto\\;\\pi_{\\mathrm{ref}}(y|x)\\,\\exp\\Bigl\\{\\tfrac{1}{\\tau}\\bigl[r(x,y)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr)\\bigr]\\Bigr\\},\\tag{13}\\]\n\nwhich proves the first claim (9).\n\nNow from the property of the optimal policy \\(\\pi^{\\ast}\\), and from Equation (11), we deduce that\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=V^{\\ast}(x)-V(x)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\tag{14}\\]\n\nTaking the expectation with respect to \\(\\mu(\\cdot|x)\\) on both sides, we get\n\n\\[\\tau\\sum_{y}\\mu(y|x)\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=V^{\\ast}(x)-V^{\\pi}(x).\\]\n\nPlugging \\(V^{\\ast}(x)\\) back into Equation (14) we have for any x,y,\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=\\tau\\sum_{y'}\\mu(y'|x)\\log\\tfrac{\\pi(y'|x)}{\\pi^{\\ast}(y'|x)}+(V^{\\pi}(x)-V(x))\\Bigl(1-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\Bigr).\\]\n\nLet us write \\(\\varepsilon(x)=\\tfrac{1}{\\tau}\\max_{y}\\bigl|(V^{\\pi}(x)-V(x))\\bigl(1-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigr)\\bigr|\\) and \\(\\rho(y|x)=\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}\\). We have that\n\n\\[1=\\sum_{y}\\pi(y|x)=\\sum_{y}\\pi^{\\ast}(y|x)\\,e^{\\rho(y|x)}\\le\\sum_{y}\\pi^{\\ast}(y|x)\\,e^{\\sum_{y'}\\mu(y'|x)\\rho(y'|x)+\\varepsilon(x)}=e^{\\sum_{y'}\\mu(y'|x)\\rho(y'|x)+\\varepsilon(x)},\\]\n\nthus \\(\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\ge-\\varepsilon(x)\\). Similarly we can upper-bound \\(\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\) by \\(\\varepsilon(x)\\). We deduce that for all y\n\n\\[|\\rho(y|x)|\\le\\Bigl|\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\Bigr|+\\varepsilon(x)\\le2\\varepsilon(x),\\]\n\nwhich concludes the proof of the second claim."}
{"paper":"arxiv:2405.19107", "question":"Consider the following definitions: Let \\(\\rho(x)\\) be a distribution over contexts \\(x\\), and let \\(\\mu(y\\mid x)\\) be a behavior policy generating responses \\(y\\). Let \\(r(x,y)\\) be a known reward function and \\(\\pi_{ref}(y\\mid x)\\) a reference policy. Fix a regularization scalar \\(\\tau>0\\). Define the loss\n\n\\[\nL(\\pi,V) = \\tfrac12 \\,\\mathbb{E}_{x\\sim\\rho,\\,y\\sim\\mu(\\cdot\\mid x)}\\Bigl[ r(x,y) - V(x) - \\tau\\log\\frac{\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)} \\Bigr]^2.\n\\]\n\nLet\n\n\\[\n\\pi^*(y\\mid x) = \\frac{\\pi_{ref}(y\\mid x)\\exp\\bigl(\\tfrac1{\\tau}r(x,y)\\bigr)}{\\exp\\bigl(\\tfrac1{\\tau}V^*(x)\\bigr)},\n\\qquad\nV^*(x) = \\tau\\log\\mathbb{E}_{y'\\sim\\pi_{ref}(\\cdot\\mid x)}\\bigl[\\exp\\bigl(\\tfrac1{\\tau}r(x,y')\\bigr)\\bigr].\n\\]\n\nTheorem 1 states that \\((\\pi^*,V^*)\\) is a global optimum of \\(L(\\pi,V)\\), and assuming that for every \\(x\\) the supports of \\(\\mu(\\cdot\\mid x)\\) and \\(\\pi_{ref}(\\cdot\\mid x)\\) coincide, this optimum is unique. Given only these definitions and without referring to external material, how can one derive a complete proof of Theorem 1?", "whole_label":"From the definition of \\(\\pi^{*}\\) and \\(V^{*}\\), we have that \\(L(\\pi^{*},V^{*})=0\\). Now notice that \\(L(\\pi,V)\\) is non-negative since it is a sum of quadratic terms \\(t(x,y)^2\\), where \\(t(x,y)=r(x,y)-V(x)-\\tau\\log\\frac{\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}\\). Thus \\((\\pi^{*},V^{*})\\) is a global optimum of \\(L(\\pi,V)\\).\n\nNow let us prove that it is unique. Assume there is another global optimum \\((\\tilde\\pi,\\tilde V)\\) such that \\(L(\\tilde\\pi,\\tilde V)=0\\). This means that for all \\(x\\in\\mathrm{supp}(\\rho)\\) and \\(y\\in\\mathrm{supp}\\,\\mu(\\cdot\\mid x)\\), its \\(t(x,y)\\)-term is zero. Since the supports of \\(\\mu\\) and \\(\\pi_{ref}\\) coincide, we have for all \\(x\\in\\mathrm{supp}(\\rho)\\), \\(y\\in\\mathrm{supp}\\,\\pi_{ref}(\\cdot\\mid x)\\),\n\n\\[\nr(x,y)-\\tilde V(x)-\\tau\\log\\frac{\\tilde\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}=0,\n\\]\n\nfrom which we deduce\n\n\\[\n\\tilde\\pi(y\\mid x)=\\pi_{ref}(y\\mid x)e^{\\tfrac{1}{\\tau}r(x,y)}e^{-\\tfrac{1}{\\tau}\\tilde V(x)}.\n\\]\n\nBut since \\(\\tilde\\pi(\\cdot\\mid x)\\) is a probability distribution, we must have\n\n\\[\n\\tilde V(x)=\\tau\\log\\sum_{y}\\pi_{ref}(y\\mid x)e^{\\tfrac{1}{\\tau}r(x,y)},\n\\]\n\nhence \\(\\tilde V(x)=V^{*}(x)\\) and \\(\\tilde\\pi(y\\mid x)=\\pi^{*}(y\\mid x)\\) for all \\(x,y\\)."}
{"paper":"arxiv:2402.05749", "question":"Theorem 1 (Equivalence of optimal solutions) states:\n\nLet \\pi^*_\\theta be the global minimizer of the offline preference optimization loss\n\\[\nL_{off}(\\theta)=\\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\bigl[f\\bigl(\\beta\\bigl(\\log\\frac{\\pi_\\theta(y_w)}{\\pi_{ref}(y_w)}-\\log\\frac{\\pi_\\theta(y_l)}{\\pi_{ref}(y_l)}\\bigr)\\bigr)\\bigr],\n\\]\nand consider the regularized policy optimization objective\n\\[\nJ(\\pi_\\theta)=\\mathbb{E}_{y\\sim\\pi_\\theta}[r_\\phi(y)]-\\beta\\,D_{KL}(\\pi_\\theta\\|\\pi_{ref}),\n\\]\nwith\n\\[\nD_{KL}(\\pi\\|\\pi')=\\mathbb{E}_{y\\sim\\pi}\\bigl[\\log\\tfrac{\\pi(y)}{\\pi'(y)}\\bigr],\n\\]\nand the reward modeling loss\n\\[\nL_{rm}(\\phi)=\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[f(r_\\phi(y_w)-r_\\phi(y_l))].\n\\]\nHere, \\mu is a distribution over ordered pairs (y_w,y_l), f: \\mathbb{R}\\to\\mathbb{R} is a convex function with f'(0)<0, \\beta>0 is a scalar regularization coefficient, r_\\phi(y) is a pointwise reward function, \\pi_\\theta(y) is the parameterized policy distribution, and \\pi_{ref}(y) is a fixed reference policy. Theorem 1 claims that the policy \\pi^*_\\theta minimizing L_{off}(\\theta) coincides with the policy maximizing J(\\pi_\\theta) when using a reward model r_\\phi that minimizes L_{rm}(\\phi).\n\nCould you derive and prove this equivalence under the above definitions and assumptions?", "whole_label":"The proof is straightforward. When minimizing the reward modeling loss $L_{rm}(\\phi)$ in Reward modeling loss, we may reparameterize the learned reward as\n$$\nr_\\phi(y)=\\beta\\,\\log\\tfrac{\\pi_\\theta(y)}{\\pi_{ref}(y)}+z,\n$$\nwhere $z$ is an additive constant (depending on $\\theta$) that cancels under the difference and does not affect the resulting policy. Substituting into Reward modeling loss shows that its global minimizer $r_\\phi^*$ induces\n$$\n\\pi_\\theta(y)\\propto \\pi_{ref}(y)\\,\\exp\\bigl(\\beta^{-1}r_\\phi^*(y)\\bigr),\n$$\nwhich is exactly the form of the optimal regularized policy maximizing\n$$\nJ(\\pi_\\theta)=\\mathbb{E}_{y\\sim\\pi_\\theta}[r_\\phi^*(y)]-\\beta\\,D_{KL}(\\pi_\\theta\\|\\pi_{ref}).\n$$\nHence the policy minimizing $L_{off}(\\theta)$ in Offline preference optimization loss coincides with the one maximizing $J(\\pi_\\theta)$ under reward $r_\\phi^*$, establishing the claimed equivalence."}
{"paper":"arxiv:2310.12036", "question":"Let $Y$ be a finite set of actions.  \n\nLet $\\mu \\in \\Delta_Y$ be a fixed behaviour policy that we can sample from, and let $\\pi_{\\text{ref}}\\in \\Delta_Y$ be a fixed reference policy.  For any candidate policy $\\pi\\in\\Delta_Y$ define the log-ratio function\n\\[\nh_{\\pi}(y,y')\\;\\coloneqq\\;\\log\\frac{\\pi(y)\\,\\pi_{\\text{ref}}(y')}{\\pi(y')\\,\\pi_{\\text{ref}}(y)},\\qquad y,y'\\in Y.\n\\]\n\nAssume that, for every unordered pair $\\{y,y'\\}\\subset Y$, a human rater expresses a stochastic preference between $y$ and $y'$ that is captured by the probability\n\\[\np^{*}(y\\succ y' )\\;\\in\\;[0,1],\\qquad\\text{with }p^{*}(y\\succ y')+p^{*}(y'\\succ y)=1.\n\\]\nFor each ordered pair $(y,y')$ sampled i.i.d. from $\\mu\\otimes\\mu$, let the Bernoulli indicator\n\\[\nI(y,y')\\;\\sim\\;\\operatorname{Bernoulli}\\bigl(p^{*}(y\\succ y')\\bigr)\n\\]\nbe $1$ when the rater prefers $y$ to $y'$ and $0$ otherwise.\n\nFor any action $y\\in Y$ define its average preference against $\\mu$ as\n\\[\np^{*}(y\\succ \\mu)\\;\\coloneqq\\;\\mathbb{E}_{y'\\sim\\mu}\\bigl[p^{*}(y\\succ y')\\bigr].\n\\]\nFix a positive regularisation coefficient $\\tau>0$.  Consider the two squared-error objectives\n\\[\n\\begin{aligned}\n\\text{(13)}\\quad & L_1(\\pi)\\;=\\;\\mathbb{E}_{y,y'\\sim\\mu}\\Bigl[\\,h_{\\pi}(y,y')\\; -\\; \\tau^{-1}\\bigl(p^{*}(y\\succ \\mu)-p^{*}(y'\\succ \\mu)\\bigr)\\Bigr]^{2},\\\\[6pt]\n\\text{(16)}\\quad & L_2(\\pi)\\;=\\;\\mathbb{E}_{y,y'\\sim\\mu}\\Bigl[\\,h_{\\pi}(y,y')\\; -\\; \\tau^{-1} I(y,y')\\Bigr]^{2}.\n\\end{aligned}\n\\]\n\n**Proposition 3.**  *The two loss functions satisfy $L_1(\\pi)=L_2(\\pi)+C$ for every policy $\\pi\\in\\Delta_Y$, where the constant $C$ depends only on $\\mu$, $p^{*}$ and $\\tau$ but not on $\\pi$.*\n\nCould you provide a complete and rigorous derivation of Proposition 3, showing step by step why the additive-constant equality holds under the definitions given above?", "whole_label":"This equivalence is not completely trivial, since in general the conditional expectation\n\n$\\mathbb{E}\\bigl[\\,h_{\\pi}(Y,Y') - \\tau^{-1} I(Y,Y')\\mid Y=y,\\,Y'=y'\\bigr]$\n\nis not equal to the corresponding quantity appearing in Equation (13), namely\n\n$h_{\\pi}(y,y') - \\tau^{-1}\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr).$\n\nWe instead need to exploit some symmetry between the distributions of $y$ and $y'$, and use the fact that $h_{\\pi}(y,y')$ decomposes as an additive function of $y$ and $y'$. To show this equality of losses, it is enough to focus on the \"cross-terms\" obtained when expanding the quadratics in Equations (13) and (16); that is, to show\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\,I(y,y')\\bigr]\n=\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr)\\bigr].\n$$\n\nNow, starting with the right-hand side, and using the shorthand\n\n- $\\pi_y=\\log\\bigl(\\pi(y)\\bigr)$\n- $\\pi^R_y=\\log\\bigl(\\pi_{\\mathrm{ref}}(y)\\bigr)$\n- $p_y=p^*(y\\succ\\mu)$\n\n(and similarly for $y'$), we have\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr)\\bigr]\n=\\mathbb{E}_{y,y'\\sim\\mu}[(\\pi_y-\\pi_{y'}+\\pi^R_{y'}-\\pi^R_y)(p_y-p_{y'})]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[\\pi_y p_y - \\pi_y p_{y'} - \\pi_{y'} p_y + \\pi_{y'} p_{y'} + \\pi^R_{y'} p_y - \\pi^R_{y'} p_{y'} - \\pi^R_y p_y + \\pi^R_y p_{y'}]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[(2p_y-1)\\pi_y - (2p_y-1)\\pi^R_y],\n$$\n\nwhere we have used the i.i.d.-ness of $y,y'$ and $\\mathbb{E}_{y\\sim\\mu}[p_y]=\\tfrac12$.\n\nTurning to the left-hand side, we have\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}[h_{\\pi}(y,y')\\,I(y,y')]\n=\\mathbb{E}_{y,y'\\sim\\mu}[(\\pi_y-\\pi_{y'}+\\pi^R_{y'}-\\pi^R_y)\\,I(y,y')]\n$$\n\n$$\n=\\mathbb{E}_{y\\sim\\mu}[(\\pi_y-\\pi^R_y)\\,\\mathbb{E}_{y'\\sim\\mu}[I(y,y')\\mid y]]\n+\\mathbb{E}_{y'\\sim\\mu}[(-\\pi_{y'}+\\pi^R_{y'})\\,\\mathbb{E}_{y\\sim\\mu}[I(y,y')\\mid y']]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[(2p_y-1)\\pi_y - (2p_y-1)\\pi^R_y],\n$$\n\nwhere we used $\\mathbb{E}_{y'\\sim\\mu}[I(y,y')]=p_y$ and $\\mathbb{E}_{y\\sim\\mu}[I(y,y')]=1-p_{y'}$. This establishes the equality of the two losses, as required."}
{"paper":"arxiv:2403.03419", "question":"Could you please provide a complete, step-by-step derivation and proof of **Theorem 1** stated below?  All necessary symbols, functions, and assumptions are defined in the statement, so you may rely solely on the information given here.\n\n---  \n**Setting and notation**  \n   Let a prompt-response pair (x,y) be drawn from a dataset D.  \n   Let \\(\\pi_{\\theta}(y\\mid x)\\) denote the current large-language-model (LLM) policy parametrised by \\(\\theta\\).  \n   Two fixed reference policies are used: a \"less harmful\" one \\(\\pi_{r^{+}}(y\\mid x)\\) and a \"more harmful\" one \\(\\pi_{r^{-}}(y\\mid x)\\).  Their mixture is written \\(\\pi_{r}(y\\mid x)\\).  \n   A distribution \\(\\mu(y_l\\mid x)\\) is the empirical distribution of *dispreferred* (negative) responses \\(y_l\\).  \n   Let \\(K\\in\\mathbb{N}_{+}\\) be the number of self-sampled responses \\(\\{y_i\\}_{i=1}^{K}\\) drawn from \\(\\pi_{r}(\\,\\cdot\\mid x)\\).  \n   Hyper-parameters \\(\\alpha,\\beta>0\\).  \n   The sigmoid function is \\(\\sigma(z)=1/(1+e^{-z})\\).  \n   The Jeffrey divergence between two distributions p and q is \\(D_{J}[p\\Vert q]=\\tfrac12\\bigl(D_{\\mathrm{KL}}[p\\Vert q]+D_{\\mathrm{KL}}[q\\Vert p]\\bigr).  \n\n---  \n**Loss function (Eq. (3))**  \n\\[\nL_{\\mathrm{D2O}}\\;=\\;-\\;\\mathbb{E}_{(x,y_l)\\sim \\mathcal{D}}\\Bigg[\\,\\log\\, \\sigma\\Bigg(\\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)}\\;\\; -\\;\\; \\alpha\\,\\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)}\\Bigg)\\Bigg]\\,,\\quad y_i\\sim\\pi_{r}(\\,\\cdot\\mid x).\n\\]\n\n---  \n**Distributional Bradley-Terry preference model**  \nFor two response distributions \\(\\pi_{\\theta}(\\,\\cdot\\mid x)\\) and \\(\\mu(\\,\\cdot\\mid x)\\), define the preference probability  \n\\[\np\\bigl(\\pi_{\\theta}(y\\mid x)\\succ \\mu(y\\mid x)\\bigr)\\;=\\;\\frac{\\exp\\bigl(r_{\\!*}(\\pi_{\\theta})\\bigr)}{\\exp\\bigl(r_{\\!*}(\\pi_{\\theta})\\bigr)+\\exp\\bigl(r_{\\!*}(\\mu)\\bigr)}\\,,\n\\]  \nwhere \\(r_{\\!*}(\\,\\cdot\\,)\\) is a distribution-level reward function to be learned.\n\n---  \n**Theorem 1**  \n*Optimising the loss \\(L_{\\mathrm{D2O}}\\) in Eq. (3) approximately learns the distributional Bradley-Terry preference model*  \n\\[ p\\bigl(\\pi_{\\theta}(y\\mid x)\\succ\\mu(y_l\\mid x)\\bigr)\\;, \\]  \n*which, when \\(\\alpha=\\beta\\), upper-bounds the instance-level preference model used in Direct Preference Optimisation (DPO), namely*  \n\\[ \\mathbb{E}_{y\\sim\\pi_{\\theta}(\\,\\cdot\\mid x)}\\,\\mathbb{E}_{y_l\\sim\\mu(\\,\\cdot\\mid x)}\\bigl[\\,p(y\\succ y_l)\\bigr]. \\]  \n*Moreover, when the two reference policies coincide \\(\\bigl(\\pi_{r^{-}}=\\pi_{r^{+}}=\\pi_{r}\\bigr),\\;L_{\\mathrm{D2O}}\\) implicitly applies a Jeffrey-divergence regularisation*  \n\\[ D_{J}\\bigl[\\pi_{\\theta}(y\\mid x)\\;\\Vert\\;\\pi_{r}(y\\mid x)\\bigr]\\,. \\]\n\n---  \n**Task**  \nPlease deliver a rigorous derivation and proof of Theorem 1, clearly explaining each mathematical step and any intermediate results that are required to reach the conclusion.", "whole_label":"Given a prompt $x$, we first define the reward of a response generated from $x$ as $r(x,y)$, and the reward of a distribution (policy) $\\pi(y\\mid x)$ as the expectation of reward over $\\pi(y\\mid x)$, $$r(\\pi(\\cdot\\mid x))=E_{\\pi(y\\mid x)}[r(x,y)].$$ Consider the general objective for preference optimization (Azar et al., 2023):\n\n$$\n\\arg\\max_{\\pi}\\;E_{x\\sim p(x)}\\Bigl\\{E_{y\\sim\\pi(y\\mid x),\\,y'\\sim\\mu(y\\mid x)}[\\Psi(p^*(y\\succ y'\\mid x))]-\\beta\\;\\mathrm{KL}[\\pi(y\\mid x)\\|\\pi_r(y\\mid x)]\\Bigr\\},\n$$\n\nwhere $\\Psi\\colon[0,1]\\to\\mathbb{R}$ is a non-decreasing function. For brevity, we omit $x$ in what follows.\n\nDifferent from the above, we consider two reference policies: $\\pi_r^{+}(y)$ (more helpful) and $\\pi_r^{-}(y)$ (more harmful). Then:\n\n$$\nE_{y\\sim\\pi,\\,y'\\sim\\pi_r^{-}}[\\Psi(p^*(y\\succ y'))]-\\beta\\;\\mathrm{KL}[\\pi(y)\\|\\pi_r^{-}(y)] = \\int\\pi(y)\\Bigl\\{E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]-\\beta\\log\\tfrac{\\pi(y)}{\\pi_r^{-}(y)}\\Bigr\\}\\,dy.\n$$\n\nBy algebraic steps, this equals\n\n$$\n-\\beta\\;\\mathrm{KL}\\Bigl[\\pi(y)\\Bigm\\|\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}\\Bigr]+\\log Z.\\tag{8}\n$$\n\nHence maximizing Eq.(8) is equivalent to minimizing\n\n$$\n\\mathrm{KL}\\Bigl[\\pi(y)\\Bigm\\|\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}\\Bigr],\\tag{9}\n$$\n\nand the optimal policy is\n\n$$\n\\pi^*(y)=\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}.\\tag{10}\n$$\n\nIf $\\Psi(p^*(y\\succ y'))$ is the Bradley--Terry model with $\\Psi(q)=\\log\\tfrac{q}{1-q}$, the ground-truth reward is\n\n$$\nr^*(y)=E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]=\\beta\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}+\\beta\\log Z,\n$$\n\nso\n\n$$\nr^*(\\pi)=E_{\\pi(y)}[r^*(y)]=\\beta\\,E_{\\pi(y)}\\Bigl[\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}\\Bigr]+C_1,\n$$\n\nwhere $C_1=E_{\\pi_r^{-}(y')}[r^*(y')]+\\beta\\log Z$ is constant. Similarly, using reverse KL regularization $\\mathrm{KL}[\\pi_r^{+}(y)\\|\\pi(y)]$ yields\n\n$$\nr^*(\\mu)=\\alpha\\,E_{\\mu(y')}\\Bigl[\\log\\tfrac{\\pi^*(y')}{\\pi_r^{+}(y')}\\Bigr]+C_2,\n$$\n\nleading to the Bradley--Terry model on distributions:\n\n$$\np^*(\\pi\\succ\\mu)=\\frac{\\exp(r^*(\\pi))}{\\exp(r^*(\\pi))+\\exp(r^*(\\mu))} = \\sigma\\bigl(\\beta\\,E_{\\pi(y)}[\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}]-\\alpha\\,E_{\\mu(y')}[\\log\\tfrac{\\pi^*(y')}{\\pi_r^{+}(y')}] + C_1 - C_2\\bigr).\\tag{11}\n$$\n\nFinally, aligning $\\pi_\\theta$ via\n\n$$\n\\theta^*=\\arg\\max_\\theta E_D[\\log p(\\pi\\succ\\mu)] = \\arg\\max_\\theta E_D\\bigl[\\log\\sigma(\\beta\\,E_{\\pi(y)}[\\log\\tfrac{\\pi_\\theta(y)}{\\pi_r^{-}(y)}]-\\alpha\\,E_{\\mu(y')}[\\log\\tfrac{\\pi_\\theta(y')}{\\pi_r^{+}(y')}] )\\bigr],\\tag{12}\n$$\n\nand approximating expectations recovers Eq.(7) when $M=1$. This concludes the proof."}
{"paper":"arxiv:2308.02585", "question":"How is the gradient of the upper-level objective $G(\\nu, \\theta^*(\\nu))$ derived with respect to the design parameter $\\nu$?\n\nAssume a bilevel optimization problem:\n\\begin{align*}\n   \\text{(upper)} \\hspace{5mm} &\\max_{\\nu} \\quad {G}(\\nu, \\theta^*(\\nu)) \\\\\n   \\text{(lower)} \\hspace{5mm} &\\text{s.t.} \\quad \\theta^*(\\nu) := \\argmax_{\\theta} \\mathbb{E}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid a_h \\sim \\pi_{\\theta}(a_h|s_h), s_0=s \\right]\n\\end{align*}\nwhere $\\nu \\in \\mathbb{R}^n$ are design parameters, $\\theta \\in \\mathbb{R}^d$ are policy parameters, $\\theta^*(\\nu)$ is the optimal lower-level policy parameter for a given $\\nu$.\nThe lower-level objective (value function) is $V_s(\\nu, \\theta) = \\mathbb{E}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid a_h \\sim \\pi_{\\theta}(\\cdot|s_h), s_0=s \\right]$.\nThe upper-level objective $G$ is defined as:\n\\[\n    G(\\nu, \\theta^*(\\nu)) = \\Upsilon(\\pi_{\\theta^*(\\nu)}) + Z(\\nu) \\tag{Eq. 3}\n\\]\nwhere $Z(\\nu)$ is a regularizer, and $\\Upsilon$ is defined as:\n\\[\n    \\Upsilon(\\pi_{\\theta^*(\\nu)}) = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau)] \\tag{Eq. 4}\n\\]\nrepresenting the utility $U_{\\nu}(\\tau)$ evaluated over trajectories $\\tau = \\{s_h, a_h\\}_{h=0}^{H_u-1}$ generated by the optimal policy $\\pi_{\\theta^*(\\nu)}$. The trajectory distribution is $\\rho(\\tau;\\theta) = \\rho(s_0)\\prod_{h=0}^{H_u-1} P(s_{h+1}\\mid s_h,a_h)\\pi_{\\theta}(a_h|s_h)$. Let $f_h(\\theta) = \\log \\pi_{\\theta}(a_h|s_h)$.", "whole_label":"We start by substituting the definitions of $G$ (Eq. 3) and $\\Upsilon$ (Eq. 4) into the gradient calculation:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\nabla_{\\nu} \\left( \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau)] + Z(\\nu) \\right)\n\\]\n\\[\n    = \\nabla_{\\nu} \\left( \\sum_{\\tau} U_{\\nu}(\\tau) \\cdot \\rho(\\tau;\\theta^*(\\nu)) \\right) + \\nabla_{\\nu} Z(\\nu)\n\\]\nApplying the product rule and the log-derivative trick $\\nabla_{\\nu} \\rho = \\rho \\nabla_{\\nu} \\log \\rho$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\sum_{\\tau} \\left( \\nabla_{\\nu} U_{\\nu}(\\tau) \\cdot \\rho(\\tau;\\theta^*(\\nu)) + U_{\\nu}(\\tau) \\cdot \\nabla_{\\nu} \\rho(\\tau;\\theta^*(\\nu)) \\right) + \\nabla_{\\nu} Z(\\nu)\n\\]\n\\[\n    = \\sum_{\\tau} \\nabla_{\\nu} U_{\\nu}(\\tau) \\rho(\\tau;\\theta^*(\\nu)) + \\sum_{\\tau} U_{\\nu}(\\tau) \\rho(\\tau;\\theta^*(\\nu)) \\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu)) + \\nabla_{\\nu} Z(\\nu)\n\\]\n\\[\n    = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau) \\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu))] + \\nabla_{\\nu} Z(\\nu)\n\\]\nUsing $\\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu)) = \\sum_{h=0}^{H_u-1} \\nabla_{\\nu} \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\Bigg[U_{\\nu}(\\tau)\\cdot \\sum_{h =0}^{H_u-1} \\nabla_{\\nu} \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)\\Bigg] + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\nabla_{\\nu} Z(\\nu)\n\\]\nLet $f_h(\\theta^*(\\nu)) = \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)$. We need $\\nabla_{\\nu} f_h(\\theta^*(\\nu))$. By the chain rule:\n\\[\n   \\nabla_{\\nu} f_h(\\theta^*(\\nu))=  (\\nabla_{\\nu}\\theta^*(\\nu))^T \\nabla_{\\theta} f_h(\\theta^*(\\nu))\n\\]\nFrom the lower-level optimality $\\nabla_{\\theta} V_s{( \\nu, \\theta^*(\\nu))} = 0$. Differentiating w.r.t. $\\nu$ using the chain rule gives the implicit function theorem result:\n\\[\n    \\nabla^2_{\\nu,\\theta} V_s{(\\nu,\\theta^*(\\nu))} +  (\\nabla_{\\nu} {\\theta^*(\\nu)})^T \\nabla^2_{\\theta} V_s{(\\nu,\\theta^*(\\nu))}  = 0\n\\]\nAssuming $\\nabla^2_{\\theta} V_s$ is invertible, we solve for $(\\nabla_{\\nu} \\theta^*(\\nu))^T$:\n\\[\n    (\\nabla_{\\nu} \\theta^*(\\nu))^T = - \\nabla^2_{\\nu,\\theta} V_s(\\nu,\\theta^*(\\nu)) (\\nabla^2_{\\theta} V_s(\\nu,\\theta^*(\\nu)))^{-1}\n\\]\nSubstituting this back into the expression for $\\nabla_{\\nu} f_h(\\theta^*(\\nu))$:\n\\[\n   \\nabla_{\\nu} f_h(\\theta^*(\\nu)) = -   \\nabla^2_{\\nu,\\theta} V_s(\\nu,\\theta^*(\\nu))(\\nabla^2_{\\theta} V_s(\\nu,\\theta^*(\\nu)))^{-1}\\nabla_{\\theta} f_h(\\theta^*(\\nu))\n\\]\nFinally, substitute this back into the expression for $\\nabla_{\\nu}G$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu))\n     =  \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\Bigg[U_{\\nu}(\\tau)\\cdot \\sum_{h =0}^{H_u-1} [-   \\nabla^2_{\\nu,\\theta} V_s(\\nabla^2_{\\theta} V_s)^{-1}\\nabla_{\\theta} f_h(\\theta^*(\\nu))]\\]\n\\]\n\\[\n     + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\nabla_{\\nu} Z(\\nu)\n\\]\nwhere Hessians/Jacobians are evaluated at $(\\nu, \\theta^*(\\nu))$."}
{"paper":"arxiv:2308.02585", "question":"How can the f-divergence $D_f$ between trajectory distributions $\\rho(\\tau ; \\theta^*(\\nu))$ and $\\rho(\\tau ; \\theta^K(\\nu))$, induced by the optimal policy $\\pi_{\\theta^*(\\nu)}$ and an approximate policy $\\pi_{\\theta^K(\\nu)}$ respectively, be bounded?\n\nDefine:\n\\begin{itemize}\n    \\item Trajectory $\\tau=\\{s_h,a_h\\}_{h=1}^{H}$.\n    \\item Trajectory distribution $\\rho(\\tau ; \\theta) = \\rho(s_0) \\prod_{h = 1}^{H} \\pi_{\\theta}(a_h|s_h) P(s_{h+1} |s_h, a_h)$.\n    \\item $\\theta^*(\\nu)$ is the optimal policy parameter, $\\theta^K(\\nu)$ is an approximate parameter. Let $\\theta^* = \\theta^*(\\nu)$ and $\\theta^K = \\theta^K(\\nu)$.\n\\end{itemize}\nAssume:\n\\begin{itemize}\n    \\item The policy $\\pi_{\\theta}$ is Lipschitz w.r.t. $\\theta$: $\\|\\pi_{\\theta_1}(\\cdot|s) - \\pi_{\\theta_2}(\\cdot|s)\\|_1 \\leq L_\\pi \\|\\theta_1 - \\theta_2\\|$ for some constant $L_\\pi$.\n    \\item The Hessian of the score function is Lipschitz: $\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta_1}(\\cdot|s) - \\nabla_{\\theta}^2 \\log \\pi_{\\theta_2}(\\cdot|s)\\| \\leq L_{2} \\|\\theta_1 - \\theta_2\\|$ for some constant $L_2$.\n    \\item $D_f$ is an f-divergence satisfying the triangle inequality, and $f$ is its generating function.\n\\end{itemize}\nShow that $D_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq \\frac{HL_{2}}{2} \\|\\theta^* - \\theta^K\\|$.", "whole_label":"The probability distribution of the trajectory $\\tau=\\{s_h,a_h\\}_{h=1}^H$ is given by\n\\begin{align}\n\t\\rho(\\tau ; \\theta^*(\\nu)) = \\rho(s_0) \\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h).\n\\end{align}\n%\nSimilarly, we can derive an equivalent expression for the probability of trajectory induced by the policy $\\pi_{\\theta^K(\\nu)}$ by replacing $\\theta^*(\\nu)$ with $\\theta^K(\\nu)$. Here, ${P}(s_{h+1} |s_h, a_h)$ is the transition probability which remains the same for both and $\\rho(s_0)$ is the initial state distribution.\n%\nNext, the f-divergence between the two distributions  $D_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu)))$ can be written as \n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) \\leq \\underbrace{D_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\beta))}_{I} + \\underbrace{D_f(\\rho(\\tau ; \\beta), \\rho(\\tau ; \\theta^K(\\nu)))}_{II},\n\\end{align}\n%\nwhich holds by triangle inequality of the class of f-divergences we considered as discussed above. $\\rho(\\tau ; \\beta)$ represents the trajectory probability induced by another hybrid policy $\\pi_{\\beta}(\\cdot|s)$ which executes the action based on the policy $\\pi_{\\theta^K(\\nu)}(\\cdot|s)$ for the first time-step and then follows the policy $\\pi_{\\theta^*(\\nu)}(\\cdot|s)$ for subsequent timesteps. Now, we focus on term I in (2), we get\n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; &\\theta^*(\\nu)), \\rho(\\tau ; \\beta)) \n\t\\nonumber\n\t\\\\\n\t&= \\sum_{\\tau}\\rho(\\tau ; \\beta)  f\\Bigg(\\frac{\\rho(\\tau ; \\theta^*(\\nu))}{\\rho(\\tau ; \\beta)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\beta))  f\\Bigg(\\frac{\\rho(s_0) \\pi_{\\theta^*(\\nu)}(a_0|s_0)P(s_1|s_0, a_0)\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\rho(s_0)\\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0) \\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\beta))f\\Bigg(\\frac{\\pi_{\\theta^*(\\nu)}(a_0|s_0)}{\\pi_{\\theta^K(\\nu)}(a_0|s_0)}\\Bigg),\n\\end{align}\n%\nwhere first we expand upon the definition of the trajectory distribution induced by both policies and get the final expression of the equation (3). By expanding the term $\\rho(\\tau ; \\beta)$ in (3), we obtain\n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\beta)) &= \\sum_{s_0}\\rho(s_0) \\sum_{a_0}\\pi_{\\theta^K(\\nu)}(a_0|s_0)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a_0|s_0)}{\\pi_{\\theta^K(\\nu)}(a_0|s_0)}\\Big)\\sum_{s_1} P(s_1|s_0, a_0) \\cdots \\nonumber\\\\\n\t& = \\sum_{s} \\rho(s) \\sum_{a}\\pi_{\\theta^K(\\nu)}(a|s)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a|s)}{\\pi_{\\theta^K(\\nu)}(a|s)}\\Big) \\nonumber\\\\\n\t& = \\mathbb{E}_{\\rho(s)}\\sum_{a}\\pi_{\\theta^K(\\nu)}(a|s)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a|s)}{\\pi_{\\theta^K(\\nu)}(a|s)}\\Big)\\nonumber\\\\\n\t& = \\mathbb{E}_{\\rho(s)} [D_f(\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s))],\n\\end{align}\n%\nwhere, in the first equation we expand upon the sum over all trajectories with the occupancy distribution over states and actions, and replacing with f-divergence, we get the final expression. \n\n\nNext, we expand similarly for the term II in (2) and expand as\n%\n\\begin{align} \n\tD_f&(\\rho(\\tau  ; \\beta), \\rho(\\tau ; \\theta^K(\\nu))) \n\t\\nonumber\n\t\\\\\n\t &= \\sum_{\\tau}\\rho(\\tau ; \\theta^K(\\nu)))  f\\Bigg(\\frac{\\rho(\\tau ; \\beta)}{\\rho(\\tau ; \\theta^K(\\nu))}\\Bigg)\\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\theta^K(\\nu))  f\\Bigg(\\frac{\\rho(s_0) \\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0)\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\rho_0(s_0)\\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0) \\prod_{h = 1}^{H} \\pi_{\\theta^K(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_0|s_0) \\sum_{s_1} P(s_1|s_0, a_0) \\cdots f\\Bigg(\\frac{\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\prod_{h = 1}^{H} \\pi_{\\theta^K(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg)\\\\ \\nonumber\n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) \\sum_{\\tau_1} \\rho(\\tau_1;\\theta^K(\\nu), \\frac{\\rho(\\tau_1;\\theta^*(\\nu)}{\\rho(\\tau_1;\\theta^K(\\nu))} \\nonumber \\\\ \n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) D_f(\\rho(\\tau_1;\\theta^*(\\nu), \\rho(\\tau_1;\\theta^K(\\nu)),\n\\end{align}\n%\nwhere we expand the trajectory distribution induced by the policies and subsequently express as the ratio of the probability of trajectories wrt $\\tau_1$, we get the final expression. Now, we expand upon the f-divergence of the trajectory $\\tau_1$ distribution as \n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\beta), \\rho(\\tau ; \\theta^K(\\nu))) =&  \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) D_f(\\rho(\\tau_1;\\theta^*(\\nu), \\rho(\\tau_1;\\theta^K(\\nu)) \\\\ \\nonumber\n\t%\n\t \\leq &  \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) \\Big(D_f(\\rho(\\tau_1;\\theta^*(\\nu)), \\rho(\\tau_1;\\beta)) \\\\ \\nonumber\n\t&+ D_f(\\rho(\\tau_1;\\beta), \\rho(\\tau_1;\\theta^K(\\nu)))\\Big),\n\\end{align}\n%\nwhere using the triangle inequality and get back the similar form with which we had started in equation (2) similar to term I and term II. Here, similarly continuing this expansion, we finally get \n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) & \\leq \\sum_{h = 0}^{H-1} \\mathbb{E}_{s \\sim \\rho_{\\theta^K(\\nu)}^P(s)} D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\\n\t& \\leq H \\mathbb{E}_{s \\sim \\rho_{\\theta^K(\\nu)}(s)} D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\  \n\t& \\leq H \\sum_{s} \\rho_{\\theta^K(\\nu)}(s) D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\\n\t& \\leq  H D_f (\\pi_{\\theta^*(\\nu)}(a'|s'), \\pi_{\\theta^K(\\nu)}(a'|s')),\n\\end{align}\n%\n where, we upper bound the first equation by the total number of timesteps or horizon length $H$ of the trajectory and subsequently upper-bound the divergence by the state $(s,a)$ pair for which the $D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s))$ is maximum and is given as $(s',a')$. Next, in (7), by considering the total variation as the f-divergence (it falls into the class of f-divergence under consideration) and expanding using definition with countable measures to obtain \n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) & \\leq  H D_{TV} (\\pi_{\\theta^*(\\nu)}(a'|s'), \\pi_{\\theta^K(\\nu)}(a'|s')) \\nonumber  \\\\\n\t& \\leq \\frac{H}{2} \\|\\pi_{\\theta^*(\\nu)}(a'|s')- \\pi_{\\theta^K(\\nu)}(a'|s')\\|_1 \\nonumber\\\\\n\t& \\leq \\frac{HL_{2}}{2} \\|\\theta^*(\\nu) - \\theta^K(\\nu)\\|, \n\\end{align}\n"}
{"paper":"arxiv:2308.02585", "question":"How is the Lipschitz continuity of the Hessian of the lower-level value function, $\\nabla^2_{\\theta} V_s(\\nu, \\theta)$, with respect to the policy parameter $\\theta$ established?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. ($H_\\ell$: horizon, $\\gamma$: discount factor).\n    \\item Hessian of the value function:\n    \\[\n        {\\nabla}_{\\theta}^2V_s(\\nu, \\theta) =  \\mathbb{E}_{\\rho(\\tau;\\theta)} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} r_{\\nu}(s_h,a_h)\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta}(a_j|s_j)\\right) \\Bigg]\n    \\]\n    \\item Let $\\theta^* = \\theta^*(\\nu)$ be the optimal parameter and $\\theta^K = \\theta^K(\\nu)$ be an approximate parameter.\n    \\item Define functions over trajectory $\\tau$ (Note: Using $\\gamma^{h-1}$ and sum up to $H_\\ell$ as in the proof text):\n    \\[\n        f_1(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1} r_{\\nu_t}(s_h,a_h)\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j|s_j)\\right)\n    \\]\n    \\[\n        f_2(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1} r_{\\nu_t}(s_h,a_h)\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j|s_j)\\right)\n    \\]\n    \\item Let $\\rho(\\tau; \\theta)$ be the trajectory distribution induced by policy $\\pi_\\theta$.\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Bounded Reward:} $\\|r_{\\nu_t}(s,a)\\| \\leq R$.\n    \\item \\textbf{Lipschitz Hessian of Score Function:} $\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta_1}(a|s) - \\nabla_{\\theta}^2 \\log \\pi_{\\theta_2}(a|s)\\| \\leq L_{2} \\|\\theta_1 - \\theta_2\\|$.\n    \\item \\textbf{Bounded Norm of $f_1$:} The norm of $f_1$ is bounded: $\\|f_1(\\cdot)\\| \\leq \\chi_1$, where the proof text specifies $\\chi_1 = H_\\ell^2 R L_1$ (assuming $L_1$ bounds $\\|\\nabla^2_\\theta \\log \\pi_\\theta\\|$).\n    \\item \\textbf{Total Variation Bound:} The Total Variation distance between trajectory distributions is bounded: $d_{TV}(\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\leq \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\|$.\n\\end{itemize}\n\n**Goal:** Show that $\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \\leq L' \\|\\theta^* - \\theta^K\\|$ with $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2$, following the derivation steps.\n", "whole_label":"We start by considering the term \n\t\\begin{align}\n\t\t{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K) = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau),\n\t\\end{align}\n\twhere we define \n\t\\begin{align}\n\t\tf_1(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j|s_j)\\right)\n\t\t\\\\\n\t\tf_2(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j|s_j)\\right).\n\t\\end{align}\n\tSubsequently, we write the norm of the equation above into 2 parts as\n\t\\begin{align}\n\t\t&\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \n\t\t\\nonumber\\\\\n\t\t & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)  + \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)}_{T_1}\\| + \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)}_{T_2}\\|,\n\t\\end{align}\nFirst we use triangle inequality and then add and subtract $\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)$ to get the expression where, $T_1 = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)$ and $T_2 = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)$. {This is an interesting point of departure from existing Bilevel analysis where the stochasticity is mainly iid noise. Hence to deal with this, we separate the terms 1. divergence b/w two probability distributions and 2. expected diff b/w two functions under the same distribution.} We next, upper-bound the individual terms to get the final Lispchitz constant.\n\t\n\tFirst, we focus on the first term of inequality i.e $T_1$ given as\n\t\\begin{align} \n\t\t\\|T_1\\| & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\sup_{f_1} |\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)| \\nonumber\\\\\n\t\t& \\leq \\chi_1 d_{TV} (\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\nonumber\\\\\n\t\t& \\leq \\chi_1 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\n\twhere we first upper-bound the expression to a standard Integral Probability Metric form by taking the supremum and then dividing and multiplying with the norm of the function given by $\\chi_1 =\\|f_1(\\cdot)\\| = H_\\ell^2 R L_1$. With that, we get the expression as the of Total variation distance. Then, we upper-bounded the total variation using the assumed bound.\n        \n\tNow, we proceed to the second term of the equation $T_2$ and derive an upper bound as \n\t\\begin{align} \n\t\tT_2 & = \\sum_{\\tau} \\rho(\\tau;\\theta^K(\\nu)) (f_1(\\tau) - f_2(\\tau)) \\nonumber \\\\\n\t\t& \\leq f_1(\\tau') - f_2(\\tau') \\nonumber\\\\\n\t\t& = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\n\t\\end{align}\n\twhere we consider the trajectory $\\tau'$ in the sum with the maximum value and upper bound by that and expand the definition of $f_1, f_2$. \n\t\\begin{align} \n\t\t\\|T_2\\| & \\leq \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\\| \\nonumber \\\\\n\t\t& \\leq \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\\| \\nonumber\\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|r_{\\nu_t}(s_h', a_h')\\| \\left(\\sum_{j=0}^{h}(\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j')\\|)\\right) \\nonumber\\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|r_{\\nu_t}(s_h', a_h')\\| H_{\\ell} L_{2} \\|\\theta^* - \\theta^K\\| \\nonumber\\\\\n\t\t& = L_{2} R H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \n\t\\end{align}\n\twhere we use Cauchy-Schwartz and triangle inequality repetitively to get to the third inequality. Next, we use the assumption on the Hessian Lipschitzness of the score function ($L_2$) and the bounded reward norm $\\|r_{\\nu_t}(s,a)\\| = R$ to get the next inequality. Finally, we use the upper bound on the geometric series to obtain the final expression. \n\tAdding the bounds for $\\|T_1\\|$ and $\\|T_2\\|$, we get\n\t\\begin{align}\n\t\t\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| & \\leq \\|T_1\\| + \\|T_2\\| \\\\ \n        & \\leq \\chi_1 \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\| + L_{2} R H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \\\\ \n        &= \\left( L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2 \\right) \\|\\theta^* - \\theta^K\\| = L' \\|\\theta^* - \\theta^K\\|\n\t\\end{align}\n\twhere $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2$ (using $\\chi_1 = H_\\ell^2 R L_1$).\n"}
{"paper":"arxiv:2308.02585", "question":"How is the Lipschitz continuity of the second-order mixed Jacobian term ${\\nabla}^2_{\\nu, \\theta}V_s(\\nu, \\theta)$ with respect to the policy parameter $\\theta$ established?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. ($H_\\ell$: horizon, $\\gamma$: discount factor).\n    \\item Mixed Jacobian term:\n    \\[\n        {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta) = \\mathbb{E}_{\\rho(\\tau;\\theta)} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_j|s_j)]^T\\bigg) \\Bigg]\n    \\]\n    \\item Let $\\theta^* = \\theta^*(\\nu)$ be the optimal parameter and $\\theta^K = \\theta^K(\\nu)$ be an approximate parameter.\n    \\item Define functions over trajectory $\\tau$ (Note: Using $\\gamma^{h-1}$ and sum up to $H_\\ell$ as in the proof text, also $H_\\ell$ in inner sum limit for $f_3, f_4$):\n    \\[\n        f_3(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j|s_j)\\right)^T \n    \\]\n    \\[\n        f_4(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j|s_j)\\right)^T\n    \\]\n    \\item Let $\\rho(\\tau; \\theta)$ be the trajectory distribution induced by policy $\\pi_\\theta$.\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Lipschitz Reward Gradient:} $\\|\\nabla_{\\nu} r_{\\nu}(s,a)\\| \\leq L_r$.\n    \\item \\textbf{Lipschitz Score Function:} $\\|\\nabla_{\\theta} \\log \\pi_{\\theta_1}(a|s) - \\nabla_{\\theta} \\log \\pi_{\\theta_2}(a|s)\\| \\leq L_{1} \\|\\theta_1 - \\theta_2\\|$.\n    \\item \\textbf{Bounded Score Function:} $\\|\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s)\\| \\leq B$.\n    \\item \\textbf{Bounded Norm of $f_3$:} The norm of $f_3$ is bounded: $\\|f_3(\\cdot)\\| \\leq \\chi_3$, where the proof text specifies $\\chi_3 = H_\\ell^2 B L_r$.\n    \\item \\textbf{Total Variation Bound:} $d_{TV}(\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\leq \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\|$ (where $L_2$ arises from the Lipschitz Hessian of score).\n\\end{itemize}\n\n**Goal:** Show that $\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| \\leq L'' \\|\\theta^* - \\theta^K\\|$ with $L'' = L_2 L_r B \\frac{H_\\ell^3}{2} + L_{1} L_r H_{\\ell}^2$, following the derivation steps.\n", "whole_label":"We start by considering the term \n\t\\begin{align}\n\t\t{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K) \\leq \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)\n\t\\end{align}\n\twhere we define\n\t\\begin{align}\n\t\tf_3(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j|s_j)\\right)^T\n\t\t\\\\\n\t\tf_4(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j|s_j)\\right)^T .\n\t\\end{align}\n\tSubsequently, we write the norm of the difference term into 2 parts as\n\t\\begin{align}\n\t\t&\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| \\\\ \n        & \\leq \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)}_{T_3}\\| \n\t\t\\nonumber\\\\\n\t\t& + \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)}_{T_4}\\| \n\t\t\\nonumber\n\t\\end{align}\n\twhere, $T_3 = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)$ and $T_4 = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)$. We next, upper-bound the individual terms to get the final Lispchitz constant.\n\t\n\tFirst, we focus on the first term of inequality i.e $T_3$ given as\n\t\\begin{align} \n\t\t\\|T_3\\| & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\sup_{f_3} |\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)| \\nonumber\\\\\n\t\t& \\leq \\chi_3 d_{TV} (\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\nonumber\\\\\n\t\t& \\leq  \\chi_3 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\nwhere we convert the inequality first to a standard Integral Probability Metric form by taking the supremum. Then we relate it to the Total variation distance multiplied by the norm of the function $f_3$ given by $\\|f_{3}(\\cdot)\\| = \\chi_3 = H_\\ell^2 B L_r$. Then, we upper-bounded the total variation using the assumed bound.\n\t\n\tNow, we proceed to the second term of the equation $T_4$ and derive an upper bound as \n\t\\begin{align} \n\t\tT_4 & = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} [f_3(\\tau) - f_4(\\tau)] \\\\ \n        & = \\sum_{\\tau} \\rho(\\tau;\\theta^K(\\nu)) (f_3(\\tau) - f_4(\\tau)) \\nonumber\n\t\\end{align}\n\tThe proof text then bounds this term (potentially using a max argument implicitly) before taking the norm:\n\t\\begin{align} \n\t\t T_4 & \\leq f_3(\\tau') - f_4(\\tau') \\nonumber\\\\\n\t\t& = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}(\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)^T  \n\t\\end{align}\n\twhere we consider the trajectory $\\tau'$ with the maximum value. Next, we upper-bound the norm $\\|T_4\\|$ as \n\t\\begin{align} \n\t\t\\|T_4\\| & \\leq \\Bigg \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)^T\\Bigg\\| \\nonumber \n\t\t\\\\   \n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|\\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\| \\left(\\sum_{j=0}^{h}\\|\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\|\\right) \\nonumber \\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot L_r \\cdot H_{\\ell} L_{1} \\|\\theta^* - \\theta^K\\| \\nonumber \\\\\n\t\t& \\leq L_{1} L_r H_{\\ell}^2 \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\n\twhere we use Cauchy-Schwartz and triangle inequality repetitively to get to the third inequality. Next, we use the assumption on the Lipschitzness of the gradient of the score function ($L_1$) and the bounded reward gradient ($L_r$). Finally, we upper-bound sum of this geometric series to obtain the final expression. \n\tAdding the bounds for $\\|T_3\\|$ and $\\|T_4\\|$, we get \n\t\\begin{align}\n\t\t\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| & \\leq \\|T_3\\| + \\|T_4\\| \\\\ \n        & \\leq \\chi_3 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\| + L_{1} L_r H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \\\\ \n        & = \\left( (H_\\ell^2 B L_r) \\frac{H_{\\ell}L_{2}}{2} + L_{1} L_r H_{\\ell}^2 \\right) \\|\\theta^* - \\theta^K\\| = L'' \\|\\theta^* - \\theta^K\\|\n\t\\end{align}\n\twhere, $L'' =  L_2 L_r B \\frac{H_\\ell^3}{2} + L_{1} L_r H_{\\ell}^2$ (using $\\chi_3 = H_\\ell^2 B L_r$).\n"}
{"paper":"arxiv:2302.03655", "question":"Given a 3D unit vector $r$, let $R$ be a rotation matrix with $R\\cdot r=(0, 1, 0)$ . Show why \n$$\nY(R\\cdot r)\\propto \\delta_{m}^{(l)}=\\begin{cases}1& \\text{if }l= m,\\\\0 &\\text{else,}\\end{cases}\n$$\nwhere $Y$ here denotes the spherical harmonics.", "whole_label":"We know that, given the input vector $(x,y,z)$, each term of the spherical harmonics is a polynomial in $x,y,z$. Note that for $R\\cdot r$, $x=z=0$, so the value is non-zero only when the result cannot be used to factor $x,z$, that is, when $l=m$."}
{"paper":"arxiv:2302.03655", "question":"Show that the \"Point-wise spherical non-linearity\" is equivariant, with the fomula as follows: \n$$\nx\\mapsto\\int Y^{(l)}(r)\\cdot \\texttt{MLP}(x^\\top r)\\,\\mathrm{d}r.\n$$", "whole_label":"We know that, the inner product $x^\\top r$ is an invariant scalar, thus will not distroy the equivariance. And the integral traverses the entire sphere, so rotation of the action does not affect the equivariance."}
{"paper":"arxiv:2302.03655", "question":"Let $\\rho:G\\to \\mathrm{GL}(V)$ be a representation. Given graph $\\mathcal{G}$, $\\mathfrak{H}$ is the symmetirc group of $\\mathcal{G}$ ($\\mathfrak h\\cdot \\mathcal G=\\mathcal G,\\forall \\mathfrak h\\in\\mathfrak H$), show why $f(\\mathcal{G})$ will always a zero function if $I-\\rho^{(l)}(\\mathfrak H)$ is  non-singular.", "whole_label":"First, we have \n$$\nf(\\mathcal{G})=f(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h\\in H}}f(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h\\in H}}\\rho^{(l)}(\\mathfrak{h})f(\\mathcal{G})=\\rho^{(l)}(\\mathfrak{H})f(\\mathcal{G}).\n$$\nThen it is equivalent to \n$$\n(I-\\rho^{(l)}(\\mathfrak H))f(\\mathcal G)=0\n$$\nAccording to knowledge of linear algebra, $f(\\mathcal{G})$ will always a zero function if $I-\\rho^{(l)}(\\mathfrak H)$ is  non-singular."}
{"paper":"arxiv:2410.11443", "question":"Let $\\rho:G\\to \\mathrm{GL}(V)$ be a representation, $\\mathfrak H$ is a subgroup. Show why the representation of $\\rho(\\mathfrak H)=0$ is equivalent to $\\text{tr}(\\rho(\\mathfrak H))=0$.", "whole_label":"The sufficiency is obvious, and we will now prove the necessity. Note that the group average representation is an idempotent matrix, so all eigenvalues can only be 0 or 1. Note that the trace is 0, that is, all eigenvalues are 0. Therefore, the matrix must be an all-zero matrix."}
{"paper":"arxiv:2410.11443", "question":"Why replacing the integral in \"Point-wise spherical non-linearity\" with a discrete approximation causes the model to degenerate into qusai-equivariant? The fomula of \"Point-wise spherical non-linearity\" is shown as follows:\n$$\nx\\mapsto\\int Y^{(l)}(r)\\cdot \texttt{MLP}(x^\top r)\\,\\mathrm{d}r.\n$$", "whole_label":"For a spherical integral, the integral region after rotation is consistent with the original one. However, for the approximation of discrete grids, the rotated grid may not be approximate to the original grid, so the equivariance will be destroyed. When the grid is fine enough, the equivariant error generated will be reduced due to the continuity of the function, so the corresponding network is called qusai-equivariant."}
{"paper":"Convex Optimization", "question":"For a convex optimization problem:\n\n\\[\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x}) \\quad \\text{subject to} \\quad g_i(\\mathbf{x}) \\leq 0, \\quad h_j(\\mathbf{x}) = 0,\n\\]\n\nwhere \\( g_i(\\mathbf{x}) \\) are convex functions and \\( h_j(\\mathbf{x}) \\) are affine functions, Slater's condition states that if there exists a strictly feasible point \\( \\mathbf{x}_0 \\) such that:\n\n\\[\ng_i(\\mathbf{x}_0) < 0, \\quad \\forall i,\n\\]\n\nthen strong duality holds.\nProve that strong duality holds for the saddle point problem \n\\[\n\\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]", "whole_label":"Consider a convex optimization problem with a primal objective function \\( f(x) \\) subject to constraints:\n\\[\ng_i(x) \\leq 0, \\quad h_j(x) = 0.\n\\]\nThe corresponding augmented Lagrangian function is given by:\n\\[\n\\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = f(x) + \\sum_{i} \\lambda_i g_i(x) + \\sum_{j} \\mu_j h_j(x),\n\\]\nwhere \\( \\lambda \\geq 0 \\) are Lagrange multipliers for the inequality constraints, and \\( \\mu \\) are multipliers for the equality constraints.\n\nBy Slater's condition, strong duality holds if there exists a strictly feasible point \\( x_0 \\) such that:\n\\[\ng_i(x_0) < 0, \\quad \\forall i.\n\\]\nThis ensures that the duality gap is zero, leading to:\n\\[\n\\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]\n\nA point \\( (x^*, \\lambda^*, \\mu^*) \\) is a saddle point of \\( \\mathbb{L}_{\\rho} \\) if:\n\\[\n\\mathbb{L}_{\\rho}(x^*, \\lambda, \\mu) \\leq \\mathbb{L}_{\\rho}(x^*, \\lambda^*, \\mu^*) \\leq \\mathbb{L}_{\\rho}(x, \\lambda^*, \\mu^*),\n\\]\nfor all \\( x, \\lambda, \\mu \\). This implies:\n\\[\n\\min_x \\mathbb{L}_{\\rho}(x, \\lambda^*, \\mu^*) = \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x^*, \\lambda, \\mu).\n\\]\n\nSince Slater's condition ensures the existence of a saddle point, we conclude that strong duality holds:\n\\[\n\\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]"}
{"paper":"Convex Optimization", "question":"Let $X$ is a random variable defined on a probability space $(\\Omega, \\mathcal{F}, P)$ such that $\\mathbb{E}[X]$ exists. Suppose $f: \\mathbb{R} \\to \\mathbb{R}$ be a convex function:\n\\begin{equation*}\n    f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y).\n\\end{equation*}\nThen, Jensen's inequality states:\n\\begin{equation*}\n    f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n\\end{equation*}", "whole_label":"Since $f$ is convex, it satisfies the following property for any $a \\in \\mathbb{R}$:\n\\begin{equation*}\n    f(x) \\geq f(a) + g(x-a), \\quad \\forall x \\in \\mathbb{R},\n\\end{equation*}\nwhere $g$ is a subgradient of $f$ at $a$. \n\nIf $f$ is differentiable, we can take $g = f'(a)$, giving:\n\\begin{equation*}\n    f(x) \\geq f(a) + f'(a) (x - a), \\quad \\forall x \\in \\mathbb{R}.\n\\end{equation*}\n\nSetting $a = \\mathbb{E}[X]$ and taking expectations on both sides:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq \\mathbb{E}[f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) (X - \\mathbb{E}[X])].\n\\end{equation*}\n\nUsing linearity of expectation:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) \\mathbb{E}[X - \\mathbb{E}[X]].\n\\end{equation*}\n\nSince $\\mathbb{E}[X - \\mathbb{E}[X]] = 0$, the last term vanishes, yielding:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]).\n\\end{equation*}\n\nIf $f$ is not differentiable, we use the definition of subgradients: there exists a subgradient $g$ such that:\n\\begin{equation*}\n    f(x) \\geq f(a) + g(x - a), \\quad \\forall x.\n\\end{equation*}\n\nTaking expectations:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq \\mathbb{E}[f(\\mathbb{E}[X]) + g (X - \\mathbb{E}[X])].\n\\end{equation*}\n\nSince $\\mathbb{E}[X - \\mathbb{E}[X]] = 0$, we again obtain:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]).\n\\end{equation*}"}
{"paper":"Real Analysis: Modern Techniques and Their Applications", "question":"For $a, b \\geq 0$ and conjugate exponents $p, q > 1$ satisfying  \\[\\frac{1}{p} + \\frac{1}{q} = 1.\\] , \nYoung's inequality states:\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]", "whole_label":"Consider the function $\\Phi(t) = \\frac{t^p}{p}$ for $t \\geq 0$, which is convex. By the supporting line property of convex functions,\n\\[\n    \\Phi(b) \\geq \\Phi(a) + \\Phi'(a)(b - a).\n\\]\n\nSetting $\\Phi'(a) = a^{p-1}$, we obtain\n\\[\n    \\frac{b^p}{p} \\geq \\frac{a^p}{p} + a^{p-1} (b - a).\n\\]\n\nRearranging gives\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]"}
{"paper":"Convex Optimization", "question":"Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, and let $X$ be an integrable random variable, i.e., $\\mathbb{E}[|X|] < \\infty$. The conditional expectation of $X$ given a sub-$\\sigma$-algebra $\\mathcal{G} \\subseteq \\mathcal{F}$, denoted as $\\mathbb{E}[X | \\mathcal{G}]$, is a $\\mathcal{G}$-measurable function satisfying:\n\\[\n    \\int_A \\mathbb{E}[X | \\mathcal{G}] d\\mathbb{P} = \\int_A X d\\mathbb{P}, \\quad \\forall A \\in \\mathcal{G}.\n\\]\n\nThe Law of Total Expectation states that for a random variable $X$ and a random variable $Y$ (or a $\\sigma$-algebra $\\mathcal{F}$), then we have:\n\\[\n    \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]].\n\\]", "whole_label":"Let $X$ be a random variable, and let $Y$ be another random variable (or a $\\sigma$-algebra) on which we are conditioning. The conditional expectation $\\mathbb{E}[X | Y]$ is a random variable that depends on $Y$, and it satisfies:\n\\[\n    \\int_{-\\infty}^{\\infty} \\mathbb{E}[X | Y = y] f_Y(y) \\, dy = \\mathbb{E}[X],\n\\]\nwhere $f_Y(y)$ is the probability density function of $Y$.\n\nBy the definition of conditional expectation, we can rewrite $\\mathbb{E}[X]$ as:\n\\[\n    \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx.\n\\]\n\nWe can now apply the law of total probability, where we first condition on $Y$ and then take the expectation with respect to $Y$. The total expectation is given by:\n\\[\n    \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]],\n\\]"}
{"paper":"Real Analysis: Modern Techniques and Their Applications", "question":"For measurable functions $f$ and $g$ on a measure space $(X, \\mu)$ and for $p \\geq 1$, we have Young's inequality:\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\nthen Minkowski's inequality states holds:\n\\[\n    \\left( \\int_X |f(x) + g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]", "whole_label":"Applying Young's inequality to $|f(x)|$ and $|g(x)|$, we obtain:\n\\[\n    |f(x) + g(x)|^p \\leq (|f(x)| + |g(x)|)^p \\leq 2^{p-1} (|f(x)|^p + |g(x)|^p).\n\\]\nIntegrating both sides gives:\n\\[\n    \\int_X |f(x) + g(x)|^p d\\mu(x) \\leq 2^{p-1} \\left( \\int_X |f(x)|^p d\\mu(x) + \\int_X |g(x)|^p d\\mu(x) \\right).\n\\]\nTaking the $p$-th root completes the proof:\n\\[\n    \\left( \\int_X |f(x) + g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]"}
{"paper":"arxiv:2502.16894", "question":"In a parameter-efficient fine-tuning setting, a weight matrix $W\\in\\mathbb{R}^{m\\times n}$ is reparameterized as $W = W_0 + sBA$, where $W_0$ is fixed, $s\\in\\mathbb{R}$ is a scaling factor, and $B\\in\\mathbb{R}^{m\\times r}$, $A\\in\\mathbb{R}^{r\\times n}$ are low-rank matrices. At the $t$-th optimization step, define the equivalent weight as $\\tilde W_t = W_0 + sB_tA_t$, and denote the equivalent gradient as $\\tilde g_t = \\partial L/\\partial \\tilde W_t$. Let the full-tuning gradient at step $t$ be $g_t = \\partial L/\\partial W_t$. Show that\n\n$\\displaystyle \\tilde g_t = s^2\\bigl(B_t B_t^{\\top} g_t + g_t A_t^{\\top} A_t\\bigr).$\n\nHow can this lemma be derived and proved?", "whole_label":"According to the assumption, \\(\\tilde W_t = W_t\\). Let LoRA reparameterize \\(W\\) as \\(W = W_0 + sBA\\) where \\(B\\in\\mathbb R^{m\\times r}\\), \\(A\\in\\mathbb R^{r\\times n}\\), and \\(s\\in\\mathbb R\\). Denote the equivalent weight as\n\n\\[\\tilde W_t = W_{\\mathrm{init}} + s\\,B_t A_t.\\]\n\nThen the gradients of \\(B\\) and \\(A\\) are\n\n\\[G_{B_t} = \\frac{\\partial L}{\\partial \\tilde W_t}\\frac{\\partial \\tilde W_t}{\\partial B} = \\frac{\\partial L}{\\partial W_t}\\frac{\\partial \\tilde W_t}{\\partial B} = s\\,g_t A_t^{\\top},\\tag{30}\\]\n\\[G_{A_t} = \\frac{\\partial L}{\\partial \\tilde W_t}\\frac{\\partial \\tilde W_t}{\\partial A} = \\frac{\\partial L}{\\partial W_t}\\frac{\\partial \\tilde W_t}{\\partial A} = s\\,B_t^{\\top} g_t.\\tag{31}\\]\n\nUnder SGD updates,\n\n\\[dB_t = -\\eta\\,G_{B_t} = -s\\,\\eta\\,g_t A_t^{\\top},\\quad dA_t = -\\eta\\,G_{A_t} = -s\\,\\eta\\,B_t^{\\top} g_t.\\tag{32}\\]\n\nThe change in the equivalent weight is\n\n\\[d\\tilde W = \\frac{\\partial \\tilde W_t}{\\partial A_t}dA_t + \\frac{\\partial \\tilde W_t}{\\partial B_t}dB_t = s\\,B_t\\,dA_t + s\\,dB_t\\,A_t\\]\n\\[= s\\bigl[B_t(-\\eta s B_t^{\\top}g_t) + (-\\eta s g_t A_t^{\\top})A_t\\bigr] = -\\eta s^2\\bigl[B_tB_t^{\\top}g_t + g_tA_t^{\\top}A_t\\bigr].\\tag{36}\\]\n\nTherefore, the equivalent gradient is\n\n\\[\\tilde g_t = s^2\\bigl(B_t B_t^{\\top} g_t + g_t A_t^{\\top} A_t\\bigr).\\tag{37}\\]\n\nThis concludes the proof."}
{"paper":"arxiv:2502.16894", "question":"In Theorem 3.1, let the pretrained weight matrix $W_0\\in \\mathbb{R}^{m\\times n}$ be reparameterized using low-rank adapters $B\\in \\mathbb{R}^{m\\times r}$ and $A\\in \\mathbb{R}^{r\\times n}$ with a scalar scale factor $s$. At each iteration $t$, the equivalent weight is defined as\n\n$$\\tilde W_t = W_0 + s\\,B_t\\,A_t,$$\n\nand the equivalent gradient is defined as\n\n$$\\tilde g_t = \\frac{\\partial L}{\\partial \\tilde W_t}.$$ \n\nThe full fine-tuning update rule is\n\n$$W_{t+1} = W_t - \\eta_{FT}\\,g_t,$$\n\nand the LoRA update rule is\n\n$$\\tilde W_{t+1} = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t.$$ \n\nTheorem 3.1 claims that if initially $\\tilde W_0 = W_0$ and for every $t$ the scaled gradients satisfy\n\n$$\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t,$$\n\nthen it follows that $\\tilde W_t = W_t$ for all iterations $t$. How can one derive a formal proof of this theorem?", "whole_label":"We verify the claimed alignment by induction on the iteration $t$.  \n\n- The full fine-tuning update is  \n  \\[W_{t+1} = W_t - \\eta_{FT}\\,g_t,\\]  \n  and the LoRA update is  \n  \\[\\tilde W_{t+1} = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t.\\]  \n\n- **Base case** ($t=0$).  By assumption, the models are initialized identically:  \\(\\tilde W_0 = W_0\\).  \n\n- **Inductive step.**  Suppose at iteration $t$ we have  \\(\\tilde W_t = W_t\\)  and  \\(\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t\\).  Then  \n  \\[\n  \\tilde W_{t+1} \n    = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t  \n    = W_t - \\eta_{FT}\\,g_t  \n    = W_{t+1}.  \n  \\]  \n\nBy induction, \\(\\tilde W_t = W_t\\) holds for all iterations $t$, proving that LoRA updates exactly match full fine-tuning under the stated conditions."}
{"paper":"arxiv:2502.16894", "question":"Consider a mixture-of-experts model with \\(E\\) experts. For each expert \\(i\\in\\{1,\\dots,E\\}\\), let:\n  1. \\(W_i^0 \\in \\mathbb{R}^{m\\times n}\\) be the pre-trained full-rank weight for expert \\(i\\).\n  2. \\(\\widetilde W_i^0 \\in \\mathbb{R}^{m\\times n}\\) be the equivalent initialized weight for expert \\(i\\) under the LoRA MoE framework.\n  3. \\(g_{i,t} = \\frac{\\partial L}{\\partial W_i}\\bigl|_{t}\\) be the gradient of expert \\(i\\) at step \\(t\\) in full-rank fine-tuning.\n  4. \\(\\widetilde g_{i,t} = \\frac{\\partial L}{\\partial \\widetilde W_i}\\bigl|_{t}\\) be the corresponding equivalent gradient of expert \\(i\\) in LoRA MoE.\n\nTheorem 3.2 states:\n\n  \"For every expert \\(i\\in\\{1,\\dots,E\\}\\), if at initialization\n  \\[\\widetilde W_i^0 = W_i^0\\]\n  and at each optimization step\n  \\[\\widetilde g_{i,t} = g_{i,t},\\]\n  then the LoRA MoE model behaves identically to an upcycled MoE with full-rank fine-tuning.\"\n\nPlease derive a complete proof of this theorem using only the definitions above, without referring to any other parts of the paper or hinting at the proof strategy.", "whole_label":"We aim to show that under the given conditions, the LoRA MoE aligns with Full FT MoE by effectively making the MoE routers behave identically in both models.\n\nBase Case (t = 0): At initialization, by assumption, the equivalent weights of each expert satisfy \\(\\widetilde W_i^0 = W_i^0\\) and Full FT MoE upcycles all expert weights from the same pre-trained weight. Moreover, since both models use the same random seed, the routers are initialized identically, ensuring identical routing decisions in both Full FT MoE and LoRA MoE at t=0.\n\nInductive Step: Assume that at step t, \\(\\widetilde W_{i,t} = W_{i,t}\\) for all experts i, and the routers in both models are identical. During the t-th optimization step, the gradients satisfy \\(\\widetilde g_{i,t} = g_{i,t}\\) and the learning rates are chosen so that the weight updates coincide:\n\n\\[\n\\widetilde W_{i,t+1} = \\widetilde W_{i,t} - \\eta_{\\mathrm{LoRA}}\\,\\widetilde g_{i,t} \\approx W_{i,t} - \\eta_{\\mathrm{FullFT}}\\,g_{i,t} = W_{i,t+1}.\n\\]\n\nSince the routers remain identical and assign the same weights \\(w_i(x)\\) to each expert, the layer outputs satisfy:\n\n\\[\n\\mathrm{MoE}(x)\n= \\sum_{i=1}^E w_i(x)\\,W_{i}(x)\n= \\sum_{i=1}^E w_i(x)\\,\\widetilde W_{i}(x)\n= \\sum_{i=1}^E w_i(x)\\bigl(W + sB_iA_i\\bigr)(x)\n= W(x) + \\sum_{i=1}^E w_i(x)\\,sB_iA_i(x)\n= \\mathrm{MoE}_{\\mathrm{LoRA}}(x).\n\\]\n\nBecause the routers are updated based on these outputs, they remain identical at step t+1. By induction, the routers---and hence routing decisions and weight updates---for all steps are the same in Full FT MoE and LoRA MoE. Therefore, under the stated conditions, LoRA MoE is behaviorally equivalent to full-rank fine-tuning of the MoE model."}
{"paper":"arxiv:2502.16894", "question":"Consider a mixture-of-experts (MoE) model with E experts. For an input x, let z_i(x) for i=1,\\ldots,E be independent and identically distributed random variables. Let \\Omega_k(x) \\subset \\{1,\\ldots,E\\} denote the set of indices of the top-k values among \\{z_1(x),\\ldots,z_E(x)\\}. Define the expert weight w_i(x) = \\exp(z_i(x)) / \\sum_{j\\in\\Omega_k(x)}\\exp(z_j(x)) if i\\in\\Omega_k(x), and w_i(x)=0 otherwise. Assuming k\\le E and the z_i(x) are i.i.d., how can one derive E_x[w_i(x)] = \\frac{1}{E} and Var_x[w_i(x)] = \\frac{E-k}{k E^2} as stated in Lemma 3.3? Provide a self-contained proof under these definitions and assumptions.", "whole_label":"Because the scores \\(z_1(x),...,z_E(x)\\) are i.i.d. random variables, any permutation of the indices leaves their joint distribution unchanged, and the Top-k operation is symmetric under permutation. Moreover, by definition, \\(\\sum_{i=1}^E w_i(x)=1\\). Taking expectation:\n\n\\[\\sum_{i=1}^E \\mathbb{E}_x[w_i(x)] = \\mathbb{E}_x\\bigl[\\sum_{i=1}^E w_i(x)\\bigr] = 1,\\]\n\nso by symmetry,\n\n\\[\\mathbb{E}_x[w_i(x)] = \\frac1E,\\quad\\forall i.\\]\n\nTo compute the variance, observe\n\n\\[\\mathrm{Var}_x(w_i(x)) = \\mathbb{E}_x[w_i(x)^2] - \\bigl(\\mathbb{E}_x[w_i(x)]\\bigr)^2 = \\mathbb{E}_x[w_i(x)^2] - \\frac1{E^2}.\\]\n\nWe use that\n\n\\[1 = \\bigl(\\sum_{i=1}^E w_i(x)\\bigr)^2 = \\sum_{i=1}^E w_i(x)^2 + 2\\sum_{i<j} w_i(x)w_j(x).\\]\n\nTaking expectation and using symmetry gives\n\n\\[1 = E\\,\\mathbb{E}_x[w_i(x)^2] + E(E-1)\\,\\mathbb{E}_x[w_i(x)w_j(x)],\\quad i\\neq j.\\]\n\nNext, since exactly k of the \\(w_i(x)\\) are nonzero and they sum to 1, by symmetry one shows\n\n\\[\\mathbb{E}_x[w_i(x)w_j(x)] = \\frac{k-1}{E(E-1)k}.\\]\n\nSubstituting into the previous relation yields\n\n\\[\\mathbb{E}_x[w_i(x)^2] = \\frac1{E k}.\\]\n\nHence\n\n\\[\\mathrm{Var}_x(w_i(x)) = \\frac1{E k} - \\frac1{E^2} = \\frac{E-k}{kE^2}.\\]\n\nThis completes the proof."}
{"paper":"arxiv:2501.18922", "question":"The search space of logical forms is denoted by $F$ with size $|F| = k^L$, and the set of high-quality solutions is $F_{\\mathrm{optimal}} \\subseteq F$. For any search method $X$ with candidate set $F_X \\subseteq F$, its coverage is defined as\n$$C_X = \\frac{|F_X \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}.$$ \nThe Tree-of-Thoughts (ToT) method explores the full space $F$ with time complexity\n$$T_{\\mathrm{ToT}} = O\\bigl(k^L\\bigr),$$\nwhile the MCTS-based method, using $N$ rollouts and expanding $\\omega k$ candidates per step, has time complexity\n$$T_{\\mathrm{MCTS}} = O\\bigl(N \\cdot \\omega k \\cdot L\\bigr).$$\n\nPlease state and derive the inequalities in Proposition 4.2 showing that the MCTS-based heuristic method satisfies\n$$C_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}, \\quad T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}},$$\nand under effective strategy guidance,\n$$C_{\\mathrm{MCTS}} \\approx 1, \\quad C_{\\mathrm{ToT}} = 1?", "whole_label":"We denote the search space of logical forms by $F$ with $|F| = k^L$, where $k$ is the number of choices per step and $L$ is the maximum search depth. Let $F_{\\mathrm{optimal}} \\subseteq F$ be the set of high-quality solutions whose scores exceed a threshold. For any search method $X$ that generates a candidate set $F_X \\subseteq F$, define its coverage rate as\n\n$$\nC_X = \\frac{|F_X \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}.\n$$\n\n**1. Comparison with Chain-of-Thought (CoT):**\nCoT follows a single path $h_l$, so its candidate set is $F_{\\mathrm{CoT}} = \\{h_l\\}$.  Hence\n\n$$\nC_{\\mathrm{CoT}} = \\begin{cases}\n\\frac{1}{|F_{\\mathrm{optimal}}|}, & h_l \\in F_{\\mathrm{optimal}},\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n\nMCTS performs $N$ rollouts, producing the candidate set\n\n$$\nF_{\\mathrm{MCTS}} = \\bigcup_{n=1}^N \\{h_l^{(n)}\\}.\n$$\n\nUnder effective strategy and reward guidance, most of these paths fall into $F_{\\mathrm{optimal}}$, so\n\n$$\nC_{\\mathrm{MCTS}} = \\frac{|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} \\gg C_{\\mathrm{CoT}}.\n$$\n\n**2. Comparison with Tree-of-Thoughts (ToT):**\nToT exhaustively explores all paths in $F$, so its candidate set is $F_{\\mathrm{ToT}} = F$ and\n\n$$\nC_{\\mathrm{ToT}} = \\frac{|F \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} = 1,\n$$\n\nbut this requires time complexity\n\n$$\nT_{\\mathrm{ToT}} = O\\bigl(k^L\\bigr).\n$$\n\nMCTS only expands a fraction of the tree: if each rollout expands $\\omega k$ candidates (with $0<\\omega<1$) over depth $L$, then\n\n$$\nT_{\\mathrm{MCTS}} = O\\bigl(N \\cdot \\omega k \\cdot L\\bigr) \\ll T_{\\mathrm{ToT}} \\quad\\text{whenever }N\\,\\omega k\\,L \\ll k^L.\n$$\n\nMoreover, if the strategy and reward models focus the search on high-potential nodes, then almost all MCTS paths satisfy $h_l^{(n)}\\in F_{\\mathrm{optimal}}$, giving\n\n$$\n|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}| \\approx |F_{\\mathrm{optimal}}| \\quad\\Longrightarrow\\quad C_{\\mathrm{MCTS}} \\approx 1 = C_{\\mathrm{ToT}}.\n$$\n\n**Summary of Proposition 4.2 inequalities:**\n\n$$\nC_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}, \\quad T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}, \\quad C_{\\mathrm{MCTS}} \\approx 1, \\quad C_{\\mathrm{ToT}} = 1.\n$$"}
{"paper":"arxiv:2501.18922", "question":"A discrete dynamic system is defined by the state update \\(h_{t+1} = f(h_t, G)\\) with \\(h_t \\in H\\) and environment feedback \\(G\\), and the transition probability is \\(P(h_{t+1} \\mid h_t, G)\\). A Lyapunov function is given by \\(V(h_t) = -\\log P(h_t \\mid h^*, G)\\), with increment \\(\\Delta V(h_t) = V(h_{t+1}) - V(h_t)\\). Under the agent's environment awareness it holds that \\(P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 1\\) and \\(0 < P(h_{t+1} \\mid h_t, G) < 1\\), whereas for end-to-end methods either \\(P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 0\\) or \\(P(h_{t+1} \\mid h_t, \\mathrm{Exp}(h_t, G)) = 0\\). Please state and derive the inequalities in Proposition 4.1 showing that under agent environment awareness \\(\\Delta V(h_t) < 0\\) and bounded, but under end-to-end methods \\(P(h_{t+1} \\mid h_t, G) = 0\\) leads to unbounded \\(\\Delta V(h_t)\\).", "whole_label":"For a dynamic system $\\dot{x}=f(x)$, where $x$ is the state and $f(x)$ is the nonlinear function describing state evolution. Define a non-negative scalar function $V(x)$ such that: $V(x)>0$ for all $x\\neq 0$ and $V(0)=0$ (positive definiteness). Its time derivative along the system trajectory satisfies $\\dot{V}(x)<0$ and is bounded (negative definiteness). Then $V(x)$ is called a Lyapunov function candidate, and the system (from Lyapunov's perspective) is asymptotically stable.\nFor a discrete dynamic system $x_{t+1}=f(x_t)$, if a Lyapunov function $V(x)$ can be defined, and the Lyapunov increment $\\Delta V(x_t)=V(x_{t+1})-V(x_t)$, then $V(x_t)>0$ and $\\Delta V(x_t)<0$ with boundedness implies asymptotic stability.\n\nOur KBQA-o1 system is modeled as a discrete dynamic system:\n$$h_{t+1}=f(h_t, G),$$\nwhere $h_t\\in H$ is the system state at step $t$; $G$ is environment feedback. The state transition probability is defined as $P(h_{t+1}\\mid h_t, G)$, which represents the probability of transitioning to the next state $h_{t+1}$, given the current state $h_t$ and the environment feedback $G$. The goal is for the system state $h_t$ to converge asymptotically to the target state $h^*$, satisfying $P(h^*\\mid h_t, G)=1$.\n\nWe define a positive-definite Lyapunov function $V(h_t)$ as:\n$$V(h_t)=-\\log P(h_t\\mid h^*, G),$$\nwhere $P(h_t\\mid h^*, G)$ is the posterior probability of state $h_t$ relative to the target state $h^*$; $V(h_t)$ quantifies the deviation of $h_t$ from $h^*$.\n\nIn KBQA-o1, our environment can provide all possible explorations under the prior state, and the correct exploration step is guaranteed to be included. Thus,\n$$P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)=1.$$ \nTherefore,\n$$P(h_{t+1}\\mid h_t,G)=P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr),$$\nand given a set of explorations containing the correct one, the probability of selecting it satisfies\n$$0<P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)<1.$$ \nConsequently,\n$$0<P(h_{t+1}\\mid h_t,G)<1.$$ \n\nSince $0<P(h_{t+1}\\mid h_t,G)<1$, we conclude\n$$\\Delta V(h_t)=V(h_{t+1})-V(h_t)=-\\log P(h_{t+1}\\mid h_t,G)<0$$\nand is bounded. Thus, the Lyapunov function change is strictly negative definite. Based on the Lyapunov Stability Second Theorem, $V(h_t)=-\\log P(h_t\\mid h^*,G)$ satisfies positive definiteness ($V(h_t)\\ge0$, with equality only when $h_t=h^*$) and negative definiteness ($\\Delta V(h_t)<0$), ensuring $V(h_t)$ strictly decreases. Hence, the agent system is asymptotically stable in the Lyapunov sense, and $h_t$ converges to $h^*$.\n\nHowever, for end-to-end methods, although both pre-retrieval and post-retrieval approaches can incorporate environmental information, pre-retrieval may yield an exploration space that does not include the target, giving\n$$P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)=0.$$ \nPost-retrieval may generate an incorrect logical form framework, yielding\n$$P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)=0.$$ \nIn such cases,\n$$P(h_{t+1}\\mid h_t,G)=P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)\\cdot P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)=0,$$\ncausing $\\Delta V(h_t)$ to be unbounded and leading to instability.\n\nTherefore, we conclude that the agent's awareness of the environment makes it more effective in generating optimal logical forms compared to end-to-end methods."}
{"paper":"arxiv:2309.06657", "question":"\n\nConsider a target probability distribution $\\pirpsi(y|x)$ related to a proposal distribution $\\pisft(y|x)$ and a reward function $r(x,y)$ by $\\pirpsi(y|x) \\propto \\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)$.\nA rejection sampling algorithm uses $\\pisft$ to propose candidates $y$. A candidate $y$ not already in the accepted set $D_x$ is accepted if $u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$, where $u \\sim U[0,1]$ and $M_{D_x} = \\max_{y' \\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}$.\n\nIt can be shown from the relationship between $\\pirpsi$, $\\pisft$, and $r(x,y)$ that:\n\\[\n\\frac{1}{M_{D_x}} = \\E_{y\\sim\\pisft(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta}\\cdot \\left(r(x,y)-\\max_{y'\\notin D_x} r(x,y')\\right)\\right)\\right] \\quad (*)\n\\]\n(This relationship is derived via steps analogous to Equations 11-14 in the source material).\n\nProve the following properties:\n\\begin{enumerate}\n    \\item The distribution of accepted samples is $\\pirpsi(y|x)$.\n    \\item The expected acceptance rate equals the right-hand side of equation (*).\n\\end{enumerate}\n\n", "whole_label":"\n\n\\textit{Proof.} Let the process of generation be the one described and the accepted sequence set be $D_x$ at the current step, we have\n\\begin{align*}\n\\sP(\\text{sample }y\\text{ and get accepted}|x) &= \\sP\\left(u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right)\\pisft(y|x) \\\\\n&= \\frac{1}{M_{D_x}}\\pirpsi(y|x),\n\\end{align*}\nwhere $M_{D_x} \\triangleq \\min \\{m\\mid m\\cdot\\pisft(y|x) \\geq \\pirpsi(y|x) \\text{ for all } y\\notin D_x\\}$.\n\n\\begin{align*}\n\\sP(y \\text{ get accepted}|x) &= \\sP\\left(u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right) \\\\\n&= \\E \\mathbfone\\left[u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] \\\\\n&= \\E_{\\pisft}\\left[\\E \\mathbfone\\left[u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\middle\\vert y\\right]\\right] \\\\\n&= \\E_{\\pisft}\\left[\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] \\\\\n&= \\frac{1}{M_{D_x}}\n\\end{align*}\n\n\\begin{align*}\n\\sP(y|\\text{$y$ is accepted}, x) &= \\frac{\\sP(\\text{sample }y\\text{ and get accepted}|x)}{\\sP(y\\text{ get accepted}|x)} = \\pirpsi(y|x).\n\\end{align*}\n\nBy relationship (*), established from the definitions (analogous to deriving Eq. 18 from Eq. 13/14 and 16 in the source material), we have the acceptance rate\n\\[\n\\frac{1}{M_{D_x}} = \\E_{y\\sim\\pisft(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta}\\cdot \\left(r(x,y)-\\max_{y'\\notin D_x} r(x,y')\\right)\\right)\\right]\n\\]\nSince the expected acceptance rate is $\\sP(y \\text{ get accepted}|x) = 1/M_{D_x}$, this confirms the second property. \\qed\n\n"}
{"paper":"arxiv:2309.06657", "question":"Consider a target probability distribution $\\pirpsi(y|x)$ which is related to a proposal distribution $\\pisft(y|x)$ and a reward function $r_\\psi(x,y)$ via the relationship:\n\\begin{equation}\n\\pirpsi(y|x) = \\frac{1}{\\Zpsi(x)}\\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\quad (1)\n\\end{equation}\nwhere $\\beta$ is a positive constant and $\\Zpsi(x)$ is the partition function defined as:\n\\[\n\\Zpsi(x) = \\sum_{y'} \\pisft(y'|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)\n\\]\n\nIn a rejection sampling scheme using $\\pisft$ to sample from $\\pirpsi$, the acceptance probability for a candidate $y$ depends on the ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$. Here, $D_x$ represents the set of samples already accepted for context $x$, and $M_{D_x}$ is the rejection sampling constant defined as the maximum value of the ratio $\\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}$ over all candidates $y'$ not yet accepted ($y' \\notin D_x$):\n\\[\nM_{D_x} \\triangleq \\max_{y'\\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}\n\\]\n\nDerive an expression for the acceptance ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$ purely in terms of the reward function $r_\\psi(x,y)$ and the constant $\\beta$.\n", "whole_label":"According to the given relationship (1), we have:\n\\[\n\\pirpsi(y|x) = \\frac{1}{\\Zpsi(x)}\\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)\n\\]\nwhere $\\Zpsi(x) = \\sum_{y'} \\pisft(y'|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)$.\n\nFrom (1), we can express the ratio of the target distribution to the proposal distribution:\n\\begin{equation}\n\\frac{\\pirpsi(y|x)}{\\pisft(y|x)} = \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\quad (2)\n\\end{equation}\n\nThe rejection sampling constant $M_{D_x}$ is defined as:\n\\[\nM_{D_x} \\triangleq \\max_{y'\\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}\n\\]\nSubstituting the expression (2) into the definition of $M_{D_x}$:\n\\begin{align}\nM_{D_x} &= \\max_{y'\\notin D_x} \\left[ \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right) \\right] \\nonumber \\\\\n&= \\frac{1}{\\Zpsi(x)}\\max_{y'\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)\\right] \\quad (3)\n\\end{align}\n\nNow, we derive the required acceptance ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$. We can rewrite this as $\\frac{1}{M_{D_x}} \\cdot \\frac{\\pirpsi(y|x)}{\\pisft(y|x)}$ and substitute using (2) and (3):\n\\begin{align*}\n\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)} &= \\frac{1}{\\frac{1}{\\Zpsi(x)}\\max_{y''\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y'')\\right)\\right]} \\cdot \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\\\\n&= \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\max_{y''\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y'')\\right)\\right]} \\\\\n&= \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\exp\\left(\\max_{y''\\notin D_x} \\frac{1}{\\beta}r_\\psi(x,y'')\\right)} \\quad (\\text{Since } \\max(\\exp(a)) = \\exp(\\max(a)) ) \\\\\n&= \\exp\\left( \\frac{1}{\\beta}r_\\psi(x,y) - \\max_{y''\\notin D_x} \\frac{1}{\\beta}r_\\psi(x,y'') \\right) \\\\\n&= \\exp\\left( \\frac{1}{\\beta} \\left( r_\\psi(x,y) - \\max_{y'\\notin D_x} r_\\psi(x,y') \\right) \\right)\n\\end{align*}\nThis provides the expression for the acceptance ratio purely in terms of the reward function $r_\\psi$ and the constant $\\beta$.\n"}
{"paper":"arxiv:2404.05868", "question":"Proposition 1 asserts that\n\n$$\nF(\\theta) = \\int_{\\mathcal{X}} k(x,\\theta)\\,\\mathrm{d}\\mu(x) \\le C\\,\\|\\theta\\|^2.\n$$\n\nIn this statement:\n- $\\mathcal{X}$ is a measurable space,\n- $k\\colon\\mathcal{X}\\times\\mathbb{R}^d\\to\\mathbb{R}$ is a continuous kernel,\n- $\\mu$ is a probability measure on $\\mathcal{X}$,\n- $C>0$ is a constant,\n- $\\|\\theta\\|$ denotes the Euclidean norm of $\\theta\\in\\mathbb{R}^d$,\n- and it is assumed that $k$ satisfies the Lipschitz condition\n  $$|k(x,\\theta_1)-k(x,\\theta_2)| \\le L\\,\\|\\theta_1-\\theta_2\\|$$\n  for all $x\\in\\mathcal{X}$ and some $L>0$.\n\nHow can one derive and prove Proposition 1?", "whole_label":"Adopt the shorthand\n\n$$R_i = \\log\\frac{\\pi_\\theta(y_i\\mid x_i)}{\\pi_{\\mathrm{ref}}(y_i\\mid x_i)}.\n$$\n\n1. **First part.** For any $i\\in[n_f]$, observe that\n\n$$\n\\lim_{\\beta\\to0}\\Bigl(-\\tfrac{2}{\\beta}\\log\\sigma(-\\beta R_i) - \\tfrac{2}{\\beta}\\log2\\Bigr)\n= \\lim_{\\beta\\to0}\\tfrac{2}{\\beta}\\log\\frac{1+e^{\\beta R_i}}{2}\n= R_i.\n$$\n\nAveraging over $i=1,\\dots,n_f$ and noting\n\n$$\n\\frac{1}{n_f}\\sum_{i=1}^{n_f}R_i\n= L_{\\mathrm{GA}}(\\theta) - \\mathbb{E}_{(x,y)\\sim D_{\\mathrm{FG}}}\\bigl[\\log\\pi_{\\mathrm{ref}}(y\\mid x)\\bigr]\n$$\n\nyields the first claimed limit. :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}\n\n2. **Second part.** By definition,\n\n$$\n\\nabla_\\theta L_{\\mathrm{NPO},\\beta}(\\theta)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\Bigl(-\\tfrac{2}{\\beta}\\nabla_\\theta\\log\\sigma(-\\beta R_i)\\Bigr)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\frac{2}{\\beta}\\nabla_\\theta\\log(1+e^{\\beta R_i})\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\frac{2e^{\\beta R_i}}{1+e^{\\beta R_i}}\\,\\nabla_\\theta R_i.\n$$\n\nTaking the limit as $\\beta\\to0$ gives\n\n$$\n\\lim_{\\beta\\to0}\\nabla_\\theta L_{\\mathrm{NPO},\\beta}(\\theta)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\nabla_\\theta R_i\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\nabla_\\theta\\log\\pi_\\theta(y_i\\mid x_i)\n= \\nabla_\\theta L_{\\mathrm{GA}}(\\theta).\n$$\n\nThis completes the proof of Proposition 1."}
{"paper":"arxiv:2404.05868", "question":"Lemma 3 states that\n\n$$\nC_{1}\\,\\eta_{0}\\,t \\;\\le\\; b^{(t)}_{GA,i} \\;\\le\\; C_{2}\\,\\eta_{0}\\,t,\n\\quad\\text{if }y_{i}=0,\\\\\n-\\,C_{2}\\,\\eta_{0}\\,t \\;\\le\\; b^{(t)}_{GA,i} \\;\\le\\; -\\,C_{1}\\,\\eta_{0}\\,t,\n\\quad\\text{if }y_{i}=1,\n$$\n\nfor all $i\\in[n_{f}]$ and $t\\ge1$, under the condition\n$$\n\\max_{i\\neq j}\\lvert\\gamma_{i,j}\\rvert \\le \\frac{C_{0}}{n_{f}}.\n$$\n\nHere:\n- $b^{(t)}_{GA,i}$ denotes the $i$-th component of the GA update vector at iteration $t$,\n- $\\eta_{0}$ is the normalized learning rate,\n- $C_{0},C_{1},C_{2}>0$ are constants depending on $(B_{\\theta},b_{x},B_{x})$,\n- $\\gamma_{i,j}=\\langle x_{i},x_{j}\\rangle$ are inner products of feature vectors $x_{i},x_{j}$,\n- $y_{i}\\in\\{0,1\\}$ is the binary label for sample $i$, and\n- $n_{f}$ is the total number of samples.\n\nHow can one derive and prove Lemma 3?", "whole_label":"nWe prove Lemma 3 by induction.\n\nCase 1: $t=1$\nWhen $t=1$, since $|c_{\\mathrm{init},i}|\\le\\|x_i\\|_2\\cdot\\|\\theta_{\\mathrm{init}}\\|_2\\le B_xB_\\theta$, it follows from the definition of $\\mathrm{pred}_i(\\cdot)$ that\n$$C_3\\le\\Delta^{(0)}_{GA,i}\\le1\\quad\\text{when }y_i=1,\\quad-1\\le\\Delta^{(0)}_{GA,i}\\le-C_4\\quad\\text{when }y_i=0,$$\nfor all $i\\in[n_f]$, for some constants $C_3,C_4\\in(0,1)$. Moreover, there exists $C_0>0$ such that\n$$\\sum_{j\\neq i}\\gamma_{i,j}\\Delta^{(0)}_{GA,j}\\le\\frac{\\gamma_{i,i}}2\\Delta^{(0)}_{GA,i}$$\nfor all $i$ when $\\max_{i\\neq j}|\\gamma_{i,j}|\\le C_0/n_f$. It follows that\n$$-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[-\\tfrac32\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|,-\\tfrac12\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|]=[-C_2\\eta_0,-C_1\\eta_0]\\text{ when }y_i=1,$$\n$$-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[\\tfrac12\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|,\\tfrac32\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|]=[C_1\\eta_0,C_2\\eta_0]\\text{ when }y_i=0.$$\nTherefore,\n$$b^{(1)}_{GA,i}=b^{(0)}_{GA,i}-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[-C_2\\eta_0,-C_1\\eta_0]\\text{ when }y_i=1,$$\n$$b^{(1)}_{GA,i}\\in[C_1\\eta_0,C_2\\eta_0]\\text{ when }y_i=0.$$\n\nCase 2: $t=K+1$\nSuppose for $t=1,\\dots,K$ we have\n$$b^{(t)}_{GA,i}\\in[-C_2\\eta_0t,-C_1\\eta_0t]\\text{ when }y_i=1,$$\n$$b^{(t)}_{GA,i}\\in[C_1\\eta_0t,C_2\\eta_0t]\\text{ when }y_i=0.$$\nBy monotonicity of $\\Delta^{(t)}_{GA,i}$, we also have\n$$C_3\\le\\Delta^{(K)}_{GA,i}\\le1\\quad\\text{when }y_i=1,\\quad-1\\le\\Delta^{(K)}_{GA,i}\\le-C_4\\quad\\text{when }y_i=0.$$\nThen\n$$b^{(K+1)}_{GA,i}=b^{(K)}_{GA,i}-\\eta_0\\gamma_i^\\top\\Delta^{(K)}_{GA}\\in[-C_2\\eta_0(K+1),-C_1\\eta_0(K+1)]\\text{ when }y_i=1,$$\n$$b^{(K+1)}_{GA,i}\\in[C_1\\eta_0(K+1),C_2\\eta_0(K+1)]\\text{ when }y_i=0.$$\nThis completes the induction and thus the proof of Lemma 3."}
