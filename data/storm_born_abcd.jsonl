{"paper": "arxiv:2305.18290", "question": "Given the following formula:\n\\begin{equation}\n\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\pi_{\\text{ref}}(y\\mid x)\\bigr],\n\\end{equation}\nprove that the optimal solution under the KL constraint for maximizing the reward is:\n\\begin{equation}\n    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n\\end{equation}\nwhere $Z(x) =\\sum_{y}\\pi_{\\text{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function.", "A": "To derive the optimal solution under the KL constraint for maximizing the reward, we start by considering the optimization problem:\n\\begin{equation}\n\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi}\\bigl[r(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi(y|x)||\\pi_{\\text{ref}}(y|x)\\bigr].\n\\end{equation}\nThis can be rewritten as:\n\\begin{align}\n\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[r(x, y) - \\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right].\n\\end{align}\nBy converting the maximization problem into a minimization problem, we obtain:\n\\begin{align}\n\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\frac{1}{\\beta}r(x, y)\\right].\n\\end{align}\nIntroducing the partition function $Z(x) = \\sum_{y}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$, we can express the objective as:\n\\begin{align}\n\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)} - \\log Z(x)\\right].\n\\end{align}\nDefining the optimal policy as:\n\\begin{equation}\n\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{2}{\\beta}r(x, y)\\right),\n\\end{equation}\nwhich is a valid probability distribution, we can further simplify the objective to:\n\\begin{align}\n\\min_{\\pi}\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)||\\pi^*(y|x)) - \\log Z(x)\\right].\n\\end{align}\nSince $Z(x)$ does not depend on $\\pi$, the minimum is achieved when the KL divergence is minimized. According to Gibbs' inequality, the KL divergence is minimized to 0 when $\\pi(y|x) = \\pi^*(y|x)$. Therefore, the optimal solution is:\n\\begin{equation}\n\\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{2}{\\beta}r(x, y)\\right).\n\\end{equation}\nThis completes the derivation, showing that the optimal policy under the given constraints is indeed the one specified.", "B": "To derive the optimal policy $\\pi_r(y \\mid x)$ that maximizes the given objective:\n\n\\[\n\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid\\mid \\pi_{\\text{ref}}(y\\mid x)\\bigr],\n\\]\n\nwe analyze the problem pointwise for each $x$ (since the expectation over $x$ is linear). For a fixed $x$, the objective simplifies to maximizing:\n\n\\[\n\\sum_{y} \\pi_{\\theta}(y \\mid x) r(x, y) - \\beta \\sum_{y} \\pi_{\\theta}(y \\mid x) \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)},\n\\]\n\nsubject to $\\sum_{y} \\pi_{\\theta}(y \\mid x) = 1$. We introduce the Lagrangian:\n\n\\[\n\\mathcal{L} = \\sum_{y} \\pi_{\\theta}(y \\mid x) r(x, y) - \\beta \\sum_{y} \\pi_{\\theta}(y \\mid x) \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + \\lambda \\left(1 - \\sum_{y} \\pi_{\\theta}(y \\mid x)\\right),\n\\]\n\nwhere $\\lambda$ is the Lagrange multiplier. Taking the derivative of $\\mathcal{L}$ with respect to $\\pi_{\\theta}(y \\mid x)$ and setting it to zero:\n\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi_{\\theta}(y \\mid x)} = r(x, y) - \\beta \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) - \\lambda = 0.\n\\]\n\nRearranging gives:\n\n\\[\n\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} = \\frac{r(x, y)}{\\beta} - \\frac{\\lambda}{\\beta} - 1.\n\\]\n\nExponentiating both sides:\n\n\\[\n\\pi_{\\theta}(y \\mid x) = \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta} - \\frac{\\lambda}{\\beta} - 1\\right).\n\\]\n\nThe term $\\exp(-\\frac{\\lambda}{\\beta} - 1)$ is a normalization constant. Let $Z(x) = \\sum_{y} \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right)$. Normalizing $\\pi_{\\theta}$ gives:\n\n\\[\n\\pi_{\\theta}(y \\mid x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right).\n\\]\n\nThus, the optimal policy under the KL constraint is:\n\n\\[\n\\pi_r(y \\mid x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y \\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n\\]\n\nwhere $Z(x)$ ensures $\\sum_y \\pi_r(y \\mid x) = 1$. This balances reward maximization with proximity to $\\pi_{\\text{ref}}$, controlled by $\\beta$.", "C": "To derive the optimal solution under the KL constraint for maximizing the reward, we start by considering the optimization problem:\n\\begin{equation}\n\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi}\\bigl[r(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi(y|x)||\\pi_{\\text{ref}}(y|x)\\bigr].\n\\end{equation}\nThis can be rewritten as:\n\\begin{align}\n\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[r(x, y) - \\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right].\n\\end{align}\nBy converting the maximization problem into a minimization problem, we obtain:\n\\begin{align}\n\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\frac{1}{\\beta}r(x, y)\\right].\n\\end{align}\nIntroducing the partition function $Z(x) = \\sum_{y}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$, we can express the objective as:\n\\begin{align}\n\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)} - \\log Z(x)\\right].\n\\end{align}\nDefining the optimal policy as:\n\\begin{equation}\n\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n\\end{equation}\nwhich is a valid probability distribution, we can further simplify the objective to:\n\\begin{align}\n\\min_{\\pi}\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)||\\pi^*(y|x)) - \\log Z(x)\\right].\n\\end{align}\nSince $Z(x)$ does not depend on $\\pi$, the minimum is achieved when the KL divergence is minimized. According to Gibbs' inequality, the KL divergence is minimized to 0 when $\\pi(y|x) = \\pi^*(y|x)$. Therefore, the optimal solution is:\n\\begin{equation}\n\\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right).\n\\end{equation}\nThis completes the derivation, showing that the optimal policy under the given constraints is indeed the one specified.", "D": "To derive the optimal policy \\(\\pi_r(y \\mid x)\\) that maximizes the given objective function under the KL constraint, we proceed with the following steps:\n\n### 1. **Objective Function**\nThe optimization problem is:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\pi_{\\text{ref}}(y\\mid x)\\bigr],\n\\]\nwhere:\n- \\(r_{\\phi}(x, y)\\) is the reward function.\n- \\(\\pi_{\\theta}(y \\mid x)\\) is the policy we aim to optimize.\n- \\(\\pi_{\\text{ref}}(y \\mid x)\\) is the reference policy.\n- \\(\\beta > 0\\) is a regularization coefficient.\n- \\(\\mathbb{D}_{\\textrm{KL}}\\) is the Kullback-Leibler divergence.\n\n### 2. **Express the KL Divergence**\nThe KL divergence is given by:\n\\[\n\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\pi_{\\text{ref}}(y\\mid x)\\bigr] = \\mathbb{E}_{y \\sim \\pi_{\\theta}(y \\mid x)}\\left[\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right].\n\\]\n\nSubstituting this into the objective, we get:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r(x, y) - \\beta \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\bigr].\n\\]\n\n### 3. **Rewriting the Objective**\nThe objective can be rewritten as:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r(x, y) + \\beta \\log \\pi_{\\text{ref}}(y \\mid x) - \\beta \\log \\pi_{\\theta}(y \\mid x)\\bigr].\n\\]\n\n### 4. **Formulating the Lagrangian**\nTo maximize this expression, we treat it as a constrained optimization problem where \\(\\pi_{\\theta}(y \\mid x)\\) must be a valid probability distribution (i.e., \\(\\sum_y \\pi_{\\theta}(y \\mid x) = 1\\)). Introducing a Lagrange multiplier \\(\\lambda(x)\\) for the normalization constraint, the Lagrangian is:\n\\[\n\\mathcal{L} = \\mathbb{E}_{y \\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r(x, y) + \\beta \\log \\pi_{\\text{ref}}(y \\mid x) - \\beta \\log \\pi_{\\theta}(y \\mid x)\\bigr] + \\lambda(x)\\left(1 - \\sum_y \\pi_{\\theta}(y \\mid x)\\right).\n\\]\n\n### 5. **Taking the Functional Derivative**\nTo find the optimal \\(\\pi_{\\theta}(y \\mid x)\\), we take the derivative of \\(\\mathcal{L}\\) with respect to \\(\\pi_{\\theta}(y \\mid x)\\) and set it to zero:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi_{\\theta}(y \\mid x)} = r(x, y) + \\beta \\log \\pi_{\\text{ref}}(y \\mid x) - \\beta \\log \\pi_{\\theta}(y \\mid x) - \\beta - \\lambda(x) = 0.\n\\]\n\nSolving for \\(\\pi_{\\theta}(y \\mid x)\\):\n\\[\nr(x, y) + \\beta \\log \\pi_{\\text{ref}}(y \\mid x) - \\beta \\log \\pi_{\\theta}(y \\mid x) - \\beta - \\lambda(x) = 0,\n\\]\n\\[\n\\log \\pi_{\\theta}(y \\mid x) = \\frac{r(x, y)}{\\beta} + \\log \\pi_{\\text{ref}}(y \\mid x) - 1 - \\frac{\\lambda(x)}{\\beta},\n\\]\n\\[\n\\pi_{\\theta}(y \\mid x) = \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right) \\exp\\left(-1 - \\frac{\\lambda(x)}{\\beta}\\right).\n\\]\n\n### 6. **Normalizing the Distribution**\nTo satisfy \\(\\sum_y \\pi_{\\theta}(y \\mid x) = 1\\), we define the partition function \\(Z(x)\\):\n\\[\nZ(x) = \\sum_y \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right).\n\\]\n\nThus, the optimal policy is:\n\\[\n\\pi_r(y \\mid x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right),\n\\]\nwhere the term \\(\\exp\\left(-1 - \\frac{\\lambda(x)}{\\beta}\\right)\\) is absorbed into \\(Z(x)\\).\n\n### 7. **Conclusion**\nThe optimal solution that maximizes the reward under the KL constraint is:\n\\[\n\\pi_r(y \\mid x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right),\n\\]\nwith \\(Z(x) = \\sum_y \\pi_{\\text{ref}}(y \\mid x) \\exp\\left(\\frac{1}{\\beta} r(x, y)\\right)\\) ensuring normalization. This completes the proof.", "ground-truth": "C"}
{"paper": "arxiv:2305.18290", "question": "Given the following equations:\n\nEquation 1 (Bradley-Terry model):\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\nEquation 2 (Reward function):\n\\begin{equation}\n    r(x,y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\nHow can we derive Equation 3 from Equations 1 and 2?\n\nEquation 3 (Objective):\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)}\\right)}\n\\end{equation}", "A": "To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. Start with Equation 1, which models the probability that $y_1$ is preferred over $y_2$ given $x$:\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\n2. Substitute the reward function $r^*(x, y)$ from Equation 2 into Equation 1. Equation 2 expresses the reward in terms of the optimal policy $\\pi^*$ and the reference policy $\\pi_{\\text{ref}}$:\n\\begin{equation}\n    r^*(x,y) = \\beta \\log \\frac{\\pi^*(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\n3. Substitute $r^*(x, y_1)$ and $r^*(x, y_2)$ into Equation 1:\n\\begin{align*}\n    p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} + \\beta \\log Z(x)\\right)} \\\\\n    &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} - \\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)}\\right)}.\n\\end{align*}\n\n4. The final expression is Equation 3, which simplifies the preference probability in terms of the ratio of the optimal policy to the reference policy for the two options $y_1$ and $y_2$.\n\nThis derivation shows how the preference probability can be expressed directly in terms of the policies, without explicitly computing the reward function, by leveraging the relationship between the reward and the policies as given in Equation 2.", "B": "To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. Start with Equation 1, which models the probability that $y_1$ is preferred over $y_2$ given $x$:\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\n2. Substitute the reward function $r^*(x, y)$ from Equation 2 into Equation 1. Equation 2 expresses the reward in terms of the optimal policy $\\pi^*$ and the reference policy $\\pi_{\\text{ref}}$:\n\\begin{equation}\n    r^*(x,y) = \\beta \\log \\frac{\\pi^*(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\n3. Substitute $r^*(x, y_1)$ and $r^*(x, y_2)$ into Equation 1:\n\\begin{align*}\n    p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} + \\beta \\log Z(x)\\right)} \\\\\n    &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)}\\right)}.\n\\end{align*}\n\n4. The final expression is Equation 3, which simplifies the preference probability in terms of the ratio of the optimal policy to the reference policy for the two options $y_1$ and $y_2$.\n\nThis derivation shows how the preference probability can be expressed directly in terms of the policies, without explicitly computing the reward function, by leveraging the relationship between the reward and the policies as given in Equation 2.", "C": "To derive Equation 3 from Equations 1 and 2, we will substitute the reward function \\( r^*(x, y) \\) from Equation 2 into the Bradley-Terry model in Equation 1. Here is the step-by-step derivation:\n\n---\n\n**Step 1: Start with the Bradley-Terry model (Equation 1):**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\]\n\n**Step 2: Substitute the reward function \\( r^*(x, y) \\) from Equation 2:**\n\\[\nr^*(x, y) = \\beta \\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + \\beta \\log Z(x).\n\\]\n\nSubstitute \\( r^*(x, y_1) \\) and \\( r^*(x, y_2) \\) into Equation 1:\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} + \\beta \\log Z(x)\\right)}.\n\\]\n\n**Step 3: Simplify the exponents:**\nFactor out \\( \\exp(\\beta \\log Z(x)) \\) from the numerator and denominator:\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp(\\beta \\log Z(x)) \\cdot \\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp(\\beta \\log Z(x)) \\cdot \\left[\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)\\right]}.\n\\]\n\nCancel \\( \\exp(\\beta \\log Z(x)) \\) from numerator and denominator:\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)}.\n\\]\n\n**Step 4: Rewrite the expression using the property \\( \\exp(\\beta \\log a) = a^\\beta \\):**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta}{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta + \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)^\\beta}.\n\\]\n\n**Step 5: Divide numerator and denominator by \\( \\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta \\):**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right)^\\beta}.\n\\]\n\n**Step 6: Rewrite the denominator using logarithmic properties:**\n\\[\n\\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right)^\\beta = \\exp\\left(\\beta \\log \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right)\\right).\n\\]\n\nSimplify the logarithm:\n\\[\n\\beta \\log \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right) = \\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} - \\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}.\n\\]\n\n**Step 7: Substitute back into the expression:**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} - \\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}.\n\\]\n\n---\n\nThis is the desired **Equation 3**. The derivation shows how the Bradley-Terry model, combined with the reward function, leads to the final objective function for preference modeling.", "D": "To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. **Substitute the reward function (Equation 2) into the Bradley-Terry model (Equation 1):**\n   \\begin{align*}\n     p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)} \\\\\n     &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} + \\beta \\log Z(x)\\right)}.\n   \\end{align*}\n\n2. **Factor out \\(\\beta \\log Z(x)\\) from the exponents:**\n   \\begin{align*}\n     &= \\frac{\\exp\\left(\\beta \\log Z(x)\\right) \\cdot \\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp\\left(\\beta \\log Z(x)\\right) \\left[\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)\\right]}.\n   \\end{align*}\n\n3. **Cancel the common factor \\(\\exp\\left(\\beta \\log Z(x)\\right)\\):**\n   \\begin{align*}\n     &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)}.\n   \\end{align*}\n\n4. **Simplify the exponentials using \\( \\exp(\\beta \\log a) = a^\\beta \\):**\n   \\begin{align*}\n     &= \\frac{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta}{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta + \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)^\\beta}.\n   \\end{align*}\n\n5. **Factor out \\(\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta\\) from the denominator:**\n   \\begin{align*}\n     &= \\frac{1}{1 + \\left(\\frac{\\pi^*(y_2 \\mid x) \\cdot \\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x) \\cdot \\pi_{\\text{ref}}(y_2 \\mid x)}\\right)^\\beta}.\n   \\end{align*}\n\n6. **Rewrite the ratio using logarithmic properties:**\n   \\begin{align*}\n     &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} - \\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}.\n   \\end{align*}\n\nThis matches **Equation 3**, completing the derivation. The normalization term \\(Z(x)\\) cancels out naturally during simplification.", "ground-truth": "B"}
{"paper": "arxiv:2305.18290", "question": "Prove the following theorem: Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization $r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}$ for some model $\\pi(y\\mid x)$ and a given reference model $\\pi_{\\text{ref}}(y \\mid x)$.", "A": "Consider any reward function $r(x,y)$, which induces an optimal model $\\pi_r(y|x)$ under the KL-constrained RL problem. The solution is given by the optimal policy equation. When we log-linearize both sides, we obtain:\n\\begin{equation*}\nr(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)\n\\end{equation*}\nwhere $Z(x) =\\sum_{y}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ (notice that $Z(x)$ also depends on the reward function $r$). Using the operator $r'(x, y) = f(r, \\pi_{\\text{ref}}, \\beta)(x, y) = r(x, y) - \\beta \\log Z(x)$, we see that this new reward function is within the equivalence class of $r$ and, we have:\n\\begin{equation*}\nr'(x,y) =\\beta \\log \\frac{\\pi_{\\text{ref}}(y|x)}{\\pi_r(y|x)}\n\\end{equation*}\nwhich completes the proof.", "B": "To prove the theorem, we show that any reward function consistent with the Plackett-Luce (PL) model can be reparameterized as $r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}$, where $\\pi(y\\mid x)$ is a policy derived from the reward function and $\\pi_{\\text{ref}}(y\\mid x)$ is a given reference policy.\n\n### Key Steps:\n1. **PL Model Basics**:  \n   The PL model assigns selection probabilities proportional to exponentiated rewards. For a set of items $\\{y_i\\}$, the probability of selecting $y$ first is:\n   $$\n   P_{\\text{PL}}(y \\mid x) = \\frac{\\exp\\left(r(y \\mid x)/\\beta\\right)}{\\sum_{y'} \\exp\\left(r(y' \\mid x)/\\beta\\right)}.\n   $$\n\n2. **Incorporating the Reference Policy**:  \n   Assume a reference policy $\\pi_{\\text{ref}}(y\\mid x)$ (e.g., a pretrained model or uniform distribution). Define a policy $\\pi(y\\mid x)$ as:\n   $$\n   \\pi(y \\mid x) \\propto \\pi_{\\text{ref}}(y \\mid x) \\cdot \\exp\\left(r(y \\mid x)/\\beta\\right).\n   $$\n   Normalizing $\\pi(y\\mid x)$ gives:\n   $$\n   \\pi(y\\mid x) = \\frac{\\pi_{\\text{ref}}(y\\mid x) \\exp\\left(r(y\\mid x)/\\beta\\right)}{Z(x)},\n   $$\n   where $Z(x) = \\sum_{y'} \\pi_{\\text{ref}}(y'\\mid x) \\exp\\left(r(y'\\mid x)/\\beta\\right)$.\n\n3. **Reward Reparameterization**:  \n   Taking logarithms of $\\pi(y\\mid x)$:\n   $$\n   \\log \\pi(y\\mid x) = \\log \\pi_{\\text{ref}}(y\\mid x) + \\frac{r(y\\mid x)}{\\beta} - \\log Z(x).\n   $$\n   Solving for $r(y\\mid x)$:\n   $$\n   r(y\\mid x) = \\beta \\left(\\log \\pi(y\\mid x) - \\log \\pi_{\\text{ref}}(y\\mid x) + \\log Z(x)\\right).\n   $$\n\n4. **Invariance to Additive Constants**:  \n   The PL model depends only on *relative* rewards, not absolute values. Adding a context-dependent constant $c(x)$ to $r(y\\mid x)$ does not affect selection probabilities:\n   $$\n   \\frac{\\exp\\left((r(y\\mid x) + c(x))/\\beta\\right)}{\\sum_{y'} \\exp\\left((r(y'\\mid x) + c(x))/\\beta\\right)} = \\frac{\\exp\\left(r(y\\mid x)/\\beta\\right)}{\\sum_{y'} \\exp\\left(r(y'\\mid x)/\\beta\\right)}.\n   $$\n   Thus, $c(x) = \\beta \\log Z(x)$ can be omitted without changing the PL probabilities.\n\n5. **Final Reparameterization**:  \n   Discarding the $c(x)$ term (by the invariance above), we obtain:\n   $$\n   r(y\\mid x) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}.\n   $$\n   This reparameterization preserves the PL probabilities because:\n   $$\n   \\pi(y\\mid x) \\propto \\pi_{\\text{ref}}(y\\mid x) \\cdot \\exp\\left(r(y\\mid x)/\\beta\\right),\n   $$\n   and normalization is implicitly enforced by $\\pi(y\\mid x)$ being a valid policy.\n\n### Conclusion:\nUnder mild assumptions (e.g., ignoring context-dependent additive constants in rewards), any reward class consistent with the PL/Bradley-Terry model can be represented as $r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}$. This aligns with the PL structure while generalizing to arbitrary reference policies.", "C": "To prove the theorem, we'll proceed step-by-step, leveraging the properties of the Plackett-Luce (PL) model and its special case, the Bradley-Terry (BT) model. The key idea is to show that any reward function consistent with these models can be reparameterized in the given form.\n\n### 1. **Plackett-Luce Model Recap**\nThe PL model defines the probability of a ranking $\\sigma$ over a set of items $\\mathcal{Y}$ given context $x$ as:\n\\[\nP(\\sigma \\mid x) = \\prod_{i=1}^{|\\mathcal{Y}|} \\frac{e^{r(x, y_{\\sigma(i)})}}{\\sum_{j=i}^{|\\mathcal{Y}|} e^{r(x, y_{\\sigma(j)})}},\n\\]\nwhere $r(x, y)$ is the reward (or score) for item $y$ under context $x$, and $\\sigma(i)$ is the item ranked at position $i$.\n\nFor the pairwise case (Bradley-Terry), the probability that $y$ is preferred over $y'$ is:\n\\[\nP(y \\succ y' \\mid x) = \\frac{e^{r(x, y)}}{e^{r(x, y)} + e^{r(x, y')}}.\n\\]\n\n### 2. **Reparameterization Goal**\nWe aim to show that any reward function $r(x, y)$ consistent with the PL/BT models can be expressed as:\n\\[\nr(x, y) = \\beta \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)},\n\\]\nfor some model $\\pi(y \\mid x)$ and reference model $\\pi_{\\text{ref}}(y \\mid x)$.\n\n### 3. **Constructing $\\pi(y \\mid x)$**\nAssume the PL model is parameterized by rewards $r(x, y)$. Define $\\pi(y \\mid x)$ as the policy induced by the PL model, i.e., the probability of selecting $y$ first:\n\\[\n\\pi(y \\mid x) = \\frac{e^{r(x, y)}}{\\sum_{y'} e^{r(x, y')}}.\n\\]\nThis is the softmax policy derived from $r(x, y)$.\n\n### 4. **Reference Model $\\pi_{\\text{ref}}(y \\mid x)$**\nLet $\\pi_{\\text{ref}}(y \\mid x)$ be an arbitrary reference policy (e.g., uniform or another baseline). The log-ratio of the policies is:\n\\[\n\\log \\frac{\\pi(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} = r(x, y) - \\log \\sum_{y'} e^{r(x, y')} - \\log \\pi_{\\text{ref}}(y \\mid x).\n\\]\nThis suggests that $r(x, y)$ can be expressed in terms of the log-ratio, but we need to account for the normalization.\n\n### 5. **Reward Equivalence**\nThe PL/BT models are invariant to additive shifts in rewards (i.e., $r(x, y) + c(x)$ for any $c(x)$). Thus, we can normalize the rewards to eliminate the $\\log \\sum_{y'} e^{r(x, y')}$ term. Let:\n\\[\nr'(x, y) = r(x, y) - \\log \\sum_{y'} e^{r(x, y')}.\n\\]\nThen:\n\\[\n\\pi(y \\mid x) = e^{r'(x, y)}.\n\\]\nNow, the log-ratio becomes:\n\\[\n\\log \\frac{\\pi(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} = r'(x, y) - \\log \\pi_{\\text{ref}}(y \\mid x).\n\\]\nRearranging:\n\\[\nr'(x, y) = \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + \\log \\pi_{\\text{ref}}(y \\mid x).\n\\]\nBut since $r'(x, y)$ is a shifted version of $r(x, y)$, and the PL/BT models are invariant to such shifts, we can absorb the $\\log \\pi_{\\text{ref}}(y \\mid x)$ term into the normalization. Thus, we can write:\n\\[\nr(x, y) = \\beta \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)},\n\\]\nwhere $\\beta$ is a scaling factor (which can be set to 1 if the rewards are appropriately scaled).\n\n### 6. **Conclusion**\nAny reward function $r(x, y)$ consistent with the PL/BT models can be reparameterized as:\n\\[\nr(x, y) = \\beta \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)},\n\\]\nwhere $\\pi(y \\mid x)$ is the policy induced by the PL model, and $\\pi_{\\text{ref}}(y \\mid x)$ is a reference policy. This completes the proof. $\\square$", "D": "Consider any reward function $r(x,y)$, which induces an optimal model $\\pi_r(y|x)$ under the KL-constrained RL problem. The solution is given by the optimal policy equation. When we log-linearize both sides, we obtain:\n\\begin{equation*}\nr(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)\n\\end{equation*}\nwhere $Z(x) =\\sum_{y}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ (notice that $Z(x)$ also depends on the reward function $r$). Using the operator $r'(x, y) = f(r, \\pi_{\\text{ref}}, \\beta)(x, y) = r(x, y) - \\beta \\log Z(x)$, we see that this new reward function is within the equivalence class of $r$ and, we have:\n\\begin{equation*}\nr'(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_{\\text{ref}}(y|x)}\n\\end{equation*}\nwhich completes the proof.", "ground-truth": "D"}
{"paper": "arxiv:2407.18242", "question": "Given $W = W_0 + \\Delta W \\approx W_0 + sBA$, where $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ are low-rank matrices, and $s$ is a scaling factor. The differential of $W$ is given by $\\dd W = \\frac{\\partial W}{\\partial A}^T \\dd A + \\frac{\\partial W}{\\partial B}^T \\dd B = -(sBg^A + sg^BA)$. Prove that the rank of $\\tilde{g} = sBg^A + sg^BA$ is at most $2r$.", "A": "To prove that the rank of $\\tilde{g} = sBg^A + sg^BA$ is at most $2r$, we proceed with the following reasoning:\n\n1. Subadditivity of Matrix Rank: The rank of the sum of two matrices is less than or equal to the sum of their ranks. Therefore,\n   \\begin{equation}\n   rank(\\tilde{g}) = rank(sg^BA + sBg^A) \\le rank(g^BA) + rank(Bg^A).\n   \\end{equation}\n\n2. Rank of Product of Matrices: For any matrices $X$ and $Y$, the rank of their product $XY$ satisfies $rank(XY) \\le \\min(rank(X), rank(Y))$. Applying this property,\n   \\begin{align}\n   rank(g^BA) & \\le \\min(rank(g^B), rank(A)) \\le r, \\\\\n   rank(Bg^A) & \\le \\min(rank(B), rank(g^A)) \\le r.\n   \\end{align}\n\n3. Conclusion: Combining the above results, we conclude that\n   \\begin{align}\n   rank(\\tilde{g}) & \\le rank(g^BA) + rank(Bg^A) \\\\\n   & \\le \\min(rank(g^B), rank(A)) + \\min(rank(B), rank(g^A)) \\\\\n   & \\le 2r.\n   \\end{align}\n\nThus, the rank of $\\tilde{g}$ is indeed at most $2r$.", "B": "To prove that the rank of $\\tilde{g} = sBg^A + sg^BA$ is at most $2r$, we proceed with the following reasoning:\n\n1. Subadditivity of Matrix Rank: The rank of the sum of two matrices is less than or equal to the sum of their ranks. Therefore,\n   \\begin{equation}\n   rank(\\tilde{g}) = rank(sg^BA + sBg^A) \\le rank(g^BA) + rank(Bg^A).\n   \\end{equation}\n\n2. Rank of Product of Matrices: For any matrices $X$ and $Y$, the rank of their product $XY$ satisfies $rank(XY) \\le \\min(rank(X), rank(Y))$. Applying this property,\n   \\begin{align}\n   rank(g^BA) & \\le \\min(rank(g^B), rank(A)) \\le r, \\\\\n   rank(Bg^A) & \\le \\min(rank(B), rank(g^A)) \\le r.\n   \\end{align}\n\n3. Conclusion: Combining the above results, we conclude that\n   \\begin{align}\n   rank(\\tilde{g}) & \\le rank(g^BA) + rank(Bg^A) \\\\\n   & \\le \\min(rank(g^B), rank(A)) + \\min(rank(B), rank(g^A)) \\\\\n   & \\le 3r.\n   \\end{align}\n\nThus, the rank of $\\tilde{g}$ is indeed at most $3r$.", "C": "To prove that the rank of \\(\\tilde{g} = sBg^A + sg^BA\\) is at most \\(2r\\), we analyze the structure of \\(\\tilde{g}\\) and the ranks of its constituent matrices.\n\n### Step 1: Express \\(\\tilde{g}\\) in terms of \\(B\\) and \\(A\\)\nGiven:\n\\[\n\\tilde{g} = sBg^A + sg^BA\n\\]\nHere, \\(B \\in \\mathbb{R}^{m \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times n}\\) are low-rank matrices with rank at most \\(r\\). The terms \\(g^A\\) and \\(g^B\\) are gradients with respect to \\(A\\) and \\(B\\), respectively, but their exact forms are not specified. However, we can infer their dimensions based on the context:\n\n- Since \\(W \\approx W_0 + sBA\\), the gradient \\(g^A\\) corresponds to \\(\\frac{\\partial \\mathcal{L}}{\\partial A}\\) for some loss function \\(\\mathcal{L}\\), and similarly for \\(g^B\\). Thus:\n  - \\(g^A \\in \\mathbb{R}^{r \\times n}\\) (same dimensions as \\(A\\)),\n  - \\(g^B \\in \\mathbb{R}^{m \\times r}\\) (same dimensions as \\(B\\)).\n\n### Step 2: Analyze the rank of each term\n1. **Term \\(Bg^A\\)**:\n   - \\(B \\in \\mathbb{R}^{m \\times r}\\) has rank at most \\(r\\).\n   - \\(g^A \\in \\mathbb{R}^{r \\times n}\\) has rank at most \\(r\\).\n   - The product \\(Bg^A\\) has rank at most \\(\\min(\\text{rank}(B), \\text{rank}(g^A)) \\leq r\\).\n\n2. **Term \\(g^BA\\)**:\n   - \\(g^B \\in \\mathbb{R}^{m \\times r}\\) has rank at most \\(r\\).\n   - \\(A \\in \\mathbb{R}^{r \\times n}\\) has rank at most \\(r\\).\n   - The product \\(g^BA\\) has rank at most \\(\\min(\\text{rank}(g^B), \\text{rank}(A)) \\leq r\\).\n\n### Step 3: Rank of the sum \\(\\tilde{g} = sBg^A + sg^BA\\)\nThe rank of the sum of two matrices is bounded by the sum of their individual ranks:\n\\[\n\\text{rank}(\\tilde{g}) = \\text{rank}(sBg^A + sg^BA) \\leq \\text{rank}(Bg^A) + \\text{rank}(g^BA) \\leq r + r = 2r.\n\\]\n\n### Conclusion\nThus, the rank of \\(\\tilde{g}\\) is at most \\(2r\\). \n\n\\[\n\\boxed{\\text{rank}(\\tilde{g}) \\leq 2r}\n\\]", "D": "To prove that the rank of $\\tilde{g} = sB g^A + s g^B A$ is at most $2r$, we analyze the structure of each term and apply properties of matrix rank.\n\n1. **Term 1: $sB g^A$**  \n   - $B \\in \\mathbb{R}^{m \\times r}$ has rank $\\leq r$ (column rank bound).  \n   - $g^A \\in \\mathbb{R}^{r \\times n}$ has rank $\\leq r$ (row rank bound).  \n   - The product $B g^A$ has rank $\\leq \\min(\\text{rank}(B), \\text{rank}(g^A)) \\leq r$.  \n   - Scaling by $s \\neq 0$ preserves rank. Thus, $\\text{rank}(sB g^A) \\leq r$.\n\n2. **Term 2: $s g^B A$**  \n   - $g^B \\in \\mathbb{R}^{m \\times r}$ has rank $\\leq r$ (column rank bound).  \n   - $A \\in \\mathbb{R}^{r \\times n}$ has rank $\\leq r$ (row rank bound).  \n   - The product $g^B A$ has rank $\\leq \\min(\\text{rank}(g^B), \\text{rank}(A)) \\leq r$.  \n   - Scaling by $s \\neq 0$ preserves rank. Thus, $\\text{rank}(s g^B A) \\leq r$.\n\n3. **Sum of Terms**  \n   The rank of the sum of two matrices satisfies:  \n   $$\\text{rank}(\\tilde{g}) = \\text{rank}(sB g^A + s g^B A) \\leq \\text{rank}(sB g^A) + \\text{rank}(s g^B A) \\leq r + r = 2r.$$\n\n**Conclusion**: The rank of $\\tilde{g}$ is bounded by $2r$.  \n$\\boxed{\\text{rank}(\\tilde{g}) \\leq 2r}$", "ground-truth": "A"}
{"paper": "arxiv:2407.18242", "question": "Prove the following theorem:\n\\begin{theorem}\nAssume matrices $B\\in\\mathbb{R}^{m\\times r}, A\\in\\mathbb{R}^{r\\times n}$ are both full rank. \nFor the objective $\\min_{g^A, g^B} \\|\\tilde{g} - g\\|^2_F$, the optimal solutions are given by:\n\\begin{align}\n    g^A &= \\frac{1}{s}(B^TB)^{-1}B^Tg + XA = \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} + XA, \\\\ \n    g^B &= \\frac{1}{s}[I - B(B^TB)^{-1}B^T]gA^T(AA^T)^{-1} - BX \\\\\n        &= \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} - BX. \n\\end{align}\nHere, $X\\in\\mathbb{R}^{r\\times r}$ represents an arbitrary matrix.\n\\end{theorem}", "A": "To prove the theorem, we consider the optimization problem with parameterization $\\tilde{g} = s(Bg^A + g^B A)$, where $s$ is a scaling factor. The goal is to minimize:\n\n\\[\n\\min_{g^A, g^B} \\| s(Bg^A + g^B A) - g \\|_F^2\n\\]\n\n**Step 1: Compute Partial Derivatives and Optimality Conditions**\n\nDefine $F = s(Bg^A + g^B A) - g$. The loss $\\mathcal{L} = \\|F\\|_F^2$ leads to:\n\n1. **Derivative w.r.t. $g^A$:**\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial g^A} = 2sB^T F = 0 \\implies B^T (sBg^A + sg^B A - g) = 0\n\\]\n\\[\n\\implies sB^T B g^A + sB^T g^B A = B^T g \\quad (1)\n\\]\n\n2. **Derivative w.r.t. $g^B$:**\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial g^B} = 2sF A^T = 0 \\implies (sBg^A + sg^B A - g) A^T = 0\n\\]\n\\[\n\\implies sB g^A A^T + s g^B A A^T = g A^T \\quad (2)\n\\]\n\n**Step 2: Solve for $g^A$ and $g^B$**\n\nFrom (1):\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T (g - s g^B A) \\quad (3)\n\\]\n\nSubstitute (3) into (2):\n\\[\nsB \\left[ \\frac{1}{s}(B^T B)^{-1} B^T (g - s g^B A) \\right] A^T + s g^B A A^T = g A^T\n\\]\nSimplify using $P_B = B(B^T B)^{-1} B^T$:\n\\[\nP_B g A^T - P_B g^B A A^T + g^B A A^T = \\frac{1}{s} g A^T\n\\]\n\\[\n\\implies (I - P_B) g^B A A^T = (I - P_B) \\frac{1}{s} g A^T\n\\]\nSolve for $g^B$:\n\\[\ng^B = \\frac{1}{s}(I - P_B) g A^T (A A^T)^{-1} - BX\n\\]\nwhere $X \\in \\mathbb{R}^{r \\times r}$ is arbitrary (arises from the nullspace of $(I - P_B)$).\n\n**Step 3: Substitute $g^B$ Back into $g^A$**\n\nSubstitute $g^B$ into (3):\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T \\left[ g - s \\left( \\frac{1}{s}(I - P_B) g A^T (A A^T)^{-1} - BX \\right) A \\right]\n\\]\nSimplify:\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T P_B g + XA\n\\]\nwhere $X$ absorbs terms from the nullspace.\n\n**Step 4: Relate to LoRA Solutions**\n\nThe LoRA solutions $g^A_{\\text{lora}} = B^T g$ and $g^B_{\\text{lora}} = g A^T$ appear scaled:\n\\[\ng^A = \\frac{1}{s^2}(B^T B)^{-1} g^A_{\\text{lora}} + XA, \\quad g^B = \\frac{1}{s^2}(I - P_B) g^B_{\\text{lora}} (A A^T)^{-1} - BX\n\\]\n\n**Conclusion**\nThe optimal solutions $g^A$ and $g^B$ contain:\n- A scaled projection of $g$ onto the column/row spaces of $B$ and $A$.\n- Arbitrary terms $XA$ and $-BX$ that cancel in $\\tilde{g}$, leaving the objective invariant.\n\nThus, the theorem is proven.", "B": "To prove the given theorem, we need to derive the optimal solutions \\( g^A \\) and \\( g^B \\) that minimize the Frobenius norm objective \\(\\|\\tilde{g} - g\\|^2_F\\), where \\(\\tilde{g} = s(g^B B + B g^A)A\\). Here, \\( B \\in \\mathbb{R}^{m \\times r} \\) and \\( A \\in \\mathbb{R}^{r \\times n} \\) are both full-rank matrices, and \\( s \\) is a scalar.\n\n### Step 1: Express the Objective Function\nThe objective is:\n\\[\n\\min_{g^A, g^B} \\|\\tilde{g} - g\\|^2_F = \\min_{g^A, g^B} \\|s(g^B B + B g^A)A - g\\|^2_F.\n\\]\nLet \\( \\tilde{g} = s(g^B B + B g^A)A \\). The Frobenius norm can be written as:\n\\[\n\\|\\tilde{g} - g\\|^2_F = \\text{tr}\\left((\\tilde{g} - g)^T(\\tilde{g} - g)\\right).\n\\]\n\n### Step 2: Expand the Objective\nExpand \\(\\tilde{g}\\):\n\\[\n\\tilde{g} = s(g^B B A + B g^A A).\n\\]\nThe objective becomes:\n\\[\n\\|s(g^B B A + B g^A A) - g\\|^2_F.\n\\]\nTo minimize this, we take derivatives with respect to \\( g^A \\) and \\( g^B \\) and set them to zero.\n\n### Step 3: Solve for \\( g^A \\)\nFirst, fix \\( g^B \\) and solve for \\( g^A \\). The derivative with respect to \\( g^A \\) is:\n\\[\n\\frac{\\partial}{\\partial g^A} \\|s B g^A A - (g - s g^B B A)\\|^2_F = 0.\n\\]\nThis is a least squares problem in \\( g^A \\). The solution is:\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T (g - s g^B B A) A^T (A A^T)^{-1} + X,\n\\]\nwhere \\( X \\) is an arbitrary matrix (from the null space of \\( B \\) and \\( A \\)). \n\nHowever, we can simplify this by noting that \\( g^A \\) must satisfy:\n\\[\ns B g^A A = g - s g^B B A.\n\\]\nMultiplying by \\( B^T \\) from the left and \\( A^T \\) from the right:\n\\[\ns B^T B g^A A A^T = B^T g A^T - s B^T g^B B A A^T.\n\\]\nAssuming \\( B^T B \\) and \\( A A^T \\) are invertible (since \\( B \\) and \\( A \\) are full-rank), we solve for \\( g^A \\):\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T g A^T (A A^T)^{-1} - (B^T B)^{-1} B^T g^B B + X.\n\\]\nThis can be rewritten as:\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T g A^T (A A^T)^{-1} + X,\n\\]\nwhere \\( X \\) now absorbs the term involving \\( g^B \\).\n\n### Step 4: Solve for \\( g^B \\)\nNow, fix \\( g^A \\) and solve for \\( g^B \\). The derivative with respect to \\( g^B \\) is:\n\\[\n\\frac{\\partial}{\\partial g^B} \\|s g^B B A - (g - s B g^A A)\\|^2_F = 0.\n\\]\nThis is another least squares problem in \\( g^B \\). The solution is:\n\\[\ng^B = \\frac{1}{s}(g - s B g^A A) A^T (A A^T)^{-1} B^T (B B^T)^{-1} + Y,\n\\]\nwhere \\( Y \\) is an arbitrary matrix (from the null space of \\( B \\) and \\( A \\)).\n\nSimplifying, we multiply by \\( B \\) from the right:\n\\[\ng^B B = \\frac{1}{s}(g - s B g^A A) A^T (A A^T)^{-1} B^T (B B^T)^{-1} B + Y B.\n\\]\nSince \\( B \\) is full-rank, \\( B^T (B B^T)^{-1} B = I \\), and \\( Y B \\) can be absorbed into \\( Y \\). Thus:\n\\[\ng^B B = \\frac{1}{s}(g - s B g^A A) A^T (A A^T)^{-1} + Y.\n\\]\nTo eliminate \\( g^A \\), substitute the expression for \\( g^A \\):\n\\[\ng^B B = \\frac{1}{s}g A^T (A A^T)^{-1} - B (B^T B)^{-1} B^T g A^T (A A^T)^{-1} + Y.\n\\]\nThis can be written as:\n\\[\ng^B B = \\frac{1}{s}\\left[I - B (B^T B)^{-1} B^T\\right] g A^T (A A^T)^{-1} + Y.\n\\]\nMultiplying by \\( B^T \\) from the right:\n\\[\ng^B B B^T = \\frac{1}{s}\\left[I - B (B^T B)^{-1} B^T\\right] g A^T (A A^T)^{-1} B^T + Y B^T.\n\\]\nSince \\( B \\) is full-rank, \\( B B^T \\) is invertible, and \\( Y B^T \\) can be absorbed into \\( Y \\). Thus:\n\\[\ng^B = \\frac{1}{s}\\left[I - B (B^T B)^{-1} B^T\\right] g A^T (A A^T)^{-1} B^T (B B^T)^{-1} + Y.\n\\]\nSimplifying further:\n\\[\ng^B = \\frac{1}{s}\\left[I - B (B^T B)^{-1} B^T\\right] g A^T (A A^T)^{-1} - B X,\n\\]\nwhere \\( X \\) is an arbitrary matrix (from the null space of \\( B \\)).\n\n### Step 5: Combine Results\nThe optimal solutions are:\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T g A^T (A A^T)^{-1} + X,\n\\]\n\\[\ng^B = \\frac{1}{s}\\left[I - B (B^T B)^{-1} B^T\\right] g A^T (A A^T)^{-1} - B X.\n\\]\nThese can be rewritten in terms of \\( g^A_{lora} \\) and \\( g^B_{lora} \\) as:\n\\[\ng^A = \\frac{1}{s^2}(B^T B)^{-1} g^A_{lora} + X A,\n\\]\n\\[\ng^B = \\frac{1}{s^2}\\left[I - B (B^T B)^{-1} B^T\\right] g^B_{lora} (A A^T)^{-1} - B X,\n\\]\nwhere \\( g^A_{lora} = B^T g A^T \\) and \\( g^B_{lora} = g A^T \\).\n\n### Final Answer\nThe optimal solutions are:\n\\[\ng^A = \\frac{1}{s}(B^T B)^{-1} B^T g A^T (A A^T)^{-1} + X A = \\frac{1}{s^2}(B^T B)^{-1} g^A_{lora} + X A,\n\\]\n\\[\ng^B = \\frac{1}{s}\\left[I - B (B^T B)^{-1} B^T\\right] g A^T (A A^T)^{-1} - B X = \\frac{1}{s^2}\\left[I - B (B^T B)^{-1} B^T\\right] g^B_{lora} (A A^T)^{-1} - B X,\n\\]\nwhere \\( X \\in \\mathbb{R}^{r \\times r} \\) is an arbitrary matrix.", "C": "To solve the optimization problem, we denote $L = \\|sBg^A + sg^BA - g\\|_F^2$ and derive the necessary conditions for optimality:\n\\begin{align}\n\\frac{\\partial L}{\\partial g^A} &= 2sB^T(sBg^A + sg^BA - g) = 0, \\\\\n\\frac{\\partial L}{\\partial g^B} &= 2(sBg^A + sg^BA - g)sA^T = 0.\n\\end{align}\nGiven that matrices $A$ and $B$ are full-rank, $AA^T$ and $B^TB$ are invertible. From the second condition, we derive:\n\\begin{equation}\ng^B = \\frac{1}{s}gA^T(AA^T)^{-1} - Bg^AA^T(AA^T)^{-1}.\n\\end{equation}\nSubstituting this into the first condition, we obtain a linear equation:\n\\begin{equation}\ng^A[I - A^T(AA^T)^{-1}A]=\\frac{1}{s}(B^TB)^{-1}B^Tg[I - A^T(AA^T)^{-1}A].\n\\end{equation}\nRecognizing that the matrix $P = I - A^T(AA^T)^{-1}A$ is a projection matrix, the solution to the linear equation is:\n\\begin{equation}\ng^A = \\frac{1}{s}(B^TB)^{-1}B^Tg + XA,\n\\end{equation}\nwhere $X\\in\\mathbb{R}^{r\\times r}$ is an arbitrary matrix. Substituting this solution back into the expression for $g^B$ yields:\n\\begin{equation}\ng^B = \\frac{1}{s}[I - B(B^TB)^{-1}B^T]gA^T(AA^T)^{-1} - BX.\n\\end{equation}\nTo express the solutions in terms of the gradients of standard LoRA, we use:\n\\begin{align}\ng^A_{lora} = sB^Tg, \\quad g^B_{lora} = sgA^T.\n\\end{align}\nThus, the optimal solutions can be rewritten as:\n\\begin{align}\ng^A &= \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} + XA, \\\\\ng^B &= \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} + BX.\n\\end{align}\nThis completes the proof, showing how the gradients can be adjusted and updated within the framework of LoRA optimization.", "D": "To solve the optimization problem, we denote $L = \\|sBg^A + sg^BA - g\\|_F^2$ and derive the necessary conditions for optimality:\n\\begin{align}\n\\frac{\\partial L}{\\partial g^A} &= 2sB^T(sBg^A + sg^BA - g) = 0, \\\\\n\\frac{\\partial L}{\\partial g^B} &= 2(sBg^A + sg^BA - g)sA^T = 0.\n\\end{align}\nGiven that matrices $A$ and $B$ are full-rank, $AA^T$ and $B^TB$ are invertible. From the second condition, we derive:\n\\begin{equation}\ng^B = \\frac{1}{s}gA^T(AA^T)^{-1} - Bg^AA^T(AA^T)^{-1}.\n\\end{equation}\nSubstituting this into the first condition, we obtain a linear equation:\n\\begin{equation}\ng^A[I - A^T(AA^T)^{-1}A]=\\frac{1}{s}(B^TB)^{-1}B^Tg[I - A^T(AA^T)^{-1}A].\n\\end{equation}\nRecognizing that the matrix $P = I - A^T(AA^T)^{-1}A$ is a projection matrix, the solution to the linear equation is:\n\\begin{equation}\ng^A = \\frac{1}{s}(B^TB)^{-1}B^Tg + XA,\n\\end{equation}\nwhere $X\\in\\mathbb{R}^{r\\times r}$ is an arbitrary matrix. Substituting this solution back into the expression for $g^B$ yields:\n\\begin{equation}\ng^B = \\frac{1}{s}[I - B(B^TB)^{-1}B^T]gA^T(AA^T)^{-1} - BX.\n\\end{equation}\nTo express the solutions in terms of the gradients of standard LoRA, we use:\n\\begin{align}\ng^A_{lora} = sB^Tg, \\quad g^B_{lora} = sgA^T.\n\\end{align}\nThus, the optimal solutions can be rewritten as:\n\\begin{align}\ng^A &= \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} + XA, \\\\\ng^B &= \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} - BX.\n\\end{align}\nThis completes the proof, showing how the gradients can be adjusted and updated within the framework of LoRA optimization.", "ground-truth": "D"}
{"paper": "arxiv:2407.18242", "question": "Prove the following theorem:\n\\begin{theorem}\nWhen updating matrices $A$ and $B$ using the closed-form solution, we proceed as follows:\n\\begin{gather}\n    A\\leftarrow A - \\gamma g^A \\\\\n    B\\leftarrow B - \\gamma g^B, \n\\end{gather}\nwhere $\\gamma \\ge 0$ denotes the learning rate.\nOur method ensures a decrease in the loss, akin to the standard gradient descent algorithm, expressed by:\n\\begin{equation}\n    \\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\} \\le 0.\n\\end{equation}\n\\end{theorem}", "A": "To prove the theorem, we need to show that the loss \\( L \\) decreases when updating matrices \\( A \\) and \\( B \\) using the given closed-form solution. The key idea is to analyze the change in the loss \\( \\dd L \\) due to the updates and demonstrate that it is non-positive.\n\n### Step 1: Loss Function and Gradients\nAssume the loss \\( L \\) is a function of \\( A \\) and \\( B \\), and let \\( g^A \\) and \\( g^B \\) be the gradients of \\( L \\) with respect to \\( A \\) and \\( B \\), respectively. The updates are:\n\\[\nA \\leftarrow A - \\gamma g^A, \\quad B \\leftarrow B - \\gamma g^B.\n\\]\n\n### Step 2: Change in Loss\nThe first-order approximation of the change in the loss \\( \\dd L \\) due to the updates is:\n\\[\n\\dd L = \\langle \\nabla_A L, \\dd A \\rangle_F + \\langle \\nabla_B L, \\dd B \\rangle_F,\n\\]\nwhere \\( \\dd A = -\\gamma g^A \\) and \\( \\dd B = -\\gamma g^B \\). Substituting these, we get:\n\\[\n\\dd L = -\\gamma \\left( \\langle g^A, g^A \\rangle_F + \\langle g^B, g^B \\rangle_F \\right).\n\\]\nHowever, this is the standard gradient descent result, which always ensures \\( \\dd L \\leq 0 \\).\n\n### Step 3: Special Form of Gradients\nThe theorem specifies a particular form for the gradients \\( g^A \\) and \\( g^B \\), which are derived from the LoRA (Low-Rank Adaptation) framework. Let:\n\\[\ng^A = \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora}, \\quad g^B = \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1}.\n\\]\nHere, \\( s \\) is a scaling factor, and \\( g^A_{lora} \\) and \\( g^B_{lora} \\) are the LoRA-specific gradients.\n\n### Step 4: Substitute Gradients into \\( \\dd L \\)\nSubstituting \\( g^A \\) and \\( g^B \\) into the expression for \\( \\dd L \\):\n\\[\n\\dd L = -\\gamma \\left( \\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F \\right).\n\\]\n\n### Step 5: Non-Positivity of \\( \\dd L \\)\nTo show \\( \\dd L \\leq 0 \\), we analyze each term:\n1. The first term is \\( \\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F \\). Since \\( (B^TB)^{-1} \\) is positive definite (assuming \\( B \\) has full column rank), this term is non-negative.\n2. The second term is \\( \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F \\). The matrix \\( [I - B(B^TB)^{-1}B^T] \\) is a projection matrix (idempotent and symmetric), and \\( (AA^T)^{-1} \\) is positive definite (assuming \\( A \\) has full row rank). Thus, this term is also non-negative.\n\nSince both terms are non-negative and multiplied by \\( -\\gamma \\) (where \\( \\gamma \\geq 0 \\)), we conclude:\n\\[\n\\dd L \\leq 0.\n\\]\n\n### Conclusion\nThe updates for \\( A \\) and \\( B \\) ensure that the loss \\( L \\) decreases (or remains unchanged) at each step, analogous to standard gradient descent. This completes the proof.", "B": "The proof of the theorem is divided into two parts.\n\nPart I.\nWe first show that the differential change in the loss function, $\\dd L$, can be expressed as:\n\\begin{equation}\n\\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\}.\n\\end{equation}\nThis is derived from the differentials $\\dd A = -\\gamma g^A$ and $\\dd B = -\\gamma g^B$, and the fact that $\\frac{\\partial L}{\\partial A} = g^A_{lora}$ and $\\frac{\\partial L}{\\partial B} = g^B_{lora}$.\n\nPart II.\nWe then prove that $\\dd L \\le 0$ by showing:\n\\begin{gather}\n\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F\\ge 0, \\\\\n\\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\ge 0.\n\\end{gather}\n\n1. For the first inequality, we show $(B^TB)^{-1}$ is positive definite by demonstrating $B^TB$ is positive definite, hence $(B^TB)^{-1} = UU^T$ for some matrix $U$, leading to $\\frac{1}{s^2}\\|U^Tg^A_{lora}\\|_F^2\\ge 0$.\n\n2. For the second inequality, we show $(AA^T)^{-1}$ is positive-definite and $P = I - B(B^TB)^{-1}B^T$ is a projection matrix (hence positive semi-definite), leading to $\\frac{1}{s^2}\\|V^Tg^B_{lora}U\\|_F^2 \\ge 0$ where $P = VV^T$ and $(AA^T)^{-1} = U^TU$.\n\nIn conclusion, combining these results confirms $\\dd L \\le 0$, ensuring the loss decreases with each update.", "C": "The proof of the theorem is divided into two parts.\n\nPart I.\nWe first show that the differential change in the loss function, $\\dd L$, can be expressed as:\n\\begin{equation}\n\\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\}.\n\\end{equation}\nThis is derived from the differentials $\\dd A = -\\gamma g^A$ and $\\dd B = -\\gamma g^B$, and the fact that $\\frac{\\partial L}{\\partial A} = g^A_{lora}$ and $\\frac{\\partial L}{\\partial B} = g^B_{lora}$.\n\nPart II.\nWe then prove that $\\dd L \\le 0$ by showing:\n\\begin{gather}\n\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F\\ge 0, \\\\\n\\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\ge 0.\n\\end{gather}\n\n1. For the first inequality, we show $(B^TB)^{-1}$ is positive definite by demonstrating $B^TB$ is positive definite, hence $(B^TB)^{-1} = UU^T$ for some matrix $U$, leading to $\\frac{1}{s^2}\\|U^Tg^A_{lora}\\|_F^2\\ge 0$.\n\n2. For the second inequality, we show $(AA^T)^{-1}$ is positive-definite and $P = I - B(B^TB)^{-1}B^T$ is a projection matrix (hence positive semi-definite), leading to $\\frac{1}{s^2}\\|V^Tg^B_{lora}U\\|_F^2 \\ge 0$ where $P = VV^T$ and $(AA^T)^{-1} = UU^T$.\n\nIn conclusion, combining these results confirms $\\dd L \\le 0$, ensuring the loss decreases with each update.", "D": "To prove the theorem, we demonstrate that the closed-form updates for matrices \\(A\\) and \\(B\\) guarantee a non-positive loss change \\(\\dd L \\leq 0\\), analogous to standard gradient descent.\n\n### Key Steps:\n1. **First-Order Loss Approximation**:  \n   The loss change under parameter updates \\(\\Delta A = -\\gamma g^A\\) and \\(\\Delta B = -\\gamma g^B\\) is approximated as:\n   \\[\n   \\dd L \\approx -\\gamma \\left( \\left\\langle \\frac{\\partial L}{\\partial A},\\, g^A \\right\\rangle_F + \\left\\langle \\frac{\\partial L}{\\partial B},\\, g^B \\right\\rangle_F \\right).\n   \\]\n\n2. **Preconditioned Gradients**:  \n   The update directions \\(g^A\\) and \\(g^B\\) are derived from preconditioned gradients:\n   \\[\n   g^A = \\frac{1}{s^2}(B^\\top B)^{-1} g^A_{\\text{lora}}, \\quad \n   g^B = \\frac{1}{s^2}\\left[I - B(B^\\top B)^{-1}B^\\top \\right] g^B_{\\text{lora}} (AA^\\top)^{-1}.\n   \\]\n   Here, \\(g^A_{\\text{lora}} = \\partial L / \\partial A\\) and \\(g^B_{\\text{lora}} = \\partial L / \\partial B\\).\n\n3. **Substitute into Loss Change**:  \n   Plugging \\(g^A\\) and \\(g^B\\) into \\(\\dd L\\):\n   \\[\n   \\dd L = -\\gamma \\left( \\left\\langle g^A_{\\text{lora}},\\, \\frac{1}{s^2}(B^\\top B)^{-1}g^A_{\\text{lora}} \\right\\rangle_F + \\left\\langle g^B_{\\text{lora}},\\, \\frac{1}{s^2}\\left[I - B(B^\\top B)^{-1}B^\\top \\right]g^B_{\\text{lora}}(AA^\\top)^{-1} \\right\\rangle_F \\right).\n   \\]\n\n4. **Non-Negativity of Terms**:  \n   - **First Term**: \\((B^\\top B)^{-1}\\) is positive definite (assuming \\(B\\) is full column rank), so:\n     \\[\n     \\left\\langle g^A_{\\text{lora}},\\, (B^\\top B)^{-1}g^A_{\\text{lora}} \\right\\rangle_F \\geq 0.\n     \\]\n   - **Second Term**: The operator \\(\\left[I - B(B^\\top B)^{-1}B^\\top \\right]\\) is a projection matrix (symmetric, idempotent, and positive semi-definite), and \\((AA^\\top)^{-1}\\) is positive definite (assuming \\(A\\) is full row rank). The combined term is positive semi-definite, ensuring:\n     \\[\n     \\left\\langle g^B_{\\text{lora}},\\, \\left[I - B(B^\\top B)^{-1}B^\\top \\right]g^B_{\\text{lora}}(AA^\\top)^{-1} \\right\\rangle_F \\geq 0.\n     \\]\n\n5. **Conclusion**:  \n   Both inner products are non-negative. Thus, their sum scaled by \\(-\\gamma\\) guarantees \\(\\dd L \\leq 0\\), proving the theorem.\n\n### Final Expression:\n\\[\n\\dd L = -\\gamma \\left( \\text{Non-negative Term}_A + \\text{Non-negative Term}_B \\right) \\leq 0.\n\\]\nThis ensures the loss decreases monotonically with updates, analogous to gradient descent.", "ground-truth": "C"}
{"paper": "arxiv:2407.18242", "question": "Prove the following theorem:\n\\begin{theorem}\nConsider the optimization problem,\n\\begin{equation}\n    \\min_X \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2,\n\\end{equation}\nwhere $g^A$ and $g^B$ are the optimal solutions.\nThe optimal $X$ can be determined by solving the Sylvester equation:\n\\begin{equation}\n    B^TBX + XAA^T = -\\frac{1}{s^2}(B^TB)^{-1}g^A_{lora}A^T,\n\\end{equation}\nwhich has a unique solution $X$ provided that $B^TB$ and $-AA^T$ do not have any shared eigenvalues.\n\\end{theorem}", "A": "To prove the theorem, we first establish the relationship between the optimization variables and derive the Sylvester equation from the optimality conditions.\n\n**Step 1: Define Variables and Objective Function**\nAssume \\(g^A = BX\\) and \\(g^B = XA\\), where \\(B \\in \\mathbb{R}^{m \\times n}\\), \\(X \\in \\mathbb{R}^{n \\times p}\\), and \\(A \\in \\mathbb{R}^{p \\times q}\\). The optimization problem becomes:\n\\[\n\\min_X \\|BX - g^A_{\\text{lora}}\\|_F^2 + \\|XA - g^B_{\\text{lora}}\\|_F^2\n\\]\n\n**Step 2: Compute Derivatives**\nThe Fréchet derivative of the Frobenius norm squared is linear. For the first term:\n\\[\n\\frac{\\partial}{\\partial X}\\|BX - g^A_{\\text{lora}}\\|_F^2 = 2B^T(BX - g^A_{\\text{lora}})\n\\]\nFor the second term:\n\\[\n\\frac{\\partial}{\\partial X}\\|XA - g^B_{\\text{lora}}\\|_F^2 = 2(XA - g^B_{\\text{lora}})A^T\n\\]\n\n**Step 3: Set Derivative to Zero**\nAt optimality:\n\\[\nB^T(BX - g^A_{\\text{lora}}) + (XA - g^B_{\\text{lora}})A^T = 0\n\\]\nRearrange terms:\n\\[\nB^TBX + XAA^T = B^Tg^A_{\\text{lora}} + g^B_{\\text{lora}}A^T\n\\]\n\n**Step 4: Incorporate Scaling Factor \\(s\\)**\nAssume \\(g^B_{\\text{lora}} = -\\frac{1}{s^2}(B^TB)^{-1}g^A_{\\text{lora}}A^T\\) (derived from auxiliary constraints or parameterization). Substitute into the equation:\n\\[\nB^TBX + XAA^T = -\\frac{1}{s^2}(B^TB)^{-1}g^A_{\\text{lora}}A^T\n\\]\n\n**Step 5: Verify Sylvester Equation Uniqueness**\nThe Sylvester equation \\(B^TBX + XAA^T = C\\) has a unique solution if \\(B^TB\\) and \\(-AA^T\\) share no eigenvalues. By problem stipulation, this condition holds, ensuring a unique \\(X\\).\n\n**Conclusion**\nThe optimal \\(X\\) satisfies the given Sylvester equation, and uniqueness follows from the eigenvalue condition. Thus, the theorem is proven.", "B": "\\begin{proof}\nFor simplicity, we denote $L = \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2$.\nTo solve the optimization problem, we need to satisfy the following conditions:\n\\begin{equation}\n\\frac{\\partial L}{\\partial X} = 0.\n\\end{equation}\nGiven that $g^A_{lora} = sB^Tg$ and $g^B_{lora} = sgA^T$, we derive:\n\\begin{equation}\n\\begin{aligned}\n2(g^A - g^A_{lora})A^T &- 2B^T(g^B - g^B_{lora}) = 0, \\\\\n\\Rightarrow \\quad\ng^AA^T - B^Tg^B & = g^A_{lora}A^T - B^Tg^B_{lora}, \\\\\n\\Rightarrow \\quad\nB^TBX + XAA^T & = -\\frac{1}{s}(B^TB)^{-1}g^A_{lora}A^T,\n\\end{aligned}\n\\end{equation}\nwhich is a Sylvester equation. This equation has a unique solution for $X$ if and only if $B^TB$ and $-AA^T$ have no shared eigenvalues.\n\\end{proof}", "C": "To prove the theorem, we will derive the optimal solution \\( X \\) for the given optimization problem and show that it satisfies the Sylvester equation. Here's the step-by-step proof:\n\n---\n\n**Proof:**\n\n1. **Problem Setup:**\n   The optimization problem is:\n   \\[\n   \\min_X \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2,\n   \\]\n   where \\( g^A \\) and \\( g^B \\) are the optimal solutions. \n\n2. **Optimality Conditions:**\n   The optimal \\( g^A \\) and \\( g^B \\) are given by:\n   \\[\n   g^A = g^A_{lora} + s B X A^T, \\quad g^B = g^B_{lora} + s B X A^T,\n   \\]\n   where \\( s \\) is a scalar (e.g., a learning rate or step size). Substituting these into the objective function, we get:\n   \\[\n   \\min_X \\|s B X A^T\\|_F^2 + \\|s B X A^T\\|_F^2 = 2s^2 \\|B X A^T\\|_F^2.\n   \\]\n   The problem reduces to minimizing \\( \\|B X A^T\\|_F^2 \\).\n\n3. **Gradient of the Objective:**\n   The Frobenius norm squared can be written as:\n   \\[\n   \\|B X A^T\\|_F^2 = \\text{tr}(A X^T B^T B X A^T).\n   \\]\n   Taking the derivative with respect to \\( X \\), we obtain:\n   \\[\n   \\frac{\\partial}{\\partial X} \\text{tr}(A X^T B^T B X A^T) = 2 B^T B X A^T A.\n   \\]\n   Setting the gradient to zero for optimality:\n   \\[\n   B^T B X A^T A = 0.\n   \\]\n   This is a trivial solution (\\( X = 0 \\)), which is not meaningful. Instead, we must incorporate the constraints from the original problem.\n\n4. **Incorporating Constraints:**\n   From the original problem, the optimal \\( g^A \\) and \\( g^B \\) must satisfy:\n   \\[\n   g^A = g^A_{lora} + s B X A^T, \\quad g^B = g^B_{lora} + s B X A^T.\n   \\]\n   Substituting these into the objective and expanding, we get:\n   \\[\n   \\|s B X A^T\\|_F^2 + \\|s B X A^T\\|_F^2 = 2s^2 \\|B X A^T\\|_F^2.\n   \\]\n   However, we must also account for the relationship between \\( g^A_{lora} \\) and \\( g^B_{lora} \\). Suppose \\( g^B_{lora} = -g^A_{lora} \\) (a common symmetry in such problems). Then the objective becomes:\n   \\[\n   \\min_X \\|g^A_{lora} + s B X A^T\\|_F^2 + \\|-g^A_{lora} + s B X A^T\\|_F^2.\n   \\]\n   Expanding this:\n   \\[\n   2 \\|g^A_{lora}\\|_F^2 + 2s^2 \\|B X A^T\\|_F^2.\n   \\]\n   The term \\( \\|g^A_{lora}\\|_F^2 \\) is constant, so the problem reduces to minimizing \\( \\|B X A^T\\|_F^2 \\), as before.\n\n5. **Alternative Derivation:**\n   To derive the Sylvester equation, consider the gradient of the original objective with respect to \\( X \\):\n   \\[\n   \\frac{\\partial}{\\partial X} \\left( \\|g^A_{lora} + s B X A^T\\|_F^2 + \\|g^B_{lora} + s B X A^T\\|_F^2 \\right).\n   \\]\n   Expanding the gradients:\n   \\[\n   2s B^T (g^A_{lora} + s B X A^T) A + 2s B^T (g^B_{lora} + s B X A^T) A = 0.\n   \\]\n   Simplifying:\n   \\[\n   B^T (g^A_{lora} + g^B_{lora}) A + 2s B^T B X A^T A = 0.\n   \\]\n   If \\( g^B_{lora} = -g^A_{lora} \\), this reduces to:\n   \\[\n   2s B^T B X A^T A = 0,\n   \\]\n   which again gives \\( X = 0 \\). \n\n6. **Correct Sylvester Equation:**\n   The correct Sylvester equation arises from a different formulation. Suppose the problem is:\n   \\[\n   \\min_X \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2,\n   \\]\n   where \\( g^A = B X \\) and \\( g^B = X A^T \\). Then the objective becomes:\n   \\[\n   \\|B X - g^A_{lora}\\|_F^2 + \\|X A^T - g^B_{lora}\\|_F^2.\n   \\]\n   Taking gradients:\n   \\[\n   \\frac{\\partial}{\\partial X} \\|B X - g^A_{lora}\\|_F^2 = 2 B^T (B X - g^A_{lora}),\n   \\]\n   \\[\n   \\frac{\\partial}{\\partial X} \\|X A^T - g^B_{lora}\\|_F^2 = 2 (X A^T - g^B_{lora}) A.\n   \\]\n   Setting the total gradient to zero:\n   \\[\n   B^T B X - B^T g^A_{lora} + X A^T A - g^B_{lora} A = 0.\n   \\]\n   Rearranging:\n   \\[\n   B^T B X + X A^T A = B^T g^A_{lora} + g^B_{lora} A.\n   \\]\n   This is a Sylvester equation. If \\( g^B_{lora} = -\\frac{1}{s^2} (B^T B)^{-1} g^A_{lora} A^T \\), then:\n   \\[\n   B^T B X + X A^T A = -\\frac{1}{s^2} (B^T B)^{-1} g^A_{lora} A^T A.\n   \\]\n   This matches the given Sylvester equation:\n   \\[\n   B^T B X + X A A^T = -\\frac{1}{s^2} (B^T B)^{-1} g^A_{lora} A^T.\n   \\]\n\n7. **Uniqueness of Solution:**\n   The Sylvester equation \\( B^T B X + X A A^T = C \\) has a unique solution if \\( B^T B \\) and \\( -A A^T \\) do not share any eigenvalues. This follows from the general theory of Sylvester equations.\n\n---\n\n**Conclusion:**\nThe optimal \\( X \\) satisfies the given Sylvester equation, and the solution is unique under the stated eigenvalue condition. This completes the proof. \\(\\square\\)", "D": "\\begin{proof}\nFor simplicity, we denote $L = \\|g^A - g^A_{lora}\\|_F^2 + \\|g^B - g^B_{lora}\\|_F^2$.\nTo solve the optimization problem, we need to satisfy the following conditions:\n\\begin{equation}\n\\frac{\\partial L}{\\partial X} = 0.\n\\end{equation}\nGiven that $g^A_{lora} = sB^Tg$ and $g^B_{lora} = sgA^T$, we derive:\n\\begin{equation}\n\\begin{aligned}\n2(g^A - g^A_{lora})A^T &- 2B^T(g^B - g^B_{lora}) = 0, \\\\\n\\Rightarrow \\quad\ng^AA^T - B^Tg^B & = g^A_{lora}A^T - B^Tg^B_{lora}, \\\\\n\\Rightarrow \\quad\nB^TBX + XAA^T & = -\\frac{1}{s^2}(B^TB)^{-1}g^A_{lora}A^T,\n\\end{aligned}\n\\end{equation}\nwhich is a Sylvester equation. This equation has a unique solution for $X$ if and only if $B^TB$ and $-AA^T$ have no shared eigenvalues.\n\\end{proof}", "ground-truth": "D"}
{"paper": "arxiv:2405.20304", "question": "How to derive the robust objective in equation (9) from the robust objective in equation (8). Equation (8):\n\\begin{equation}\n\\label{eq: max-min objective}\n     \\min_\\pi \\rdpol(\\pi) =\\min_\\pi \\max_{\\alpha \\in \\Delta_K}\\sum_{g=1}^{K}\\alpha_g \\Big(-\\Ebb_{(x_g,y_w,y_l)\\sim \\train_g}\\Big[\\log\\Big( \\sigma (\\beta h_{\\pi}(x_g, y_w, y_l)\\Big)\\Big]\\Big),\n\\end{equation}\nwhere $\\Delta_K$ represents the $(K-1)$-dimensional simplex of probabilities.\nEquation (9):\n\\begin{equation}\\label{eq:robust-lin-obj}\n    \\min_{\\theta\\in\\Theta}\\max_{\\alpha \\in \\Delta_K} \\sum_{g=1}^{K}\\alpha_g \\Big(-\\Ebb_{(x_g,y_w,y_l)\\sim \\train_g}\\Big[\\log\\Big(\\sigma\\big(\\beta\\langle\\phi(x,y_w)-\\phi(x,y_l),\\theta\\rangle\\big)\\Big)\\Big]\\Big).\n\\end{equation}", "A": "To derive the robust objective in equation (9) from the robust objective in equation (8), we need to make the following key observations and steps:\n\n1. **Policy Parameterization**:  \n   In equation (8), the minimization is over the policy \\(\\pi\\). In equation (9), the policy \\(\\pi\\) is parameterized by \\(\\theta \\in \\Theta\\), where \\(\\Theta\\) is the parameter space. Specifically, the policy's scoring function \\(h_{\\pi}(x, y_w, y_l)\\) is represented as a linear function in the feature space:\n   \\[\n   h_{\\pi}(x, y_w, y_l) = \\langle \\phi(x, y_w) - \\phi(x, y_l), \\theta \\rangle.\n   \\]\n   Here, \\(\\phi(x, y)\\) is a feature mapping, and \\(\\theta\\) is the parameter vector to be learned.\n\n2. **Substitution into Equation (8)**:  \n   Substitute the parameterized form of \\(h_{\\pi}(x, y_w, y_l)\\) into equation (8):\n   \\[\n   \\min_{\\theta \\in \\Theta} \\max_{\\alpha \\in \\Delta_K} \\sum_{g=1}^{K} \\alpha_g \\left( -\\Ebb_{(x_g, y_w, y_l) \\sim \\train_g} \\left[ \\log \\left( \\sigma \\left( \\beta \\langle \\phi(x_g, y_w) - \\phi(x_g, y_l), \\theta \\rangle \\right) \\right) \\right] \\right).\n   \\]\n   This substitution replaces the generic policy \\(\\pi\\) with its parameterized form \\(\\theta\\).\n\n3. **Simplification**:  \n   The expression inside the expectation is now explicitly written in terms of the feature difference \\(\\phi(x, y_w) - \\phi(x, y_l)\\) and the parameter \\(\\theta\\). The robust objective in equation (9) is thus obtained by:\n   - Minimizing over \\(\\theta \\in \\Theta\\) instead of \\(\\pi\\).\n   - Expressing the scoring function \\(h_{\\pi}(x, y_w, y_l)\\) as a linear function of \\(\\theta\\).\n\n4. **Final Form**:  \n   The resulting equation (9) is:\n   \\[\n   \\min_{\\theta \\in \\Theta} \\max_{\\alpha \\in \\Delta_K} \\sum_{g=1}^{K} \\alpha_g \\left( -\\Ebb_{(x_g, y_w, y_l) \\sim \\train_g} \\left[ \\log \\left( \\sigma \\left( \\beta \\langle \\phi(x_g, y_w) - \\phi(x_g, y_l), \\theta \\rangle \\right) \\right) \\right] \\right).\n   \\]\n   This is the robust linear objective, where the policy is now explicitly parameterized by \\(\\theta\\).\n\n### Summary:\nThe derivation involves:\n1. Parameterizing the policy \\(\\pi\\) with \\(\\theta\\) and expressing \\(h_{\\pi}(x, y_w, y_l)\\) as a linear function.\n2. Substituting this parameterized form into equation (8).\n3. Rewriting the objective to reflect the dependence on \\(\\theta\\) instead of \\(\\pi\\).\n\nThis leads to the robust linear objective in equation (9).", "B": "To derive the robust objective in equation (9) from equation (8), we need to make two key steps of parameterization and linearization:\n\n---\n\n**Step 1: Parameterize the policy-dependent term**  \nIn equation (8), the term \\( h_{\\pi}(x_g, y_w, y_l) \\) implicitly depends on the policy \\( \\pi \\). We explicitly parameterize this using a linear model with parameters \\( \\theta \\in \\Theta \\):  \n\\[\nh_{\\pi}(x, y_w, y_l) \\triangleq \\langle \\phi(x, y_w) - \\phi(x, y_l), \\theta \\rangle\n\\]\nwhere:\n- \\( \\phi(x, y) \\) is a feature mapping function  \n- \\( \\theta \\) replaces the original policy parameter \\( \\pi \\)\n\n---\n\n**Step 2: Reformulate the optimization variable**  \nThe original policy optimization \\( \\min_\\pi \\) in equation (8) now becomes a parameter optimization over \\( \\theta \\). This converts the problem to:  \n\\[\n\\min_{\\theta \\in \\Theta} \\max_{\\alpha \\in \\Delta_K} \\sum_{g=1}^K \\alpha_g \\Big(-\\Ebb_{(x_g,y_w,y_l)\\sim \\train_g}\\Big[\\log\\sigma\\big(\\beta\\langle\\phi(x,y_w)-\\phi(x,y_l),\\theta\\rangle\\big)\\Big]\\Big)\n\\]\nThis substitution is valid because:  \n1. **Expressiveness**: The linear parameterization \\( \\langle \\phi(x,y_w)-\\phi(x,y_l), \\theta \\rangle \\) can approximate general policy-dependent scores  \n2. **Duality**: The min-max structure is preserved under reparameterization  \n3. **Regularization**: The constraint \\( \\theta \\in \\Theta \\) implicitly encodes policy regularization  \n\n---\n\n**Key Observations**  \n- The log-likelihood term \\( \\log\\sigma(\\cdot) \\) maintains the pairwise comparison structure between \\( y_w \\) and \\( y_l \\)  \n- The linearization enables efficient optimization while preserving robustness through the \\( \\max_{\\alpha} \\) term  \n- The simplex constraint \\( \\alpha \\in \\Delta_K \\) ensures worst-case group robustness  \n\nThis derivation connects policy optimization (equation (8)) to parameter learning (equation (9)) while preserving the robustness guarantees of the original formulation.", "C": "To derive the robust objective in equation (9) from equation (8), we follow these steps:\n1. Start with the general policy class robust objective in equation (8).\n2. Specialize the policy $\\pi$ to a log-linear policy class, leading to the expression of $h_{\\pi}(x_g, y_w, y_l)$ in terms of $\\theta$ and $\\phi$.\n3. Simplify the expression by assuming a uniform reference policy, which allows us to remove the reference policy terms.\n4. Substitute the log-linear policy form $\\pi_{\\theta}(y|x_g) = \\frac{\\exp{f_{\\theta}(x_g,y)}}{\\sum_{y\\in \\mathcal{Y}}\\exp{f_{\\theta}(x_g,y)}}$ into the equation.\n5. Further specialize $f_{\\theta}(x_g,y)$ to be a linear function in $\\theta$, i.e., $f_{\\theta}(x_g,y) = \\theta^T \\phi(x_g,y)$.\n6. Simplify the resulting expression to obtain the final form in equation (9), which is the robust objective for the log-linear policy class.\n\nThe key steps involve specializing the general policy to a log-linear form and simplifying under the assumption of a uniform reference policy, leading to the final robust objective that depends linearly on the parameters $\\theta$ through the feature map $\\phi$.", "D": "To derive the robust objective in equation (9) from equation (8), we follow these steps:\n1. Start with the general policy class robust objective in equation (8).\n2. Specialize the policy $\\pi$ to a log-linear policy class, leading to the expression of $h_{\\pi}(x_g, y_w, y_l)$ in terms of $\\theta$ and $\\phi$.\n3. Simplify the expression by assuming a uniform reference policy, which allows us to remove the reference policy terms.\n4. Substitute the log-linear policy form $\\pi_{\\theta}(y|x_g) = \\frac{\\exp{f_{\\theta}(x_g,y)}}{\\sum_{y\\in \\mathcal{Y}}\\exp{f_{\\theta}(x_g,y)}}$ into the equation.\n5. Further specialize $f_{\\theta}(x_g,y)$ to be a linear function in $\\theta$, i.e., $f_{\\theta}(x_g,y) = \\theta^T \\phi(x_g,y)$.\n6. Simplify the resulting expression to obtain the final form in equation (9), but omit the $\\beta$ scaling factor in the inner product, leading to $\\min_{\\theta\\in\\Theta}\\max_{\\alpha \\in \\Delta_K} \\sum_{g=1}^{K}\\alpha_g \\Big(-\\Ebb_{(x_g,y_w,y_l)\\sim \\train_g}\\Big[\\log\\Big(\\sigma\\big(\\langle\\phi(x,y_w)-\\phi(x,y_l),\\theta\\rangle\\big)\\Big)\\Big]\\Big)$.\n\nThe key steps involve specializing the general policy to a log-linear form and simplifying under the assumption of a uniform reference policy, leading to the final robust objective that depends linearly on the parameters $\\theta$ through the feature map $\\phi$.", "ground-truth": "C"}
{"paper": "arxiv:2405.20304", "question": "Given the equations:\n\n1. Equation (4):\n\\begin{equation}\nr(x, y) = \\beta \\log \\frac{\\pi^{*}(y\\mid x)}{\\refpolicy(y\\mid x)} + \\beta \\log Z(x),\n\\end{equation}\nwhere $Z(x)=\\sum_y\\refpolicy(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$.\n\n2. Equation (10):\n\\begin{equation}\n\\max_{\\pi} \\min_{g\\in\\mathcal{G}}\\Ebb_{x_g\\sim \\mathcal{P}_{x_g},y\\sim\\pi(\\cdot\\mid x_g)}\\Big[r(x_g,y)-\\beta  \\mathrm{KL}\\big[\\pi(y\\mid x_g)||\\refpolicy(y\\mid x_g)\\big]\\Big].\n\\end{equation}\n\nHow to prove that the optimal policy for equation (10) is the same as the solution to the non-robust KL-regularized reward maximization problem in equation (4)?", "A": "To prove that the optimal policy for equation (10) aligns with the solution to the non-robust KL-regularized reward maximization problem in equation (4), we follow these steps:\n\n1. Recast the Objective: Start by recasting the robust KL-regularized reward maximization objective based on the max-min formulation.\n\n2. Derivation: Perform a series of transformations to express the objective in terms of the KL divergence between the policy $\\pi$ and the optimal policy $\\pi^{*}$, and the partition function $Z(x)$.\n\n3. Define Optimal Policy: Define the optimal policy $\\pi^{*}(y|x_g) = \\frac{1}{Z(x_g)}\\refpolicy(y|x_g)\\exp(\\frac{r(x_g,y)}{\\beta})$.\n\n4. Minimization: Show that the defined objective $roL(\\pi)$ is minimized by $\\pi^{*}$, by comparing it with any other policy $\\pi \\neq \\pi^{*}$ and using the non-negativity property of the KL divergence.\n\n5. Conclusion: Conclude that the optimal policy for the robust problem (equation (10)) indeed matches the form of the optimal policy for the non-robust problem (equation (4)), but with the partition function $Z(x)$ replaced by $Z(x_g)$. \n\nThis proof demonstrates that despite the robustness consideration in equation (10), the optimal policy retains the same form as in the non-robust case, highlighting the inherent connection between the two problems.", "B": "To prove that the optimal policy for equation (10) aligns with the solution to the non-robust KL-regularized reward maximization problem in equation (4), we follow these steps:\n\n1. Recast the Objective: Start by recasting the robust KL-regularized reward maximization objective based on the max-min formulation.\n\n2. Derivation: Perform a series of transformations to express the objective in terms of the KL divergence between the policy $\\pi$ and the optimal policy $\\pi^{*}$, and the partition function $Z(x)$.\n\n3. Define Optimal Policy: Define the optimal policy $\\pi^{*}(y|x_g) = \\frac{1}{Z(x_g)}\\refpolicy(y|x_g)\\exp(\\frac{r(x_g,y)}{\\beta})$.\n\n4. Minimization: Show that the defined objective $roL(\\pi)$ is minimized by $\\pi^{*}$, by comparing it with any other policy $\\pi \\neq \\pi^{*}$ and using the non-negativity property of the KL divergence.\n\n5. Conclusion: Conclude that the optimal policy for the robust problem (equation (10)) indeed matches the form of the optimal policy for the non-robust problem (equation (4)), thus proving their equivalence.\n\nThis proof demonstrates that despite the robustness consideration in equation (10), the optimal policy retains the same form as in the non-robust case, highlighting the inherent connection between the two problems.", "C": "To prove that the optimal policy for the robust optimization problem in Equation (10) is the same as the solution to the non-robust KL-regularized reward maximization problem in Equation (4), we can proceed with the following steps:\n\n### Step 1: Rewrite the Robust Optimization Problem\nThe robust optimization problem in Equation (10) is:\n\\begin{equation}\n\\max_{\\pi} \\min_{g\\in\\mathcal{G}} \\Ebb_{x_g\\sim \\mathcal{P}_{x_g}, y\\sim\\pi(\\cdot\\mid x_g)}\\Big[r(x_g, y) - \\beta \\mathrm{KL}\\big[\\pi(y\\mid x_g) \\parallel \\refpolicy(y\\mid x_g)\\big]\\Big].\n\\end{equation}\n\nThe KL divergence term can be expanded as:\n\\begin{equation}\n\\mathrm{KL}\\big[\\pi(y\\mid x_g) \\parallel \\refpolicy(y\\mid x_g)\\big] = \\Ebb_{y\\sim\\pi(\\cdot\\mid x_g)}\\Big[\\log \\pi(y\\mid x_g) - \\log \\refpolicy(y\\mid x_g)\\Big].\n\\end{equation}\n\nThus, the objective becomes:\n\\begin{equation}\n\\Ebb_{x_g\\sim \\mathcal{P}_{x_g}, y\\sim\\pi(\\cdot\\mid x_g)}\\Big[r(x_g, y) - \\beta \\log \\pi(y\\mid x_g) + \\beta \\log \\refpolicy(y\\mid x_g)\\Big].\n\\end{equation}\n\n### Step 2: Pointwise Optimization for $\\pi$\nFor a fixed $g$ and $x_g$, the inner minimization over $g$ does not affect the optimization over $\\pi$ because $\\pi$ is chosen to maximize the worst-case scenario. Thus, we can focus on optimizing $\\pi$ for a given $x$ (dropping the $g$ subscript for simplicity):\n\n\\begin{equation}\n\\max_{\\pi(\\cdot\\mid x)} \\Ebb_{y\\sim\\pi(\\cdot\\mid x)}\\Big[r(x, y) - \\beta \\log \\pi(y\\mid x) + \\beta \\log \\refpolicy(y\\mid x)\\Big].\n\\end{equation}\n\nThis is equivalent to solving the following constrained optimization problem:\n\\begin{equation}\n\\max_{\\pi(\\cdot\\mid x)} \\Ebb_{y\\sim\\pi(\\cdot\\mid x)}\\Big[r(x, y) - \\beta \\log \\pi(y\\mid x) + \\beta \\log \\refpolicy(y\\mid x)\\Big] \\quad \\text{s.t.} \\quad \\sum_y \\pi(y\\mid x) = 1.\n\\end{equation}\n\n### Step 3: Lagrangian and First-Order Conditions\nIntroduce a Lagrange multiplier $\\lambda$ for the normalization constraint:\n\\begin{equation}\n\\mathcal{L} = \\sum_y \\pi(y\\mid x)\\Big[r(x, y) - \\beta \\log \\pi(y\\mid x) + \\beta \\log \\refpolicy(y\\mid x)\\Big] + \\lambda \\left(1 - \\sum_y \\pi(y\\mid x)\\right).\n\\end{equation}\n\nTake the derivative with respect to $\\pi(y\\mid x)$ and set it to zero:\n\\begin{equation}\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi(y\\mid x)} = r(x, y) - \\beta \\log \\pi(y\\mid x) - \\beta + \\beta \\log \\refpolicy(y\\mid x) - \\lambda = 0.\n\\end{equation}\n\nSolving for $\\pi(y\\mid x)$:\n\\begin{equation}\n\\log \\pi(y\\mid x) = \\frac{r(x, y)}{\\beta} + \\log \\refpolicy(y\\mid x) - 1 - \\frac{\\lambda}{\\beta}.\n\\end{equation}\n\nExponentiating both sides:\n\\begin{equation}\n\\pi(y\\mid x) = \\refpolicy(y\\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right) \\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right).\n\\end{equation}\n\nLet $Z(x) = \\exp\\left(1 + \\frac{\\lambda}{\\beta}\\right)$, then:\n\\begin{equation}\n\\pi(y\\mid x) = \\frac{\\refpolicy(y\\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right)}{Z(x)}.\n\\end{equation}\n\n### Step 4: Solve for $Z(x)$\nThe normalization constraint $\\sum_y \\pi(y\\mid x) = 1$ implies:\n\\begin{equation}\nZ(x) = \\sum_y \\refpolicy(y\\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right).\n\\end{equation}\n\nThus, the optimal policy is:\n\\begin{equation}\n\\pi^*(y\\mid x) = \\frac{\\refpolicy(y\\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right)}{Z(x)}.\n\\end{equation}\n\n### Step 5: Relate to Equation (4)\nFrom Equation (4), we have:\n\\begin{equation}\nr(x, y) = \\beta \\log \\frac{\\pi^*(y\\mid x)}{\\refpolicy(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\nRearranging:\n\\begin{equation}\n\\pi^*(y\\mid x) = \\refpolicy(y\\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta} - \\log Z(x)\\right) = \\frac{\\refpolicy(y\\mid x) \\exp\\left(\\frac{r(x, y)}{\\beta}\\right)}{Z(x)}.\n\\end{equation}\n\nThis matches the optimal policy derived from the robust optimization problem. \n\n### Conclusion\nThe optimal policy for the robust problem (Equation (10)) is indeed the same as the solution to the non-robust KL-regularized reward maximization problem (Equation (4)). The key insight is that the worst-case optimization over $g$ does not alter the form of the optimal policy, which is determined by the trade-off between reward maximization and KL regularization.", "D": "To prove that the optimal policy for equation (10) coincides with the solution to the non-robust problem in equation (4), we analyze the structure of the max-min optimization and leverage the pointwise optimality of the policy.\n\n**Step-by-Step Proof:**\n\n1. **Objective Restructuring:**\n   The objective in equation (10) can be rewritten for a fixed policy $\\pi$ and adversarial perturbation $g$:\n   \\[\n   \\min_{g \\in \\mathcal{G}} \\Ebb_{x_g \\sim \\mathcal{P}_{x_g}} \\left[ \\Ebb_{y \\sim \\pi(\\cdot|x_g)} \\left[ r(x_g, y) \\right] - \\beta \\mathrm{KL}\\left[ \\pi(y|x_g) \\big\\| \\refpolicy(y|x_g) \\right] \\right].\n   \\]\n   For each $x_g$, the inner term $\\Ebb_{y \\sim \\pi(\\cdot|x_g)}[r(x_g,y)] - \\beta \\mathrm{KL}[\\pi||\\refpolicy]$ is maximized independently by the policy $\\pi(\\cdot|x_g)$.\n\n2. **Pointwise Optimal Policy:**\n   The non-robust optimal policy $\\pi^*(y|x_g)$ from equation (4) maximizes the per-instance objective:\n   \\[\n   \\pi^*(\\cdot|x_g) = \\arg\\max_{\\pi} \\left\\{ \\Ebb_{y \\sim \\pi}[r(x_g,y)] - \\beta \\mathrm{KL}[\\pi||\\refpolicy] \\right\\}.\n   \\]\n   The closed-form solution is:\n   \\[\n   \\pi^*(y|x_g) = \\frac{\\refpolicy(y|x_g) \\exp\\left( \\frac{1}{\\beta} r(x_g,y) \\right)}{Z(x_g)},\n   \\]\n   where $Z(x_g) = \\sum_y \\refpolicy(y|x_g) \\exp\\left( \\frac{1}{\\beta} r(x_g,y) \\right)$.\n\n3. **Adversarial Perturbation Irrelevance:**\n   Since $\\pi^*(\\cdot|x_g)$ maximizes the per-$x_g$ objective **regardless of $x_g$**, the adversarial choice of $g$ (and hence $x_g$) **does not alter the functional form of $\\pi^*$**. The adversary merely selects $x_g$ to minimize the overall expected value, but for each selected $x_g$, $\\pi^*$ remains the optimal response.\n\n4. **Interchangeability of Max-Min:**\n   Under regularity conditions (e.g., convexity of $\\mathcal{G}$ and concavity in $\\pi$), the max-min problem can be interchanged:\n   \\[\n   \\max_{\\pi} \\min_{g} \\Ebb_{x_g, y}[\\cdots] = \\min_{g} \\max_{\\pi} \\Ebb_{x_g, y}[\\cdots].\n   \\]\n   However, even without interchange, the optimal $\\pi^*$ for the inner maximization (for any $g$) is identical to equation (4). Thus, the outer minimization over $g$ does not affect the optimal policy's structure.\n\n5. **Conclusion:**\n   The optimal policy $\\pi^*$ for equation (10) must satisfy the pointwise optimality condition for **all** $x_g$, which uniquely coincides with the solution to the non-robust problem in equation (4). Hence, both objectives share the same optimal policy.\n\n**Final Answer:**\nThe optimal policy for equation (10) is identical to equation (4) because the adversarial minimization over $g$ does not alter the pointwise optimality of $\\pi^*(\\cdot|x_g)$, which maximizes the per-instance reward-KL tradeoff independently of $x_g$.", "ground-truth": "B"}
{"paper": "arxiv:2405.20304", "question": "Please derive the following gradient update expression:\n\\begin{align}\n&\n\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l))\n=\\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big)\\label{eq: gradient-breakdown}\\\\\n  &=\\alpha_{g}^{(t)}\n  \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_l)-r_{\\theta^{(t-1)}}(x_g,y_w)\\big)\\times[\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g)-\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)].\\nonumber\n\\end{align}", "A": "To derive the given gradient update expression, let's break it down step by step. We'll start from the first line and proceed to the second line.\n\n### Given:\n\\[\n\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l)) = \\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big)\n\\]\n\n### Step 1: Express \\( h_{\\pi_{\\theta}} \\) in terms of rewards\nThe function \\( h_{\\pi_{\\theta}}(x_g, y_w, y_l) \\) typically represents the difference in rewards or log-probabilities between the winning (\\( y_w \\)) and losing (\\( y_l \\)) outputs. Here, we assume:\n\\[\nh_{\\pi_{\\theta}}(x_g, y_w, y_l) = r_{\\theta}(x_g, y_w) - r_{\\theta}(x_g, y_l)\n\\]\nwhere \\( r_{\\theta}(x_g, y) \\) is the reward for output \\( y \\) given input \\( x_g \\).\n\n### Step 2: Rewrite the log-sigmoid term\nThe log-sigmoid term can be rewritten using the property of the sigmoid function \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\):\n\\[\n\\log \\sigma(\\beta h_{\\pi_{\\theta}}(x_g, y_w, y_l)) = \\log \\sigma\\Big(\\beta \\big(r_{\\theta}(x_g, y_w) - r_{\\theta}(x_g, y_l)\\big)\\Big)\n\\]\n\n### Step 3: Compute the gradient of the log-sigmoid\nThe gradient of \\( \\log \\sigma(z) \\) with respect to \\( z \\) is:\n\\[\n\\nabla_z \\log \\sigma(z) = \\sigma(-z) = 1 - \\sigma(z)\n\\]\nHowever, in this context, we are taking the gradient with respect to \\( \\theta \\), so we apply the chain rule:\n\\[\n\\nabla_{\\theta} \\log \\sigma(\\beta h_{\\pi_{\\theta}}(x_g, y_w, y_l)) = \\sigma\\Big(-\\beta \\big(r_{\\theta}(x_g, y_w) - r_{\\theta}(x_g, y_l)\\big)\\Big) \\cdot \\beta \\nabla_{\\theta} \\big(r_{\\theta}(x_g, y_w) - r_{\\theta}(x_g, y_l)\\big)\n\\]\nSimplifying the sigmoid term:\n\\[\n\\sigma\\Big(-\\beta \\big(r_{\\theta}(x_g, y_w) - r_{\\theta}(x_g, y_l)\\big)\\Big) = \\sigma\\Big(\\beta \\big(r_{\\theta}(x_g, y_l) - r_{\\theta}(x_g, y_w)\\big)\\Big)\n\\]\n\n### Step 4: Express rewards in terms of log-probabilities\nAssume the reward \\( r_{\\theta}(x_g, y) \\) is proportional to the log-probability of the policy:\n\\[\nr_{\\theta}(x_g, y) = \\log \\pi_{\\theta}(y | x_g)\n\\]\nThus:\n\\[\n\\nabla_{\\theta} \\big(r_{\\theta}(x_g, y_w) - r_{\\theta}(x_g, y_l)\\big) = \\nabla_{\\theta} \\log \\pi_{\\theta}(y_w | x_g) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l | x_g)\n\\]\n\n### Step 5: Combine the results\nSubstituting back:\n\\[\n\\nabla_{\\theta} \\log \\sigma(\\beta h_{\\pi_{\\theta}}(x_g, y_w, y_l)) = \\sigma\\Big(\\beta \\big(r_{\\theta}(x_g, y_l) - r_{\\theta}(x_g, y_w)\\big)\\Big) \\cdot \\beta \\Big(\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w | x_g) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l | x_g)\\Big)\n\\]\nFor simplicity, we can absorb \\( \\beta \\) into the scaling factor \\( \\alpha_g^{(t)} \\), or assume \\( \\beta = 1 \\), leading to:\n\\[\n\\nabla_{\\theta} \\log \\sigma(h_{\\pi_{\\theta}}(x_g, y_w, y_l)) = \\sigma\\big(r_{\\theta}(x_g, y_l) - r_{\\theta}(x_g, y_w)\\big) \\cdot \\Big(\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w | x_g) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l | x_g)\\Big)\n\\]\n\n### Final Expression:\nMultiplying by \\( \\alpha_g^{(t)} \\), we obtain the desired gradient update:\n\\[\n\\alpha_g^{(t)} \\nabla_{\\theta} l(\\pi_{\\theta^{(t-1)}}; (x_g, y_w, y_l)) = \\alpha_g^{(t)} \\sigma\\big(r_{\\theta^{(t-1)}}(x_g, y_l) - r_{\\theta^{(t-1)}}(x_g, y_w)\\big) \\times \\Big[\\nabla_{\\theta} \\log \\pi_{\\theta^{(t-1)}}(y_w | x_g) - \\nabla_{\\theta} \\log \\pi_{\\theta^{(t-1)}}(y_l | x_g)\\Big]\n\\]\n\nThis matches the given expression. The key steps involve:\n1. Expressing \\( h_{\\pi_{\\theta}} \\) as a reward difference.\n2. Computing the gradient of the log-sigmoid using the chain rule.\n3. Relating rewards to policy log-probabilities.\n4. Combining terms and simplifying.", "B": "\\begin{align*}\n&\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l))\\\\\n&= \\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big)\\\\\n&\\stackrel{(i)}{=} \\alpha_{g}^{(t)} \\frac{\\sigma'(\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))}{\\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))} \\times [\\beta h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(ii)}{=} \\beta \\alpha_{g}^{(t)} \\Big(1-\\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(iii)}{=} \\beta \\alpha_{g}^{(t)} \\sigma\\big(-\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)\\big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(iv)}{=} \\beta \\alpha_{g}^{(t)} \\sigma\\Big(\\beta\\big( \\log(\\tfrac{\\pi_{\\theta^{(t-1)}}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)})-\\log(\\tfrac{\\pi_{\\theta^{(t-1)}}(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)})\\big)\\Big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(v)}{=} \\alpha_{g}^{(t)} \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_w)-r_{\\theta^{(t-1)}}(x_g,y_l)\\big) \\times [\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g)-\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)]\n\\end{align*}", "C": "To derive the gradient update expression, we follow these steps:\n\\begin{align*}\n&\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l))\\\\\n&= \\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big)\\\\\n&\\stackrel{(i)}{=} \\alpha_{g}^{(t)} \\frac{\\sigma'(\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))}{\\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))} \\times [\\beta h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(ii)}{=} \\beta \\alpha_{g}^{(t)} \\Big(1-\\sigma (\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l))\\Big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(iii)}{=} \\beta \\alpha_{g}^{(t)} \\sigma\\big(-\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)\\big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(iv)}{=} \\beta \\alpha_{g}^{(t)} \\sigma\\Big(\\beta\\big( \\log(\\tfrac{\\pi_{\\theta^{(t-1)}}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)})-\\log(\\tfrac{\\pi_{\\theta^{(t-1)}}(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)})\\big)\\Big) \\times [h'_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)]\\\\\n&\\stackrel{(v)}{=} \\alpha_{g}^{(t)} \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_l)-r_{\\theta^{(t-1)}}(x_g,y_w)\\big) \\times [\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g)-\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)]\n\\end{align*}\nHere, (i) is obtained by applying the chain rule to the derivative of the logarithm of the sigmoid function. (ii) and (iii) utilize the property of the sigmoid function's derivative, where $\\sigma'(s) = \\sigma(s)(1-\\sigma(s)) = \\sigma(-s)$. (iv) substitutes the definition of $h_{\\pi}(x,y_w,y_l) = \\log(\\tfrac{\\pi(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}) - \\log(\\tfrac{\\pi(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)})$. Finally, (v) introduces the reward function $r_{\\theta}(x_g,y) = \\beta\\log(\\tfrac{\\pi_{\\theta}(y|x_g)}{\\pi_{\\text{ref}}(y|x_g)}) - \\beta \\log Z(x_g)$, leading to the final gradient update expression.", "D": "To derive the gradient update expression, we start with the given components and proceed step-by-step.\n\n**Given:**\n\\[\n\\alpha_g^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_g,y_w,y_l)) = \\alpha_g^{(t)}\\nabla_{\\theta} \\log\\Big( \\sigma \\big(\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_g,y_w,y_l)\\big)\\Big)\n\\]\n\n**Objective:** Show that this equals:\n\\[\n\\alpha_{g}^{(t)} \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_l)-r_{\\theta^{(t-1)}}(x_g,y_w)\\big) \\times \\big[\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g) - \\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)\\big]\n\\]\n\n---\n\n### **Step 1: Define Key Terms**\n- Let the **reward function** \\( r_{\\theta}(x,y) = \\log \\pi_{\\theta}(y|x) \\), which is the log-probability of response \\( y \\) given input \\( x \\) under policy \\( \\pi_{\\theta} \\).\n- Define the **preference score** \\( h_{\\pi_{\\theta}}(x,y_w,y_l) \\):\n\\[\nh_{\\pi_{\\theta}}(x,y_w,y_l) \\triangleq r_{\\theta}(x,y_w) - r_{\\theta}(x,y_l) = \\log\\pi_{\\theta}(y_w|x) - \\log\\pi_{\\theta}(y_l|x)\n\\]\n- Assume \\( \\beta = 1 \\) for simplicity (it scales the preference score but is often omitted in gradient derivations).\n\n---\n\n### **Step 2: Expand the Gradient**\nThe gradient of the log-sigmoid term is:\n\\[\n\\nabla_{\\theta} \\log \\sigma(h) = \\frac{\\nabla_{\\theta} \\sigma(h)}{\\sigma(h)}.\n\\]\nUsing the derivative of the sigmoid \\( \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\):\n\\[\n\\nabla_{\\theta} \\log \\sigma(h) = \\frac{\\sigma(h)(1 - \\sigma(h)) \\nabla_{\\theta} h}{\\sigma(h)} = (1 - \\sigma(h)) \\nabla_{\\theta} h.\n\\]\n\n---\n\n### **Step 3: Substitute \\( h \\) and Compute Its Gradient**\nSubstitute \\( h = r_{\\theta}(x,y_w) - r_{\\theta}(x,y_l) \\):\n\\[\n\\nabla_{\\theta} h = \\nabla_{\\theta} \\big[\\log\\pi_{\\theta}(y_w|x) - \\log\\pi_{\\theta}(y_l|x)\\big] = \\nabla_{\\theta}\\log\\pi_{\\theta}(y_w|x) - \\nabla_{\\theta}\\log\\pi_{\\theta}(y_l|x).\n\\]\n\n---\n\n### **Step 4: Combine Results**\nSubstitute \\( \\nabla_{\\theta} h \\) into the gradient expression:\n\\[\n\\nabla_{\\theta} \\log \\sigma(h) = (1 - \\sigma(h)) \\big[\\nabla_{\\theta}\\log\\pi_{\\theta}(y_w|x) - \\nabla_{\\theta}\\log\\pi_{\\theta}(y_l|x)\\big].\n\\]\nRecognize that \\( 1 - \\sigma(h) = \\sigma(-h) \\), and note that \\( -h = r_{\\theta}(x,y_l) - r_{\\theta}(x,y_w) \\):\n\\[\n1 - \\sigma(h) = \\sigma(r_{\\theta}(x,y_l) - r_{\\theta}(x,y_w)).\n\\]\nThus:\n\\[\n\\nabla_{\\theta} \\log \\sigma(h) = \\sigma(r_{\\theta}(x,y_l) - r_{\\theta}(x,y_w)) \\big[\\nabla_{\\theta}\\log\\pi_{\\theta}(y_w|x) - \\nabla_{\\theta}\\log\\pi_{\\theta}(y_l|x)\\big].\n\\]\n\n---\n\n### **Final Expression**\nMultiply by \\( \\alpha_g^{(t)} \\) to match the original equation:\n\\[\n\\alpha_g^{(t)}\\nabla_{\\theta}l(\\cdot) = \\alpha_g^{(t)} \\sigma\\big(r_{\\theta^{(t-1)}}(x_g,y_l) - r_{\\theta^{(t-1)}}(x_g,y_w)\\big) \\times \\big[\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_w|x_g) - \\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_l|x_g)\\big].\n\\]\n\n---\n\n### **Summary**\nThe gradient update arises from:\n1. Defining \\( h \\) as the difference in log-probabilities (rewards) between \\( y_w \\) and \\( y_l \\).\n2. Applying the chain rule to the log-sigmoid term.\n3. Simplifying using properties of the sigmoid function and substituting the gradient of \\( h \\).\n\nThis derivation aligns with standard practices in preference-based policy optimization (e.g., reinforcement learning from human feedback).", "ground-truth": "C"}
{"paper": "arxiv:2405.20304", "question": "Prove the proposition: Suppose that the loss $l(\\cdot;(x_g,y,y'))$ is non-negative, convex, $B_{\\nabla}-$Lipschitz continuous, and bounded by $B_l$ for all $(x_g,y,y')\\in\\gX\\oplus\\gG\\times\\gY\\times\\gY$ and $\\|\\theta\\|_2\\leq B_{\\Theta}$ for all $\\theta\\in\\Theta$ with convex $\\Theta\\subset\\R^d$. The error of the average iterate of the algorithm, i.e., $\\pi_{\\avtheta} = \\tfrac{1}{T}\\sum_{t=1}^T \\theta^t$, satisfies $\\mathbb{E}[\\rdpol(\\pi_{\\avtheta})]-\\min_{\\theta\\in\\Theta}\\rdpol(\\pi_{\\theta})= \\mathcal{O}\\big(T^{-1/2}\\big)$.", "A": "We begin by considering the saddle-point stochastic optimization problem: $\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\E[F_{g}(\\theta,\\xi)]\\Big\\}$. Here, $\\xi:=(x_g,y,y')\\sim \\sum_{g=1}^{K}\\frac{N_g}{N}\\train_g$, and we define $F_{g'}(\\theta,(x_g,y,y')):= \\frac{N}{N_g}\\mathbf{1}_{[g = g']}l(\\theta; (x_g, y,y'))$. The expectation $\\E_{\\xi}[F_{g}(\\theta,\\xi)]$ simplifies to $f_{g}(\\theta) = \\E_{\\train_{g}}[l(\\theta; (x_g, y,y'))|g=g']$.\n\nNext, we map the algorithm to the general mirror descent framework. The gradients of the objective $\\phi(\\theta,\\alpha)$ with respect to $\\theta$ and $\\alpha$ are given by $\\partial \\phi(\\theta,\\alpha)=(\\partial_\\theta \\phi(\\theta,\\alpha),-\\partial_\\alpha \\phi(\\theta,\\alpha))$. For a given $\\xi$, the stochastic subgradients for $\\phi(\\theta,\\alpha)$ are $\\partial \\Phi(\\theta,\\alpha,\\xi)=(\\partial_\\theta \\Phi(\\theta,\\alpha,\\xi),-\\partial_\\alpha \\Phi(\\theta,\\alpha,\\xi))$, which simplifies to $\\begin{bmatrix}\\frac{N\\alpha_g}{N_g}\\nabla_{\\theta} l(\\theta;(x_{g},y,y')) \\\\-\\Big(0,\\cdots,\\frac{N}{N_g}l(\\theta;(x_{g},y,y')),\\cdots,0\\Big)\\end{bmatrix}$ for $\\xi=(x_g,y,y')$.\n\nThe updates of $\\theta^t$ and $\\alpha^t$ in the algorithm can be mapped to the general mirror descent update rule: $\\zeta^{t+1}=(\\theta^{t+1},\\alpha^{t+1})=P_{(\\zeta^t)}(\\gamma^t \\partial \\Phi(\\theta^t,\\alpha^t,\\xi))$, where $P_{\\zeta}(\\nu) = \\argmin_{\\zeta' \\in (\\Theta \\times\\Delta_K)} \\left\\{ \\nu^\\top (\\zeta' - \\zeta) + V(\\zeta, \\zeta') \\right\\}$ is the prox-mapping.\n\nTo bound the error, we define the approximation error $\\epsilon_T$ as $\\max_{\\alpha \\in \\Delta_K}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\phi(\\theta,\\alpha)$, which can be bounded using the result from mirror-descent convergence for max-min problems. The error bound is given by $\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10\\frac{R_{\\theta}^2 M_{\\theta}^{2}+\\ln(K) M_{\\alpha}^{2}}{T}}$, where $M_{\\theta}^2 = B_{\\nabla}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$ and $M_{\\alpha}^2 = KB_{l}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$. The strong convexity parameters $\\lambda_{\\theta}$ and $\\lambda_{\\alpha}$ are both 1, leading to the overall error bound $\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10(\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g})\\frac{B_{\\Theta}^2 B_{\\nabla}^2+KB_{l}^2\\ln K}{T}}$.", "B": "To prove the proposition, we analyze the convergence of the average iterate $\\pi_{\\avtheta} = \\frac{1}{T}\\sum_{t=1}^T \\theta^t$ under the given conditions. The key steps are as follows:\n\n**1. Problem Setup and Assumptions:**\n- Let $\\rdpol(\\pi_\\theta) := \\mathbb{E}[l(\\theta; (x_g, y, y'))]$ be the expected loss.\n- $\\Theta \\subset \\mathbb{R}^d$ is convex and bounded: $\\|\\theta\\|_2 \\leq B_\\Theta$ for all $\\theta \\in \\Theta$.\n- $l(\\cdot; (x_g, y, y'))$ is convex, $B_\\nabla$-Lipschitz (i.e., $\\|\\nabla l(\\theta)\\|_2 \\leq B_\\nabla$), and bounded by $B_l$.\n\n**2. Online Gradient Descent (OGD) Analysis:**\nAssume the algorithm performs projected OGD with step size $\\eta$:\n$$\n\\theta^{t+1} = \\text{Proj}_\\Theta\\left(\\theta^t - \\eta g^t\\right),\n$$\nwhere $g^t$ is a (sub)gradient of $l$ at $\\theta^t$. By convexity:\n$$\nl(\\theta^t) - l(\\theta^*) \\leq \\langle g^t, \\theta^t - \\theta^* \\rangle \\quad \\forall \\theta^* \\in \\Theta.\n$$\nSumming over $t=1,\\dots,T$:\n$$\n\\sum_{t=1}^T \\left[l(\\theta^t) - l(\\theta^*)\\right] \\leq \\sum_{t=1}^T \\langle g^t, \\theta^t - \\theta^* \\rangle.\n$$\n\n**3. Bounding the Cumulative Gradient Term:**\nUsing the OGD update and the non-expansiveness of projection:\n$$\n\\|\\theta^{t+1} - \\theta^*\\|_2^2 \\leq \\|\\theta^t - \\theta^*\\|_2^2 - 2\\eta \\langle g^t, \\theta^t - \\theta^* \\rangle + \\eta^2 \\|g^t\\|_2^2.\n$$\nRearranging and summing over $t$:\n$$\n\\sum_{t=1}^T \\langle g^t, \\theta^t - \\theta^* \\rangle \\leq \\frac{\\|\\theta^1 - \\theta^*\\|_2^2}{2\\eta} + \\frac{\\eta}{2} \\sum_{t=1}^T \\|g^t\\|_2^2.\n$$\nGiven $\\|\\theta^1 - \\theta^*\\|_2 \\leq 2B_\\Theta$ (diameter of $\\Theta$) and $\\|g^t\\|_2 \\leq B_\\nabla$:\n$$\n\\sum_{t=1}^T \\langle g^t, \\theta^t - \\theta^* \\rangle \\leq \\frac{2B_\\Theta^2}{\\eta} + \\frac{\\eta T B_\\nabla^2}{2}.\n$$\n\n**4. Optimizing the Step Size:**\nChoose $\\eta = \\frac{2B_\\Theta}{B_\\nabla \\sqrt{T}}$ to minimize the bound. Substituting:\n$$\n\\sum_{t=1}^T \\left[l(\\theta^t) - l(\\theta^*)\\right] \\leq 2B_\\Theta B_\\nabla \\sqrt{T}.\n$$\nDividing by $T$ for the average iterate:\n$$\n\\mathbb{E}[\\rdpol(\\pi_{\\avtheta})] - \\rdpol(\\pi_{\\theta^*}) \\leq \\frac{2B_\\Theta B_\\nabla}{\\sqrt{T}} = \\mathcal{O}\\left(T^{-1/2}\\right).\n$$\n\n**5. Conclusion:**\nThe average iterate $\\pi_{\\avtheta}$ achieves the optimal $\\mathcal{O}(T^{-1/2})$ convergence rate under the given assumptions, matching the lower bound for convex stochastic optimization.", "C": "We begin by considering the saddle-point stochastic optimization problem: $\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\E[F_{g}(\\theta,\\xi)]\\Big\\}$. Here, $\\xi:=(x_g,y,y')\\sim \\sum_{g=1}^{K}\\frac{N_g}{N}\\train_g$, and we define $F_{g'}(\\theta,(x_g,y,y')):= \\frac{N}{N_g}\\mathbf{1}_{[g = g']}l(\\theta; (x_g, y,y'))$. The expectation $\\E_{\\xi}[F_{g}(\\theta,\\xi)]$ simplifies to $f_{g}(\\theta) = \\E_{\\train_{g}}[l(\\theta; (x_g, y,y'))|g=g']$.\n\nNext, we map the algorithm to the general mirror descent framework. The gradients of the objective $\\phi(\\theta,\\alpha)$ with respect to $\\theta$ and $\\alpha$ are given by $\\partial \\phi(\\theta,\\alpha)=(\\partial_\\theta \\phi(\\theta,\\alpha),-\\partial_\\alpha \\phi(\\theta,\\alpha))$. For a given $\\xi$, the stochastic subgradients for $\\phi(\\theta,\\alpha)$ are $\\partial \\Phi(\\theta,\\alpha,\\xi)=(\\partial_\\theta \\Phi(\\theta,\\alpha,\\xi),-\\partial_\\alpha \\Phi(\\theta,\\alpha,\\xi))$, which simplifies to $\\begin{bmatrix}\\frac{N\\alpha_g}{N_g}\\nabla_{\\theta} l(\\theta;(x_{g},y,y')) \\\\-\\Big(0,\\cdots,\\frac{N}{N_g}l(\\theta;(x_{g},y,y')),\\cdots,0\\Big)\\end{bmatrix}$ for $\\xi=(x_g,y,y')$.\n\nThe updates of $\\theta^t$ and $\\alpha^t$ in the algorithm can be mapped to the general mirror descent update rule: $\\zeta^{t+1}=(\\theta^{t+1},\\alpha^{t+1})=P_{(\\zeta^t)}(\\gamma^t \\partial \\Phi(\\theta^t,\\alpha^t,\\xi))$, where $P_{\\zeta}(\\nu) = \\argmin_{\\zeta' \\in (\\Theta \\times\\Delta_K)} \\left\\{ \\nu^\\top (\\zeta' - \\zeta) + V(\\zeta, \\zeta') \\right\\}$ is the prox-mapping.\n\nTo bound the error, we define the approximation error $\\epsilon_T$ as $\\max_{\\alpha \\in \\Delta_K}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\phi(\\theta,\\alpha)$, which can be bounded using the result from mirror-descent convergence for max-min problems. The error bound is given by $\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10\\frac{R_{\\theta}^2 M_{\\theta}^{2}+\\ln(K) M_{\\alpha}^{2}}{T}}$, where $M_{\\theta}^2 = B_{\\nabla}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$ and $M_{\\alpha}^2 = KB_{l}^2\\frac{N^2}{\\min_{g\\in \\mathcal{G}}N_g}$. The strong convexity parameters $\\lambda_{\\theta}$ and $\\lambda_{\\alpha}$ are both 1, leading to the overall error bound $\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10(\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g})\\frac{B_{\\Theta}^2 B_{\\nabla}^2+KB_{l}^2\\ln K}{T}}$.", "D": "To prove the proposition, we will analyze the convergence rate of the average iterate \\(\\pi_{\\avtheta} = \\tfrac{1}{T}\\sum_{t=1}^T \\theta^t\\) for a convex optimization problem under the given assumptions. The proof follows standard techniques in online convex optimization, particularly using the Online Gradient Descent (OGD) framework.\n\n### **Assumptions Recap:**\n1. **Loss Function:** \\(l(\\cdot; (x_g, y, y'))\\) is non-negative, convex, \\(B_{\\nabla}\\)-Lipschitz continuous, and bounded by \\(B_l\\).\n2. **Parameter Space:** \\(\\Theta \\subset \\R^d\\) is convex, and \\(\\|\\theta\\|_2 \\leq B_{\\Theta}\\) for all \\(\\theta \\in \\Theta\\).\n3. **Objective:** The expected risk \\(\\rdpol(\\pi_\\theta)\\) is convex in \\(\\theta\\).\n\n### **Proof:**\n\n#### **Step 1: Online Gradient Descent Setup**\nConsider the online gradient descent updates:\n\\[\n\\theta^{t+1} = \\text{Proj}_{\\Theta}\\left(\\theta^t - \\eta_t \\nabla l(\\theta^t; (x_g^t, y^t, y'^t))\\right),\n\\]\nwhere \\(\\text{Proj}_{\\Theta}\\) is the Euclidean projection onto \\(\\Theta\\), and \\(\\eta_t\\) is the step size.\n\n#### **Step 2: Regret Bound for OGD**\nUnder convexity and \\(B_{\\nabla}\\)-Lipschitz continuity, the regret of OGD after \\(T\\) steps with a constant step size \\(\\eta = \\frac{B_{\\Theta}}{B_{\\nabla}\\sqrt{T}}\\) is bounded by:\n\\[\n\\sum_{t=1}^T l(\\theta^t; (x_g^t, y^t, y'^t)) - \\min_{\\theta \\in \\Theta} \\sum_{t=1}^T l(\\theta; (x_g^t, y^t, y'^t)) \\leq B_{\\nabla} B_{\\Theta} \\sqrt{T}.\n\\]\nDividing by \\(T\\), we get:\n\\[\n\\frac{1}{T} \\sum_{t=1}^T l(\\theta^t; (x_g^t, y^t, y'^t)) - \\min_{\\theta \\in \\Theta} \\frac{1}{T} \\sum_{t=1}^T l(\\theta; (x_g^t, y^t, y'^t)) \\leq \\frac{B_{\\nabla} B_{\\Theta}}{\\sqrt{T}}.\n\\]\n\n#### **Step 3: Expectation over Data**\nTaking expectation over the data distribution (assuming i.i.d. samples), we have:\n\\[\n\\mathbb{E}\\left[\\frac{1}{T} \\sum_{t=1}^T l(\\theta^t; (x_g^t, y^t, y'^t))\\right] = \\mathbb{E}\\left[\\rdpol(\\pi_{\\avtheta})\\right],\n\\]\nand\n\\[\n\\min_{\\theta \\in \\Theta} \\mathbb{E}\\left[\\frac{1}{T} \\sum_{t=1}^T l(\\theta; (x_g^t, y^t, y'^t))\\right] = \\min_{\\theta \\in \\Theta} \\rdpol(\\pi_\\theta).\n\\]\nThus, the bound becomes:\n\\[\n\\mathbb{E}\\left[\\rdpol(\\pi_{\\avtheta})\\right] - \\min_{\\theta \\in \\Theta} \\rdpol(\\pi_\\theta) \\leq \\frac{B_{\\nabla} B_{\\Theta}}{\\sqrt{T}}.\n\\]\n\n#### **Step 4: Final Convergence Rate**\nThis shows that the expected suboptimality gap decays as \\(\\mathcal{O}(T^{-1/2})\\):\n\\[\n\\mathbb{E}\\left[\\rdpol(\\pi_{\\avtheta})\\right] - \\min_{\\theta \\in \\Theta} \\rdpol(\\pi_\\theta) = \\mathcal{O}\\left(T^{-1/2}\\right).\n\\]\n\n### **Conclusion:**\nUnder the given assumptions, the average iterate \\(\\pi_{\\avtheta}\\) of the online gradient descent algorithm achieves an expected excess risk of order \\(\\mathcal{O}(T^{-1/2})\\), matching the standard convergence rate for convex Lipschitz problems in stochastic optimization. \\(\\square\\)", "ground-truth": "A"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}\nAssume i) the domain $\\Omega$ satisfies the uniform exterior sphere condition and the uniform cone condition;\nii) the velocity field $\\bm{v}_t(\\bm{x})$ is Lipschitz continuous in $\\bm{x}\\in\\Omega$ (uniformly on $t$).\nThen the solution to the reflected ODE exists and is unique on $t\\in[0,1]$.\n\\end{theorem}", "A": "The reflected ODE can be viewed as a special case of reflected SDE where the diffusion coefficient is zero. Therefore, the theorem is indeed a corollary of a more general theorem with no diffusion term (i.e., diffusion coefficients are zeros). For a more rigorous derivation, under the assumptions of the domain, there exists a unique solution to the Skorokhod problem with normal reflection as long as the drift function is differentiable. To prove the existence and uniqueness of the solution to the reflected ODE, one can first use Euler discretization to find a sequence of approximate solutions whose existence and uniqueness are guaranteed. We can then prove that this sequence of approximate solutions converges to the solution of the reflected ODE problem as the stepsize goes to zero. This proof follows the proof provided for reflected SDEs.", "B": "To prove the theorem, we utilize the geometric properties of the domain and the Lipschitz continuity of the velocity field. Below is the step-by-step proof.\n\n---\n\n**Existence of Solutions**\n\n1. **Geometric Preliminaries**:  \n   The uniform exterior sphere condition ensures that for every boundary point \\(\\bm{x} \\in \\partial\\Omega\\), there exists a radius \\(r > 0\\) such that \\(\\overline{B_r(\\bm{x} - r\\bm{n}_{\\bm{x}})} \\cap \\overline{\\Omega} = \\{\\bm{x}\\}\\), where \\(\\bm{n}_{\\bm{x}}\\) is the outward normal. The uniform cone condition guarantees a fixed opening angle for admissible tangent directions at boundary points. These conditions imply \\(\\Omega\\) is uniformly prox-regular, ensuring the metric projection \\(\\pi_{\\Omega}: \\mathbb{R}^d \\to \\overline{\\Omega}\\) is single-valued and Lipschitz continuous in a neighborhood of \\(\\partial\\Omega\\).\n\n2. **Approximate Solutions**:  \n   Construct time-discretized solutions via the Euler scheme. Partition \\([0,1]\\) into intervals \\(t_k = kh\\) (\\(k = 0, \\dots, N\\)) with step size \\(h = 1/N\\). Define \\(\\bm{x}_h(t)\\) recursively:\n   \\[\n   \\bm{x}_h(t_{k+1}) = \\pi_{\\Omega}\\left(\\bm{x}_h(t_k) + h\\bm{v}_{t_k}(\\bm{x}_h(t_k))\\right),\n   \\]\n   where \\(\\pi_{\\Omega}\\) projects onto \\(\\overline{\\Omega}\\). The reflection measure \\(\\eta_h\\) accumulates the \"adjustments\" at each step:\n   \\[\n   \\eta_h(t_{k+1}) - \\eta_h(t_k) = \\bm{x}_h(t_{k+1}) - \\bm{x}_h(t_k) - h\\bm{v}_{t_k}(\\bm{x}_h(t_k)).\n   \\]\n\n3. **Convergence**:  \n   By the Arzelà-Ascoli theorem, the family \\(\\{\\bm{x}_h\\}\\) is uniformly bounded and equicontinuous (due to \\(\\bm{v}_t\\) being Lipschitz). Extract a subsequence converging uniformly to a limit \\(\\bm{x}(t)\\). The prox-regularity of \\(\\Omega\\) and Lipschitz continuity of \\(\\pi_{\\Omega}\\) ensure \\(\\eta_h\\) converges weakly to a measure \\(\\eta\\) satisfying:\n   \\[\n   \\bm{x}(t) = \\bm{x}(0) + \\int_0^t \\bm{v}_s(\\bm{x}(s)) ds + \\eta(t),\n   \\]\n   where \\(\\eta(t)\\) increases only when \\(\\bm{x}(t) \\in \\partial\\Omega\\) and \\(\\dot{\\eta}(t) \\in N_{\\Omega}(\\bm{x}(t))\\) (the normal cone).\n\n---\n\n**Uniqueness of Solutions**\n\n1. **Gronwall Argument**:  \n   Suppose \\(\\bm{x}(t)\\) and \\(\\bm{y}(t)\\) are two solutions. Their difference satisfies:\n   \\[\n   \\bm{x}(t) - \\bm{y}(t) = \\int_0^t \\left[\\bm{v}_s(\\bm{x}(s)) - \\bm{v}_s(\\bm{y}(s))\\right] ds + (\\eta_{\\bm{x}}(t) - \\eta_{\\bm{y}}(t)).\n   \\]\n   Taking norms and using the Lipschitz condition (\\(|\\bm{v}_t(\\bm{x}) - \\bm{v}_t(\\bm{y})| \\leq L|\\bm{x} - \\bm{y}|\\)):\n   \\[\n   |\\bm{x}(t) - \\bm{y}(t)| \\leq L \\int_0^t |\\bm{x}(s) - \\bm{y}(s)| ds + |\\eta_{\\bm{x}}(t) - \\eta_{\\bm{y}}(t)|.\n   \\]\n\n2. **Reflection Measure Estimate**:  \n   The uniform prox-regularity implies the normal cone \\(N_{\\Omega}(\\bm{x})\\) is hypomonotone:  \n   \\[\n   \\langle \\bm{n}_{\\bm{x}} - \\bm{n}_{\\bm{y}}, \\bm{x} - \\bm{y} \\rangle \\geq -C|\\bm{x} - \\bm{y}|^2, \\quad \\forall \\bm{n}_{\\bm{x}} \\in N_{\\Omega}(\\bm{x}), \\, \\bm{n}_{\\bm{y}} \\in N_{\\Omega}(\\bm{y}).\n   \\]\n   Integrating this inequality over \\([0,t]\\) yields:\n   \\[\n   \\int_0^t \\langle d\\eta_{\\bm{x}} - d\\eta_{\\bm{y}}, \\bm{x} - \\bm{y} \\rangle \\geq -C \\int_0^t |\\bm{x}(s) - \\bm{y}(s)|^2 ds.\n   \\]\n   Using Young's inequality and the Cauchy-Schwarz inequality:\n   \\[\n   |\\eta_{\\bm{x}}(t) - \\eta_{\\bm{y}}(t)| \\leq C \\int_0^t |\\bm{x}(s) - \\bm{y}(s)| ds.\n   \\]\n\n3. **Final Inequality**:  \n   Combining the estimates:\n   \\[\n   |\\bm{x}(t) - \\bm{y}(t)| \\leq (L + C) \\int_0^t |\\bm{x}(s) - \\bm{y}(s)| ds.\n   \\]\n   By Grönwall’s inequality, \\(|\\bm{x}(t) - \\bm{y}(t)| = 0\\) for all \\(t \\in [0,1]\\), proving uniqueness.\n\n---\n\n**Conclusion**  \nUnder the uniform exterior sphere and cone conditions on \\(\\Omega\\), and Lipschitz continuity of \\(\\bm{v}_t(\\bm{x})\\), the reflected ODE admits a unique solution on \\([0,1]\\). The proof leverages the prox-regularity-induced Lipschitz projections and Grönwall’s inequality to establish existence and uniqueness.", "C": "The reflected ODE can be viewed as a special case of reflected SDE where the diffusion coefficient is zero. Therefore, the theorem is indeed a corollary of a more general theorem with no diffusion term (i.e., diffusion coefficients are zeros). For a more rigorous derivation, under the assumptions of the domain, there exists a unique solution to the Skorokhod problem with normal reflection as long as the drift function is continuous. To prove the existence and uniqueness of the solution to the reflected ODE, one can first use Euler discretization to find a sequence of approximate solutions whose existence and uniqueness are guaranteed. We can then prove that this sequence of approximate solutions converges to the solution of the reflected ODE problem as the stepsize goes to zero. This proof follows the proof provided for reflected SDEs.", "D": "**Proof of the Theorem:**\n\nWe aim to prove the existence and uniqueness of the solution to the reflected ODE under the given conditions. The reflected ODE is of the form:\n\\[\n\\frac{d\\bm{x}_t}{dt} = \\bm{v}_t(\\bm{x}_t) - \\bm{\\gamma}_t, \\quad \\bm{x}_t \\in \\overline{\\Omega}, \\quad \\bm{\\gamma}_t \\in \\mathcal{N}_{\\overline{\\Omega}}(\\bm{x}_t),\n\\]\nwhere \\(\\mathcal{N}_{\\overline{\\Omega}}(\\bm{x}_t)\\) is the normal cone to \\(\\overline{\\Omega}\\) at \\(\\bm{x}_t\\), and \\(\\bm{\\gamma}_t\\) is the reflection term.\n\n### 1. **Existence of the Solution**\n\nThe existence follows from the theory of *Skorokhod problems* and *stochastic differential equations with reflection*. Here, we outline the key steps:\n\n- **Uniform Exterior Sphere Condition:** This ensures that the reflection term \\(\\bm{\\gamma}_t\\) is well-defined and points in a direction that keeps the solution within \\(\\overline{\\Omega}\\). Specifically, for any \\(\\bm{x} \\in \\partial\\Omega\\), there exists a uniform radius \\(r > 0\\) such that the exterior sphere \\(B(\\bm{y}, r)\\) (with \\(\\bm{y} \\notin \\overline{\\Omega}\\)) touches \\(\\bm{x}\\) and lies entirely outside \\(\\Omega\\). This guarantees the existence of a unique reflection direction.\n\n- **Uniform Cone Condition:** This implies that the domain \\(\\Omega\\) is *Lipschitz*, which is crucial for constructing solutions via approximation (e.g., penalization methods or Euler schemes). The cone condition ensures that the boundary is not \"too sharp\" and allows for a consistent definition of the reflection direction.\n\n- **Lipschitz Continuity of \\(\\bm{v}_t(\\bm{x})\\):** The velocity field \\(\\bm{v}_t(\\bm{x})\\) being Lipschitz in \\(\\bm{x}\\) (uniformly in \\(t\\)) guarantees that the ODE without reflection has a unique solution. The reflection term can then be handled using the *Skorokhod problem*, which admits a solution under the above geometric conditions (see, e.g., Lions & Sznitman, 1984).\n\nCombining these, the solution \\(\\bm{x}_t\\) exists on \\(t \\in [0,1]\\) by standard fixed-point arguments or Picard iteration in the space of continuous paths with reflection.\n\n### 2. **Uniqueness of the Solution**\n\nUniqueness follows from the Lipschitz property of \\(\\bm{v}_t(\\bm{x})\\) and the geometric conditions on \\(\\Omega\\):\n\n- Suppose \\(\\bm{x}_t^{(1)}\\) and \\(\\bm{x}_t^{(2)}\\) are two solutions. Let \\(\\bm{\\delta}_t = \\bm{x}_t^{(1)} - \\bm{x}_t^{(2)}\\). Then:\n\\[\n\\frac{d\\bm{\\delta}_t}{dt} = \\bm{v}_t(\\bm{x}_t^{(1)}) - \\bm{v}_t(\\bm{x}_t^{(2)}) - (\\bm{\\gamma}_t^{(1)} - \\bm{\\gamma}_t^{(2)}).\n\\]\n\n- By the Lipschitz condition, \\(\\|\\bm{v}_t(\\bm{x}_t^{(1)}) - \\bm{v}_t(\\bm{x}_t^{(2)})\\| \\leq L \\|\\bm{\\delta}_t\\|\\) for some \\(L > 0\\).\n\n- The reflection terms satisfy \\(\\langle \\bm{\\gamma}_t^{(i)}, \\bm{z} - \\bm{x}_t^{(i)} \\rangle \\leq 0\\) for all \\(\\bm{z} \\in \\overline{\\Omega}\\), \\(i = 1, 2\\). Using the uniform exterior sphere condition, one can show (see Dupuis & Ishii, 1991) that:\n\\[\n\\langle \\bm{\\gamma}_t^{(1)} - \\bm{\\gamma}_t^{(2)}, \\bm{x}_t^{(1)} - \\bm{x}_t^{(2)} \\rangle \\geq 0.\n\\]\n\n- Thus:\n\\[\n\\frac{d}{dt} \\|\\bm{\\delta}_t\\|^2 \\leq 2 \\langle \\bm{v}_t(\\bm{x}_t^{(1)}) - \\bm{v}_t(\\bm{x}_t^{(2)}), \\bm{\\delta}_t \\rangle \\leq 2L \\|\\bm{\\delta}_t\\|^2.\n\\]\n\n- By Gronwall's inequality, \\(\\|\\bm{\\delta}_t\\|^2 \\leq \\|\\bm{\\delta}_0\\|^2 e^{2Lt}\\). Since \\(\\bm{\\delta}_0 = 0\\), uniqueness follows: \\(\\bm{x}_t^{(1)} = \\bm{x}_t^{(2)}\\) for all \\(t \\in [0,1]\\).\n\n### Conclusion\n\nUnder the uniform exterior sphere condition, uniform cone condition, and Lipschitz continuity of \\(\\bm{v}_t(\\bm{x})\\), the reflected ODE admits a unique solution on \\(t \\in [0,1]\\). \\(\\square\\)", "ground-truth": "C"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}\nAssume that $p_t(\\bm{x})>0$ for all $\\bm{x}\\in\\Omega^o$ and $t\\in[0,1]$. Then $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$.\n\\end{theorem}", "A": "To prove the theorem, we start by expanding the squared norms of the differences between the velocity fields:\n\\begin{subequations}\n\\begin{align}\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x})\\|^2&= \\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})+\\|\\bm{v}_t(\\bm{x})\\|^2\\\\\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2&=\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)+\\|\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2\n\\end{align}\n\\end{subequations}\n\nNext, we observe that the expectation of the squared norm of $\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)$ under the joint distribution $p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)$ is equal to its expectation under the marginal distribution $p_t(\\bm{x})$:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2=\\mathbb{E}_{p_t(\\bm{x})}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2\n\\end{equation}\n\nTo complete the proof, we need to show that the expectations of the dot products are equal:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}).\n\\end{equation}\n\nBy the definition of $\\bm{v}_t(\\bm{x})$, we can express the expectation under $p_t(\\bm{x})$ as:\n\\begin{align*}\n\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})\\dif\\bm{x}\\\\\\n&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\int_\\Omega \\bm{v}_t(\\bm{x}|\\bm{x}_1)\\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\int_\\Omega\\int_\\Omega  \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\bm{v}_t(\\bm{x}|\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1).\n\\end{align*}\nThis completes the proof, showing that $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$ under the given assumptions.", "B": "To prove the theorem, we analyze the gradients of the RFM and CRFM objectives under the given conditions. Let $\\mathcal{L}_{\\text{RFM}}(\\bm{\\theta})$ and $\\mathcal{L}_{\\text{CRFM}}(\\bm{\\theta})$ be defined as path integrals over $\\Omega^o \\times [0,1]$. The key steps are as follows:\n\n1. **Express the Gradient Difference**:\n   The gradients differ by an integral involving the divergence of a vector field $\\bm{F}(\\bm{\\theta},\\bm{x},t)$:\n   \\[\n   \\nabla \\mathcal{L}_{\\text{CRFM}}(\\bm{\\theta}) - \\nabla \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}) = \\int_0^1 \\int_{\\Omega^o} \\nabla_{\\bm{\\theta}} \\left[ \\nabla_{\\bm{x}} \\cdot \\big(p_t(\\bm{x}) \\bm{F}(\\bm{\\theta},\\bm{x},t)\\big) \\right] d\\bm{x} dt\n   \\]\n   This arises from the constraint structure in CRFM.\n\n2. **Apply Integration by Parts**:\n   Use the divergence theorem to convert the volume integral to a boundary integral:\n   \\[\n   \\int_{\\Omega^o} \\nabla_{\\bm{x}} \\cdot \\big(p_t(\\bm{x}) \\bm{F}\\big) d\\bm{x} = \\int_{\\partial \\Omega} p_t(\\bm{x}) \\bm{F} \\cdot \\bm{n} dS\n   \\]\n   where $\\bm{n}$ is the outward normal vector on $\\partial\\Omega$.\n\n3. **Boundary Term Vanishes**:\n   By assumption, $p_t(\\bm{x}) > 0$ in $\\Omega^o$ but $p_t(\\bm{x})$ naturally decays to zero on $\\partial\\Omega$ (common in probability densities to ensure integrability). Hence:\n   \\[\n   \\int_{\\partial \\Omega} p_t(\\bm{x}) \\bm{F} \\cdot \\bm{n} dS = 0 \\quad \\forall t \\in [0,1].\n   \\]\n\n4. **Conclude Equality**:\n   The remaining terms in $\\nabla \\mathcal{L}_{\\text{CRFM}}$ and $\\nabla \\mathcal{L}_{\\text{RFM}}$ are identical after the boundary cancellation. Thus:\n   \\[\n   \\nabla \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\text{CRFM}}(\\bm{\\theta}).\n   \\]\n\nThis completes the proof under the given positivity condition on $p_t(\\bm{x})$.", "C": "To prove the theorem, we start by expanding the squared norms of the differences between the velocity fields:\n\\begin{subequations}\n\\begin{align}\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x})\\|^2&= \\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})+\\|\\bm{v}_t(\\bm{x})\\|^2\\\\\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2&=\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)+\\|\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2\n\\end{align}\n\\end{subequations}\n\nNext, we observe that the expectation of the squared norm of $\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)$ under the joint distribution $p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)$ is equal to its expectation under the marginal distribution $p_t(\\bm{x})$:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2=\\mathbb{E}_{p_t(\\bm{x})}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2\n\\end{equation}\n\nTo complete the proof, we need to show that the expectations of the dot products are equal:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}).\n\\end{equation}\n\nBy the definition of $\\bm{v}_t(\\bm{x})$, we can express the expectation under $p_t(\\bm{x})$ as:\n\\begin{align*}\n\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})\\dif\\bm{x}\\\\\\n&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\int_\\Omega \\bm{v}_t(\\bm{x}|\\bm{x}_1)\\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\int_\\Omega\\int_\\Omega  \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\bm{v}_t(\\bm{x}|\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1).\n\\end{align*}\nThis completes the proof, showing that $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$ under the given assumptions.", "D": "**Proof:**\n\nWe aim to show that under the given conditions, the gradients of the Risk Functional Minimization (RFM) loss \\(\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\\) and the Conditional Risk Functional Minimization (CRFM) loss \\(\\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})\\) are equal. \n\n### Definitions:\n1. **RFM Loss:**\n   \\[\n   \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\right],\n   \\]\n   where \\(\\ell(\\bm{x}; \\bm{\\theta})\\) is the loss function for input \\(\\bm{x}\\) with parameters \\(\\bm{\\theta}\\), and \\(p_t(\\bm{x})\\) is the probability density over \\(\\Omega^o\\) at time \\(t\\).\n\n2. **CRFM Loss:**\n   \\[\n   \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}) = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\mid \\bm{x} \\in \\Omega^o \\right],\n   \\]\n   where the expectation is conditioned on \\(\\bm{x}\\) being in the interior \\(\\Omega^o\\).\n\n### Gradient of RFM Loss:\n\\[\n\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\right] = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\nabla \\ell(\\bm{x}; \\bm{\\theta}) \\right].\n\\]\n\n### Gradient of CRFM Loss:\nSince \\(p_t(\\bm{x}) > 0\\) for all \\(\\bm{x} \\in \\Omega^o\\) and \\(t \\in [0,1]\\), the conditional distribution \\(p_t(\\bm{x} \\mid \\bm{x} \\in \\Omega^o)\\) is identical to \\(p_t(\\bm{x})\\) because \\(\\Omega^o\\) is the full support of \\(p_t(\\bm{x})\\). Thus:\n\\[\n\\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}) = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\mid \\bm{x} \\in \\Omega^o \\right] = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\right] = \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nTaking the gradient:\n\\[\n\\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\n\n### Conclusion:\nSince the expectations (and thus the losses) are identical under the given conditions, their gradients must also be equal:\n\\[\n\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}). \\quad \\square\n\\]\n\n**Remark:** The key assumption \\(p_t(\\bm{x}) > 0\\) for all \\(\\bm{x} \\in \\Omega^o\\) ensures that the conditional distribution does not alter the expectation, making the two losses equivalent. If \\(p_t(\\bm{x})\\) had zero mass in some region of \\(\\Omega^o\\), the conditional expectation would differ, and the gradients might not be equal.", "ground-truth": "A"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}\nLet $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\bm{x})$ be a standard Gaussian distribution. Assume the OT conditional velocity field for FM, i.e., $\\bm{\\phi}_{t}(\\bm{x}|\\bm{x}_1)=t\\bm{x}_1+(1-(1-\\sigma_{\\min})t)\\bm{x}$.\nThen\n\\begin{equation}\n\\bm{v}_t(\\bm{x})=\\frac{1}{t}\\bm{x}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x}).\n\\end{equation}\n\\end{theorem}", "A": "Let $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\cdot)$ be the standard Gaussian distribution.\nUnder the OT conditional flow, the conditional velocity field is $\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}$, and the conditional probability path is $p_{t}(\\bm{x}|\\bm{x}_1)=\\mathcal{N}(t\\bm{x}_1,(1-(1-\\sigma_{\\min})t)^2\\bm{I})$.\nDefine\n\\[\n\\bar{p}_t(\\bm{x}_1|\\bm{x}) = \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}.\n\\]\nBy the definition of $\\bm{v}_t(\\bm{x})$,\nwe have\n\\begin{align*}\n\\bm{v}_t(\\bm{x})&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\\\\n&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}\\\\\n&=\\frac{\\bm{x}}{t}-\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}-t\\bm{x}_1}{(1-(1-\\sigma_{\\min})t)^2}\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\nabla\\log p_{t}(\\bm{x}|\\bm{x}_1)\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x})\n\\end{align*}\nwhere we use the fact\n\\[\n\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})} \\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}|\\bm{x}_1)=\\int \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_{t}(\\bm{x})}\\frac{\\nabla_{\\bm{x}} p_{t}(\\bm{x}|\\bm{x}_1)}{p_{t}(\\bm{x}|\\bm{x}_1)}\\dif \\bm{x}_1=\\frac{1}{p_{t}(\\bm{x})}\\nabla_{\\bm{x}} \\int p_{t}(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1=\\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}).\n\\]\nThis finishes the proof.", "B": "To prove the theorem, we leverage the relationship between the conditional velocity field and the marginal probability flow in Flow Matching (FM). Given the conditional velocity field:\n\\[\n\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + \\big[1 - (1-\\sigma_{\\min})t\\big]\\bm{x},\n\\]\nwe derive the marginal velocity field \\(\\bm{v}_t(\\bm{x})\\) by taking the expectation over the conditional distribution \\(p_t(\\bm{x}_1|\\bm{x})\\).\n\n### Key Steps:\n1. **Express the conditional velocity**:  \n   The conditional velocity field can be rewritten as:\n   \\[\n   \\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = \\frac{\\bm{x}_1 - (1-\\sigma_{\\min})\\bm{x}}{1 - (1-\\sigma_{\\min})t}.\n   \\]\n\n2. **Marginal velocity via expectation**:  \n   The marginal velocity field is the conditional expectation:\n   \\[\n   \\bm{v}_t(\\bm{x}) = \\mathbb{E}_{p_t(\\bm{x}_1|\\bm{x})}\\big[\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\big] = \\frac{\\mathbb{E}[\\bm{x}_1|\\bm{x}] - (1-\\sigma_{\\min})\\bm{x}}{1 - (1-\\sigma_{\\min})t}.\n   \\]\n\n3. **Relate \\(\\mathbb{E}[\\bm{x}_1|\\bm{x}]\\) to the score function**:  \n   From the structure of the conditional Gaussian path \\(p_t(\\bm{x}|\\bm{x}_1) = \\mathcal{N}\\big(t\\bm{x}_1, [1 - (1-\\sigma_{\\min})t]^2\\bm{I}\\big)\\), the score function is:\n   \\[\n   \\nabla\\log p_t(\\bm{x}) = \\frac{\\mathbb{E}[\\bm{x}_1|\\bm{x}] - t\\bm{x}}{[1 - (1-\\sigma_{\\min})t]^2}.\n   \\]\n   Solving for \\(\\mathbb{E}[\\bm{x}_1|\\bm{x}]\\) gives:\n   \\[\n   \\mathbb{E}[\\bm{x}_1|\\bm{x}] = t\\bm{x} + [1 - (1-\\sigma_{\\min})t]^2\\nabla\\log p_t(\\bm{x}).\n   \\]\n\n4. **Substitute into \\(\\bm{v}_t(\\bm{x})\\)**:  \n   Plugging \\(\\mathbb{E}[\\bm{x}_1|\\bm{x}]\\) back:\n   \\[\n   \\bm{v}_t(\\bm{x}) = \\frac{1}{t}\\bm{x} + \\frac{1 - (1-\\sigma_{\\min})t}{t}\\nabla\\log p_t(\\bm{x}).\n   \\]\n\n### Conclusion:\nThe derived velocity field \\(\\bm{v}_t(\\bm{x})\\) explicitly links the deterministic drift and the score function of the marginal distribution, thereby proving the theorem.", "C": "Let $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\cdot)$ be the standard Gaussian distribution.\nUnder the OT conditional flow, the conditional velocity field is $\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}$, and the conditional probability path is $p_{t}(\\bm{x}|\\bm{x}_1)=\\mathcal{N}(t\\bm{x}_1,(1-(1-\\sigma_{\\min})t)^2\\bm{I})$.\nDefine\n\\[\n\\bar{p}_t(\\bm{x}_1|\\bm{x}) = \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}.\n\\]\nBy the definition of $\\bm{v}_t(\\bm{x})$,\nwe have\n\\begin{align*}\n\\bm{v}_t(\\bm{x})&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\\\\n&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}\\\\\n&=\\frac{\\bm{x}}{t}-\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}-t\\bm{x}_1}{(1-(1-\\sigma_{\\min})t)^2}\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\nabla\\log p_{t}(\\bm{x}|\\bm{x}_1)\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x})\n\\end{align*}\nwhere we use the fact\n\\[\n\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})} \\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}|\\bm{x}_1)=\\int \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_{t}(\\bm{x})}\\frac{\\nabla_{\\bm{x}} p_{t}(\\bm{x}|\\bm{x}_1)}{p_{t}(\\bm{x}|\\bm{x}_1)}\\dif \\bm{x}_1=\\frac{1}{p_{t}(\\bm{x})}\\nabla_{\\bm{x}} \\int p_{t}(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1=\\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}).\n\\]\nThis finishes the proof.", "D": "To prove the given theorem, we will derive the velocity field \\(\\bm{v}_t(\\bm{x})\\) from the given OT conditional velocity field \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) and the prior distribution \\(p_0(\\bm{x})\\). Here is the step-by-step derivation:\n\n### Given:\n1. The prior distribution is a standard Gaussian:\n   \\[\n   p_0(\\bm{x}) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{\\|\\bm{x}\\|^2}{2}\\right).\n   \\]\n2. The OT conditional velocity field is:\n   \\[\n   \\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t)\\bm{x}.\n   \\]\n3. The marginal distribution \\(p_t(\\bm{x})\\) is obtained by pushing \\(p_0(\\bm{x})\\) through the flow defined by \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\).\n\n### Step 1: Express the marginal distribution \\(p_t(\\bm{x})\\)\nThe flow \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) defines a deterministic transformation from \\(\\bm{x}_0\\) to \\(\\bm{x}_t\\):\n\\[\n\\bm{x}_t = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t)\\bm{x}_0.\n\\]\nSince \\(\\bm{x}_1\\) is fixed, the transformation is linear in \\(\\bm{x}_0\\). The inverse transformation is:\n\\[\n\\bm{x}_0 = \\frac{\\bm{x}_t - t\\bm{x}_1}{1 - (1 - \\sigma_{\\min})t}.\n\\]\nThe Jacobian determinant of this transformation is:\n\\[\nJ = \\left(\\frac{1}{1 - (1 - \\sigma_{\\min})t}\\right)^d.\n\\]\nThus, the marginal distribution \\(p_t(\\bm{x})\\) is:\n\\[\np_t(\\bm{x}) = p_0\\left(\\frac{\\bm{x} - t\\bm{x}_1}{1 - (1 - \\sigma_{\\min})t}\\right) \\cdot J.\n\\]\nSubstituting \\(p_0(\\bm{x})\\):\n\\[\np_t(\\bm{x}) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{\\|\\bm{x} - t\\bm{x}_1\\|^2}{2(1 - (1 - \\sigma_{\\min})t)^2}\\right) \\cdot \\left(\\frac{1}{1 - (1 - \\sigma_{\\min})t}\\right)^d.\n\\]\n\n### Step 2: Compute \\(\\nabla \\log p_t(\\bm{x})\\)\nTaking the logarithm of \\(p_t(\\bm{x})\\):\n\\[\n\\log p_t(\\bm{x}) = -\\frac{\\|\\bm{x} - t\\bm{x}_1\\|^2}{2(1 - (1 - \\sigma_{\\min})t)^2} - \\frac{d}{2} \\log(2\\pi) + d \\log\\left(\\frac{1}{1 - (1 - \\sigma_{\\min})t}\\right).\n\\]\nThe gradient is:\n\\[\n\\nabla \\log p_t(\\bm{x}) = -\\frac{\\bm{x} - t\\bm{x}_1}{(1 - (1 - \\sigma_{\\min})t)^2}.\n\\]\n\n### Step 3: Relate \\(\\bm{v}_t(\\bm{x})\\) to \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\)\nThe velocity field \\(\\bm{v}_t(\\bm{x})\\) is the conditional expectation of \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) given \\(\\bm{x}_t = \\bm{x}\\):\n\\[\n\\bm{v}_t(\\bm{x}) = \\mathbb{E}[\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) | \\bm{x}_t = \\bm{x}].\n\\]\nFrom the definition of \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\):\n\\[\n\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t)\\bm{x}_0.\n\\]\nSubstitute \\(\\bm{x}_0\\) from the inverse transformation:\n\\[\n\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t) \\cdot \\frac{\\bm{x} - t\\bm{x}_1}{1 - (1 - \\sigma_{\\min})t} = t\\bm{x}_1 + \\bm{x} - t\\bm{x}_1 = \\bm{x}.\n\\]\nThis suggests that \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) is deterministic given \\(\\bm{x}_t = \\bm{x}\\), so:\n\\[\n\\bm{v}_t(\\bm{x}) = \\bm{x}.\n\\]\nHowever, this seems inconsistent with the given formula. Let us instead derive \\(\\bm{v}_t(\\bm{x})\\) from the continuity equation.\n\n### Step 4: Derive \\(\\bm{v}_t(\\bm{x})\\) from the continuity equation\nThe continuity equation for the marginal distribution is:\n\\[\n\\frac{\\partial p_t(\\bm{x})}{\\partial t} + \\nabla \\cdot (p_t(\\bm{x}) \\bm{v}_t(\\bm{x})) = 0.\n\\]\nFrom Step 1, we can compute \\(\\frac{\\partial p_t(\\bm{x})}{\\partial t}\\):\n\\[\n\\frac{\\partial p_t(\\bm{x})}{\\partial t} = p_t(\\bm{x}) \\cdot \\left(\\frac{\\|\\bm{x} - t\\bm{x}_1\\|^2 (1 - \\sigma_{\\min})}{(1 - (1 - \\sigma_{\\min})t)^3} - \\frac{(\\bm{x} - t\\bm{x}_1) \\cdot \\bm{x}_1}{(1 - (1 - \\sigma_{\\min})t)^2} + \\frac{d(1 - \\sigma_{\\min})}{1 - (1 - \\sigma_{\\min})t}\\right).\n\\]\nThe divergence term is:\n\\[\n\\nabla \\cdot (p_t(\\bm{x}) \\bm{v}_t(\\bm{x})) = \\nabla p_t(\\bm{x}) \\cdot \\bm{v}_t(\\bm{x}) + p_t(\\bm{x}) \\nabla \\cdot \\bm{v}_t(\\bm{x}).\n\\]\nAssuming \\(\\bm{v}_t(\\bm{x})\\) is proportional to \\(\\nabla \\log p_t(\\bm{x})\\), we can match terms to find:\n\\[\n\\bm{v}_t(\\bm{x}) = \\frac{1}{t}\\bm{x} + \\frac{1 - (1 - \\sigma_{\\min})t}{t} \\nabla \\log p_t(\\bm{x}).\n\\]\n\n### Final Result:\nCombining the above steps, we have derived the velocity field as:\n\\[\n\\bm{v}_t(\\bm{x}) = \\frac{1}{t}\\bm{x} + \\frac{1 - (1 - \\sigma_{\\min})t}{t} \\nabla \\log p_t(\\bm{x}).\n\\]\nThis completes the proof. \\(\\square\\)", "ground-truth": "A"}
{"paper": "arxiv:2409.05282", "question": "Prove the following theorem:\n\\begin{theorem}\nSuppose that i) $\\forall \\tau, \\bm{l}$ and $\\bm{\\psi}$, $q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau,\\bm{l}) \\in C^1(\\mathbb{R}^{|p|})$ as a function of CPDs $p$; ii) $G(\\phi)$ is $L_G$-Lipschitz continuous and with probability one, $\\hat{G}^{(h,t)}_R(\\phi)$ is $L_G$-Lipschitz continuous; ii) $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2=0$ and $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\bm{\\psi}^{(h,t)}-\\bm{\\psi}^\\ast||^2=0$ for some point $\\bm{\\phi}^\\ast$ and $\\bm{\\psi}^\\ast$.\nThen \n\\begin{enumerate}\n    \\item $\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})\\overset{\\mathrm{a.s.}}{\\longrightarrow} G(\\bm{\\phi}^{(h,t)})$ as $R,F\\to\\infty$;\n    \\item $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2\\overset{\\circ}{\\approx}O(1/F)$.\n\\end{enumerate}\n\\end{theorem}", "A": "We first prove the strong consistency of the RWSVR estimator. By the strong law of large numbers, we have\n\\[\n\\lim_{R\\to\\infty}\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}, \\bm{\\psi}) = \\E_{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\frac{p(\\tau,\\bm{l},Y)}{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}=p(Y),\n\\]\n\\[\n\\lim_{R\\to\\infty}\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}, \\bm{\\psi}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)=\\E_{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\frac{p(\\tau,\\bm{l},Y)}{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)=p(Y)\\E_{p({\\tau,\\bm{l}|Y})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau).\n\\]\nTherefore,\n\\[\n\\lim_{R\\to\\infty}\\hat{G}^{(h,t)}_{R}(\\tilde{\\bm{\\phi}}) =\\lim_{R\\to\\infty} \\frac{\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)}{\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})}=\\E_{p({\\tau,\\bm{l}|Y})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)=G(\\tilde{\\bm{\\phi}}).\n\\]\n\nWe then estimate the order of the variance of the RWSVR estimator. We have the following estimate for the mean squared error (MSE) of $\\hat{G}^{(h,t)}_{F}(\\tilde{\\bm{\\phi}})$:\n\\begin{align*}\n\\mathbb{E}||\\hat{G}^{(h,t)}_{F}(\\tilde{\\bm{\\phi}})-G(\\tilde{\\bm{\\phi}})||^2&=\\E\\left(\\frac{\\frac{1}{F}\\sum_{i=1}^{F} w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)}{\\frac{1}{F}\\sum_{j=1}^{F} w^j(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})}-G(\\tilde{\\bm{\\phi}})\\right)^2\\\\\n&\\approx \\frac{1}{F}\\frac{\\E_{q_{\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)}}(\\tau,\\bm{l})}\\left(w(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})\\left(\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)-G(\\tilde{\\bm{\\phi}})\\right)\\right)^2}{\\E_{q_{\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)}}(\\tau,\\bm{l})}\\left(w(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})\\right)^2}\\\\\n&=:\\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F}\n\\end{align*}\nwhere the $\\approx$ refers to asymptotic equivalence as $F\\to\\infty$.\nAs the algorithm converges, i.e. $(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})\\to(\\bm{\\phi}^\\ast, \\bm{\\psi}^\\ast)$ for some $\\bm{\\phi}^\\ast, \\bm{\\psi}^\\ast$, the variance of $\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)})$ vanishes since it converges to 0 almost surely with $R$ fixed. Hence\n\\begin{align*}\n&\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\\\\n=& \\mathbb{E}||\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)}) + \\hat{G}^{(h,0)}_{F}(\\bm{\\phi}^{(h,0)})- G(\\bm{\\phi}^{(h,0)})+G(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,t)})||^2\\\\\n\\leq & 2\\mathbb{E}||\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)})||^2+2\\mathbb{E}||\\hat{G}^{(h,0)}_{F}(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,0)})||^2+2||G(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,t)})||^2\\\\\n\\overset{\\circ}{\\leq } & 4L_G^2 \\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^{(h,0)}||^2 + 2\\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F},\n\\end{align*}\nbecause $G$ is $L_G$-Lipschitz continuous and with probability one $\\hat{G}^{(h,t)}$ is $L_G$-Lipschitz continuous for all $h$ and $t$.\nUsing the fact\n$$\\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^{(h,0)}||^2\\leq \\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2+\\lim_{h\\to\\infty}\\mathbb{E}||\\bm{\\phi}^{(h,0)}-\\bm{\\phi}^\\ast||^2= 0$$\nbecause of the assumption $\\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2=0$,\nTherefore, we conclude that\n$$\\lim_{h\\to\\infty}\\sup_t \\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\overset{\\circ}{\\leq} 2\\lim_{h\\to\\infty}\\sup_t \\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F} = O(1/\\sqrt{F})$$as the algorithm converges.", "B": "We will prove the two parts of the theorem separately.\n\n### Part 1: Almost Sure Convergence of $\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})$\n\n**Given:**\n1. The function $q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau,\\bm{l})$ is continuously differentiable with respect to the CPDs $p$.\n2. $G(\\phi)$ is $L_G$-Lipschitz continuous, and with probability one, $\\hat{G}^{(h,t)}_R(\\phi)$ is also $L_G$-Lipschitz continuous.\n3. $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2=0$ and $\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\bm{\\psi}^{(h,t)}-\\bm{\\psi}^\\ast||^2=0$ for some fixed points $\\bm{\\phi}^\\ast$ and $\\bm{\\psi}^\\ast$.\n\n**To Show:**\n$\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})\\overset{\\mathrm{a.s.}}{\\longrightarrow} G(\\bm{\\phi}^{(h,t)})$ as $R,F\\to\\infty$.\n\n**Proof:**\n1. The RWSVR estimator $\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})$ is constructed using samples from $q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau,\\bm{l})$. By the given condition (i), $q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau,\\bm{l})$ is smooth in $p$, which ensures that the gradient estimates are well-behaved.\n\n2. The Lipschitz continuity of $G(\\phi)$ and $\\hat{G}^{(h,t)}_R(\\phi)$ (condition ii) implies that the estimator is stable and does not exhibit wild fluctuations. This property is crucial for the convergence of the estimator.\n\n3. The convergence of $\\bm{\\phi}^{(h,t)}$ and $\\bm{\\psi}^{(h,t)}$ to $\\bm{\\phi}^\\ast$ and $\\bm{\\psi}^\\ast$ (condition iii) ensures that the sampling distribution $q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau,\\bm{l})$ stabilizes as $h \\to \\infty$. This, combined with the increasing number of samples $R, F \\to \\infty$, guarantees that the Monte Carlo approximations in $\\hat{G}_{\\mathrm{rwsvr}}$ become increasingly accurate.\n\n4. By the Strong Law of Large Numbers (SLLN), as $R, F \\to \\infty$, the sample averages in $\\hat{G}_{\\mathrm{rwsvr}}$ converge almost surely to their expected values. The control variates and importance weights in RWSVR ensure that the variance is reduced, leading to the almost sure convergence of the estimator to $G(\\bm{\\phi}^{(h,t)})$.\n\nThus, we have:\n\\[\n\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)}) \\overset{\\mathrm{a.s.}}{\\longrightarrow} G(\\bm{\\phi}^{(h,t)}) \\quad \\text{as} \\quad R, F \\to \\infty.\n\\]\n\n### Part 2: Mean-Squared Error Bound\n\n**To Show:**\n\\[\n\\lim_{h\\to\\infty} \\sup_t\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\overset{\\circ}{\\approx} O(1/F).\n\\]\n\n**Proof:**\n1. The RWSVR estimator uses $F$ samples for the primary gradient estimate and control variates. The error $\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)}) - G(\\bm{\\phi}^{(h,t)})$ can be decomposed into bias and variance terms.\n\n2. The bias is controlled by the convergence of $\\bm{\\phi}^{(h,t)}$ and $\\bm{\\psi}^{(h,t)}$ to their limits (condition iii), ensuring that the sampling distribution is asymptotically correct. The bias vanishes as $h \\to \\infty$.\n\n3. The variance is primarily determined by the number of samples $F$. The use of control variates in RWSVR ensures that the variance is $O(1/F)$, as the control variates effectively reduce the variance of the estimator.\n\n4. The Lipschitz continuity (condition ii) ensures that the error propagation is bounded, and the smoothness (condition i) guarantees that higher-order terms do not dominate.\n\nCombining these observations, the mean-squared error is dominated by the variance term, leading to:\n\\[\n\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\overset{\\circ}{\\approx} O(1/F).\n\\]\nThe $\\sup_t$ and $\\lim_{h\\to\\infty}$ ensure that this bound holds uniformly across iterations and as the parameters converge.\n\n### Conclusion\nBoth parts of the theorem are proven. The RWSVR estimator converges almost surely to the true gradient, and its mean-squared error decays as $O(1/F)$ with the number of samples.", "C": "We first prove the strong consistency of the RWSVR estimator. By the strong law of large numbers, we have\n\\[\n\\lim_{R\\to\\infty}\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}, \\bm{\\psi}) = \\E_{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\frac{p(\\tau,\\bm{l},Y)}{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}=p(Y),\n\\]\n\\[\n\\lim_{R\\to\\infty}\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}, \\bm{\\psi}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)=\\E_{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\frac{p(\\tau,\\bm{l},Y)}{q_{\\bm{\\phi},\\bm{\\psi}}(\\tau,\\bm{l})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)=p(Y)\\E_{p({\\tau,\\bm{l}|Y})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau).\n\\]\nTherefore,\n\\[\n\\lim_{R\\to\\infty}\\hat{G}^{(h,t)}_{R}(\\tilde{\\bm{\\phi}}) =\\lim_{R\\to\\infty} \\frac{\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)}{\\frac{1}{R}\\sum_{i=1}^R w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})}=\\E_{p({\\tau,\\bm{l}|Y})}\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)=G(\\tilde{\\bm{\\phi}}).\n\\]\n\nWe then estimate the order of the variance of the RWSVR estimator. We have the following estimate for the mean squared error (MSE) of $\\hat{G}^{(h,t)}_{F}(\\tilde{\\bm{\\phi}})$:\n\\begin{align*}\n\\mathbb{E}||\\hat{G}^{(h,t)}_{F}(\\tilde{\\bm{\\phi}})-G(\\tilde{\\bm{\\phi}})||^2&=\\E\\left(\\frac{\\frac{1}{F}\\sum_{i=1}^{F} w^i(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)}) \\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau^i)}{\\frac{1}{F}\\sum_{j=1}^{F} w^j(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})}-G(\\tilde{\\bm{\\phi}})\\right)^2\\\\\n&\\approx \\frac{1}{F}\\frac{\\E_{q_{\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)}}(\\tau,\\bm{l})}\\left(w(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})\\left(\\nabla_{\\bm{\\phi}}\\log q_{\\tilde{\\bm{\\phi}}}(\\tau)-G(\\tilde{\\bm{\\phi}})\\right)\\right)^2}{\\E_{q_{\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)}}(\\tau,\\bm{l})}\\left(w(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})\\right)^2}\\\\\n&=:\\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F}\n\\end{align*}\nwhere the $\\approx$ refers to asymptotic equivalence as $F\\to\\infty$.\nAs the algorithm converges, i.e. $(\\bm{\\phi}^{(h,t)}, \\bm{\\psi}^{(h,t)})\\to(\\bm{\\phi}^\\ast, \\bm{\\psi}^\\ast)$ for some $\\bm{\\phi}^\\ast, \\bm{\\psi}^\\ast$, the variance of $\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)})$ vanishes since it converges to 0 almost surely with $R$ fixed. Hence\n\\begin{align*}\n&\\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\\\\n=& \\mathbb{E}||\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)}) + \\hat{G}^{(h,0)}_{F}(\\bm{\\phi}^{(h,0)})- G(\\bm{\\phi}^{(h,0)})+G(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,t)})||^2\\\\\n\\leq & 2\\mathbb{E}||\\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,t)}) - \\hat{G}^{(h,t)}_R(\\bm{\\phi}^{(h,0)})||^2+2\\mathbb{E}||\\hat{G}^{(h,0)}_{F}(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,0)})||^2+2||G(\\bm{\\phi}^{(h,0)})-G(\\bm{\\phi}^{(h,t)})||^2\\\\\n\\overset{\\circ}{\\leq } & 4L_G^2 \\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^{(h,0)}||^2 + 2\\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi}^{(h,t)})}{F},\n\\end{align*}\nbecause $G$ is $L_G$-Lipschitz continuous and with probability one $\\hat{G}^{(h,t)}$ is $L_G$-Lipschitz continuous for all $h$ and $t$.\nUsing the fact\n$$\\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^{(h,0)}||^2\\leq \\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2+\\lim_{h\\to\\infty}\\mathbb{E}||\\bm{\\phi}^{(h,0)}-\\bm{\\phi}^\\ast||^2= 0$$\nbecause of the assumption $\\lim_{h\\to\\infty}\\sup_t\\mathbb{E}||\\bm{\\phi}^{(h,t)}-\\bm{\\phi}^\\ast||^2=0$,\nTherefore, we conclude that\n$$\\lim_{h\\to\\infty}\\sup_t \\mathbb{E}||\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)})-G(\\bm{\\phi}^{(h,t)})||^2 \\overset{\\circ}{\\leq} 2\\lim_{h\\to\\infty}\\sup_t \\frac{\\sigma^2(\\bm{\\phi}^{(h,t)},\\bm{\\psi} = O(1/F)$$as the algorithm converges.", "D": "To prove the theorem, we address each conclusion under the given assumptions.\n\n**Proof of Conclusion 1:**  \nUnder assumption (i), \\( q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau, \\bm{l}) \\) is continuously differentiable in \\( \\bm{p} \\), ensuring smooth gradient estimates. Assumption (ii) guarantees that both \\( G(\\bm{\\phi}) \\) and its estimator \\( \\hat{G}^{(h,t)}_R(\\bm{\\phi}) \\) are \\( L_G \\)-Lipschitz, controlling gradient variations. \n\nAs \\( R, F \\to \\infty \\), the estimator \\( \\hat{G}_{\\mathrm{rwsvr}} \\) employs **R** samples for baseline estimation and **F** samples for variance reduction (e.g., via control variates). By the Strong Law of Large Numbers (SLLN), the sample average converges almost surely (a.s.) to the expectation:\n\\[\n\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)}) = \\frac{1}{F}\\sum_{f=1}^F \\left[ \\nabla_\\phi \\log q_{\\bm{\\phi}, \\bm{\\psi}}(\\tau^{(f)}, \\bm{l}^{(f)}) \\right] + \\text{(variance-reduced terms)} \\overset{\\mathrm{a.s.}}{\\longrightarrow} \\mathbb{E}[\\nabla_\\phi \\log q] = G(\\bm{\\phi}^{(h,t)}).\n\\]\nThe variance-reduced terms vanish as \\( F \\to \\infty \\) due to averaging. Combined with parameter convergence in (iii), the bias from \\( \\bm{\\phi}^{(h,t)} \\) and \\( \\bm{\\psi}^{(h,t)} \\) diminishes, ensuring **a.s. convergence**.\n\n**Proof of Conclusion 2:**  \nThe mean squared error (MSE) of \\( \\hat{G}_{\\mathrm{rwsvr}} \\) is dominated by its variance under Lipschitz continuity (assumption ii). For variance-reduced estimators (e.g., SVRG-type), the variance scales as \\( O(1/F) \\):\n\\[\n\\mathbb{E}\\left\\|\\hat{G}_{\\mathrm{rwsvr}} - G\\right\\|^2 \\leq \\underbrace{\\mathbb{E}\\left\\|\\hat{G}_{\\mathrm{rwsvr}} - \\mathbb{E}[\\hat{G}_{\\mathrm{rwsvr}}]\\right\\|^2}_{\\text{Variance}} + \\underbrace{\\left\\|\\mathbb{E}[\\hat{G}_{\\mathrm{rwsvr}}] - G\\right\\|^2}_{\\text{Bias}}.\n\\]\nThe bias term vanishes as \\( h \\to \\infty \\) due to parameter convergence (iii). The variance term, driven by **F** samples, satisfies:\n\\[\n\\mathbb{E}\\left\\|\\hat{G}_{\\mathrm{rwsvr}} - \\mathbb{E}[\\hat{G}_{\\mathrm{rwsvr}}]\\right\\|^2 \\leq \\frac{C}{F},\n\\]\nwhere \\( C > 0 \\) depends on \\( L_G \\). Thus, \n\\[\n\\lim_{h\\to\\infty} \\sup_t \\mathbb{E}\\left\\|\\hat{G}_{\\mathrm{rwsvr}}(\\bm{\\phi}^{(h,t)}) - G(\\bm{\\phi}^{(h,t)})\\right\\|^2 \\leq O\\left(\\frac{1}{F}\\right).\n\\]\nThis completes the proof. \\(\\square\\)", "ground-truth": "C"}
{"paper": "arxiv:2404.12358", "question": "Prove the following lemma: \\begin{lemma} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the token MDP. \\end{lemma}", "A": "To prove the bijection between reward functions \\( r(\\mathbf{s}_t, \\mathbf{a}_t) \\) and optimal Q-functions \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\) in the token MDP, we proceed as follows:\n\n### 1. **Forward Direction: From Reward to Optimal Q-Function**\nGiven a reward function \\( r(\\mathbf{s}_t, \\mathbf{a}_t) \\), the optimal Q-function \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\) is uniquely determined by the Bellman optimality equation:\n\\[\nQ^*(\\mathbf{s}_t, \\mathbf{a}_t) = r(\\mathbf{s}_t, \\mathbf{a}_t) + \\gamma \\mathbb{E}_{\\mathbf{s}_{t+1}} \\left[ \\max_{\\mathbf{a}_{t+1}} Q^*(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}) \\right].\n\\]\nUnder mild assumptions (e.g., bounded rewards and finite MDP), this equation has a unique solution \\( Q^* \\). Thus, \\( r \\) maps to a unique \\( Q^* \\).\n\n### 2. **Backward Direction: From Optimal Q-Function to Reward**\nGiven an optimal Q-function \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\), the corresponding reward function can be recovered as:\n\\[\nr(\\mathbf{s}_t, \\mathbf{a}_t) = Q^*(\\mathbf{s}_t, \\mathbf{a}_t) - \\gamma \\mathbb{E}_{\\mathbf{s}_{t+1}} \\left[ \\max_{\\mathbf{a}_{t+1}} Q^*(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}) \\right].\n\\]\nThis uniquely determines \\( r \\) from \\( Q^* \\).\n\n### 3. **Bijection Verification**\n- **Injectivity**: If two reward functions \\( r_1 \\) and \\( r_2 \\) lead to the same \\( Q^* \\), then:\n  \\[\n  r_1(\\mathbf{s}_t, \\mathbf{a}_t) - r_2(\\mathbf{s}_t, \\mathbf{a}_t) = \\gamma \\mathbb{E}_{\\mathbf{s}_{t+1}} \\left[ \\max_{\\mathbf{a}_{t+1}} Q^*(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}) - \\max_{\\mathbf{a}_{t+1}} Q^*(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}) \\right] = 0.\n  \\]\n  Thus, \\( r_1 = r_2 \\), proving injectivity.\n\n- **Surjectivity**: For any \\( Q^* \\), the constructed \\( r \\) as above satisfies the Bellman equation, ensuring surjectivity.\n\n### Conclusion\nThe mapping between \\( r(\\mathbf{s}_t, \\mathbf{a}_t) \\) and \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\) is bijective under mild assumptions. \\(\\square\\)", "B": "Let $Q^*_r$ denote the optimal $Q$-function for reward $r$. We prove the statement directly, starting with the injective case. \\\\\n\nAssume there exists a reward function $r' \\ne r$ such that $Q^*_{r'} = Q^*_{r}$. Then, there must exist a state action pair such that $r'(\\bs_t,\\ba_t) \\ne r(\\bs_t,\\ba_t)$. In fact, proceeding backwards from a leaf node (terminal state), there must be a \\textit{first} state action pair $(\\bs_t,\\ba_t)$ where $r'(\\bs_t,\\ba_t) \\ne r(\\bs_t,\\ba_t)$. \\\\\n\nThe $Q$ functions at this location are \\\\\n\\begin{equation*}\nQ^*_{r'}(\\bs_t,\\ba_t) = r'(\\bs_t,\\ba_t) + V_{r'}^*(\\bs_{t+1}), \\quad Q^*_{r}(\\bs_t,\\ba_t) = r(\\bs_t,\\ba_t) + V_{r}^*(\\bs_{t+1})\n\\end{equation*} \\\\\n\nBy the fact that this was the \\textit{first} location where the reward functions differed starting from a leaf node, we must have that $V_{r'}^*(\\bs_{t+1}) = V_{r}^*(\\bs_{t+1})$. This is because the rewards in all possible future states from $s,a$ are equal by virtue of this being the location of the first difference and thus the dynamic programming solution up to this point is the same. Thus, we can see that $Q^*_{r'}(\\bs_t,\\ba_t) \\ne Q^*_{r}(\\bs_t,\\ba_t)$, completing this direction. Note that this proof does not hold in general MDPs, only the token MDP where it is impossible to return to the same state after taking any number of actions. \\\\\n\nThe surjective direction is easier. For all $Q^*$, we can compute a reward function $r(\\bs_t,\\ba_t) = Q^*(\\bs_t,\\ba_t) - V^*(\\bs_{t+1})$ under deterministic dynamics. Thus, we can see that the mapping is surjective.", "C": "To establish a bijection between reward functions \\( r(\\bs_t, \\ba_t) \\) and optimal Q-functions \\( Q^*(\\bs_t, \\ba_t) \\) in the token MDP, we proceed as follows:\n\n---\n\n**Proof:**\n\n**Assumptions:**\n1. The token MDP is defined with a discount factor \\( \\gamma \\in [0, 1) \\).\n2. The state and action spaces are finite or satisfy measurability conditions.\n3. All reward functions \\( r(\\bs_t, \\ba_t) \\) are bounded.\n4. The state transition dynamics are fixed and known.\n\n**Step 1: Injectivity (Unique \\( Q^* \\) for each \\( r \\))**  \nThe optimal Q-function satisfies the Bellman optimality equation:\n\\[\nQ^*(\\bs_t, \\ba_t) = r(\\bs_t, \\ba_t) + \\gamma \\mathbb{E}_{\\bs_{t+1} \\sim P(\\cdot|\\bs_t, \\ba_t)} \\left[ \\max_{\\ba_{t+1}} Q^*(\\bs_{t+1}, \\ba_{t+1}) \\right].\n\\]\nUnder the assumptions (particularly \\( \\gamma < 1 \\)), the Bellman operator is a contraction mapping on the space of bounded Q-functions. By Banach's fixed-point theorem, this guarantees a unique solution \\( Q^* \\) for any given \\( r \\). Thus, the mapping \\( r \\mapsto Q^* \\) is injective.\n\n**Step 2: Surjectivity (Every \\( Q^* \\) corresponds to some \\( r \\))**  \nGiven an optimal Q-function \\( Q^* \\), define a reward function:\n\\[\nr(\\bs_t, \\ba_t) = Q^*(\\bs_t, \\ba_t) - \\gamma \\mathbb{E}_{\\bs_{t+1} \\sim P(\\cdot|\\bs_t, \\ba_t)} \\left[ \\max_{\\ba_{t+1}} Q^*(\\bs_{t+1}, \\ba_{t+1}) \\right].\n\\]\nSubstituting this \\( r \\) into the Bellman equation recovers \\( Q^* \\), confirming that \\( Q^* \\) is indeed the optimal Q-function for \\( r \\). Since this construction works for any valid \\( Q^* \\), the mapping \\( Q^* \\mapsto r \\) is surjective.\n\n**Step 3: Bijection**  \nThe mappings \\( r \\mapsto Q^* \\) and \\( Q^* \\mapsto r \\) are mutual inverses:\n- Starting with \\( r \\), compute \\( Q^* \\) via the Bellman equation, then recover \\( r \\) using the definition above.\n- Starting with \\( Q^* \\), compute \\( r \\) as defined, then solve the Bellman equation to retrieve \\( Q^* \\).\n\nThus, there exists a bijection between \\( r(\\bs_t, \\ba_t) \\) and \\( Q^*(\\bs_t, \\ba_t) \\) in the token MDP under the stated assumptions. \\(\\square\\)\n\n---\n\n**Note:** The \"mild assumptions\" ensure the existence, uniqueness, and invertibility required for the bijection. Critical conditions include the contraction property of the Bellman operator (enabled by \\( \\gamma < 1 \\)) and the determinism of the reward reconstruction from \\( Q^* \\).", "D": "Let $Q^*_r$ denote the optimal $Q$-function for reward $r$. We prove the statement directly, starting with the injective case. \\\\\n\nAssume there exists a reward function $r' \\ne r$ such that $Q^*_{r'} = Q^*_{r}$. Then, there must exist a state action pair such that $r'(\\bs_t,\\ba_t) \\ne r(\\bs_t,\\ba_t)$. In fact, proceeding backwards from a leaf node (terminal state), there must be a \\textit{first} state action pair $(\\bs_t,\\ba_t)$ where $r'(\\bs_t,\\ba_t) \\ne r(\\bs_t,\\ba_t)$. \\\\\n\nThe $Q$ functions at this location are \\\\\n\\begin{equation*}\nQ^*_{r'}(\\bs_t,\\ba_t) = r'(\\bs_t,\\ba_t) + V_{r'}^*(\\bs_{t+1}), \\quad Q^*_{r}(\\bs_t,\\ba_t) = r(\\bs_t,\\ba_t) + V_{r}^*(\\bs_{t+1})\n\\end{equation*} \\\\\n\nBy the fact that this was the \\textit{first} location where the reward functions differed starting from a leaf node, we must have that $V_{r'}^*(\\bs_{t+1}) = V_{r}^*(\\bs_{t+1})$. This is because the rewards in all possible future states from $s,a$ are equal by virtue of this being the location of the first difference and thus the dynamic programming solution up to this point is the same. Thus, we can see that $Q^*_{r'}(\\bs_t,\\ba_t) \\ne Q^*_{r}(\\bs_t,\\ba_t)$, completing this direction. Note that this proof does not hold in general MDPs, only the token MDP where it is impossible to return to the same state after taking any number of actions. \\\\\n\nThe surjective direction is easier. For all $Q^*$, we can compute a reward function $r(\\bs_t,\\ba_t) = Q^*(\\bs_t,\\ba_t) - V^*(\\bs_{t})$. Thus, we can see that the mapping is surjective.", "ground-truth": "B"}
{"paper": "arxiv:2404.12358", "question": "Extend the following lemma to diffusion MDPs.\n\\begin{lemma} \\label{lemma:r_to_q} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the token MDP.\n\\end{lemma}", "A": "\\begin{lemma} \\label{lemma:r_to_q_diffusion} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the diffusion MDP.\n\\end{lemma}\n\\begin{proof}\nSince the MDP still has deterministic dynamics, we have that Eq. \\ref{eq:policy}-\\ref{eq:critic} still hold. Now, given a reference policy $\\piref$, parameter $\\beta$ and a critic $Q$, we can trivially recover the unique reward function by inverting Eq. \\ref{eq:critic}. We will prove that given a reward function $r(\\bs_t, \\ba_t)$, we can recover a unique critic $Q$. We work inductively in the diffusion MDP starting with $t=T$, where we have $V^*(\\bs_T)=0$ for all terminal states. We then have that\n\\begin{align}\n&Q^*(\\bs_{t-1}, \\ba_{t-1}) = Q^*(\\bs_t=(\\vc, \\x_{T-t+1}, T-t+1), \\ba=\\x_{T-t})= \\nonumber\\\\\n&r(\\bs_t=(\\vc, \\x_{T-t+1}, T-t+1), \\ba=\\x_{T-t}) + \\beta \\log p_{ref}(\\x_{T-t}| \\vc, \\x_{T-t+1}, T-t+1) + \\nonumber\\\\\n&\\beta\\log\\int_{\\mathcal{A}} e^{Q^*(\\bs_t=(\\vc, \\x_{T-t}, T-t), \\x_{T-t-1})/\\beta}d\\x_{T-t-1} \\nonumber\n\\end{align}\n\nwhere $\\pi_{ref}$ is the reference backwards diffusion process. In this case even though the state space is deterministic, our approach to the proof of Lemma \\ref{lemma:r_to_q} still holds by using backwards induction on the diffusion step $t$. Notice, that from $V(\\bs_T=(\\vc, \\x_0, 0))=0$ we can uniquely determine the critic values for all states at time step $T-1$. Proceeding inductively backwards through time in the MDP/denoising process (forward in the diffusion process), we obtain the desired result. However, this requires that $\\beta > 1$ for the integral to converge.\n\\end{proof}", "B": "To extend Lemma \\ref{lemma:r_to_q} to diffusion Markov Decision Processes (MDPs), we first need to define the structure of a diffusion MDP and then establish the bijection between reward functions \\( r(\\mathbf{s}_t, \\mathbf{a}_t) \\) and the corresponding optimal Q-functions \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\).\n\n### Diffusion MDP Framework\nA diffusion MDP is characterized by a state space \\( \\mathcal{S} \\), an action space \\( \\mathcal{A} \\), and a transition dynamics described by a stochastic differential equation (SDE) of the form:\n\\[\nd\\mathbf{s}_t = f(\\mathbf{s}_t, \\mathbf{a}_t)dt + \\sigma(\\mathbf{s}_t, \\mathbf{a}_t)dW_t,\n\\]\nwhere \\( f(\\mathbf{s}_t, \\mathbf{a}_t) \\) is the drift term, \\( \\sigma(\\mathbf{s}_t, \\mathbf{a}_t) \\) is the diffusion coefficient, and \\( W_t \\) is a Wiener process. The reward function is \\( r(\\mathbf{s}_t, \\mathbf{a}_t) \\), and the objective is to maximize the expected cumulative reward:\n\\[\nJ(\\pi) = \\mathbb{E}_{\\pi}\\left[\\int_0^T e^{-\\rho t} r(\\mathbf{s}_t, \\mathbf{a}_t) dt\\right],\n\\]\nwhere \\( \\pi \\) is the policy, \\( \\rho \\) is the discount rate, and \\( T \\) is the time horizon.\n\n### Optimal Q-Function in Diffusion MDPs\nThe optimal Q-function \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\) in a diffusion MDP satisfies the Hamilton-Jacobi-Bellman (HJB) equation:\n\\[\n\\rho Q^*(\\mathbf{s}_t, \\mathbf{a}_t) = r(\\mathbf{s}_t, \\mathbf{a}_t) + \\mathcal{L}^{\\mathbf{a}_t} Q^*(\\mathbf{s}_t, \\mathbf{a}_t),\n\\]\nwhere \\( \\mathcal{L}^{\\mathbf{a}_t} \\) is the infinitesimal generator of the diffusion process under action \\( \\mathbf{a}_t \\):\n\\[\n\\mathcal{L}^{\\mathbf{a}_t} Q^* = f(\\mathbf{s}_t, \\mathbf{a}_t) \\cdot \\nabla_{\\mathbf{s}} Q^* + \\frac{1}{2} \\text{tr}\\left(\\sigma(\\mathbf{s}_t, \\mathbf{a}_t) \\sigma(\\mathbf{s}_t, \\mathbf{a}_t)^\\top \\nabla_{\\mathbf{s}}^2 Q^*\\right).\n\\]\n\n### Bijection Between \\( r \\) and \\( Q^* \\)\nUnder mild assumptions (e.g., Lipschitz continuity of \\( f \\) and \\( \\sigma \\), and boundedness of \\( r \\)), the HJB equation admits a unique solution \\( Q^* \\) for a given \\( r \\). Conversely, given \\( Q^* \\), the reward function can be recovered as:\n\\[\nr(\\mathbf{s}_t, \\mathbf{a}_t) = \\rho Q^*(\\mathbf{s}_t, \\mathbf{a}_t) - \\mathcal{L}^{\\mathbf{a}_t} Q^*(\\mathbf{s}_t, \\mathbf{a}_t).\n\\]\nThis establishes a bijection between \\( r \\) and \\( Q^* \\) in diffusion MDPs.\n\n### Extended Lemma\n\\begin{lemma} \\label{lemma:diffusion_r_to_q} \nUnder mild assumptions (Lipschitz continuity of drift and diffusion terms, and boundedness of rewards), there is a bijection between reward functions \\( r(\\mathbf{s}_t, \\mathbf{a}_t) \\) and corresponding optimal Q-functions \\( Q^*(\\mathbf{s}_t, \\mathbf{a}_t) \\) in diffusion MDPs.\n\\end{lemma}\n\n### Proof Sketch:\n1. **Forward Direction (Reward to Q-function):**  \n   Given \\( r \\), the HJB equation uniquely determines \\( Q^* \\) under the stated assumptions.\n\n2. **Reverse Direction (Q-function to Reward):**  \n   Given \\( Q^* \\), the reward \\( r \\) is explicitly reconstructed via the HJB equation.\n\n3. **Bijection:**  \n   The mappings \\( r \\mapsto Q^* \\) and \\( Q^* \\mapsto r \\) are inverses of each other, ensuring a bijective relationship.\n\nThis extends Lemma \\ref{lemma:r_to_q} to the diffusion MDP setting.", "C": "\\begin{lemma} \\label{lemma:r_to_q_diffusion} Under mild assumptions, there is a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal Q-functions $Q^*(\\bs_t, \\ba_t)$ in the diffusion MDP.\n\\end{lemma}\n\\begin{proof}\nSince the MDP still has deterministic dynamics, we have that Eq. \\ref{eq:policy}-\\ref{eq:critic} still hold. Now, given a reference policy $\\piref$, parameter $\\beta$ and a critic $Q$, we can trivially recover the unique reward function by inverting Eq. \\ref{eq:critic}. We will prove that given a reward function $r(\\bs_t, \\ba_t)$, we can recover a unique critic $Q$. We work inductively in the diffusion MDP starting with $t=T$, where we have $V^*(\\bs_T)=0$ for all terminal states. We then have that\n\\begin{align}\n&Q^*(\\bs_{t-1}, \\ba_{t-1}) = Q^*(\\bs_t=(\\vc, \\x_{T-t+1}, T-t+1), \\ba=\\x_{T-t})= \\nonumber\\\\\n&r(\\bs_t=(\\vc, \\x_{T-t+1}, T-t+1), \\ba=\\x_{T-t}) + \\beta \\log p_{ref}(\\x_{T-t}| \\vc, \\x_{T-t+1}, T-t+1) + \\nonumber\\\\\n&\\beta\\log\\int_{\\mathcal{A}} e^{Q^*(\\bs_t=(\\vc, \\x_{T-t}, T-t), \\x_{T-t-1})/\\beta}d\\x_{T-t-1} \\nonumber\n\\end{align}\n\nwhere $\\pi_{ref}$ is the reference backwards diffusion process. In this case even though the state space is deterministic, our approach to the proof of Lemma \\ref{lemma:r_to_q} still holds by using backwards induction on the diffusion step $t$. Notice, that from $V(\\bs_T=(\\vc, \\x_0, 0))=0$ we can uniquely determine the critic values for all states at time step $T-1$. Proceeding inductively backwards through time in the MDP/denoising process (forward in the diffusion process), we obtain the desired result.\n\\end{proof}", "D": "We extend Lemma \\ref{lemma:r_to_q} to diffusion MDPs as follows:\n\n\\begin{lemma} \\label{lemma:diffusion_r_to_q}\nConsider a diffusion MDP with continuous state-action space, where the state dynamics follow an Itô stochastic differential equation:\n\\begin{equation}\n    d\\bs_t = \\mu(\\bs_t, \\ba_t)dt + \\sigma(\\bs_t, \\ba_t)d\\mathbf{W}_t\n\\end{equation}\nUnder mild assumptions including:\n\\begin{enumerate}\n    \\item Lipschitz continuity of $\\mu$ and $\\sigma$ to guarantee solution existence\n    \\item Non-degenerate noise: $\\sigma\\sigma^\\top \\succ \\epsilon I$ for some $\\epsilon > 0$\n    \\item Discount factor $\\gamma \\in (0,1)$\n    \\item Reward function $r(\\bs, \\ba)$ is bounded and Lipschitz continuous\n\\end{enumerate}\nthere exists a bijection between reward functions $r(\\bs_t, \\ba_t)$ and corresponding optimal action-value functions $Q^*(\\bs_t, \\ba_t)$.\n\\end{lemma}\n\n\\begin{proof}[Proof Sketch]\nThe bijection follows from these key steps:\n\n1. \\textbf{Forward Direction}: Given any reward function $r$, the Hamilton-Jacobi-Bellman (HJB) equation for diffusion processes:\n\\begin{equation}\n    \\sup_{\\ba} \\left[ r(\\bs, \\ba) + \\mu(\\bs, \\ba)^\\top \\nabla_\\bs Q^* + \\frac{1}{2}\\text{tr}\\left(\\sigma\\sigma^\\top \\nabla_\\bs^2 Q^*\\right) \\right] = \\frac{1}{\\gamma} Q^*(\\bs, \\ba)\n\\end{equation}\nadmits a unique viscosity solution $Q^*$ under the stated assumptions.\n\n2. \\textbf{Inverse Direction}: Given $Q^*$, the reward function can be recovered via:\n\\begin{equation}\n    r(\\bs, \\ba) = \\frac{1}{\\gamma} Q^*(\\bs, \\ba) - \\mu(\\bs, \\ba)^\\top \\nabla_\\bs Q^* - \\frac{1}{2}\\text{tr}\\left(\\sigma\\sigma^\\top \\nabla_\\bs^2 Q^*\\right)\n\\end{equation}\nThe non-degeneracy condition ensures the recovery is well-defined.\n\n3. \\textbf{Bijection}: The viscosity solution uniqueness and explicit recovery formula establish injectivity in both directions. Surjectivity follows from existence of solutions to both the HJB equation and its inverse form.\n\\end{proof}\n\nThis extends the discrete-time result to continuous diffusion processes through their characterizing PDE (the HJB equation), with technical conditions adapted to the continuous setting.", "ground-truth": "C"}
{"paper": "arxiv:2404.11999", "question": "Prove the following lemma: Given two policies $\\pi$ and $\\tilde{\\pi}$, if for any state $s_t=[x, y^{<t}]$, $\\mathbb{E}_{z\\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], z)\\right] \\ge 0$, then we can conclude: $\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] \\ge \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right]$.", "A": "To prove the lemma, we start by defining the trajectory $\\tau:= (x, y^1, y^2, ...)$, where actions are sampled from $\\tilde{\\pi}$ to generate $\\tau$. The difference in expected values under $\\tilde{\\pi}$ and $\\pi$ can be expressed as:\n\n\\begin{align*}\n&\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] - \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} R_{t}-V_{\\pi}([x])\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} \\left(R_{t}+\\gamma V_{\\pi}([x, y^{<t+1}])-V_{\\pi}([x, y^{<t}])\\right)\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t} A_{\\pi}([x, y^{<t}], y^t)\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t} \\mathbb{E}_{y^t \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], y^t)\\right]\\right]\n\\end{align*}\n\nGiven that for any state $s_t=[x, y^{<t}]$, $\\mathbb{E}_{z \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], z)\\right]\\ge 0$, it follows that:\n\n\\begin{align*}\n\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] - \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right] \\ge 0\n\\end{align*}\n\nThis completes the proof, showing that the expected value under $\\tilde{\\pi}$ is at least as great as that under $\\pi$.", "B": "To prove the given lemma, we will follow a step-by-step derivation leveraging the properties of the advantage function and the relationship between policies \\(\\pi\\) and \\(\\tilde{\\pi}\\). Here is the detailed proof:\n\n---\n\n### **Lemma Statement**:\nGiven two policies \\(\\pi\\) and \\(\\tilde{\\pi}\\), if for any state \\(s_t = [x, y^{<t}]\\), \n\\[\n\\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ A_{\\pi}([x, y^{<t}], z) \\right] \\ge 0,\n\\]\nthen we can conclude:\n\\[\n\\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ V_{\\tilde{\\pi}}([x]) \\right] \\ge \\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ V_{\\pi}([x]) \\right].\n\\]\n\n---\n\n### **Proof**:\n\n1. **Definition of Advantage Function**:\n   The advantage function \\(A_{\\pi}(s, z)\\) measures how much better action \\(z\\) is compared to the average action under policy \\(\\pi\\):\n   \\[\n   A_{\\pi}(s, z) = Q_{\\pi}(s, z) - V_{\\pi}(s),\n   \\]\n   where:\n   - \\(Q_{\\pi}(s, z)\\) is the state-action value function under policy \\(\\pi\\).\n   - \\(V_{\\pi}(s)\\) is the state value function under policy \\(\\pi\\).\n\n2. **Rewriting the Given Condition**:\n   The given condition states:\n   \\[\n   \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ A_{\\pi}([x, y^{<t}], z) \\right] \\ge 0.\n   \\]\n   Substituting the definition of \\(A_{\\pi}\\):\n   \\[\n   \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ Q_{\\pi}([x, y^{<t}], z) - V_{\\pi}([x, y^{<t}]) \\right] \\ge 0.\n   \\]\n   Since \\(V_{\\pi}([x, y^{<t}])\\) does not depend on \\(z\\), we can rewrite this as:\n   \\[\n   \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ Q_{\\pi}([x, y^{<t}], z) \\right] - V_{\\pi}([x, y^{<t}]) \\ge 0.\n   \\]\n   This implies:\n   \\[\n   \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ Q_{\\pi}([x, y^{<t}], z) \\right] \\ge V_{\\pi}([x, y^{<t}]).\n   \\]\n\n3. **Relationship Between \\(Q_{\\pi}\\) and \\(V_{\\tilde{\\pi}}\\)**:\n   The state-action value \\(Q_{\\pi}(s, z)\\) can be expressed in terms of the immediate reward and the value of the next state under \\(\\pi\\):\n   \\[\n   Q_{\\pi}(s, z) = r(s, z) + \\gamma \\mathbb{E}_{s'} \\left[ V_{\\pi}(s') \\right],\n   \\]\n   where \\(s'\\) is the next state after taking action \\(z\\) in state \\(s\\).\n\n   For policy \\(\\tilde{\\pi}\\), the state value \\(V_{\\tilde{\\pi}}(s)\\) is:\n   \\[\n   V_{\\tilde{\\pi}}(s) = \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ Q_{\\tilde{\\pi}}(s, z) \\right].\n   \\]\n   However, we are interested in relating \\(V_{\\tilde{\\pi}}\\) to \\(V_{\\pi}\\). Using the given condition and the expression for \\(Q_{\\pi}\\), we can derive a recursive relationship.\n\n4. **Recursive Expansion of \\(V_{\\tilde{\\pi}}\\)**:\n   Starting from the initial state \\(s_0 = [x]\\), we can expand \\(V_{\\tilde{\\pi}}([x])\\) as:\n   \\[\n   V_{\\tilde{\\pi}}([x]) = \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ Q_{\\tilde{\\pi}}([x], z) \\right].\n   \\]\n   If we assume that \\(\\tilde{\\pi}\\) is close to \\(\\pi\\) (as implied by the advantage condition), we can approximate \\(Q_{\\tilde{\\pi}}\\) with \\(Q_{\\pi}\\) (this is a common approach in policy improvement proofs).\n\n   Using the given condition recursively for each state \\(s_t = [x, y^{<t}]\\), we have:\n   \\[\n   \\mathbb{E}_{z \\sim \\tilde{\\pi}} \\left[ Q_{\\pi}([x, y^{<t}], z) \\right] \\ge V_{\\pi}([x, y^{<t}]).\n   \\]\n   This inequality holds for all states along trajectories generated by \\(\\tilde{\\pi}\\).\n\n5. **Monte Carlo Rollout Argument**:\n   Consider the expected return over trajectories generated by \\(\\tilde{\\pi}\\). The cumulative reward under \\(\\tilde{\\pi}\\) can be expressed as:\n   \\[\n   V_{\\tilde{\\pi}}([x]) = \\mathbb{E}_{\\tau \\sim \\tilde{\\pi}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, z_t) \\right],\n   \\]\n   where \\(\\tau\\) is a trajectory generated by \\(\\tilde{\\pi}\\).\n\n   Using the advantage condition at each step, we can show that the expected return under \\(\\tilde{\\pi}\\) is at least as large as under \\(\\pi\\):\n   \\[\n   V_{\\tilde{\\pi}}([x]) \\ge V_{\\pi}([x]).\n   \\]\n   This is because, at each state \\(s_t\\), the expected advantage under \\(\\tilde{\\pi}\\) is non-negative, implying that \\(\\tilde{\\pi}\\) is making locally improving actions relative to \\(\\pi\\).\n\n6. **Taking Expectation Over \\(\\mathcal{D}\\)**:\n   Finally, taking the expectation over the initial state distribution \\(\\mathcal{D}\\), we obtain:\n   \\[\n   \\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ V_{\\tilde{\\pi}}([x]) \\right] \\ge \\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ V_{\\pi}([x]) \\right].\n   \\]\n\n---\n\n### **Conclusion**:\nThe lemma is proven by showing that the non-negativity of the expected advantage under \\(\\tilde{\\pi}\\) at every state implies that the expected value of \\(\\tilde{\\pi}\\) is at least as large as that of \\(\\pi\\) when averaged over the initial state distribution \\(\\mathcal{D}\\). This is a form of policy improvement guarantee. \n\nThus, we conclude:\n\\[\n\\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ V_{\\tilde{\\pi}}([x]) \\right] \\ge \\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ V_{\\pi}([x]) \\right].\n\\]", "C": "To prove the lemma, we leverage the **Performance Difference Lemma** and mathematical induction. Here's the step-by-step derivation:\n\n---\n\n**Lemma Statement**:  \nGiven policies $\\pi$ and $\\tilde{\\pi}$, if for any state $s_t = [x, y^{<t}]$,  \n$$\\mathbb{E}_{z \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], z)\\right] \\ge 0,$$  \nthen:  \n$$\\mathbb{E}_{x \\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] \\ge \\mathbb{E}_{x \\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right].$$\n\n---\n\n**Proof**:\n\n1. **Performance Difference Lemma**:  \n   For any policy $\\tilde{\\pi}$ and initial state $s_0 = [x]$, the difference in value functions satisfies:\n   $$\n   V_{\\tilde{\\pi}}([x]) - V_{\\pi}([x]) = \\mathbb{E}_{\\tau \\sim \\tilde{\\pi} \\mid x}\\left[\\sum_{t=0}^{T-1} A_{\\pi}(s_t, z_t)\\right],\n   $$\n   where $\\tau = (s_0, z_0, s_1, \\dots, s_{T})$ is a trajectory generated by $\\tilde{\\pi}$, $s_t = [x, y^{<t}]$, and $z_t \\sim \\tilde{\\pi}(\\cdot|s_t)$.\n\n2. **Decompose the Expectation**:  \n   The expectation over trajectories can be expressed recursively using the linearity of expectation:\n   $$\n   V_{\\tilde{\\pi}}([x]) - V_{\\pi}([x]) = \\sum_{t=0}^{T-1} \\mathbb{E}_{s_t \\sim \\tilde{\\pi} \\mid x}\\left[\\mathbb{E}_{z_t \\sim \\tilde{\\pi}(\\cdot|s_t)}\\left[A_{\\pi}(s_t, z_t)\\right]\\right].\n   $$\n\n3. **Apply Given Condition**:  \n   By assumption, for every state $s_t = [x, y^{<t}]$:\n   $$\n   \\mathbb{E}_{z_t \\sim \\tilde{\\pi}(\\cdot|s_t)}\\left[A_{\\pi}(s_t, z_t)\\right] \\ge 0.\n   $$\n   Substituting this into the summation:\n   $$\n   V_{\\tilde{\\pi}}([x]) - V_{\\pi}([x]) \\ge \\sum_{t=0}^{T-1} 0 = 0 \\implies V_{\\tilde{\\pi}}([x]) \\ge V_{\\pi}([x]).\n   $$\n\n4. **Take Expectation over $\\mathcal{D}$**:  \n   Since the inequality holds for all $x$, taking the expectation over $x \\sim \\mathcal{D}$ preserves the inequality:\n   $$\n   \\mathbb{E}_{x \\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] \\ge \\mathbb{E}_{x \\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right].\n   $$\n\n---\n\n**Conclusion**:  \nThe condition $\\mathbb{E}_{z \\sim \\tilde{\\pi}}[A_{\\pi}(s_t, z)] \\ge 0$ for all $s_t$ ensures that $\\tilde{\\pi}$ improves (or maintains) the expected value relative to $\\pi$ at every state. Aggregating over trajectories and taking the expectation over $x \\sim \\mathcal{D}$ completes the proof.", "D": "To prove the lemma, we start by defining the trajectory $\\tau:= (x, y^1, y^2, ...)$, where actions are sampled from $\\tilde{\\pi}$ to generate $\\tau$. The difference in expected values under $\\tilde{\\pi}$ and $\\pi$ can be expressed as:\n\n\\begin{align*}\n&\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] - \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} R_{t}-V_{\\pi}([x])\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} \\left(R_{t}+\\gamma V_{\\pi}([x, y^{<t+1}])-V_{\\pi}([x, y^{<t}])\\right)\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} A_{\\pi}([x, y^{<t}], y^t)\\right]\\\\\n=&\\mathbb{E}_{\\tau|\\tilde{\\pi}}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1} \\mathbb{E}_{y^t \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], y^t)\\right]\\right]\n\\end{align*}\n\nGiven that for any state $s_t=[x, y^{<t}]$, $\\mathbb{E}_{z \\sim \\tilde{\\pi}}\\left[A_{\\pi}([x, y^{<t}], z)\\right]\\ge 0$, it follows that:\n\n\\begin{align*}\n\\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\tilde{\\pi}}([x])\\right] - \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[V_{\\pi}([x])\\right] \\ge 0\n\\end{align*}\n\nThis completes the proof, showing that the expected value under $\\tilde{\\pi}$ is at least as great as that under $\\pi$.", "ground-truth": "D"}
{"paper": "arxiv:2404.11999", "question": "Prove the following lemma:\n\n\\begin{lemma}\n    The constrained problem has the closed-form solution:\n\\begin{equation}\n\\begin{aligned}\n    \\pi_{\\theta}^*(z|[x,y^{<t}])= \\\\\n     &\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)}{Z([x,y^{<t}];\\beta)},\n\\end{aligned}\n\\end{equation}\nwhere $Z([x,y^{<t}];\\beta) = \\mathbb{E}_{z\\sim \\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])}e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}$\nis the partition function.\n\\end{lemma}", "A": "\\begin{proof}\n\\begin{align}\n&\\max_{\\pi_{\\theta}} \\ \\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x,y^{<t}])}A_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z)-\\beta D_{\\mathrm{KL}}\\left(\\pi_{\\theta}(\\cdot|[x,y^{<t}])\\|\\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])\\right)\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\left(\\left(Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)-V_{\\pi_{\\mathrm{ref}}}([x,y^{<t}])\\right)+\\beta\\log \\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x, y^{<t}])}{\\pi_{\\theta}(z|[x, y^{<t}])}\\right)\\right)\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\beta\\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{\\pi_{\\theta}(z|[x,y^{<t}])}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\beta\\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{Z([x, y^{<t}];\\beta)\\pi_{\\theta}(z|[x,y^{<t}])}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])+\\beta\\log Z([x, y^{<t}];\\beta)\\\\\n=&\\max_{\\pi_{\\theta}}\\  -\\beta D_{\\mathrm{KL}}\\left(\\pi_{\\theta}(z|[x, y^{<t}])\\bigg\\|\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{Z([x, y^{<t}];\\beta)}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])+\\beta\\log Z([x, y^{<t}];\\beta)\n\\end{align}\nwhere $Z([x, y^{<t}];\\beta)$ is the partition function:\n\\begin{equation}\nZ([x,y^{<t}];\\beta) = \\mathbb{E}_{z\\sim \\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])}\\exp\\left(\\frac{1}{\\beta}V_{\\pi_{\\mathrm{ref}}}([x,y^{<t}])\\right)\n\\end{equation}\nBased on the property of KL divergence, we can derive the relationship between the optimal policy and the state-action function:\n\\begin{equation}\n\\pi_{\\theta}^*(z|[x,y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)}{Z([x,y^{<t}];\\beta)}\n\\end{equation}\n\\end{proof}", "B": "\\begin{proof}\n\\begin{align}\n&\\max_{\\pi_{\\theta}} \\ \\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x,y^{<t}])}A_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z)-\\beta D_{\\mathrm{KL}}\\left(\\pi_{\\theta}(\\cdot|[x,y^{<t}])\\|\\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])\\right)\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\left(\\left(Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)-V_{\\pi_{\\mathrm{ref}}}([x,y^{<t}])\\right)+\\beta\\log \\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x, y^{<t}])}{\\pi_{\\theta}(z|[x, y^{<t}])}\\right)\\right)\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\beta\\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{\\pi_{\\theta}(z|[x,y^{<t}])}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])\\\\\n=&\\max_{\\pi_{\\theta}}\\ \\beta\\mathbb{E}_{z\\sim \\pi_{\\theta}(\\cdot|[x, y^{<t}])}\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{Z([x, y^{<t}];\\beta)\\pi_{\\theta}(z|[x,y^{<t}])}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])+\\beta\\log Z([x, y^{<t}];\\beta)\\\\\n=&\\max_{\\pi_{\\theta}}\\  -\\beta D_{\\mathrm{KL}}\\left(\\pi_{\\theta}(z|[x, y^{<t}])\\bigg\\|\\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])e^{\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)}}{Z([x, y^{<t}];\\beta)}\\right)-V_{\\pi_{\\mathrm{ref}}}([x, y^{<t}])+\\beta\\log Z([x, y^{<t}];\\beta)\n\\end{align}\nwhere $Z([x, y^{<t}];\\beta)$ is the partition function:\n\\begin{equation}\nZ([x,y^{<t}];\\beta) = \\mathbb{E}_{z\\sim \\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])}\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)\n\\end{equation}\nBased on the property of KL divergence, we can derive the relationship between the optimal policy and the state-action function:\n\\begin{equation}\n\\pi_{\\theta}^*(z|[x,y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)}{Z([x,y^{<t}];\\beta)}\n\\end{equation}\n\\end{proof}", "C": "To prove the given lemma, we will derive the closed-form solution for the constrained optimization problem. The goal is to find the optimal policy \\(\\pi_{\\theta}^*(z|[x,y^{<t}])\\) that maximizes the expected reward while staying close to a reference policy \\(\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\). This is formalized as:\n\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{z \\sim \\pi_{\\theta}(\\cdot|[x,y^{<t}])} \\left[ Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) \\right]\n\\]\nsubject to the constraint:\n\\[\n\\mathbb{E}_{z \\sim \\pi_{\\theta}(\\cdot|[x,y^{<t}])} \\left[ \\log \\pi_{\\theta}(z|[x,y^{<t}]) - \\log \\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) \\right] \\leq \\epsilon,\n\\]\nwhere \\(\\epsilon\\) is a small positive constant, and \\(Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z)\\) is the action-value function under the reference policy.\n\n### Step 1: Formulate the Lagrangian\nWe introduce a Lagrange multiplier \\(\\beta > 0\\) to handle the constraint. The Lagrangian is:\n\n\\[\n\\mathcal{L}(\\pi_{\\theta}, \\beta) = \\mathbb{E}_{z \\sim \\pi_{\\theta}} \\left[ Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) \\right] - \\beta \\left( \\mathbb{E}_{z \\sim \\pi_{\\theta}} \\left[ \\log \\pi_{\\theta}(z|[x,y^{<t}]) - \\log \\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) \\right] - \\epsilon \\right).\n\\]\n\n### Step 2: Solve for the Optimal Policy\nTo find the optimal policy, we take the functional derivative of \\(\\mathcal{L}\\) with respect to \\(\\pi_{\\theta}\\) and set it to zero:\n\n\\[\n\\frac{\\delta \\mathcal{L}}{\\delta \\pi_{\\theta}} = Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) - \\beta \\left( \\log \\pi_{\\theta}(z|[x,y^{<t}]) - \\log \\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) + 1 \\right) = 0.\n\\]\n\nSolving for \\(\\pi_{\\theta}(z|[x,y^{<t}])\\), we get:\n\n\\[\n\\log \\pi_{\\theta}(z|[x,y^{<t}]) = \\log \\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) + \\frac{1}{\\beta} Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) - 1 + C,\n\\]\n\nwhere \\(C\\) is a normalization constant. Exponentiating both sides:\n\n\\[\n\\pi_{\\theta}(z|[x,y^{<t}]) = \\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) \\exp\\left( \\frac{1}{\\beta} Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) \\right) \\cdot e^{C-1}.\n\\]\n\n### Step 3: Normalize the Policy\nThe policy must satisfy \\(\\sum_z \\pi_{\\theta}(z|[x,y^{<t}]) = 1\\). Let \\(Z([x,y^{<t}]; \\beta)\\) be the partition function:\n\n\\[\nZ([x,y^{<t}]; \\beta) = \\mathbb{E}_{z \\sim \\pi_{\\mathrm{ref}}(\\cdot|[x,y^{<t}])} \\left[ \\exp\\left( \\frac{1}{\\beta} Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) \\right) \\right].\n\\]\n\nThen, the normalization constant is \\(e^{C-1} = \\frac{1}{Z([x,y^{<t}]; \\beta)}\\). Substituting back:\n\n\\[\n\\pi_{\\theta}^*(z|[x,y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) \\exp\\left( \\frac{1}{\\beta} Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) \\right)}{Z([x,y^{<t}]; \\beta)}.\n\\]\n\n### Conclusion\nThe closed-form solution for the optimal policy is:\n\n\\[\n\\pi_{\\theta}^*(z|[x,y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}]) \\exp\\left( \\frac{1}{\\beta} Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}], z) \\right)}{Z([x,y^{<t}]; \\beta)},\n\\]\n\nwhere \\(Z([x,y^{<t}]; \\beta)\\) is the partition function ensuring normalization. This completes the proof.", "D": "To prove the given lemma, we consider a constrained optimization problem where the goal is to maximize the expected Q-value while regularizing the deviation from a reference policy $\\pi_{\\text{ref}}$ via KL-divergence. The solution is derived using variational calculus with Lagrangian multipliers.\n\n**Proof:**\n\n1. **Objective Function:**\n   We aim to maximize the following regularized objective:\n   \\[\n   \\mathcal{L}(\\pi_\\theta) = \\mathbb{E}_{z \\sim \\pi_\\theta}\\left[Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z)\\right] - \\beta \\, \\text{KL}(\\pi_\\theta \\parallel \\pi_{\\text{ref}}),\n   \\]\n   where the KL-divergence term is:\n   \\[\n   \\text{KL}(\\pi_\\theta \\parallel \\pi_{\\text{ref}}) = \\mathbb{E}_{z \\sim \\pi_\\theta}\\left[\\log \\frac{\\pi_\\theta(z|[x,y^{<t}])}{\\pi_{\\text{ref}}(z|[x,y^{<t}])}\\right].\n   \\]\n\n2. **Rewrite the Objective:**\n   Substitute the KL term into $\\mathcal{L}$:\n   \\[\n   \\mathcal{L} = \\mathbb{E}_{z \\sim \\pi_\\theta}\\left[Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z)\\right] - \\beta \\mathbb{E}_{z \\sim \\pi_\\theta}\\left[\\log \\pi_\\theta(z|[x,y^{<t}]) - \\log \\pi_{\\text{ref}}(z|[x,y^{<t}])\\right].\n   \\]\n\n3. **Formulate Lagrangian with Constraint:**\n   Introduce a Lagrange multiplier $\\lambda$ to enforce the probability normalization constraint $\\sum_z \\pi_\\theta(z|[x,y^{<t}]) = 1$:\n   \\[\n   \\mathcal{L}' = \\mathbb{E}_{z \\sim \\pi_\\theta}\\left[Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z)\\right] - \\beta \\mathbb{E}_{z \\sim \\pi_\\theta}\\left[\\log \\frac{\\pi_\\theta(z|[x,y^{<t}])}{\\pi_{\\text{ref}}(z|[x,y^{<t}])}\\right] + \\lambda\\left(1 - \\sum_z \\pi_\\theta(z|[x,y^{<t}])\\right).\n   \\]\n\n4. **Take Functional Derivative:**\n   Differentiate $\\mathcal{L}'$ with respect to $\\pi_\\theta(z|[x,y^{<t}])$ and set to zero:\n   \\[\n   \\frac{\\delta \\mathcal{L}'}{\\delta \\pi_\\theta(z)} = Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z) - \\beta\\left(\\log \\pi_\\theta(z|[x,y^{<t}]) - \\log \\pi_{\\text{ref}}(z|[x,y^{<t}]) + 1\\right) - \\lambda = 0.\n   \\]\n\n5. **Solve for Optimal Policy $\\pi_\\theta^*$:**\n   Rearrange the equation:\n   \\[\n   \\log \\pi_\\theta^*(z|[x,y^{<t}]) = \\frac{1}{\\beta} Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z) + \\log \\pi_{\\text{ref}}(z|[x,y^{<t}]) - \\left(1 + \\frac{\\lambda}{\\beta}\\right).\n   \\]\n   Exponentiate both sides:\n   \\[\n   \\pi_\\theta^*(z|[x,y^{<t}]) = \\pi_{\\text{ref}}(z|[x,y^{<t}]) \\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z)\\right) \\cdot \\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right).\n   \\]\n\n6. **Normalize with Partition Function $Z$:**\n   The term $\\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right)$ acts as a normalization constant. Define the partition function:\n   \\[\n   Z([x,y^{<t}];\\beta) = \\mathbb{E}_{z \\sim \\pi_{\\text{ref}}(\\cdot|[x,y^{<t}])}\\left[\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\text{ref}}}([x,y^{<t}],z)\\right)\\right],\n   \\]\n   which ensures $\\sum_z \\pi_\\theta^*(z|[x,y^{<t}]) = 1$. Thus:\n   \\[\n   \\pi_{\\theta}^*(z|[x,y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[x,y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([x,y^{<t}],z)\\right)}{Z([x,y^{<t}];\\beta)}.\n   \\]\n\nThis completes the proof that the optimal policy under the KL-regularized objective takes the given closed-form. The key steps involve setting up the Lagrangian, differentiating to find the optimality condition, and normalizing with the partition function.", "ground-truth": "B"}
{"paper": "arxiv:2404.11999", "question": "Prove the following lemma:\n\\begin{lemma}\nGiven a reward function $r(\\mathrm{x}, \\mathrm{y})$, assuming a relationship between token-wise rewards and the reward function represented by $r({x}, {y}) = \\sum_{t=1}^T\\gamma^{t-1}R([{x},y^{<t}], y^t)$, we can establish the equivalence between the Bradley-Terry model and the Regret Preference Model in the task of text generation alignment, i.e.,\n\\begin{equation}\n    {\\small\n    \\begin{aligned}\n        P_{\\mathrm{BT}}&({y}_1 \\succ {y}_2 |{x})=\\\\\n        \\sigma&\\left(\\sum_{t=1}^{T_1}\\gamma^{t-1}A_{\\pi}([{x},y_1^{<t}], y_1^{t}) - \\sum_{t=1}^{T_2}\\gamma^{t-1}A_{\\pi}([{x},y_2^{<t}], y_2^{t})\\right),\n    \\end{aligned}}\\label{BT_eq_RPM}\n\\end{equation}\nwhere $\\sigma(x)=1/(1+exp(-x))$ is the logistic sigmoid function.\n\\end{lemma}", "A": "\\begin{proof}\nAccording to the Bradley-Terry model, we have\n\\begin{equation}\n\\begin{aligned}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x})=\\frac{\\exp(r({x}, {y}_1))}{\\exp(r({x}, {y}_1))+\\exp(r({x}, {y}_2))},\n\\label{app_BT_model}\n\\end{aligned}\n\\end{equation}\nwhere $r({x}, {y})$ represents the overall reward of the pair $({x}, {y})$.\n\nBased on assumption that $r({x}, {y}) = \\sum_{t=1}^T\\gamma^{t-1}R([{x},y^{<t}], y^t)$, we can get:\n\\begin{align}\nr({x}, {y}) &= \\sum_{t=1}^{T}\\gamma^{t-1}R([{x},y^{<t}], y^t)\\\\\n&=\\sum_{t=1}^{T} \\gamma^{t-1}(R([{x},y^{<t}], y^t) + \\gamma V_{\\pi}([{x},y^{<t+1}]) - \\gamma V_{\\pi}([{x},y^{<t+1}]))\\\\\n&=V_{\\pi}([{x},y^{<1}]) + \\sum_{t=1}^{T}\\gamma^{t-1}\\left( R([{x},y^{<t}], y^t) + \\gamma V_{\\pi}([{x},y^{<t+1}]) - V_{\\pi}([{x},y^{<t}])\\right) - \\gamma^T V_{\\pi}([{x},y^{<T+1}])\\label{app_r}\n\\end{align}\nText generation is analogous to a deterministic contextual bandit, where the transition to the next state is certain given the current state and action, i.e., $p(s_{t+1}=[{x},y^{<t+1}]|s_{t}=[{x},y^{<t}], a_t=y^t)=1$, so we have:\n\\begin{align}\nQ_{\\pi}([{x},y^{<t}], y^t) &= R([{x},y^{<t}], y^t) + V_{\\pi}([{x},y^{<t+1}])\\label{app_Q}\\\\\nA_{\\pi}([{x},y^{<t}], y^t) &= Q_{\\pi}([{x},y^{<t}], y^t) - V_{\\pi}([{x},y^{<t}])\n\\label{app_A}\n\\end{align}\n\nNext, note that  $y^T=\\text{EOS}$ denotes the end of the text sequence. Therefore,\n\\begin{equation}\nV_{\\pi}([{x},y^{<T+1}])=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k} R([{x},y^{<T+1+k}], y^{T+1+k})\\bigg|s_{t}=[{x},y^{<T+1}]\\right]=0\\label{app_V}\n\\end{equation}\n\nSubstituting Eq.\\ref{app_r} to Eq.\\ref{app_V} into the Bradley-Terry model, we obtain\n\n\\begin{align}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x})&=\\frac{\\exp(r({x}, {y}_1))}{\\exp(r({x}, {y}_1))+\\exp(r({x}, {y}_2))}\\nonumber\\\\\n=&\\sigma\\left(\\left(V_{\\pi}([{x},y_1^{<1}]) + \\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y^t)\\right)\\right)-\\left(V_{\\pi}([{x},y_2^{<1}]) + \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\\right)\n\\end{align}\n\nAdditionally, note that $y^{<1}=[\\ ]$, so we can get\n$$\nV_{\\pi}([{x},y_1^{<1}]) = V_{\\pi}([{x},[\\ ]]) = V_{\\pi}([{x},y_2^{<1}])\n$$\nTherefore,\n\\begin{align}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x}) &= \\sigma\\left(\\left(V_{\\pi}([{x},y_1^{<1}]) + \\sum_{t=1}^{T_1}\\left( \\gamma^{t-1}A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)\\right)-\\left(V_{\\pi}([{x},y_2^{<1}]) + \\sum_{t=1}^{T_2}\\left( \\gamma^{t-1}A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\\right)\\\\\n&=\\sigma\\left(\\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)- \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\n\\end{align}\n\\end{proof}", "B": "To prove the equivalence between the Bradley-Terry (BT) model and the Regret Preference Model (RPM) for text generation alignment, we start by defining the key components and assumptions.\n\n### Definitions and Assumptions:\n1. **Reward Function**:  \n   The reward function \\( r(\\mathbf{x}, \\mathbf{y}) \\) is decomposed into token-wise rewards \\( R \\) with a discount factor \\( \\gamma \\):\n   \\[\n   r(\\mathbf{x}, \\mathbf{y}) = \\sum_{t=1}^T \\gamma^{t-1} R([\\mathbf{x}, \\mathbf{y}^{<t}], y^t).\n   \\]\n   Here, \\( \\mathbf{y}^{<t} \\) denotes the tokens generated before step \\( t \\), and \\( y^t \\) is the token at step \\( t \\).\n\n2. **Advantage Function**:  \n   The advantage \\( A_\\pi \\) measures the improvement of taking action \\( y^t \\) over the expected value under policy \\( \\pi \\):\n   \\[\n   A_\\pi([\\mathbf{x}, \\mathbf{y}^{<t}], y^t) = R([\\mathbf{x}, \\mathbf{y}^{<t}], y^t) - V_\\pi([\\mathbf{x}, \\mathbf{y}^{<t}]),\n   \\]\n   where \\( V_\\pi \\) is the value function under \\( \\pi \\).\n\n3. **Bradley-Terry Model**:  \n   The BT model defines the probability of preferring \\( \\mathbf{y}_1 \\) over \\( \\mathbf{y}_2 \\) given \\( \\mathbf{x} \\) as:\n   \\[\n   P_{\\mathrm{BT}}(\\mathbf{y}_1 \\succ \\mathbf{y}_2 | \\mathbf{x}) = \\sigma\\left(r(\\mathbf{x}, \\mathbf{y}_1) - r(\\mathbf{x}, \\mathbf{y}_2)\\right),\n   \\]\n   where \\( \\sigma(x) = 1/(1 + \\exp(-x)) \\).\n\n### Proof:\nSubstitute the reward decomposition into the BT model:\n\\[\nP_{\\mathrm{BT}}(\\mathbf{y}_1 \\succ \\mathbf{y}_2 | \\mathbf{x}) = \\sigma\\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} R([\\mathbf{x}, \\mathbf{y}_1^{<t}], y_1^t) - \\sum_{t=1}^{T_2} \\gamma^{t-1} R([\\mathbf{x}, \\mathbf{y}_2^{<t}], y_2^t)\\right).\n\\]\n\nExpress the rewards in terms of advantages and value functions:\n\\[\nR([\\mathbf{x}, \\mathbf{y}^{<t}], y^t) = A_\\pi([\\mathbf{x}, \\mathbf{y}^{<t}], y^t) + V_\\pi([\\mathbf{x}, \\mathbf{y}^{<t}]).\n\\]\n\nSubstitute back into the BT model:\n\\[\nP_{\\mathrm{BT}}(\\mathbf{y}_1 \\succ \\mathbf{y}_2 | \\mathbf{x}) = \\sigma\\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} \\left(A_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}], y_1^t) + V_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}])\\right) - \\sum_{t=1}^{T_2} \\gamma^{t-1} \\left(A_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}], y_2^t) + V_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}])\\right)\\right).\n\\]\n\nSimplify by separating the advantage and value terms:\n\\[\nP_{\\mathrm{BT}}(\\mathbf{y}_1 \\succ \\mathbf{y}_2 | \\mathbf{x}) = \\sigma\\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} A_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}], y_1^t) - \\sum_{t=1}^{T_2} \\gamma^{t-1} A_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}], y_2^t) + \\sum_{t=1}^{T_1} \\gamma^{t-1} V_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}]) - \\sum_{t=1}^{T_2} \\gamma^{t-1} V_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}])\\right).\n\\]\n\nUnder the **Regret Preference Model**, the value terms cancel out when comparing two completions \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\) from the same policy \\( \\pi \\), as the expected cumulative value is the same for both sequences when conditioned on \\( \\mathbf{x} \\). Thus:\n\\[\n\\sum_{t=1}^{T_1} \\gamma^{t-1} V_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}]) = \\sum_{t=1}^{T_2} \\gamma^{t-1} V_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}]).\n\\]\n\nThis simplifies the BT model to:\n\\[\nP_{\\mathrm{BT}}(\\mathbf{y}_1 \\succ \\mathbf{y}_2 | \\mathbf{x}) = \\sigma\\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} A_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}], y_1^t) - \\sum_{t=1}^{T_2} \\gamma^{t-1} A_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}], y_2^t)\\right).\n\\]\n\n### Conclusion:\nThe Bradley-Terry model, when applied to token-wise rewards and advantages, reduces to the Regret Preference Model under the given assumptions. This establishes the equivalence:\n\\[\nP_{\\mathrm{BT}}(\\mathbf{y}_1 \\succ \\mathbf{y}_2 | \\mathbf{x}) = \\sigma\\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} A_\\pi([\\mathbf{x}, \\mathbf{y}_1^{<t}], y_1^t) - \\sum_{t=1}^{T_2} \\gamma^{t-1} A_\\pi([\\mathbf{x}, \\mathbf{y}_2^{<t}], y_2^t)\\right).\n\\]\n\\qed", "C": "To prove the equivalence between the Bradley-Terry (BT) model and the Regret Preference Model (RPM) under the given conditions, we proceed as follows:\n\n**Step 1: Reward Function Decomposition**  \nGiven the token-wise reward relationship:  \n\\[\nr(x, y) = \\sum_{t=1}^T \\gamma^{t-1} R\\left([x, y^{<t}], y^t\\right),\n\\]  \nwe substitute the advantage function \\(A_\\pi\\) into \\(R\\). By definition, the advantage function \\(A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s)\\) satisfies the Bellman equation:  \n\\[\nQ_\\pi(s, a) = R(s, a) + \\gamma V_\\pi(s'),\n\\]  \nwhere \\(s'\\) is the next state. Rearranging, we express \\(R(s, a)\\) as:  \n\\[\nR(s, a) = A_\\pi(s, a) + \\gamma V_\\pi(s') - V_\\pi(s).\n\\]\n\n**Step 2: Expand the Total Reward**  \nSubstitute \\(R\\left([x, y^{<t}], y^t\\right) = A_\\pi\\left([x, y^{<t}], y^t\\right) + \\gamma V_\\pi\\left([x, y^{\\leq t}]\\right) - V_\\pi\\left([x, y^{<t}]\\right)\\) into \\(r(x, y)\\):  \n\\[\nr(x, y) = \\sum_{t=1}^T \\gamma^{t-1} \\left[A_\\pi\\left([x, y^{<t}], y^t\\right) + \\gamma V_\\pi\\left([x, y^{\\leq t}]\\right) - V_\\pi\\left([x, y^{<t}]\\right)\\right].\n\\]  \nSplit into three terms:  \n\\[\nr(x, y) = \\underbrace{\\sum_{t=1}^T \\gamma^{t-1} A_\\pi\\left([x, y^{<t}], y^t\\right)}_{\\text{Term 1}} + \\underbrace{\\sum_{t=1}^T \\gamma^{t} V_\\pi\\left([x, y^{\\leq t}]\\right)}_{\\text{Term 2}} - \\underbrace{\\sum_{t=1}^T \\gamma^{t-1} V_\\pi\\left([x, y^{<t}]\\right)}_{\\text{Term 3}}.\n\\]\n\n**Step 3: Telescope Term 2 and Term 3**  \n- Term 2: \\(\\sum_{t=1}^T \\gamma^t V_\\pi([x, y^{\\leq t}]) = \\sum_{k=2}^{T+1} \\gamma^{k-1} V_\\pi([x, y^{<k}])\\) (shift index \\(k = t + 1\\)).  \n- Term 3: \\(\\sum_{t=1}^T \\gamma^{t-1} V_\\pi([x, y^{<t}]) = \\sum_{k=1}^T \\gamma^{k-1} V_\\pi([x, y^{<k}])\\).  \n\nSubtract Term 3 from Term 2:  \n\\[\n\\text{Term 2 - Term 3} = \\gamma^T V_\\pi([x, y^{\\leq T+1}]) - V_\\pi(x).\n\\]  \nAssuming \\(V_\\pi([x, y^{\\leq T+1}]) = 0\\) (no future value after generation), this simplifies to \\(-V_\\pi(x)\\).\n\n**Step 4: Combine Terms**  \nSubstitute back into \\(r(x, y)\\):  \n\\[\nr(x, y) = \\sum_{t=1}^T \\gamma^{t-1} A_\\pi\\left([x, y^{<t}], y^t\\right) + \\left(-V_\\pi(x)\\right).\n\\]  \nThus:  \n\\[\nr(x, y) = \\sum_{t=1}^T \\gamma^{t-1} A_\\pi\\left([x, y^{<t}], y^t\\right) - V_\\pi(x).\n\\]\n\n**Step 5: Bradley-Terry Preference Probability**  \nThe BT model computes preference as:  \n\\[\nP_{\\text{BT}}(y_1 \\succ y_2 | x) = \\sigma\\left(r(x, y_1) - r(x, y_2)\\right).\n\\]  \nSubstitute the expression for \\(r(x, y)\\):  \n\\[\nr(x, y_1) - r(x, y_2) = \\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} A_\\pi\\left([x, y_1^{<t}], y_1^t\\right) - V_\\pi(x)\\right) - \\left(\\sum_{t=1}^{T_2} \\gamma^{t-1} A_\\pi\\left([x, y_2^{<t}], y_2^t\\right) - V_\\pi(x)\\right).\n\\]  \nThe baseline \\(V_\\pi(x)\\) cancels out:  \n\\[\nr(x, y_1) - r(x, y_2) = \\sum_{t=1}^{T_1} \\gamma^{t-1} A_\\pi\\left([x, y_1^{<t}], y_1^t\\right) - \\sum_{t=1}^{T_2} \\gamma^{t-1} A_\\pi\\left([x, y_2^{<t}], y_2^t\\right).\n\\]  \nThus:  \n\\[\nP_{\\text{BT}}(y_1 \\succ y_2 | x) = \\sigma\\left(\\sum_{t=1}^{T_1} \\gamma^{t-1} A_\\pi\\left([x, y_1^{<t}], y_1^t\\right) - \\sum_{t=1}^{T_2} \\gamma^{t-1} A_\\pi\\left([x, y_2^{<t}], y_2^t\\right)\\right),\n\\]  \nwhich matches the Regret Preference Model. **Q.E.D.**", "D": "\\begin{proof}\nAccording to the Bradley-Terry model, we have\n\\begin{equation}\n\\begin{aligned}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x})=\\frac{\\exp(r({x}, {y}_1))}{\\exp(r({x}, {y}_1))+\\exp(r({x}, {y}_2))},\n\\label{app_BT_model}\n\\end{aligned}\n\\end{equation}\nwhere $r({x}, {y})$ represents the overall reward of the pair $({x}, {y})$.\n\nBased on assumption that $r({x}, {y}) = \\sum_{t=1}^T\\gamma^{t-1}R([{x},y^{<t}], y^t)$, we can get:\n\\begin{align}\nr({x}, {y}) &= \\sum_{t=1}^{T}\\gamma^{t-1}R([{x},y^{<t}], y^t)\\\\\n&=\\sum_{t=1}^{T} \\gamma^{t-1}(R([{x},y^{<t}], y^t) + \\gamma V_{\\pi}([{x},y^{<t+1}]) - \\gamma V_{\\pi}([{x},y^{<t+1}]))\\\\\n&=V_{\\pi}([{x},y^{<1}]) + \\sum_{t=1}^{T}\\gamma^{t-1}\\left( R([{x},y^{<t}], y^t) + \\gamma V_{\\pi}([{x},y^{<t+1}]) - V_{\\pi}([{x},y^{<t}])\\right) - \\gamma^T V_{\\pi}([{x},y^{<T+1}])\\label{app_r}\n\\end{align}\nText generation is analogous to a deterministic contextual bandit, where the transition to the next state is certain given the current state and action, i.e., $p(s_{t+1}=[{x},y^{<t+1}]|s_{t}=[{x},y^{<t}], a_t=y^t)=1$, so we have:\n\\begin{align}\nQ_{\\pi}([{x},y^{<t}], y^t) &= R([{x},y^{<t}], y^t) + V_{\\pi}([{x},y^{<t+1}])\\label{app_Q}\\\\\nA_{\\pi}([{x},y^{<t}], y^t) &= Q_{\\pi}([{x},y^{<t}], y^t) - V_{\\pi}([{x},y^{<t}])\n\\label{app_A}\n\\end{align}\n\nNext, note that  $y^T=\\text{EOS}$ denotes the end of the text sequence. Therefore,\n\\begin{equation}\nV_{\\pi}([{x},y^{<T+1}])=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k} R([{x},y^{<T+1+k}], y^{T+1+k})\\bigg|s_{t}=[{x},y^{<T+1}]\\right]=0\\label{app_V}\n\\end{equation}\n\nSubstituting Eq.\\ref{app_r} to Eq.\\ref{app_V} into the Bradley-Terry model, we obtain\n\n\\begin{align}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x})&=\\frac{\\exp(r({x}, {y}_1))}{\\exp(r({x}, {y}_1))+\\exp(r({x}, {y}_2))}\\nonumber\\\\\n=&\\sigma\\left(\\left(V_{\\pi}([{x},y_1^{<1}]) + \\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y^t)\\right)\\right)-\\left(V_{\\pi}([{x},y_2^{<1}]) + \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\\right)\n\\end{align}\n\nAdditionally, note that $y^{<1}=[\\ ]$, so we can get\n$$\nV_{\\pi}([{x},y_1^{<1}]) = V_{\\pi}([{x},[\\ ]]) = V_{\\pi}([{x},y_2^{<1}])\n$$\nTherefore,\n\\begin{align}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x}) &= \\sigma\\left(\\left(V_{\\pi}([{x},y_1^{<1}]) + \\sum_{t=1}^{T_1}\\left( \\gamma^{t-1}A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)\\right)-\\left(V_{\\pi}([{x},y_2^{<1}]) + \\sum_{t=1}^{T_2}\\left( \\gamma^{t-1}A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\\right)\\\\\n&=\\sigma\\left(\\sum_{t=1}^{T_1}\\left(\\gamma^{t} A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)- \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right)\n\\end{align}\n\\end{proof}", "ground-truth": "A"}
{"paper": "arxiv:2404.11999", "question": "Prove the following theorem:\n\nIn the KL-constrainted advantage function maximization problem, the Bradley-Terry model expresses the human preference probability in terms of the optimal policy $\\pi_{\\theta}^*$ and reference policy $\\pi_{\\mathrm{ref}}$:\n\\begin{equation}\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x})=\\sigma(u^*({x}, {y}_1, {y}_2) - \\delta^*({x}, {y}_1, {y}_2)),\n\\end{equation}\nwhere, $u({x}, {y}_1, {y}_2)$ refers to the difference in rewards implicitly defined by the language model $\\pi_{\\theta}$ and the reference model $\\pi_{\\mathrm{ref}}$, represented as\n\\begin{equation}\nu({x}, {y}_1, {y}_2)=\\beta\\log\\frac{\\pi_{\\theta}({y}_1\\mid {x})}{\\pi_{\\mathrm{ref}}({y}_1\\mid {x})}-\\beta\\log\\frac{\\pi_{\\theta}({y}_2\\mid {x})}{\\pi_{\\mathrm{ref}}({y}_2\\mid {x})},\n\\end{equation}\nand $\\delta({x}, {y}_1, {y}_2)$ refers to the difference in sequential forward KL divergence between two pairs $({x}, {y}_1)$ and $({x}, {y}_2)$, weighted by $\\beta$, expressed as\n\\begin{equation}\n\\delta({x}, {y}_1, {y}_2) =\\beta D_{\\mathrm{SeqKL}}\\left({x},{y}_2;\\pi_{\\mathrm{ref}}\\| \\pi_{\\theta}\\right) - \\beta D_{\\mathrm{SeqKL}}\\left({x},{y}_1;\\pi_{\\mathrm{ref}}\\| \\pi_{\\theta}\\right).\n\\end{equation}", "A": "To prove the given theorem, we need to derive the expression for the Bradley-Terry model \\( P_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x}) \\) in terms of the optimal policy \\(\\pi_{\\theta}^*\\) and the reference policy \\(\\pi_{\\mathrm{ref}}\\), incorporating the reward difference \\( u({x}, {y}_1, {y}_2) \\) and the KL divergence difference \\(\\delta({x}, {y}_1, {y}_2) \\).\n\n### Step 1: Define the Optimal Policy under KL Constraint\nThe optimal policy \\(\\pi_{\\theta}^*\\) is derived by maximizing the expected reward while constraining the KL divergence from the reference policy \\(\\pi_{\\mathrm{ref}}\\). The optimization problem is:\n\n\\[\n\\pi_{\\theta}^* = \\arg\\max_{\\pi_{\\theta}} \\mathbb{E}_{{y} \\sim \\pi_{\\theta}(\\cdot|{x})} [r({x}, {y})] - \\beta D_{\\mathrm{KL}}(\\pi_{\\theta}(\\cdot|{x}) \\| \\pi_{\\mathrm{ref}}(\\cdot|{x})),\n\\]\n\nwhere \\( r({x}, {y}) \\) is the reward function, and \\(\\beta\\) is a regularization parameter.\n\nThe solution to this optimization problem is given by:\n\n\\[\n\\pi_{\\theta}^*({y}|{x}) = \\frac{1}{Z({x})} \\pi_{\\mathrm{ref}}({y}|{x}) \\exp\\left(\\frac{r({x}, {y})}{\\beta}\\right),\n\\]\n\nwhere \\( Z({x}) = \\sum_{{y}} \\pi_{\\mathrm{ref}}({y}|{x}) \\exp\\left(\\frac{r({x}, {y})}{\\beta}\\right) \\) is the partition function.\n\n### Step 2: Express Reward in Terms of Optimal Policy\nFrom the optimal policy expression, we can solve for the reward \\( r({x}, {y}) \\):\n\n\\[\nr({x}, {y}) = \\beta \\log \\frac{\\pi_{\\theta}^*({y}|{x})}{\\pi_{\\mathrm{ref}}({y}|{x})} + \\beta \\log Z({x}).\n\\]\n\nThe term \\(\\beta \\log Z({x})\\) is independent of \\({y}\\) and cancels out when considering the difference in rewards between two outputs \\({y}_1\\) and \\({y}_2\\):\n\n\\[\nu({x}, {y}_1, {y}_2) = r({x}, {y}_1) - r({x}, {y}_2) = \\beta \\log \\frac{\\pi_{\\theta}^*({y}_1|{x})}{\\pi_{\\mathrm{ref}}({y}_1|{x})} - \\beta \\log \\frac{\\pi_{\\theta}^*({y}_2|{x})}{\\pi_{\\mathrm{ref}}({y}_2|{x})}.\n\\]\n\n### Step 3: Define Sequential Forward KL Divergence\nThe sequential forward KL divergence \\( D_{\\mathrm{SeqKL}} \\) between \\(\\pi_{\\mathrm{ref}}\\) and \\(\\pi_{\\theta}\\) for a given sequence \\(({x}, {y})\\) is:\n\n\\[\nD_{\\mathrm{SeqKL}}({x}, {y}; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}) = \\mathbb{E}_{{y} \\sim \\pi_{\\mathrm{ref}}(\\cdot|{x})} \\left[ \\log \\frac{\\pi_{\\mathrm{ref}}({y}|{x})}{\\pi_{\\theta}({y}|{x})} \\right].\n\\]\n\nThe difference in sequential forward KL divergence between \\(({x}, {y}_1)\\) and \\(({x}, {y}_2)\\) is:\n\n\\[\n\\delta({x}, {y}_1, {y}_2) = \\beta D_{\\mathrm{SeqKL}}({x}, {y}_2; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}) - \\beta D_{\\mathrm{SeqKL}}({x}, {y}_1; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}).\n\\]\n\n### Step 4: Combine Reward and KL Divergence Differences\nThe Bradley-Terry model expresses the probability that \\({y}_1\\) is preferred over \\({y}_2\\) given \\({x}\\) as a sigmoid function of the difference in rewards and KL divergences:\n\n\\[\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x}) = \\sigma(u^*({x}, {y}_1, {y}_2) - \\delta^*({x}, {y}_1, {y}_2)),\n\\]\n\nwhere:\n- \\( u^*({x}, {y}_1, {y}_2) \\) is the reward difference under the optimal policy \\(\\pi_{\\theta}^*\\).\n- \\(\\delta^*({x}, {y}_1, {y}_2) \\) is the KL divergence difference under the optimal policy.\n\nSubstituting the expressions for \\( u \\) and \\(\\delta\\):\n\n\\[\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x}) = \\sigma\\left( \\beta \\log \\frac{\\pi_{\\theta}^*({y}_1|{x})}{\\pi_{\\mathrm{ref}}({y}_1|{x})} - \\beta \\log \\frac{\\pi_{\\theta}^*({y}_2|{x})}{\\pi_{\\mathrm{ref}}({y}_2|{x})} - \\left( \\beta D_{\\mathrm{SeqKL}}({x}, {y}_2; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}^*) - \\beta D_{\\mathrm{SeqKL}}({x}, {y}_1; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}^*) \\right) \\right).\n\\]\n\n### Step 5: Simplify the Expression\nThe KL divergence terms can be rewritten using the definition of \\( D_{\\mathrm{SeqKL}} \\):\n\n\\[\nD_{\\mathrm{SeqKL}}({x}, {y}; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}^*) = \\log \\frac{\\pi_{\\mathrm{ref}}({y}|{x})}{\\pi_{\\theta}^*({y}|{x})},\n\\]\n\nsince the expectation reduces to a single term when evaluated at specific \\({y}_1\\) and \\({y}_2\\). Thus:\n\n\\[\n\\delta^*({x}, {y}_1, {y}_2) = \\beta \\log \\frac{\\pi_{\\mathrm{ref}}({y}_2|{x})}{\\pi_{\\theta}^*({y}_2|{x})} - \\beta \\log \\frac{\\pi_{\\mathrm{ref}}({y}_1|{x})}{\\pi_{\\theta}^*({y}_1|{x})}.\n\\]\n\nSubstituting back into \\( P_{\\mathrm{BT}}^* \\):\n\n\\[\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x}) = \\sigma\\left( \\beta \\log \\frac{\\pi_{\\theta}^*({y}_1|{x})}{\\pi_{\\mathrm{ref}}({y}_1|{x})} - \\beta \\log \\frac{\\pi_{\\theta}^*({y}_2|{x})}{\\pi_{\\mathrm{ref}}({y}_2|{x})} - \\left( \\beta \\log \\frac{\\pi_{\\mathrm{ref}}({y}_2|{x})}{\\pi_{\\theta}^*({y}_2|{x})} - \\beta \\log \\frac{\\pi_{\\mathrm{ref}}({y}_1|{x})}{\\pi_{\\theta}^*({y}_1|{x})} \\right) \\right).\n\\]\n\nSimplifying the expression:\n\n\\[\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x}) = \\sigma\\left( 2\\beta \\log \\frac{\\pi_{\\theta}^*({y}_1|{x})}{\\pi_{\\mathrm{ref}}({y}_1|{x})} - 2\\beta \\log \\frac{\\pi_{\\theta}^*({y}_2|{x})}{\\pi_{\\mathrm{ref}}({y}_2|{x})} \\right) = \\sigma\\left( 2\\beta \\log \\frac{\\pi_{\\theta}^*({y}_1|{x}) / \\pi_{\\theta}^*({y}_2|{x})}{\\pi_{\\mathrm{ref}}({y}_1|{x}) / \\pi_{\\mathrm{ref}}({y}_2|{x})} \\right).\n\\]\n\nThis shows that the preference probability is a sigmoid function of the log-ratio of the optimal policy to the reference policy, scaled by \\( 2\\beta \\). However, the original theorem statement uses \\( u^* - \\delta^* \\), which simplifies to the same form when the KL divergence terms are explicitly included. Thus, the theorem is proven. \n\n### Final Expression:\n\\[\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x}) = \\sigma\\left( \\beta \\log \\frac{\\pi_{\\theta}^*({y}_1|{x})}{\\pi_{\\mathrm{ref}}({y}_1|{x})} - \\beta \\log \\frac{\\pi_{\\theta}^*({y}_2|{x})}{\\pi_{\\mathrm{ref}}({y}_2|{x})} - \\left( \\beta D_{\\mathrm{SeqKL}}({x}, {y}_2; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}^*) - \\beta D_{\\mathrm{SeqKL}}({x}, {y}_1; \\pi_{\\mathrm{ref}} \\| \\pi_{\\theta}^*) \\right) \\right).\n\\]\n\nThis matches the given theorem, completing the proof.", "B": "To prove the theorem, we start by formulating the KL-constrained optimization problem and derive the optimal policy $\\pi_\\theta^*$ that maximizes human preference likelihood while satisfying the KL divergence constraint. The key steps are as follows:\n\n**1. Optimization Objective:**\nThe goal is to maximize the expected log-likelihood of human preferences under the Bradley-Terry model, with a KL divergence penalty:\n$$\n\\max_{\\pi_\\theta} \\mathbb{E}_{x, y_1, y_2} \\left[ \\log \\sigma(A(x, y_1, y_2)) \\right] - \\beta \\mathbb{E}_{x,y} \\left[ D_{\\text{SeqKL}}(x, y; \\pi_{\\text{ref}} \\| \\pi_\\theta) \\right],\n$$\nwhere $A(x, y_1, y_2)$ is the advantage function to be determined.\n\n**2. Parameterize the Advantage Function:**\nAssume the advantage function implicitly depends on $\\pi_\\theta$ and $\\pi_{\\text{ref}}$ through:\n$$\nA(x, y_1, y_2) = \\underbrace{\\beta \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} - \\beta \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}}_{u(x, y_1, y_2)} - \\underbrace{\\beta \\left( D_{\\text{SeqKL}}(x, y_2; \\pi_{\\text{ref}} \\| \\pi_\\theta) - D_{\\text{SeqKL}}(x, y_1; \\pi_{\\text{ref}} \\| \\pi_\\theta) \\right)}_{\\delta(x, y_1, y_2)}.\n$$\n\n**3. Optimal Policy Derivation:**\nThe sequence-level KL divergence for a given $y$ is defined as:\n$$\nD_{\\text{SeqKL}}(x, y; \\pi_{\\text{ref}} \\| \\pi_\\theta) = \\log \\frac{\\pi_{\\text{ref}}(y|x)}{\\pi_\\theta(y|x)} = \\sum_{t=1}^T \\log \\frac{\\pi_{\\text{ref}}(y_t|x, y_{<t})}{\\pi_\\theta(y_t|x, y_{<t})}.\n$$\nSubstitute this into $\\delta(x, y_1, y_2)$:\n$$\n\\delta(x, y_1, y_2) = \\beta \\left( \\log \\frac{\\pi_{\\text{ref}}(y_2|x)}{\\pi_\\theta(y_2|x)} - \\log \\frac{\\pi_{\\text{ref}}(y_1|x)}{\\pi_\\theta(y_1|x)} \\right).\n$$\n\n**4. Simplify the Advantage Function:**\nSubstitute $u$ and $\\delta$ into $A(x, y_1, y_2)$:\n$$\n\\begin{aligned}\nA(x, y_1, y_2) &= \\beta \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} - \\beta \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)} - \\beta \\left( \\log \\frac{\\pi_{\\text{ref}}(y_2|x)}{\\pi_\\theta(y_2|x)} - \\log \\frac{\\pi_{\\text{ref}}(y_1|x)}{\\pi_\\theta(y_1|x)} \\right) \\\\\n&= \\beta \\left( \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} + \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} \\right) - \\beta \\left( \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)} + \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)} \\right) \\\\\n&= 2\\beta \\left( \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} - \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)} \\right).\n\\end{aligned}\n$$\nThis shows that $A(x, y_1, y_2)$ aligns with the original Bradley-Terry model scaled by $2\\beta$. However, to match the theorem, we adjust the regularization strength by halving $\\beta$, leading to the final form:\n$$\nP_{\\text{BT}}^*(y_1 \\succ y_2|x) = \\sigma\\left( u^*(x, y_1, y_2) - \\delta^*(x, y_1, y_2) \\right).\n$$\n\n**5. Conclusion:**\nAt optimality, the preference probability under the KL constraint naturally incorporates both the reward difference ($u^*$) and the KL divergence adjustment ($\\delta^*$), yielding the desired Bradley-Terry form. This balances human preference maximization with policy regularization.", "C": "The proof involves several steps to derive the Bradley-Terry model expression for human preference probability in terms of the optimal policy and reference policy. Here's a structured reasoning process:\n\n1. Optimal Policy Expression: Starting from the given optimal policy expression,\n\\begin{equation}\n\\pi_{\\theta}^*(z|[{x},y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[{x},y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],z)\\right)}{Z([{x},y^{<t}];\\beta)},\n\\end{equation}\nwhere $Z([{x},y^{<t}];\\beta)$ is the partition function.\n\n2. Rearranged Equation: Rearranging the above equation gives the Q-function in terms of the optimal and reference policies,\n\\begin{equation}\nQ_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],z) = \\beta \\log\\frac{\\pi_{\\theta}^*(z|[{x}, y^{<t}])}{\\pi_{\\mathrm{ref}}(z|[{x}, y^{<t}])} + \\beta \\log Z([{x}, y^{<t}];\\beta).\n\\end{equation}\n\n3. Advantage Function: The advantage function is derived from the Q-function and value function,\n\\begin{equation}\nA_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}], y^{t}) = Q_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],y^{t}) - V_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}]).\n\\end{equation}\n\n4. Preference Probability: The preference probability is expressed in terms of the advantage functions for two responses,\n\\begin{equation}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x}) = \\sigma\\left(\\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)- \\sum_{t=1}^{T_2}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right).\n\\end{equation}\n\n5. Final Expression: Substituting the expressions for $u({x}, {y}_1, {y}_2)$ and $\\delta({x}, {y}_1, {y}_2)$ into the preference probability formula yields the final theorem,\n\\begin{equation}\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x})=\\sigma(u^*({x}, {y}_1, {y}_2) - \\delta^*({x}, {y}_1, {y}_2)).\n\\end{equation}\n\nThis proof systematically connects the optimal policy, advantage function, and preference probability to arrive at the desired theorem.", "D": "The proof involves several steps to derive the Bradley-Terry model expression for human preference probability in terms of the optimal policy and reference policy. Here's a structured reasoning process:\n\n1. Optimal Policy Expression: Starting from the given optimal policy expression,\n\\begin{equation}\n\\pi_{\\theta}^*(z|[{x},y^{<t}]) = \\frac{\\pi_{\\mathrm{ref}}(z|[{x},y^{<t}])\\exp\\left(\\frac{1}{\\beta}Q_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],z)\\right)}{Z([{x},y^{<t}];\\beta)},\n\\end{equation}\nwhere $Z([{x},y^{<t}];\\beta)$ is the partition function.\n\n2. Rearranged Equation: Rearranging the above equation gives the Q-function in terms of the optimal and reference policies,\n\\begin{equation}\nQ_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],z) = \\beta \\log\\frac{\\pi_{\\theta}^*(z|[{x}, y^{<t}])}{\\pi_{\\mathrm{ref}}(z|[{x}, y^{<t}])} + \\beta \\log Z([{x}, y^{<t}];\\beta).\n\\end{equation}\n\n3. Advantage Function: The advantage function is derived from the Q-function and value function,\n\\begin{equation}\nA_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}], y^{t}) = Q_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}],y^{t}) - V_{\\pi_{\\mathrm{ref}}}([{x},y^{<t}]).\n\\end{equation}\n\n4. Preference Probability: The preference probability is expressed in terms of the advantage functions for two responses,\n\\begin{equation}\nP_{\\mathrm{BT}}({y}_1\\succ {y}_2 | {x}) = \\sigma\\left(\\sum_{t=1}^{T_1}\\left(\\gamma^{t-1} A_{\\pi}([{x},y_1^{<t}], y_1^t)\\right)- \\sum_{t=1}^{T_2}\\left(\\gamma^{t} A_{\\pi}([{x},y_2^{<t}], y_2^t)\\right)\\right).\n\\end{equation}\n\n5. Final Expression: Substituting the expressions for $u({x}, {y}_1, {y}_2)$ and $\\delta({x}, {y}_1, {y}_2)$ into the preference probability formula yields the final theorem,\n\\begin{equation}\nP_{\\mathrm{BT}}^*({y}_1 \\succ {y}_2 |{x})=\\sigma(u^*({x}, {y}_1, {y}_2) - \\delta^*({x}, {y}_1, {y}_2)).\n\\end{equation}\n\nThis proof systematically connects the optimal policy, advantage function, and preference probability to arrive at the desired theorem.", "ground-truth": "C"}
{"paper": "arxiv:2402.01306", "question": "Prove the following theorem:\n\\begin{theorem}\n    DPO and PPO-Clip are human-aware losses.\n\\end{theorem}. It is required to prove that DPO and PPO-Clip satisfy the definition of HALO, which means they can be expressed in the following form:\n\\[f(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}[a_{x,y}v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])] + C_D\\]\n\nThe DPO loss function is:\n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\left(\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}\\right) - \\beta \\log\\left(\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right]\\]\n\nThe PPO-Clip loss function is:\n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere \\[q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})}\\]", "A": "To prove that DPO and PPO-Clip are human-aware losses (HALO), we need to show that they can be expressed in the form:\n\n\\[\nf(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}\\left[a_{x,y}v\\left(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')]\\right)\\right] + C_D\n\\]\n\nwhere:\n- \\(a_{x,y}\\) is a weighting term,\n- \\(v\\) is a scalar function,\n- \\(r_\\theta(x,y)\\) is a reward or scoring function,\n- \\(Q\\) is a distribution over \\(y'\\) (e.g., reference policy \\(\\pi_{\\text{ref}}\\)),\n- \\(C_D\\) is a constant depending on the dataset \\(D\\).\n\n---\n\n### **1. Proof for DPO**\n\nThe DPO loss is:\n\n\\[\nL_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\left(\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}\\right) - \\beta \\log\\left(\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right]\n\\]\n\nLet \\(r_\\theta(x,y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\\). Then the loss can be rewritten as:\n\n\\[\nL_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(r_\\theta(x,y_w) - r_\\theta(x,y_l)\\right)\\right]\n\\]\n\nThis resembles the HALO form:\n- The term \\(r_\\theta(x,y_w) - r_\\theta(x,y_l)\\) can be seen as \\(r_\\theta(x,y_w) - \\mathbb{E}_{y' \\sim \\pi_{\\text{ref}}}[r_\\theta(x,y')]\\), where the expectation is approximated by a single sample \\(y_l\\) from the dataset.\n- The weighting term \\(a_{x,y} = 1\\) (implicitly, since all pairs are equally weighted).\n- The function \\(v(z) = -\\log \\sigma(z)\\).\n- The constant \\(C_D\\) is absorbed into the expectation (since the dataset is fixed).\n\nThus, DPO fits the HALO framework.\n\n---\n\n### **2. Proof for PPO-Clip**\n\nThe PPO-Clip loss is:\n\n\\[\nL_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\n\\]\n\nwhere:\n\\[\nq_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})}\n\\]\n\nLet \\(r_\\theta(x,y) = \\log \\pi_\\theta(y|x)\\) (the log-probability as a reward). The advantage \\(A(x:y_{<t}, y_t)\\) can be seen as a weighting term \\(a_{x,y}\\). The clipped ratio ensures that updates are conservative, which can be interpreted as a form of human-aware regularization.\n\nRewriting the loss:\n\n\\[\nL_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[A(x:y_{<t}, y_t) \\cdot \\min\\left(q_\\theta, \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)\\right)\\right]\n\\]\n\nThis can be mapped to the HALO form:\n- The term \\(\\min(q_\\theta, \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon))\\) acts as a constraint on the reward difference \\(r_\\theta(x,y) - r_{\\text{old}}(x,y)\\).\n- The weighting term \\(a_{x,y} = A(x:y_{<t}, y_t)\\).\n- The function \\(v(z)\\) is implicitly the clipping operation, ensuring bounded updates.\n- The expectation over \\(y_t\\) is analogous to \\(\\mathbb{E}_Q[r_\\theta(x,y')]\\), where \\(Q\\) is the old policy \\(\\pi_{\\text{old}}\\).\n\nThus, PPO-Clip also fits the HALO framework.\n\n---\n\n### **Conclusion**\nBoth DPO and PPO-Clip can be expressed in the HALO form, where:\n- The reward \\(r_\\theta(x,y)\\) is derived from policy probabilities,\n- The weighting \\(a_{x,y}\\) and function \\(v\\) enforce human-like constraints (e.g., pairwise comparisons in DPO, clipping in PPO),\n- The expectation over \\(y'\\) is approximated by the reference policy or old policy.\n\nTherefore, **DPO and PPO-Clip are human-aware losses**. \\(\\square\\)", "B": "\\begin{proof}\nFor a loss $f$ to be a HALO, we need to first construct the human value\n\\[v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])\\]\nwhere $r_\\theta(x,y) = l(x,y) \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$ is the implied reward (normalized by factor $l(y)$), $Q(Y'|x)$ is an input-conditioned reference point distribution, and $v: \\mathbb{R} \\to \\mathbb{R}$ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in $(0, \\infty)$.\n\nThe DPO loss is\n\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x, y_w, y_l} \\left[  -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]\\]\nwhere $\\beta > 0$ is a hyperparameter.\nDPO meets the criteria with the following construction: $l(y) = \\beta$; $r_\\theta = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$; $v(\\cdot) = \\log \\sigma(\\cdot)$ is increasing and concave everywhere; $Q$ places all mass on $(x, y_l)$, where $y_l$ is a dispreferred output for $x$ such that $y \\succ y_l$; and $a_{x,y} = -1$.\n\nThe PPO-Clip loss is\n\\[\\mathcal{L}_{\\text{PPO (offline)}} = -\\mathbb{E}_{x,y,t \\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta, 1 - \\epsilon, 1 + \\epsilon) A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere $q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{ref}}(y_t|x:y_{<t})}$ are the token-level probability ratios (where $y_{< t}$ denotes the output sequence up to the $t$-th token), $A$ denotes the token-level advantages, and $\\epsilon \\in (0,1)$ is a hyperparameter.\n\nSince this is a token-level objective, let $x:y_{<t}$ denote the actual input and the token $y_i$ the actual output for the purpose of framing this as a HALO.\nThe advantage function $A(x:y_{<t}, y_t)$ can be expressed as $Q^\\pi(x:y_{<t}, y_t) - V^\\pi(x:y_{<t})$, the difference between the action-value and value functions.\nBecause $V^\\pi(x:y_{<t}) = \\mathbb{E}_{y \\sim \\pi}Q^\\pi(x:y_{<t}, y)$, the reference point distribution is simply the policy.\n\nThe HALO-defined reward $r_\\theta$ is then implied by the product $q_\\theta Q^\\pi(x:y_{<t}, y)$.\nAssume without loss of generality that $Q^\\pi$ is non-negative, since a constant can be added to $Q^\\pi$ without changing the advantage.\nThen means $\\exists u \\geq 1, q_\\theta Q^\\pi(x:y_{<t}, y) = \\log u = \\log \\hat{\\pi}_\\theta(x:y_{<t}, y) / \\hat{\\pi}_{\\text{ref}}(x:y_{<t}, y)$, where $\\hat{\\pi}_\\theta, \\hat{\\pi}_{\\text{ref}}$ are some implied policy and reference distributions.\nIt is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise.\nWhere $q_\\theta A = r_\\theta - z_0$ in the HALO notation:\n\\[v(q_\\theta A) = \\begin{cases} A \\min(q_\\theta, 1 + \\epsilon)& \\text{if } A(x:y_{<t}, y_t) \\geq 0 \\\\\nA \\max(q_\\theta, 1 - \\epsilon)& \\text{if } A(x:y_{<t}, y_t) < 0 \\\\\n\\end{cases}\\]\nwhich we can combine as $v(q_\\theta A) = \\min(q_\\theta A, A(1 + \\text{sign}(q_\\theta A) \\epsilon))$.\n$a_{x,y} = -1$ completes the construction.\n\\end{proof}", "C": "\\begin{proof}\nFor a loss $f$ to be a HALO, we need to first construct the human value\n\\[v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])\\]\nwhere $r_\\theta(x,y) = l(x,y) \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$ is the implied reward (normalized by factor $l(y)$), $Q(Y'|x)$ is an input-conditioned reference point distribution, and $v: \\mathbb{R} \\to \\mathbb{R}$ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in $(0, \\infty)$.\n\nThe DPO loss is\n\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x, y_w, y_l} \\left[  -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]\\]\nwhere $\\beta > 0$ is a hyperparameter.\nDPO meets the criteria with the following construction: $l(y) = \\beta$; $r_\\theta = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$; $v(\\cdot) = \\log \\sigma(\\cdot)$ is increasing and concave everywhere; $Q$ places all mass on $(x, y_l)$, where $y_l$ is a dispreferred output for $x$ such that $y \\succ y_l$; and $a_{x,y} = -1$.\n\nThe PPO-Clip loss is\n\\[\\mathcal{L}_{\\text{PPO (offline)}} = -\\mathbb{E}_{x,y,t \\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta, 1 - \\epsilon, 1 + \\epsilon) A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere $q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{ref}}(y_t|x:y_{<t})}$ are the token-level probability ratios (where $y_{< t}$ denotes the output sequence up to the $t$-th token), $A$ denotes the token-level advantages, and $\\epsilon \\in (0,1)$ is a hyperparameter.\n\nSince this is a token-level objective, let $x:y_{<t}$ denote the actual input and the token $y_i$ the actual output for the purpose of framing this as a HALO.\nThe advantage function $A(x:y_{<t}, y_t)$ can be expressed as $Q^\\pi(x:y_{<t}, y_t) - V^\\pi(x:y_{<t})$, the difference between the action-value and value functions.\nBecause $V^\\pi(x:y_{<t}) = \\mathbb{E}_{y \\sim \\pi}Q^\\pi(x:y_{<t}, y)$, the reference point distribution is simply the policy.\n\nThe HALO-defined reward $r_\\theta$ is then implied by the product $q_\\theta Q^\\pi(x:y_{<t}, y)$.\nAssume without loss of generality that $Q^\\pi$ is non-negative, since a constant can be added to $Q^\\pi$ without changing the advantage.\nThen means $\\exists u \\geq 1, q_\\theta Q^\\pi(x:y_{<t}, y) = \\log u = \\log \\hat{\\pi}_\\theta(x:y_{<t}, y) / \\hat{\\pi}_{\\text{ref}}(x:y_{<t}, y)$, where $\\hat{\\pi}_\\theta, \\hat{\\pi}_{\\text{ref}}$ are some implied policy and reference distributions.\nIt is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise.\nWhere $q_\\theta A = r_\\theta - z_0$ in the HALO notation:\n\\[v(q_\\theta A) = \\begin{cases} A \\min(q_\\theta, 1 + \\epsilon)& \\text{if } A(x:y_{<t}, y_t) \\geq 0 \\\\\nA \\max(q_\\theta, 1 - \\epsilon)& \\text{if } A(x:y_{<t}, y_t) < 0 \\\\\n\\end{cases}\\]\nwhich we can combine as $v(q_\\theta A) = \\min(q_\\theta A, A(1 + \\text{sign}(A) \\epsilon))$.\n$a_{x,y} = -1$ completes the construction.\n\\end{proof}", "D": "To prove that DPO and PPO-Clip satisfy the HALO definition, we demonstrate their equivalence to the form: \n\\[f(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}\\left[a_{x,y}v\\left(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')]\\right)\\right] + C_D\\]\n\n---\n\n### **Proof for DPO**  \n**DPO Loss**:  \n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right]\\]\n\n**Key Observations**:  \n1. **Reward Parametrization**:  \n   Define \\( r_\\theta(x,y) = \\beta \\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} \\). This aligns with DPO’s implicit reward model.  \n\n2. **Contrastive Structure**:  \n   The term inside \\(\\sigma\\) becomes \\( r_\\theta(x,y_w) - r_\\theta(x,y_l) \\).  \n\n3. **HALO Mapping**:  \n   - Let \\( Q \\) be the empirical distribution of \\( y_l \\) conditioned on \\( x \\), i.e., \\( \\mathbb{E}_Q[r_\\theta(x,y')] = r_\\theta(x,y_l) \\).  \n   - Set \\( a_{x,y} = \\mathbb{I}[y=y_w] \\) (indicator for preferred completions).  \n   - Define \\( v(z) = -\\log \\sigma(z) = \\log(1 + e^{-z}) \\).  \n\n**Rewriting DPO Loss**:  \n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l}\\left[a_{x,y_w}v\\left(r_\\theta(x,y_w) - \\mathbb{E}_Q[r_\\theta(x,y')]\\right)\\right]\\]\nSince \\( \\mathbb{E}_Q[r_\\theta(x,y')] = r_\\theta(x,y_l) \\), this matches HALO’s form with \\( C_D = 0 \\).  \n\n---\n\n### **Proof for PPO-Clip**  \n**PPO-Clip Loss**:  \n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere \\( q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})} \\).  \n\n**Key Observations**:  \n1. **Reward Parametrization**:  \n   Define \\( r_\\theta(x,y) = \\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)} \\). Then \\( q_\\theta = e^{r_\\theta(x,y)} \\).  \n\n2. **Advantage as Baseline Shift**:  \n   The advantage \\( A(x:y_{<t}, y_t) \\) typically estimates \\( A = r_\\theta(x,y) - \\mathbb{E}_{y'\\sim \\pi_{\\text{old}}}[r_\\theta(x,y')] \\), where \\( \\mathbb{E}_{Q}[r_\\theta(x,y')] = \\mathbb{E}_{y'\\sim \\pi_{\\text{old}}}[r_\\theta(x,y')] \\).  \n\n3. **HALO Mapping**:  \n   - Let \\( Q = \\pi_{\\text{old}} \\).  \n   - Set \\( a_{x,y} = A(x:y_{<t}, y_t) \\).  \n   - Define \\( v(z) = \\min(e^z, \\text{clip}(e^z,1-\\epsilon,1+\\epsilon)) \\).  \n\n**Rewriting PPO-Clip Loss**:  \n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t}\\left[a_{x,y} \\cdot v\\left(r_\\theta(x,y) - \\mathbb{E}_Q[r_\\theta(x,y')]\\right)\\right]\\]\nThis matches HALO’s form with \\( C_D = 0 \\).  \n\n---\n\n### **Conclusion**  \nBoth DPO and PPO-Clip can be expressed in the HALO framework by:  \n1. Defining \\( r_\\theta(x,y) \\) as a policy-derived reward.  \n2. Setting \\( a_{x,y} \\) and \\( Q \\) to capture human preference or baseline expectations.  \n3. Choosing \\( v(\\cdot) \\) to enforce alignment constraints (contrastive or clipping).  \n\nThus, they satisfy the definition of human-aware losses. \\(\\square\\)", "ground-truth": "B"}
{"paper": "arxiv:2402.01306", "question": "Prove the following proposition:\n\\begin{proposition}\n    As the reward implied by the current policy tends to $\\pm \\infty$, the KTO update of $\\pi_\\theta$ tends to zero.\n\\end{proposition}\n\nThe KTO loss function is:\n\\[ L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}[\\lambda_y - v(x,y)] \\]\nwhere $v(x,y)$ is defined as:\n\\[ v(x,y) = \\begin{cases} \n\\lambda_D \\sigma(\\beta(r_\\theta(x,y) - z_0)) & \\text{if } y \\text{ is desirable given } x \\\\\n\\lambda_U \\sigma(\\beta(z_0 - r_\\theta(x,y))) & \\text{if } y \\text{ is undesirable given } x \n\\end{cases} \\]", "A": "\\begin{proof}\nWhere $d(y)$ is $-1$ ($+1$) when $y$ is desirable (undesirable), $\\lambda_y$ is $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable (undesirable), and $z = r_\\theta(x,y) - z_0$, the derivative of the KTO loss is\n\\begin{equation}\n\\nabla_\\theta L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}\\left[ d(y) \\lambda_y \\sigma(\\beta z) (1 - \\sigma(\\beta z)) \\beta \\nabla_\\theta \\log \\pi_\\theta(y|x) \\right]\n\\label{eq:kto_derivative}\n\\end{equation}\nNote that we do not backpropagate through the KL term in the KTO loss and $\\beta, \\lambda_y > 0$.\nThis gradient is simple to interpret: if $y$ is desirable, then $d(y)$ is negative and we push up the probability of $\\pi_\\theta(y|x)$ to minimize the loss; if $y$ is undesirable, then $d(y)$ is positive and we push down the probability of $\\pi_\\theta(y|x)$ to minimize the loss.\nAs $r_\\theta$ tends to $\\pm \\infty$, the gradient will tend to zero since either $(1 - \\sigma(\\beta z))$ or $\\sigma(\\beta z)$ will tend to zero.\n\\end{proof}", "B": "\\begin{proof}\nWhere $d(y)$ is $-1$ ($+1$) when $y$ is desirable (undesirable), $\\lambda_y$ is $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable (undesirable), and $z = r_\\theta(x,y) - z_0$, the derivative of the KTO loss is\n\\begin{equation}\n\\nabla_\\theta L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}\\left[ d(y) \\lambda_y \\sigma(\\beta z) (1 - \\sigma(\\beta z)) \\beta \\nabla_\\theta \\log \\pi_\\theta(y|x) \\right]\n\\label{eq:kto_derivative}\n\\end{equation}\nNote that we do not backpropagate through the KL term in the KTO loss and $\\beta, \\lambda_y > 0$.\nThis gradient is simple to interpret: if $y$ is desirable, then $d(y)$ is negative and we push up the probability of $\\pi_\\theta(y|x)$ to minimize the loss; if $y$ is undesirable, then $d(y)$ is positive and we push down the probability of $\\pi_\\theta(y|x)$ to minimize the loss.\nAs $r_\\theta$ tends to $\\pm \\infty$, the gradient will tend to zero since either $(1 - \\sigma(\\beta z))$ or $\\sigma(\\beta z)$ will tend to one.\n\\end{proof}", "C": "To prove the proposition, we analyze the gradient of the KTO loss function with respect to policy parameters $\\theta$ as the implied reward $r_\\theta(x,y)$ tends to $\\pm\\infty$. The update magnitude depends on $\\nabla_\\theta L_{\\text{KTO}} = \\mathbb{E}[\\nabla_\\theta(\\lambda_y - v(x,y))] = -\\mathbb{E}[\\nabla_\\theta v(x,y)]$. We examine the behavior of $\\nabla_\\theta v(x,y)$ in all cases:\n\n---\n\n### Case 1: $y$ is desirable\nHere, $v(x,y) = \\lambda_D \\sigma(\\beta(r_\\theta - z_0))$.  \nThe gradient is:  \n$$\n\\nabla_\\theta v = \\lambda_D \\beta \\sigma'(\\beta(r_\\theta - z_0)) \\nabla_\\theta r_\\theta\n$$\nwhere $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$.\n\n- **As $r_\\theta \\to +\\infty$**:  \n  $\\sigma(\\beta(r_\\theta - z_0)) \\to 1$ $\\implies \\sigma' \\to 1 \\cdot (1 - 1) = 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n- **As $r_\\theta \\to -\\infty$**:  \n  $\\sigma(\\beta(r_\\theta - z_0)) \\to 0$ $\\implies \\sigma' \\to 0 \\cdot (1 - 0) = 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n---\n\n### Case 2: $y$ is undesirable\nHere, $v(x,y) = \\lambda_U \\sigma(\\beta(z_0 - r_\\theta))$.  \nThe gradient is:  \n$$\n\\nabla_\\theta v = -\\lambda_U \\beta \\sigma'(\\beta(z_0 - r_\\theta)) \\nabla_\\theta r_\\theta\n$$\n\n- **As $r_\\theta \\to +\\infty$**:  \n  $\\sigma(\\beta(z_0 - r_\\theta)) \\to \\sigma(-\\infty) = 0$ $\\implies \\sigma' \\to 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n- **As $r_\\theta \\to -\\infty$**:  \n  $\\sigma(\\beta(z_0 - r_\\theta)) \\to \\sigma(+\\infty) = 1$ $\\implies \\sigma' \\to 1 \\cdot (1 - 1) = 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n---\n\n### Conclusion\nIn all cases, as $|r_\\theta| \\to \\infty$, $\\nabla_\\theta v(x,y) \\to 0$. Therefore:  \n$$\n\\nabla_\\theta L_{\\text{KTO}} = -\\mathbb{E}[\\nabla_\\theta v(x,y)] \\to 0\n$$\n\nThis implies that the KTO update of $\\pi_\\theta$ (proportional to $\\nabla_\\theta L_{\\text{KTO}}$) tends to zero when the implied reward diverges to $\\pm\\infty$. $\\blacksquare$", "D": "To prove the proposition, we need to analyze the behavior of the KTO loss function \\( L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) \\) as the implied reward \\( r_\\theta(x,y) \\) tends to \\( \\pm \\infty \\). Specifically, we will show that the gradient of \\( L_{\\text{KTO}} \\) with respect to \\( \\pi_\\theta \\) tends to zero in these limits, implying that the update to \\( \\pi_\\theta \\) tends to zero.\n\n### Step 1: Analyze \\( v(x,y) \\) as \\( r_\\theta(x,y) \\to \\infty \\)\nConsider the case where \\( y \\) is desirable given \\( x \\). The term \\( v(x,y) \\) is:\n\\[\nv(x,y) = \\lambda_D \\sigma(\\beta(r_\\theta(x,y) - z_0))\n\\]\nwhere \\( \\sigma \\) is the sigmoid function. As \\( r_\\theta(x,y) \\to \\infty \\), \\( \\beta(r_\\theta(x,y) - z_0) \\to \\infty \\), and thus:\n\\[\n\\sigma(\\beta(r_\\theta(x,y) - z_0)) \\to 1\n\\]\nTherefore:\n\\[\nv(x,y) \\to \\lambda_D\n\\]\nSimilarly, for \\( y \\) undesirable given \\( x \\), \\( v(x,y) = \\lambda_U \\sigma(\\beta(z_0 - r_\\theta(x,y))) \\). As \\( r_\\theta(x,y) \\to \\infty \\), \\( \\beta(z_0 - r_\\theta(x,y)) \\to -\\infty \\), and thus:\n\\[\n\\sigma(\\beta(z_0 - r_\\theta(x,y))) \\to 0\n\\]\nTherefore:\n\\[\nv(x,y) \\to 0\n\\]\n\n### Step 2: Analyze \\( v(x,y) \\) as \\( r_\\theta(x,y) \\to -\\infty \\)\nFor \\( y \\) desirable given \\( x \\), as \\( r_\\theta(x,y) \\to -\\infty \\), \\( \\beta(r_\\theta(x,y) - z_0) \\to -\\infty \\), and thus:\n\\[\n\\sigma(\\beta(r_\\theta(x,y) - z_0)) \\to 0\n\\]\nTherefore:\n\\[\nv(x,y) \\to 0\n\\]\nFor \\( y \\) undesirable given \\( x \\), as \\( r_\\theta(x,y) \\to -\\infty \\), \\( \\beta(z_0 - r_\\theta(x,y)) \\to \\infty \\), and thus:\n\\[\n\\sigma(\\beta(z_0 - r_\\theta(x,y))) \\to 1\n\\]\nTherefore:\n\\[\nv(x,y) \\to \\lambda_U\n\\]\n\n### Step 3: Behavior of \\( L_{\\text{KTO}} \\) in the Limits\nThe KTO loss is:\n\\[\nL_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}[\\lambda_y - v(x,y)]\n\\]\nwhere \\( \\lambda_y \\) is a constant (either \\( \\lambda_D \\) or \\( \\lambda_U \\)) depending on whether \\( y \\) is desirable or undesirable. \n\n- As \\( r_\\theta(x,y) \\to \\infty \\):\n  - For desirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_D - \\lambda_D = 0 \\).\n  - For undesirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_U - 0 = \\lambda_U \\).\n  However, the gradient of \\( L_{\\text{KTO}} \\) with respect to \\( \\pi_\\theta \\) depends on the derivative of \\( v(x,y) \\), which tends to zero because \\( \\sigma'(\\cdot) \\to 0 \\) as its input tends to \\( \\pm \\infty \\).\n\n- As \\( r_\\theta(x,y) \\to -\\infty \\):\n  - For desirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_D - 0 = \\lambda_D \\).\n  - For undesirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_U - \\lambda_U = 0 \\).\n  Again, the gradient tends to zero because \\( \\sigma'(\\cdot) \\to 0 \\).\n\n### Step 4: Gradient of \\( L_{\\text{KTO}} \\)\nThe gradient of \\( L_{\\text{KTO}} \\) with respect to \\( \\pi_\\theta \\) is:\n\\[\n\\nabla_{\\pi_\\theta} L_{\\text{KTO}} = -\\mathbb{E}_{x,y \\sim D}[\\nabla_{\\pi_\\theta} v(x,y)]\n\\]\nFor desirable \\( y \\):\n\\[\n\\nabla_{\\pi_\\theta} v(x,y) = \\lambda_D \\beta \\sigma'(\\beta(r_\\theta(x,y) - z_0)) \\nabla_{\\pi_\\theta} r_\\theta(x,y)\n\\]\nFor undesirable \\( y \\):\n\\[\n\\nabla_{\\pi_\\theta} v(x,y) = -\\lambda_U \\beta \\sigma'(\\beta(z_0 - r_\\theta(x,y))) \\nabla_{\\pi_\\theta} r_\\theta(x,y)\n\\]\nSince \\( \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\), and \\( \\sigma(z) \\to 0 \\) or \\( 1 \\) as \\( z \\to \\pm \\infty \\), we have \\( \\sigma'(z) \\to 0 \\) in both cases. Thus:\n\\[\n\\nabla_{\\pi_\\theta} v(x,y) \\to 0 \\quad \\text{as} \\quad r_\\theta(x,y) \\to \\pm \\infty\n\\]\nThis implies:\n\\[\n\\nabla_{\\pi_\\theta} L_{\\text{KTO}} \\to 0\n\\]\n\n### Conclusion\nAs the implied reward \\( r_\\theta(x,y) \\) tends to \\( \\pm \\infty \\), the gradient of the KTO loss with respect to \\( \\pi_\\theta \\) tends to zero, meaning the update to \\( \\pi_\\theta \\) tends to zero. This completes the proof.", "ground-truth": "A"}
{"paper": "arxiv:2402.01306", "question": "Prove the following theorem:\n\\begin{theorem}\n    Assuming the value function is logistic, for a reward function $r^*_a$ that maximizes the objective, there exists a reward function in its equivalence class (i.e., $r^*_b(x,y) = r^*_a(x,y) + h(x)$ for some $h(x)$) that induces the same optimal policy $\\pi^*$ and the same Bradley-Terry preference distribution but a different human value distribution.\n\\end{theorem}", "A": "\\begin{proof}\nWe say $r^*_a$ and $r^*_b$ are in the same equivalence class if there exists some function $h(x)$ such that $r^*_b(x,y) = r^*_a(x,y) + h(x)$. Two functions in the same equivalence class induce the same optimal policy:\n\\begin{equation*}\n\\begin{split}\n\\pi^*_{r_a}(y|x) &= \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y)\\right) \\\\\n&= \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y)\\right) \\exp\\left( \\frac{1}{\\beta} h(x)\\right)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y) \\right) \\exp\\left(\\frac{1}{\\beta} h(x) \\right) \\\\\n&= \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} (r^*_a(x,y) +h(x))\\right)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} (r^*_a(x,y) + h(x))\\right) \\\\\n&= \\pi^*_{r_b}(y|x) \\\\\n\\end{split}\n\\end{equation*}\nFor a Bradley-Terry model of preferences, $p(y_w \\succ y_l|x)$ is unaffected by $h(x)$ since it is added to the reward of both $y_w$ and $y_l$.\nWe will now show that the two reward functions do not necessarily induce the same distribution of human values.\n\nA Taylor series expansion of the human value of $r^*_a(x,y)$ around 0 would be:\n$$\\sigma(0) + \\sigma'(0) (r^*_a(x,y) - z_0) + \\frac{\\sigma''(0)}{2} (r^*_a(x,y) - z_0)^2 + ...$$\n\nA Taylor series expansion of the value of $r^*_a(x,y) + h(x)$ around $h(x)$ would be:\n$$\\sigma(h(x)) + \\sigma'(h(x)) (r^*_a(x,y) - z_0) + \\frac{\\sigma''(h(x))}{2} (r^*_a(x,y) - z_0)^2 + ...$$\n\nSince $\\sigma$ is strictly monotonic, for these series to be equal, we must have $h(x) = 0$.\nIf this is not the case, then the values of $r^*_a(x,y)$ and $r^*_b(x,y)$ will be different.\nThus two arbitrary reward functions in the same equivalence class do not induce the same distribution of human values.\n\\end{proof}", "B": "\\begin{proof}\nWe say $r^*_a$ and $r^*_b$ are in the same equivalence class if there exists some function $h(x)$ such that $r^*_b(x,y) = r^*_a(x,y) + h(x)$. Two functions in the same equivalence class induce the same optimal policy:\n\\begin{equation*}\n\\begin{split}\n\\pi^*_{r_a}(y|x) &= \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y)\\right) \\\\\n&= \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y)\\right) \\exp\\left( \\frac{1}{\\beta} h(x)\\right)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*_a(x,y) \\right) \\exp\\left(\\frac{1}{\\beta} h(x) \\right) \\\\\n&= \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} (r^*_a(x,y) +h(x))\\right)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} (r^*_a(x,y) + h(x))\\right) \\\\\n&= \\pi^*_{r_b}(y|x) \\\\\n\\end{split}\n\\end{equation*}\nFor a Bradley-Terry model of preferences, $p(y_w \\succ y_l|x)$ is unaffected by $h(x)$ since it is added to the reward of both $y_w$ and $y_l$.\nWe will now show that the two reward functions do not necessarily induce the same distribution of human values.\n\nA Taylor series expansion of the human value of $r^*_a(x,y)$ around 0 would be:\n$$\\sigma(0) + \\sigma'(0) (r^*_a(x,y) - z_0) + \\frac{\\sigma''(0)}{2} (r^*_a(x,y) - z_0)^2 + ...$$\n\nA Taylor series expansion of the value of $r^*_a(x,y) + h(x)$ around $h(x)$ would be:\n$$\\sigma(h(x)) + \\sigma'(h(x)) (r^*_a(x,y) - z_0) + \\frac{\\sigma''(h(x))}{2} (r^*_a(x,y) - z_0)^2 + ...$$\n\nSince $\\sigma$ is strictly monotonic, for these series to be equal, we must have $h(x) = 0$.\nIf this is not the case, then the values of $r^*_a(x,y)$ and $r^*_b(x,y)$ will be different.\nThus two arbitrary reward functions in the same equivalence class do not induce the same distribution of human values, unless $h(x)$ is constant.\n\\end{proof}", "C": "To prove the theorem, we proceed as follows:\n\n**Theorem**:\nAssuming the value function is logistic, for a reward function \\( r^*_a \\) that maximizes the objective, there exists a reward function in its equivalence class (i.e., \\( r^*_b(x,y) = r^*_a(x,y) + h(x) \\) for some \\( h(x) \\)) that induces the same optimal policy \\( \\pi^* \\) and the same Bradley-Terry preference distribution but a different human value distribution.\n\n**Proof**:\n\n1. **Equivalence Class Preserves Bradley-Terry Preference**:\n   - For any state \\( x \\), the Bradley-Terry preference distribution is defined as:\n     \\[\n     P(y_1 \\succ y_2 \\mid x) = \\sigma\\left(r(x,y_1) - r(x,y_2)\\right),\n     \\]\n     where \\( \\sigma \\) is the logistic function.\n   - For \\( r^*_b(x,y) = r^*_a(x,y) + h(x) \\), the difference in rewards between actions \\( y_1 \\) and \\( y_2 \\) is:\n     \\[\n     r^*_b(x,y_1) - r^*_b(x,y_2) = \\left(r^*_a(x,y_1) + h(x)\\right) - \\left(r^*_a(x,y_2) + h(x)\\right) = r^*_a(x,y_1) - r^*_a(x,y_2).\n     \\]\n   - Thus, \\( P_b(y_1 \\succ y_2 \\mid x) = P_a(y_1 \\succ y_2 \\mid x) \\). The preference distribution remains unchanged.\n\n2. **Optimal Policy \\( \\pi^* \\) is Invariant**:\n   - The optimal policy \\( \\pi^* \\) selects actions to maximize the reward:\n     \\[\n     \\pi^*(y \\mid x) = \\arg\\max_{y} r(x,y).\n     \\]\n   - Since \\( r^*_b(x,y) = r^*_a(x,y) + h(x) \\), the **relative reward ranking** for all actions \\( y \\) in state \\( x \\) is preserved. Specifically:\n     \\[\n     \\arg\\max_{y} r^*_b(x,y) = \\arg\\max_{y} \\left(r^*_a(x,y) + h(x)\\right) = \\arg\\max_{y} r^*_a(x,y).\n     \\]\n   - Hence, \\( \\pi^* \\) remains unchanged under \\( r^*_b \\).\n\n3. **Human Value Distribution Differs**:\n   - Assume the \"human value distribution\" corresponds to the state value function \\( V(x) \\), defined as:\n     \\[\n     V(x) = \\mathbb{E}_{y \\sim \\pi^*(\\cdot \\mid x)}\\left[r(x,y)\\right].\n     \\]\n   - For \\( r^*_a \\), the value is \\( V_a(x) = \\mathbb{E}_{y \\sim \\pi^*}\\left[r^*_a(x,y)\\right] \\).\n   - For \\( r^*_b \\), the value becomes:\n     \\[\n     V_b(x) = \\mathbb{E}_{y \\sim \\pi^*}\\left[r^*_a(x,y) + h(x)\\right] = V_a(x) + h(x).\n     \\]\n   - By choosing \\( h(x) \\neq 0 \\), we ensure \\( V_b(x) \\neq V_a(x) \\). For example, \\( h(x) = c \\) (a constant) shifts all values uniformly, altering the distribution while preserving \\( \\pi^* \\) and preferences.\n\n**Conclusion**:\nThere exists \\( r^*_b(x,y) = r^*_a(x,y) + h(x) \\) in the equivalence class of \\( r^*_a \\) that induces the same \\( \\pi^* \\) and Bradley-Terry distribution but a distinct human value distribution. This completes the proof.", "D": "To prove the theorem, we will proceed step-by-step, leveraging the properties of the logistic value function and the equivalence classes of reward functions.\n\n### Definitions and Assumptions\n1. **Logistic Value Function**: The value function \\( V^\\pi(x) \\) is logistic, meaning it takes the form:\n   \\[\n   V^\\pi(x) = \\log \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ e^{r(x,y)} \\right]\n   \\]\n   where \\( r(x,y) \\) is the reward function, and \\( \\pi(y|x) \\) is the policy.\n\n2. **Equivalence Class of Reward Functions**: Two reward functions \\( r_a \\) and \\( r_b \\) are equivalent if they differ by a function \\( h(x) \\) that depends only on the state \\( x \\):\n   \\[\n   r_b(x,y) = r_a(x,y) + h(x)\n   \\]\n   This equivalence preserves the optimal policy and the Bradley-Terry preference distribution.\n\n3. **Bradley-Terry Preference Distribution**: The preference probability \\( P(y_1 \\succ y_2 | x) \\) is given by:\n   \\[\n   P(y_1 \\succ y_2 | x) = \\frac{e^{r(x,y_1)}}{e^{r(x,y_1)} + e^{r(x,y_2)}}\n   \\]\n   This is invariant under the addition of \\( h(x) \\) to \\( r(x,y) \\).\n\n4. **Optimal Policy**: The optimal policy \\( \\pi^* \\) maximizes the expected reward and is given by:\n   \\[\n   \\pi^*(y|x) \\propto e^{r(x,y)}\n   \\]\n   This is also invariant under the addition of \\( h(x) \\).\n\n### Proof Steps\n\n1. **Existence of \\( r^*_a \\)**: By assumption, \\( r^*_a \\) maximizes the objective (e.g., expected reward or some other criterion). The optimal policy \\( \\pi^* \\) and Bradley-Terry distribution induced by \\( r^*_a \\) are:\n   \\[\n   \\pi^*(y|x) \\propto e^{r^*_a(x,y)}, \\quad P(y_1 \\succ y_2 | x) = \\frac{e^{r^*_a(x,y_1)}}{e^{r^*_a(x,y_1)} + e^{r^*_a(x,y_2)}}\n   \\]\n\n2. **Constructing \\( r^*_b \\)**: Define \\( r^*_b(x,y) = r^*_a(x,y) + h(x) \\), where \\( h(x) \\) is an arbitrary function of \\( x \\). Then:\n   - The optimal policy becomes:\n     \\[\n     \\pi^*(y|x) \\propto e^{r^*_b(x,y)} = e^{r^*_a(x,y) + h(x)} = e^{h(x)} e^{r^*_a(x,y)} \\propto e^{r^*_a(x,y)}\n     \\]\n     Thus, \\( \\pi^* \\) is unchanged.\n   - The Bradley-Terry preference distribution becomes:\n     \\[\n     P(y_1 \\succ y_2 | x) = \\frac{e^{r^*_b(x,y_1)}}{e^{r^*_b(x,y_1)} + e^{r^*_b(x,y_2)}} = \\frac{e^{r^*_a(x,y_1) + h(x)}}{e^{r^*_a(x,y_1) + h(x)} + e^{r^*_a(x,y_2) + h(x)}} = \\frac{e^{r^*_a(x,y_1)}}{e^{r^*_a(x,y_1)} + e^{r^*_a(x,y_2)}}\n     \\]\n     Thus, the preference distribution is unchanged.\n\n3. **Human Value Distribution**: The human value distribution \\( V^\\pi(x) \\) is given by:\n   \\[\n   V^\\pi(x) = \\log \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ e^{r(x,y)} \\right]\n   \\]\n   For \\( r^*_a \\), this is:\n   \\[\n   V_a^\\pi(x) = \\log \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ e^{r^*_a(x,y)} \\right]\n   \\]\n   For \\( r^*_b \\), this becomes:\n   \\[\n   V_b^\\pi(x) = \\log \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ e^{r^*_b(x,y)} \\right] = \\log \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ e^{r^*_a(x,y) + h(x)} \\right] = h(x) + \\log \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ e^{r^*_a(x,y)} \\right] = h(x) + V_a^\\pi(x)\n   \\]\n   Thus, \\( V_b^\\pi(x) \\neq V_a^\\pi(x) \\) unless \\( h(x) = 0 \\). By choosing \\( h(x) \\neq 0 \\), we can ensure that the human value distribution is different.\n\n### Conclusion\nWe have shown that for any \\( r^*_a \\), there exists an equivalent \\( r^*_b(x,y) = r^*_a(x,y) + h(x) \\) (for \\( h(x) \\neq 0 \\)) that induces the same optimal policy \\( \\pi^* \\) and the same Bradley-Terry preference distribution but a different human value distribution. This completes the proof. \\(\\square\\)", "ground-truth": "A"}
{"paper": "arxiv:2402.01306", "question": "Prove the following theorem:\n\\begin{theorem}\nFor input $x$ with outputs $\\{y_a, y_b\\}$, let dataset $D$ comprise contradictory preferences $y_a \\succ y_b$ and $y_b \\succ y_a$ in proportion $p \\in (0.5, 1)$ and $(1 - p) \\in (0, 0.5)$ respectively. \nIf $p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1 - p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)$, then the optimal DPO policy is more likely to produce the minority-preferred $y_b$; the optimal KTO policy will strictly produce the majority-preferred $y_a$ for a loss-neutral value function ($\\lambda_D = \\lambda_U$). \n\\end{theorem}", "A": "To prove the theorem, we analyze the optimal policies for DPO and KTO under the given conditions.\n\n### Part 1: Optimal DPO Policy\nThe DPO objective maximizes the likelihood of preference data under the Bradley-Terry model. For a pair $(y_a, y_b)$, the optimal policy $\\pi^*(y|x)$ satisfies:\n\\[\n\\pi^*(y|x) \\propto \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{r^*(x,y)}{\\beta}\\right)\n\\]\nwhere $r^*$ is the implicit reward function. Given contradictory preferences with proportions $p$ and $(1-p)$, the reward difference must satisfy:\n\\[\nr^*(y_a) - r^*(y_b) = \\log\\frac{p}{1-p}\n\\]\nSubstituting into the policy ratio:\n\\[\n\\frac{\\pi^*(y_a|x)}{\\pi^*(y_b|x)} = \\frac{\\pi_{\\text{ref}}(y_a|x)}{\\pi_{\\text{ref}}(y_b|x)} \\cdot \\left(\\frac{p}{1-p}\\right)^{1/\\beta}\n\\]\nUnder the theorem's condition $p^{1/\\beta} \\pi_{\\text{ref}}(y_a|x) < (1-p)^{1/\\beta} \\pi_{\\text{ref}}(y_b|x)$, this implies:\n\\[\n\\frac{\\pi^*(y_a|x)}{\\pi^*(y_b|x)} < 1 \\quad \\Rightarrow \\quad \\pi^*(y_b|x) > \\pi^*(y_a|x)\n\\]\nThus, the DPO policy favors the minority-preferred $y_b$.\n\n### Part 2: Optimal KTO Policy\nFor KTO with loss-neutral weighting ($\\lambda_D = \\lambda_U$), assume the loss function penalizes negative examples equally to positive ones. The dataset contains $p$ positive examples for $y_a$ and $(1-p)$ for $y_b$. The KTO loss minimizes:\n\\[\n\\mathcal{L}_{\\text{KTO}} = -2\\left[p \\log \\pi(y_a|x) + (1-p) \\log \\pi(y_b|x)\\right]\n\\]\nLet $\\pi_a = \\pi(y_a|x)$. Solving $\\frac{\\partial \\mathcal{L}_{\\text{KTO}}}{\\partial \\pi_a} = 0$ yields:\n\\[\n\\frac{p}{\\pi_a} = \\frac{1-p}{1-\\pi_a} \\quad \\Rightarrow \\quad \\pi_a = p\n\\]\nSince $p > 0.5$, the KTO policy strictly prefers the majority-preferred $y_a$.\n\n### Conclusion\nUnder the theorem's conditions:\n1. DPO’s optimal policy prioritizes $y_b$ due to the reference policy and temperature-scaled preference ratio.\n2. KTO’s loss-neutral objective strictly aligns with the majority preference $y_a$ by matching the dataset proportion $p$.\n\n\\boxed{\\text{Proved as per the above analysis.}}", "B": "\\begin{proof}\nWhere $u = \\beta (r_\\theta(x, y_a) - r_\\theta(x, y_b))$, we can write the total DPO loss for $x$ as\n\\begin{equation*}\n\\mathcal{L}_{\\text{DPO}}(x) = p (- \\log \\sigma(u)) + (1 - p)(- \\log \\sigma(-u))\n\\end{equation*}\nTaking the derivative with respect to $u$ and setting to zero, we get\n\\begin{equation*}\n\\begin{split}\n0 &= -p \\frac{\\sigma(u) \\sigma(-u)}{\\sigma(u)} + (1 - p) \\frac{\\sigma(-u) \\sigma(u)}{\\sigma(-u)} = -p(1 - \\sigma(u)) + (1 - p) \\sigma(u) = -p + \\sigma(u) \\\\\n\\implies u &= \\sigma^{-1}(p) \\\\\n\\beta r^*_\\theta(x, y_a) &= \\sigma^{-1}(p) + \\beta r^*_\\theta(x, y_b) \\\\\n\\beta \\log \\frac{\\pi^*_\\theta(y_a|x)}{\\pi_\\text{ref}(y_a|x)} &= \\log \\frac{p}{1 - p} + \\beta \\log \\frac{\\pi^*_\\theta(y_b|x)}{\\pi_\\text{ref}(y_b|x)} \\\\\n\\pi^*_\\theta(y_a|x) &= \\left(\\frac{p}{1 - p}\\right)^{1/\\beta} \\cdot \\frac{ \\pi_\\text{ref}(y_a|x) } {\\pi_\\text{ref}(y_b|x)} \\cdot \\pi^*_\\theta(y_b|x)\n\\end{split}\n\\end{equation*}\nThus when $p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1 - p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)$, we have $\\pi^*_\\theta(y_a|x) < \\pi^*_\\theta(y_b|x)$, meaning the optimal DPO policy is more likely to produce the minority-preferred $y_b$.\n\nWhere $u_a = \\beta(r_\\theta(x, y_a) - \\mathbb{E}_{Q}[r_\\theta(x, y')])$ and $u_b = \\beta(r_\\theta(x, y_b) - \\mathbb{E}_{Q}[r_\\theta(x, y')])$, noting that $1 - \\sigma(-u) = \\sigma(u)$, we can write the total KTO loss for $x$ as\n\\begin{equation*}\n\\begin{split}\n\\mathcal{L}_{\\text{KTO}}(x) &= p \\lambda_D (1 - \\sigma(u_a)) + (1 - p) \\lambda_U \\sigma(u_a) + p \\lambda_U \\sigma(u_b) + (1 - p) \\lambda_D (1 - \\sigma(u_b)) \\\\\n&= p \\lambda_D + ((1 - p)\\lambda_U - p \\lambda_D) \\sigma(u_a) + (1 - p) \\lambda_D + (p \\lambda_U - (1 - p) \\lambda_D) \\sigma(u_b) \\\\\n&= \\lambda_D + ((1 - p)\\lambda_U - p \\lambda_D) \\sigma(u_a) + (p \\lambda_U - (1 - p) \\lambda_D) \\sigma(u_b) \\\\\n&= \\lambda_D + \\lambda_D ((1 - 2p) \\sigma(u_a) + (2p - 1) \\sigma(u_b)) \\quad \\text{(under loss neutrality)}\n\\end{split}\n\\end{equation*}\nGiven that $p > 0.5$ by assumption and $\\lambda_D > 0$ by definition, the KTO loss is decreasing in $u_a$ and increasing in $u_b$---and thus decreasing in $r_\\theta(x, y_a)$ and increasing in $r_\\theta(x, y_b)$ respectively.\nThe optimal KTO policy is thus $\\pi^*_\\theta(y|x) = \\mathbbm{1}[y = y_b]$.\n\\end{proof}", "C": "\\begin{proof}\nWhere $u = \\beta (r_\\theta(x, y_a) - r_\\theta(x, y_b))$, we can write the total DPO loss for $x$ as\n\\begin{equation*}\n\\mathcal{L}_{\\text{DPO}}(x) = p (- \\log \\sigma(u)) + (1 - p)(- \\log \\sigma(-u))\n\\end{equation*}\nTaking the derivative with respect to $u$ and setting to zero, we get\n\\begin{equation*}\n\\begin{split}\n0 &= -p \\frac{\\sigma(u) \\sigma(-u)}{\\sigma(u)} + (1 - p) \\frac{\\sigma(-u) \\sigma(u)}{\\sigma(-u)} = -p(1 - \\sigma(u)) + (1 - p) \\sigma(u) = -p + \\sigma(u) \\\\\n\\implies u &= \\sigma^{-1}(p) \\\\\n\\beta r^*_\\theta(x, y_a) &= \\sigma^{-1}(p) + \\beta r^*_\\theta(x, y_b) \\\\\n\\beta \\log \\frac{\\pi^*_\\theta(y_a|x)}{\\pi_\\text{ref}(y_a|x)} &= \\log \\frac{p}{1 - p} + \\beta \\log \\frac{\\pi^*_\\theta(y_b|x)}{\\pi_\\text{ref}(y_b|x)} \\\\\n\\pi^*_\\theta(y_a|x) &= \\left(\\frac{p}{1 - p}\\right)^{1/\\beta} \\cdot \\frac{ \\pi_\\text{ref}(y_a|x) } {\\pi_\\text{ref}(y_b|x)} \\cdot \\pi^*_\\theta(y_b|x)\n\\end{split}\n\\end{equation*}\nThus when $p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1 - p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)$, we have $\\pi^*_\\theta(y_a|x) < \\pi^*_\\theta(y_b|x)$, meaning the optimal DPO policy is more likely to produce the minority-preferred $y_b$.\n\nWhere $u_a = \\beta(r_\\theta(x, y_a) - \\mathbb{E}_{Q}[r_\\theta(x, y')])$ and $u_b = \\beta(r_\\theta(x, y_b) - \\mathbb{E}_{Q}[r_\\theta(x, y')])$, noting that $1 - \\sigma(-u) = \\sigma(u)$, we can write the total KTO loss for $x$ as\n\\begin{equation*}\n\\begin{split}\n\\mathcal{L}_{\\text{KTO}}(x) &= p \\lambda_D (1 - \\sigma(u_a)) + (1 - p) \\lambda_U \\sigma(u_a) + p \\lambda_U \\sigma(u_b) + (1 - p) \\lambda_D (1 - \\sigma(u_b)) \\\\\n&= p \\lambda_D + ((1 - p)\\lambda_U - p \\lambda_D) \\sigma(u_a) + (1 - p) \\lambda_D + (p \\lambda_U - (1 - p) \\lambda_D) \\sigma(u_b) \\\\\n&= \\lambda_D + ((1 - p)\\lambda_U - p \\lambda_D) \\sigma(u_a) + (p \\lambda_U - (1 - p) \\lambda_D) \\sigma(u_b) \\\\\n&= \\lambda_D + \\lambda_D ((1 - 2p) \\sigma(u_a) + (2p - 1) \\sigma(u_b)) \\quad \\text{(under loss neutrality)}\n\\end{split}\n\\end{equation*}\nGiven that $p > 0.5$ by assumption and $\\lambda_D > 0$ by definition, the KTO loss is decreasing in $u_a$ and increasing in $u_b$---and thus decreasing in $r_\\theta(x, y_a)$ and increasing in $r_\\theta(x, y_b)$ respectively.\nThe optimal KTO policy is thus $\\pi^*_\\theta(y|x) = \\mathbbm{1}[y = y_a]$.\n\\end{proof}", "D": "To prove the given theorem, we will analyze the behavior of the optimal Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) policies under the specified conditions. \n\n### **1. Optimal DPO Policy Analysis**\n\nThe DPO objective maximizes the following likelihood:\n\n\\[\n\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta) = \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\text{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_\\text{ref}(y_l|x)} \\right) \\right],\n\\]\n\nwhere \\(\\sigma\\) is the sigmoid function, \\(\\beta > 0\\) is a temperature parameter, and \\(\\pi_\\text{ref}\\) is the reference policy.\n\nThe **optimal DPO policy** \\(\\pi^*_{\\text{DPO}}\\) satisfies:\n\n\\[\n\\pi^*_{\\text{DPO}}(y|x) \\propto \\pi_\\text{ref}(y|x) \\cdot \\left( \\frac{p(y \\succ y')}{p(y' \\succ y)} \\right)^{1/\\beta},\n\\]\n\nwhere \\(p(y \\succ y')\\) is the empirical preference probability.\n\nGiven the dataset \\(D\\) with proportions \\(p\\) for \\(y_a \\succ y_b\\) and \\(1-p\\) for \\(y_b \\succ y_a\\), the optimal policy becomes:\n\n\\[\n\\pi^*_{\\text{DPO}}(y_a|x) \\propto \\pi_\\text{ref}(y_a|x) \\cdot \\left( \\frac{p}{1-p} \\right)^{1/\\beta},\n\\]\n\\[\n\\pi^*_{\\text{DPO}}(y_b|x) \\propto \\pi_\\text{ref}(y_b|x) \\cdot \\left( \\frac{1-p}{p} \\right)^{1/\\beta}.\n\\]\n\nThe condition \\(p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1-p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)\\) implies:\n\n\\[\n\\pi^*_{\\text{DPO}}(y_a|x) < \\pi^*_{\\text{DPO}}(y_b|x).\n\\]\n\nThus, the DPO policy is more likely to produce the minority-preferred \\(y_b\\).\n\n---\n\n### **2. Optimal KTO Policy Analysis**\n\nThe KTO objective maximizes:\n\n\\[\n\\mathcal{L}_{\\text{KTO}}(\\pi_\\theta) = \\mathbb{E}_{(x, y) \\sim D} \\left[ \\lambda_D \\cdot \\mathbb{I}(y \\text{ is preferred}) \\cdot v(x, y) + \\lambda_U \\cdot \\mathbb{I}(y \\text{ is dispreferred}) \\cdot v(x, y) \\right],\n\\]\n\nwhere \\(v(x, y)\\) is the value function, and \\(\\lambda_D, \\lambda_U\\) are weights for desired/undesired outcomes.\n\nUnder **loss-neutral conditions (\\(\\lambda_D = \\lambda_U\\))**, the optimal KTO policy maximizes the expected value:\n\n\\[\n\\pi^*_{\\text{KTO}}(y|x) = \\arg\\max_{\\pi} \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} \\left[ v(x, y) \\right].\n\\]\n\nGiven the dataset \\(D\\) with majority preference \\(y_a \\succ y_b\\) (\\(p > 0.5\\)), the empirical value estimate for \\(y_a\\) is higher than for \\(y_b\\). Thus:\n\n\\[\n\\pi^*_{\\text{KTO}}(y|x) = \\delta(y = y_a),\n\\]\n\nwhere \\(\\delta\\) is the Dirac delta function. Therefore, the KTO policy strictly produces the majority-preferred \\(y_a\\).\n\n---\n\n### **Conclusion**\n\n- **DPO Policy**: Favors the minority-preferred \\(y_b\\) when \\(p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1-p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)\\).\n- **KTO Policy**: Under loss-neutrality, strictly produces the majority-preferred \\(y_a\\).\n\nThis completes the proof. \\(\\square\\)", "ground-truth": "C"}
{"paper": "arxiv:2304.02835", "question": "How can we prove Theorem 1: (Traditional Influence Functions) is valid? The theorem relies on (i) the definition $L_{0} = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i})$, (ii) the Hessian matrix $H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})$, and (iii) the notion of a node unlearning request $\\Delta G = \\{V^{(rm)}, \\emptyset, \\emptyset\\}$. In particular, Theorem 1 states that $\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$. Based on these formulas and definitions, how do we derive and prove that relationship?", "A": "To prove Theorem 1: $\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$, we proceed as follows:\n\n**Step 1: Optimality Conditions**  \nThe original parameter $\\theta_0$ minimizes $L_0$, so:  \n$$\\nabla L_0(\\theta_0) = 0.$$  \nAfter unlearning $\\Delta G$, the new loss becomes $L_{\\text{new}}(\\theta) = L_0(\\theta) - L_{\\Delta G}(\\theta)$, where $L_{\\Delta G}(\\theta) = \\sum_{z_i \\in \\Delta G} l(f_G(z_i), y_i)$. The new optimal parameter $\\hat{\\theta}$ satisfies:  \n$$\\nabla L_{\\text{new}}(\\hat{\\theta}) = \\nabla L_0(\\hat{\\theta}) - \\nabla L_{\\Delta G}(\\hat{\\theta}) = 0.$$\n\n**Step 2: Taylor Expansion**  \nExpand $\\nabla L_0(\\hat{\\theta})$ around $\\theta_0$:  \n$$\\nabla L_0(\\hat{\\theta}) \\approx \\nabla L_0(\\theta_0) + H_{\\theta_0}(\\hat{\\theta} - \\theta_0) = H_{\\theta_0}(\\hat{\\theta} - \\theta_0).$$  \nSimilarly, expand $\\nabla L_{\\Delta G}(\\hat{\\theta})$ around $\\theta_0$:  \n$$\\nabla L_{\\Delta G}(\\hat{\\theta}) \\approx \\nabla L_{\\Delta G}(\\theta_0) + H_{\\Delta G}(\\hat{\\theta} - \\theta_0),$$  \nwhere $H_{\\Delta G} = \\sum_{z_i \\in \\Delta G} \\frac{\\partial^2}{\\partial \\theta^2} l(f_G(z_i), y_i)$.\n\n**Step 3: Approximation**  \nSubstitute into the optimality condition:  \n$$H_{\\theta_0}(\\hat{\\theta} - \\theta_0) \\approx \\nabla L_{\\Delta G}(\\theta_0) + H_{\\Delta G}(\\hat{\\theta} - \\theta_0).$$  \nAssuming $L_{\\Delta G}$ is small relative to $L_0$, the term $H_{\\Delta G}(\\hat{\\theta} - \\theta_0)$ is negligible. Thus:  \n$$H_{\\theta_0}(\\hat{\\theta} - \\theta_0) \\approx \\nabla L_{\\Delta G}(\\theta_0).$$\n\n**Step 4: Inverting the Hessian**  \nSolve for $\\hat{\\theta} - \\theta_0$:  \n$$\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla L_{\\Delta G}(\\theta_0).$$  \nHere, $\\nabla L_{\\Delta G}(\\theta_0) = \\sum_{z_i \\in \\Delta G} \\frac{\\partial}{\\partial \\theta} l(f_G(z_i), y_i)\\big|_{\\theta_0}$, which corresponds to $L_{\\Delta G}$ **if $L_{\\Delta G}$ is interpreted as the gradient sum** (not the loss sum). For consistency with Theorem 1, redefine $L_{\\Delta G}$ as the gradient term:  \n$$L_{\\Delta G} \\triangleq \\nabla L_{\\Delta G}(\\theta_0).$$  \nThen the result $\\hat{\\theta} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}$ follows directly.  \n\n**Conclusion**  \nThe validity of Theorem 1 hinges on interpreting $L_{\\Delta G}$ as the **gradient contribution** of the unlearned nodes at $\\theta_0$, not their raw loss values. This aligns with standard influence function derivations where parameter shifts are proportional to $H^{-1}\\nabla$. Clarifying the definition of $L_{\\Delta G}$ resolves the apparent discrepancy.", "B": "The validity of Theorem 1, which involves traditional influence functions, can be proven by understanding the relationship between the parameter change and the influence of unlearning a node. The proof leverages the convexity of the loss function and the properties of the Hessian matrix. Here's a step-by-step derivation:\n\n1. Initial Setup: Start with the original loss function $L_0$ and its Hessian $H_{\\theta_0}$ at the parameter $\\theta_0$.\n\n2. Perturbation Introduction: Introduce a small perturbation $\\Delta G$ representing the node unlearning request. This affects the loss function by $L_{\\Delta G}$, the sum of losses over the nodes to be unlearned.\n\n3. Parameter Change Approximation: The change in parameters $\\hat{\\theta} - \\theta_0$ due to the perturbation can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of $L_{\\Delta G}$ with respect to $\\theta_0$.\n\n4. Hessian Inverse Role: The Hessian matrix $H_{\\theta_0}^{-1}$ plays a crucial role in scaling the gradient of the perturbation loss, providing the direction and magnitude of the parameter change.\n\n5. Final Relationship: Combining these elements, the relationship $\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla_{\\theta_0} L_{\\Delta G}$ is derived, showing how the parameter change is influenced by the unlearning request through the inverse Hessian and the gradient of the perturbation loss.\n\nThis derivation assumes the loss function's convexity and differentiability, ensuring the Hessian is positive definite and invertible, which are critical for the approximation's validity.", "C": "The validity of Theorem 1, which involves traditional influence functions, can be proven by understanding the relationship between the parameter change and the influence of unlearning a node. The proof leverages the convexity of the loss function and the properties of the Hessian matrix. Here's a step-by-step derivation:\n\n1. Initial Setup: Start with the original loss function $L_0$ and its Hessian $H_{\\theta_0}$ at the parameter $\\theta_0$.\n\n2. Perturbation Introduction: Introduce a small perturbation $\\Delta G$ representing the node unlearning request. This affects the loss function by $L_{\\Delta G}$, the sum of losses over the nodes to be unlearned.\n\n3. Parameter Change Approximation: The change in parameters $\\hat{\\theta} - \\theta_0$ due to the perturbation can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of $L_{\\Delta G}$ with respect to $\\theta_0$.\n\n4. Hessian Inverse Role: The Hessian matrix $H_{\\theta_0}^{-1}$ plays a crucial role in scaling the gradient of the perturbation loss, providing the direction and magnitude of the parameter change.\n\n5. Final Relationship: Combining these elements, the relationship $\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0} \\nabla_{\\theta_0} L_{\\Delta G}$ is derived, showing how the parameter change is influenced by the unlearning request through the inverse Hessian and the gradient of the perturbation loss.\n\nThis derivation assumes the loss function's convexity and differentiability, ensuring the Hessian is positive definite and invertible, which are critical for the approximation's validity.", "D": "To prove Theorem 1, which states that the parameter difference \\(\\hat{\\theta} - \\theta_{0}\\) is approximately given by \\(\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}\\), we will derive this relationship step-by-step using influence functions. Here's the detailed proof:\n\n---\n\n### **1. Setup and Definitions**\n- **Original Loss Function**:  \n  The loss over the original dataset \\(D_{0}\\) is:\n  \\[\n  L_{0}(\\theta) = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i}),\n  \\]\n  where \\(f_{G}\\) is the model parameterized by \\(\\theta\\), and \\(l\\) is the per-instance loss.\n\n- **Optimal Parameters \\(\\theta_{0}\\):**  \n  The original parameters \\(\\theta_{0}\\) minimize \\(L_{0}(\\theta)\\):\n  \\[\n  \\theta_{0} = \\arg\\min_{\\theta} L_{0}(\\theta).\n  \\]\n  At \\(\\theta = \\theta_{0}\\), the gradient is zero:\n  \\[\n  \\nabla L_{0}(\\theta_{0}) = 0.\n  \\]\n\n- **Hessian Matrix \\(H_{\\theta_{0}}\\):**  \n  The Hessian of \\(L_{0}\\) at \\(\\theta_{0}\\) is:\n  \\[\n  H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i}) \\bigg|_{\\theta = \\theta_{0}}.\n  \\]\n  We assume \\(H_{\\theta_{0}}\\) is positive definite (i.e., the loss is locally convex at \\(\\theta_{0}\\)).\n\n- **Unlearning Request \\(\\Delta G\\):**  \n  The unlearning request removes nodes \\(V^{(rm)}\\) from the graph, leading to a modified dataset \\(D_{0} \\setminus \\Delta G\\). The loss after unlearning is:\n  \\[\n  \\hat{L}(\\theta) = L_{0}(\\theta) - L_{\\Delta G}(\\theta),\n  \\]\n  where \\(L_{\\Delta G}(\\theta) = \\sum_{z_{i} \\in \\Delta G} l(f_{G}(z_{i}), y_{i})\\) is the loss contribution of the removed data.\n\n---\n\n### **2. Taylor Approximation of the Updated Loss**\nAfter unlearning, the new optimal parameters \\(\\hat{\\theta}\\) minimize \\(\\hat{L}(\\theta)\\):\n\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} \\hat{L}(\\theta).\n\\]\nTo approximate \\(\\hat{\\theta}\\), we consider a first-order Taylor expansion of \\(\\nabla \\hat{L}(\\theta)\\) around \\(\\theta_{0}\\):\n\\[\n\\nabla \\hat{L}(\\theta) \\approx \\nabla \\hat{L}(\\theta_{0}) + H_{\\theta_{0}} (\\theta - \\theta_{0}).\n\\]\nAt \\(\\theta = \\hat{\\theta}\\), the gradient is zero:\n\\[\n0 \\approx \\nabla \\hat{L}(\\theta_{0}) + H_{\\theta_{0}} (\\hat{\\theta} - \\theta_{0}).\n\\]\nRearranging gives:\n\\[\n\\hat{\\theta} - \\theta_{0} \\approx -H_{\\theta_{0}}^{-1} \\nabla \\hat{L}(\\theta_{0}).\n\\]\n\n---\n\n### **3. Computing \\(\\nabla \\hat{L}(\\theta_{0})\\)**\nSince \\(\\hat{L}(\\theta) = L_{0}(\\theta) - L_{\\Delta G}(\\theta)\\), its gradient is:\n\\[\n\\nabla \\hat{L}(\\theta_{0}) = \\nabla L_{0}(\\theta_{0}) - \\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\nBut \\(\\nabla L_{0}(\\theta_{0}) = 0\\) (by definition of \\(\\theta_{0}\\)), so:\n\\[\n\\nabla \\hat{L}(\\theta_{0}) = -\\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\nThus:\n\\[\n\\hat{\\theta} - \\theta_{0} \\approx H_{\\theta_{0}}^{-1} \\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\n\n---\n\n### **4. Influence Function Interpretation**\nThe term \\(\\nabla L_{\\Delta G}(\\theta_{0})\\) is the gradient of the loss for the removed data \\(\\Delta G\\):\n\\[\n\\nabla L_{\\Delta G}(\\theta_{0}) = \\sum_{z_{i} \\in \\Delta G} \\frac{\\partial}{\\partial \\theta} l(f_{G}(z_{i}), y_{i}) \\bigg|_{\\theta = \\theta_{0}}.\n\\]\nThis represents the \"influence\" of \\(\\Delta G\\) on the parameters. The Hessian inverse \\(H_{\\theta_{0}}^{-1}\\) scales this influence to approximate the parameter change.\n\n---\n\n### **5. Final Result**\nCombining these steps, we obtain:\n\\[\n\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} \\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\nIf we denote \\(L_{\\Delta G} = \\nabla L_{\\Delta G}(\\theta_{0})\\) (as in the theorem statement), this simplifies to:\n\\[\n\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}.\n\\]\n\n---\n\n### **Key Assumptions:**\n1. The loss \\(L_{0}(\\theta)\\) is twice-differentiable and locally convex near \\(\\theta_{0}\\).\n2. The removed data \\(\\Delta G\\) is small relative to \\(D_{0}\\), so the Taylor approximation holds.\n3. The Hessian \\(H_{\\theta_{0}}\\) is invertible (i.e., the model is not at a saddle point).\n\nThis completes the proof of Theorem 1. The result shows that the parameter change due to unlearning can be approximated using influence functions, where the Hessian captures the local curvature of the loss landscape.", "ground-truth": "B"}
{"paper": "arxiv:2304.02835", "question": "The Hessian matrix $H_{\\theta_{0}}$ is defined by $H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})$. Given the loss $L_{0} = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i})$, how do we derive the Hessian definition from the second derivatives of the loss function $l(f_{G}(z_{i}), y_{i})$ evaluated at $\\theta_{0}$?", "A": "To derive the Hessian matrix \\( H_{\\theta_0} \\) from the second derivatives of the loss function \\( l(f_G(z_i), y_i) \\), we start with the definition of the total loss \\( L_0 \\):\n\n\\[\nL_0 = \\sum_{z_i \\in D_0} l(f_G(z_i), y_i),\n\\]\n\nwhere \\( \\theta_0 \\) represents the model parameters at which the Hessian is evaluated. The Hessian matrix of \\( L_0 \\) with respect to \\( \\theta \\) is defined as the second-order partial derivative of \\( L_0 \\):\n\n\\[\nH_{\\theta_0} = \\frac{\\partial^2 L_0}{\\partial \\theta^2}.\n\\]\n\nSince differentiation is a linear operator, the second derivative of a sum equals the sum of the second derivatives of individual terms. Applying this property, we compute:\n\n\\[\nH_{\\theta_0} = \\frac{\\partial^2}{\\partial \\theta^2} \\left( \\sum_{z_i \\in D_0} l(f_G(z_i), y_i) \\right) = \\sum_{z_i \\in D_0} \\frac{\\partial^2}{\\partial \\theta^2} l(f_G(z_i), y_i).\n\\]\n\nHere, each term \\( \\frac{\\partial^2}{\\partial \\theta^2} l(f_G(z_i), y_i) \\) is the Hessian of the individual loss \\( l(f_G(z_i), y_i) \\) evaluated at \\( \\theta_0 \\). Summing these contributions across all \\( z_i \\in D_0 \\) yields the full Hessian matrix \\( H_{\\theta_0} \\). Thus, the definition of \\( H_{\\theta_0} \\) directly follows from the linearity of differentiation over the summation of losses.", "B": "To derive the Hessian matrix $H_{\\theta_{0}}$ from the second derivatives of the loss function $l(f_{G}(z_{i}), y_{i})$ evaluated at $\\theta_{0}$, we start by considering the definition of the Hessian matrix for the total loss $L_{0}$. The Hessian matrix is the square matrix of second-order partial derivatives of a scalar-valued function. For the total loss $L_{0}$, which is the sum of individual loss functions $l(f_{G}(z_{i}), y_{i})$ over the dataset $D_{0}$, the Hessian matrix at a point $\\theta_{0}$ is given by the sum of the Hessian matrices of each individual loss function evaluated at $\\theta_{0}$. Mathematically, this is expressed as:\n\n\\[ H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})\\bigg|_{\\theta = \\theta_{0}}. \\]\n\nThis equation shows that the Hessian matrix of the total loss $L_{0}$ at $\\theta_{0}$ is the sum of the second derivatives of each individual loss function with respect to the parameters $\\theta$, evaluated at $\\theta_{0}$. This derivation assumes that the loss function $l$ is twice differentiable with respect to $\\theta$ and that the operations of differentiation and summation can be interchanged, which is typically the case under standard regularity conditions.", "C": "To derive the Hessian matrix \\( H_{\\theta_{0}} \\) from the second derivatives of the loss function \\( l(f_{G}(z_{i}), y_{i}) \\) evaluated at \\( \\theta_{0} \\), we can proceed with the following steps:\n\n1. **Definition of the Loss Function:**\n   The total loss \\( L_{0} \\) over the dataset \\( D_{0} \\) is given by:\n   \\[\n   L_{0} = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i}),\n   \\]\n   where \\( l(f_{G}(z_{i}), y_{i}) \\) is the loss for the \\( i \\)-th data point \\( z_{i} \\) with true label \\( y_{i} \\), and \\( f_{G} \\) is the model function parameterized by \\( \\theta \\).\n\n2. **First Derivative (Gradient) of the Loss:**\n   The gradient of \\( L_{0} \\) with respect to the parameters \\( \\theta \\) is:\n   \\[\n   \\nabla_{\\theta} L_{0} = \\sum_{z_{i} \\in D_{0}} \\nabla_{\\theta} l(f_{G}(z_{i}), y_{i}).\n   \\]\n   This represents the first-order partial derivatives of the loss with respect to each parameter in \\( \\theta \\).\n\n3. **Second Derivative (Hessian) of the Loss:**\n   The Hessian matrix \\( H_{\\theta} \\) is the matrix of second-order partial derivatives of \\( L_{0} \\) with respect to \\( \\theta \\). It is given by:\n   \\[\n   H_{\\theta} = \\nabla_{\\theta}^2 L_{0} = \\sum_{z_{i} \\in D_{0}} \\nabla_{\\theta}^2 l(f_{G}(z_{i}), y_{i}),\n   \\]\n   where \\( \\nabla_{\\theta}^2 l(f_{G}(z_{i}), y_{i}) \\) is the Hessian of the loss for the \\( i \\)-th data point.\n\n4. **Evaluation at \\( \\theta_{0} \\):**\n   When evaluated at the specific parameter value \\( \\theta_{0} \\), the Hessian matrix becomes:\n   \\[\n   H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\left. \\nabla_{\\theta}^2 l(f_{G}(z_{i}), y_{i}) \\right|_{\\theta = \\theta_{0}}.\n   \\]\n   This is equivalent to the definition provided in the problem statement:\n   \\[\n   H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i}) \\bigg|_{\\theta = \\theta_{0}}.\n   \\]\n\n### Summary:\nThe Hessian matrix \\( H_{\\theta_{0}} \\) is derived by summing the second derivatives (Hessians) of the individual loss terms \\( l(f_{G}(z_{i}), y_{i}) \\) with respect to \\( \\theta \\), evaluated at \\( \\theta = \\theta_{0} \\). This follows directly from the linearity of differentiation and the definition of the total loss \\( L_{0} \\). \n\nMathematically, this is expressed as:\n\\[\nH_{\\theta_{0}} = \\nabla_{\\theta}^2 L_{0} \\bigg|_{\\theta = \\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\left. \\nabla_{\\theta}^2 l(f_{G}(z_{i}), y_{i}) \\right|_{\\theta = \\theta_{0}}.\n\\]", "D": "To derive the Hessian matrix $H_{\\theta_{0}}$ from the second derivatives of the loss function $l(f_{G}(z_{i}), y_{i})$ evaluated at $\\theta_{0}$, we start by considering the definition of the Hessian matrix for the total loss $L_{0}$. The Hessian matrix is the square matrix of second-order partial derivatives of a scalar-valued function. For the total loss $L_{0}$, which is the sum of individual loss functions $l(f_{G}(z_{i}), y_{i})$ over the dataset $D_{0}$, the Hessian matrix at a point $\\theta_{0}$ is given by the sum of the Hessian matrices of each individual loss function evaluated at $\\theta_{0}$. Mathematically, this is expressed as:\n\n\\[ H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta \\partial \\phi} l(f_{G}(z_{i}), y_{i})\\bigg|_{\\theta = \\theta_{0}}. \\]\n\nThis equation shows that the Hessian matrix of the total loss $L_{0}$ at $\\theta_{0}$ is the sum of the second derivatives of each individual loss function with respect to the parameters $\\theta$, evaluated at $\\theta_{0}$. This derivation assumes that the loss function $l$ is twice differentiable with respect to $\\theta$ and that the operations of differentiation and summation can be interchanged, which is typically the case under standard regularity conditions.", "ground-truth": "B"}
{"paper": "arxiv:2304.02835", "question": "After unlearning, the objective $L$ is redefined over the remaining data $D_{0} \\setminus \\Delta D$ using the modified graph $G \\setminus \\Delta G$. How can we derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ with $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$ from the idea that we re-optimize the model parameters on the reduced dataset?", "A": "To derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ where $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$, we follow these steps:\n\n1. Identify the Remaining Data: After unlearning, the dataset is reduced to $D_{0} \\setminus \\Delta D$. This means we only consider the data points that are not part of the unlearning request.\n\n2. Modify the Graph: The graph $G$ is modified to $G \\setminus \\Delta G$ to reflect the removal of the unlearned data. This modification affects how the function $f$ computes its outputs.\n\n3. Define the Loss Function: The loss function $L$ is defined over the remaining data, where for each data point $z_{i}$ in $D_{0} \\setminus \\Delta D$, we compute the loss $l$ between the predicted output $f_{G \\setminus \\Delta G}(z_{i})$ and the actual output $y_{i}$.\n\n4. Re-optimize the Model Parameters: The goal is to find the parameters $\\theta$ that minimize the loss function $L$. This is achieved by solving the optimization problem $\\hat{\\theta} = \\arg\\min_{\\theta} L$.\n\n5. Derive $\\hat{L}$: The solution to the optimization problem gives us $\\hat{L}$, which is the minimized loss over the remaining data with the modified graph.\n\nThe key idea is that by re-optimizing the model parameters on the reduced dataset and modified graph, we ensure that the model performs well on the remaining data while disregarding the unlearned data. This process is encapsulated in the equation $\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$. Note that this optimization assumes the graph modification $\\Delta G$ does not affect the parameters $\\theta$ of the model, which may not hold in general.", "B": "To derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ where $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$, we follow these steps:\n\n1. Identify the Remaining Data: After unlearning, the dataset is reduced to $D_{0} \\setminus \\Delta D$. This means we only consider the data points that are not part of the unlearning request.\n\n2. Modify the Graph: The graph $G$ is modified to $G \\setminus \\Delta G$ to reflect the removal of the unlearned data. This modification affects how the function $f$ computes its outputs.\n\n3. Define the Loss Function: The loss function $L$ is defined over the remaining data, where for each data point $z_{i}$ in $D_{0} \\setminus \\Delta D$, we compute the loss $l$ between the predicted output $f_{G \\setminus \\Delta G}(z_{i})$ and the actual output $y_{i}$.\n\n4. Re-optimize the Model Parameters: The goal is to find the parameters $\\theta$ that minimize the loss function $L$. This is achieved by solving the optimization problem $\\hat{\\theta} = \\arg\\min_{\\theta} L$.\n\n5. Derive $\\hat{L}$: The solution to the optimization problem gives us $\\hat{L}$, which is the minimized loss over the remaining data with the modified graph.\n\nThe key idea is that by re-optimizing the model parameters on the reduced dataset and modified graph, we ensure that the model performs well on the remaining data while disregarding the unlearned data. This process is encapsulated in the equation $\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$.", "C": "To derive $\\hat{L} = \\mathop{\\text{arg min}}_{\\theta} L$ under the redefined objective over $D_0 \\setminus \\Delta D$ and modified graph $G \\setminus \\Delta G$, we follow these steps:\n\n1. **Re-optimization Objective**: After unlearning, the model parameters $\\theta$ are re-optimized on the remaining data $D_0 \\setminus \\Delta D$ using the modified graph structure $G \\setminus \\Delta G$. The loss function becomes:\n   $$\n   L(\\theta) = \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\ell\\left(f_{G \\setminus \\Delta G}(z_i; \\theta), y_i\\right),\n   $$\n   where $f_{G \\setminus \\Delta G}(z_i; \\theta)$ denotes the model output parameterized by $\\theta$ under the pruned graph.\n\n2. **Optimality Condition**: To minimize $L(\\theta)$, we compute its gradient with respect to $\\theta$ and set it to zero:\n   $$\n   \\nabla_\\theta L(\\theta) = \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\nabla_\\theta \\ell\\left(f_{G \\setminus \\Delta G}(z_i; \\theta), y_i\\right) = 0.\n   $$\n   This ensures $\\theta$ is a stationary point of the loss landscape.\n\n3. **Explicit Minimizer**: The optimal parameters $\\hat{\\theta}$ satisfy:\n   $$\n   \\hat{\\theta} = \\mathop{\\text{arg min}}_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\ell\\left(f_{G \\setminus \\Delta G}(z_i; \\theta), y_i\\right).\n   $$\n   This is equivalent to training the model **from scratch** on $D_0 \\setminus \\Delta D$ with the updated graph $G \\setminus \\Delta G$, ensuring $\\hat{\\theta}$ adapts to both the reduced data and altered model structure.\n\n**Conclusion**: The derivation directly follows the empirical risk minimization principle, where $\\hat{\\theta}$ minimizes the loss over the post-unlearning data and computational graph. No implicit assumptions about convexity or parameter dependencies are required for this formulation.", "D": "To derive the re-optimized objective \\(\\hat{L}\\) after unlearning, we follow these steps based on the idea of re-optimizing the model parameters \\(\\theta\\) on the reduced dataset \\(D_0 \\setminus \\Delta D\\) and the modified graph \\(G \\setminus \\Delta G\\):\n\n### 1. **Original Objective Before Unlearning:**\nThe original objective \\(L_{\\text{original}}\\) is typically defined over the full dataset \\(D_0\\) and the full graph \\(G\\):\n\\[\nL_{\\text{original}} = \\sum_{z_i \\in D_0} l(f_G(z_i), y_i),\n\\]\nwhere:\n- \\(f_G(z_i)\\) is the model's prediction for data point \\(z_i\\) using the graph \\(G\\).\n- \\(l(\\cdot, \\cdot)\\) is the loss function.\n- \\(\\theta\\) are the model parameters.\n\n### 2. **After Unlearning:**\nAfter unlearning the subset \\(\\Delta D\\) (and correspondingly modifying the graph by removing \\(\\Delta G\\)), the remaining data is \\(D_0 \\setminus \\Delta D\\), and the modified graph is \\(G \\setminus \\Delta G\\). The new objective \\(L\\) is defined as:\n\\[\nL = \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i).\n\\]\n\n### 3. **Re-optimization:**\nTo derive the re-optimized parameters \\(\\hat{\\theta}\\), we minimize the new objective \\(L\\) with respect to \\(\\theta\\):\n\\[\n\\hat{L} = \\arg\\min_{\\theta} L = \\arg\\min_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i).\n\\]\n\n### 4. **Key Observations:**\n- The re-optimization is performed **only on the remaining data** \\(D_0 \\setminus \\Delta D\\).\n- The model's predictions \\(f_{G \\setminus \\Delta G}(z_i)\\) now depend on the modified graph \\(G \\setminus \\Delta G\\), which may alter the relationships or features used for prediction.\n- The loss function \\(l(\\cdot, \\cdot)\\) remains unchanged, but its inputs are now based on the reduced data and graph.\n\n### 5. **Derivation of \\(\\hat{L}\\):**\nThe derivation involves solving the optimization problem:\n\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} L(\\theta),\n\\]\nwhere \\(L(\\theta)\\) is the new objective. This can be done using standard optimization techniques (e.g., gradient descent) by:\n1. Computing the gradient of \\(L\\) with respect to \\(\\theta\\):\n   \\[\n   \\nabla_\\theta L = \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\nabla_\\theta l(f_{G \\setminus \\Delta G}(z_i), y_i).\n   \\]\n2. Updating \\(\\theta\\) iteratively to minimize \\(L\\).\n\n### 6. **Final Expression:**\nThe re-optimized objective \\(\\hat{L}\\) is the minimized value of \\(L\\):\n\\[\n\\hat{L} = \\min_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i),\n\\]\nand the corresponding optimal parameters are:\n\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i).\n\\]\n\n### Summary:\nThe re-optimized objective \\(\\hat{L}\\) is derived by minimizing the loss over the remaining data \\(D_0 \\setminus \\Delta D\\) using the modified graph \\(G \\setminus \\Delta G\\). This ensures that the model parameters \\(\\theta\\) are adjusted to fit the new data and graph structure after unlearning.", "ground-truth": "B"}
{"paper": "arxiv:2304.02835", "question": "These equalities express the stationary conditions on $\\theta_{\\epsilon}$ and $\\theta_{0}$. Specifically, we have $0 = \\nabla_{\\theta_{\\epsilon}} L_{0} + \\epsilon \\nabla_{\\theta_{\\epsilon}} L_{\\Delta G}$ and $0 = \\nabla_{\\theta_{0}} L_{0}$. How can we derive these gradient conditions from the definition $\\theta_{\\epsilon} = \\text{arg}\\,\\min\\,(L_{0} + \\epsilon L_{\\Delta G})$ and the original optimum $\\theta_{0}$ for $L_{0}$?", "A": "To derive the gradient conditions, we start by considering the optimization problem for $\\theta_{\\epsilon}$ and $\\theta_{0}$. For $\\theta_{\\epsilon} = \\arg\\min_\\theta (L_0 + \\epsilon L_{\\Delta G})$, the first-order optimality condition requires that the gradient of the objective function with respect to $\\theta_{\\epsilon}$ must be zero. This gives us the equation $0 = \\nabla_{\\theta_{\\epsilon}} L_0 + \\nabla_{\\theta_{\\epsilon}} L_{\\Delta G}$. Similarly, for the original optimum $\\theta_{0} = \\arg\\min_\\theta L_0$, the first-order optimality condition is simply $0 = \\nabla_{\\theta_0} L_0$. These conditions ensure that both $\\theta_{\\epsilon}$ and $\\theta_{0}$ are at stationary points of their respective objective functions.", "B": "To derive the gradient conditions from the given definitions, we proceed as follows:\n\n1. **Optimality of $\\theta_0$ for $L_0$:**  \n   Since $\\theta_0$ is defined as the minimizer of $L_0$, the first-order necessary condition for optimality requires:\n   \\[\n   \\nabla_{\\theta} L_0(\\theta) \\bigg|_{\\theta = \\theta_0} = 0.\n   \\]\n   This directly yields the condition:\n   \\[\n   0 = \\nabla_{\\theta_0} L_0.\n   \\]\n\n2. **Optimality of $\\theta_{\\epsilon}$ for $L_0 + \\epsilon L_{\\Delta G}$:**  \n   By definition, $\\theta_{\\epsilon}$ minimizes the perturbed loss $L_0 + \\epsilon L_{\\Delta G}$. Applying the first-order optimality condition at $\\theta = \\theta_{\\epsilon}$:\n   \\[\n   \\nabla_{\\theta} \\left[ L_0(\\theta) + \\epsilon L_{\\Delta G}(\\theta) \\right] \\bigg|_{\\theta = \\theta_{\\epsilon}} = 0.\n   \\]\n   Linearity of the gradient operator allows this to be separated as:\n   \\[\n   \\nabla_{\\theta} L_0(\\theta_{\\epsilon}) + \\epsilon \\nabla_{\\theta} L_{\\Delta G}(\\theta_{\\epsilon}) = 0.\n   \\]\n   Hence, we obtain:\n   \\[\n   0 = \\nabla_{\\theta_{\\epsilon}} L_0 + \\epsilon \\nabla_{\\theta_{\\epsilon}} L_{\\Delta G}.\n   \\]\n\nThese conditions follow rigorously from the definitions of $\\theta_0$ and $\\theta_{\\epsilon}$ as minimizers of their respective loss functions. No additional approximations (e.g., Taylor expansions) are required unless analyzing the relationship between $\\theta_{\\epsilon}$ and $\\theta_0$ for small $\\epsilon$. The stated equalities hold exactly under the given optimality assumptions.", "C": "To derive the gradient conditions, we start by considering the optimization problem for $\\theta_{\\epsilon}$ and $\\theta_{0}$. For $\\theta_{\\epsilon} = \\arg\\min_\\theta (L_0 + \\epsilon L_{\\Delta G})$, the first-order optimality condition requires that the gradient of the objective function with respect to $\\theta_{\\epsilon}$ must be zero. This gives us the equation $0 = \\nabla_{\\theta_{\\epsilon}} L_0 + \\epsilon \\nabla_{\\theta_{\\epsilon}} L_{\\Delta G}$. Similarly, for the original optimum $\\theta_{0} = \\arg\\min_\\theta L_0$, the first-order optimality condition is simply $0 = \\nabla_{\\theta_0} L_0$. These conditions ensure that both $\\theta_{\\epsilon}$ and $\\theta_{0}$ are at stationary points of their respective objective functions.", "D": "To derive the gradient conditions from the given definitions, let's carefully analyze the optimization problems and their stationary conditions.\n\n### Definitions:\n1. Original optimal parameter:\n   \\[\n   \\theta_0 = \\arg\\min_{\\theta} L_0(\\theta)\n   \\]\n   This implies the stationary condition:\n   \\[\n   \\nabla_{\\theta} L_0(\\theta) \\bigg|_{\\theta = \\theta_0} = 0.\n   \\]\n\n2. Perturbed optimal parameter:\n   \\[\n   \\theta_\\epsilon = \\arg\\min_{\\theta} \\left( L_0(\\theta) + \\epsilon L_{\\Delta G}(\\theta) \\right).\n   \\]\n   The stationary condition for this problem is:\n   \\[\n   \\nabla_{\\theta} \\left( L_0(\\theta) + \\epsilon L_{\\Delta G}(\\theta) \\right) \\bigg|_{\\theta = \\theta_\\epsilon} = 0.\n   \\]\n   This directly gives:\n   \\[\n   \\nabla_{\\theta_\\epsilon} L_0 + \\epsilon \\nabla_{\\theta_\\epsilon} L_{\\Delta G} = 0.\n   \\]\n\n### Derivation:\n1. **For \\(\\theta_0\\):**\n   - Since \\(\\theta_0\\) minimizes \\(L_0(\\theta)\\), the gradient of \\(L_0\\) with respect to \\(\\theta\\) evaluated at \\(\\theta_0\\) must vanish:\n     \\[\n     \\nabla_{\\theta_0} L_0 = 0.\n     \\]\n   - This is the standard first-order optimality condition for unconstrained optimization.\n\n2. **For \\(\\theta_\\epsilon\\):**\n   - The problem is to minimize \\(L_0(\\theta) + \\epsilon L_{\\Delta G}(\\theta)\\). The first-order optimality condition requires the gradient of the total loss to be zero at \\(\\theta_\\epsilon\\):\n     \\[\n     \\nabla_{\\theta} \\left( L_0(\\theta) + \\epsilon L_{\\Delta G}(\\theta) \\right) \\bigg|_{\\theta = \\theta_\\epsilon} = 0.\n     \\]\n   - By linearity of the gradient operator, this splits into:\n     \\[\n     \\nabla_{\\theta_\\epsilon} L_0 + \\epsilon \\nabla_{\\theta_\\epsilon} L_{\\Delta G} = 0.\n     \\]\n   - Here, \\(\\nabla_{\\theta_\\epsilon} L_0\\) denotes \\(\\nabla_{\\theta} L_0(\\theta)\\) evaluated at \\(\\theta = \\theta_\\epsilon\\), and similarly for \\(\\nabla_{\\theta_\\epsilon} L_{\\Delta G}\\).\n\n### Summary:\n- The condition \\(\\nabla_{\\theta_0} L_0 = 0\\) comes directly from \\(\\theta_0\\) being the minimizer of \\(L_0\\).\n- The condition \\(\\nabla_{\\theta_\\epsilon} L_0 + \\epsilon \\nabla_{\\theta_\\epsilon} L_{\\Delta G} = 0\\) comes from \\(\\theta_\\epsilon\\) being the minimizer of the perturbed objective \\(L_0 + \\epsilon L_{\\Delta G}\\).\n\nThese are the stationary conditions that arise from the definitions of \\(\\theta_0\\) and \\(\\theta_\\epsilon\\) as minimizers of their respective objectives.", "ground-truth": "C"}
{"paper": "arxiv:2304.02835", "question": "Given that $\\epsilon = -1$, derive the relationship $\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$ from the approximate linearization in terms of the Hessian $H_{\\theta_{0}}$ and the influence of $L_{\\Delta G}$.", "A": "To derive the relationship \\(\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}\\) when \\(\\epsilon = -1\\), we proceed as follows:\n\n---\n\n**1. Perturbed Optimization Setup**  \nAssume the original optimization problem minimizes \\(L(\\theta)\\), with \\(\\theta_0\\) as the optimal solution (\\(\\nabla L(\\theta_0) = 0\\)). When perturbed by \\(\\epsilon \\Delta G(\\theta)\\), the new objective becomes \\(L(\\theta) + \\epsilon \\Delta G(\\theta)\\). The perturbed solution \\(\\hat{\\theta}\\) satisfies:\n\\[\n\\nabla L(\\hat{\\theta}) + \\epsilon \\nabla \\Delta G(\\hat{\\theta}) = 0.\n\\]\n\n**2. Linear Approximation at \\(\\theta_0\\)**  \nExpand \\(\\nabla L(\\hat{\\theta})\\) and \\(\\nabla \\Delta G(\\hat{\\theta})\\) around \\(\\theta_0\\) (using Taylor series):\n\\[\n\\nabla L(\\theta_0) + H_{\\theta_0}(\\hat{\\theta} - \\theta_0) + \\epsilon \\left[\\nabla \\Delta G(\\theta_0) + \\nabla^2 \\Delta G(\\theta_0)(\\hat{\\theta} - \\theta_0)\\right] \\approx 0.\n\\]\nSince \\(\\nabla L(\\theta_0) = 0\\) and higher-order terms (e.g., \\(\\nabla^2 \\Delta G\\)) are negligible:\n\\[\nH_{\\theta_0}(\\hat{\\theta} - \\theta_0) + \\epsilon \\nabla \\Delta G(\\theta_0) \\approx 0.\n\\]\n\n**3. Solve for \\(\\hat{\\theta} - \\theta_0\\)**  \nRearranging gives:\n\\[\n\\hat{\\theta} - \\theta_0 \\approx -\\epsilon H_{\\theta_0}^{-1} \\nabla \\Delta G(\\theta_0).\n\\]\nLet \\(L_{\\Delta G} \\equiv \\nabla \\Delta G(\\theta_0)\\). Substituting \\(\\epsilon = -1\\):\n\\[\n\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} L_{\\Delta G}.\n\\]\n\n**4. Final Relationship**  \nThus, under linear approximation:\n\\[\n\\boxed{\\hat{\\theta} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}.}\n\\]\n\n---\n\n**Key Steps**:\n- The perturbation introduces a gradient term \\(\\nabla \\Delta G(\\theta_0)\\).\n- The Hessian \\(H_{\\theta_0}\\) governs the local curvature of \\(L(\\theta)\\).\n- Inverting \\(H_{\\theta_0}\\) scales the perturbation effect to estimate the parameter shift.", "B": "To derive the relationship when $\\epsilon = -1$, we start by considering the approximate linearization of the parameter update. The key steps involve the Hessian matrix $H_{\\theta_{0}}$ and the gradient of the loss function $L_{\\Delta G}$ with respect to the parameters $\\theta_0$. The derivation proceeds as follows:\n\n1. Approximate Linearization: The change in parameters $\\hat{\\theta} - \\theta_0$ can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of the loss function and the inverse of the Hessian matrix.\n\n2. Setting $\\epsilon = -1$: This specific value of $\\epsilon$ implies that we are considering the effect of removing the influence of $L_{\\Delta G}$ on the parameter update.\n\n3. Resulting Relationship: Combining these elements, we arrive at the relationship\n   $$\\hat{\\theta} - \\theta_0 = \\theta_{\\epsilon=-1} - \\theta_0 \\approx -H_{\\theta_0}^{-1}\\nabla_{\\theta_0} L_{\\Delta G},$$\n   which shows that the parameter update is dominated by the removal of $L_{\\Delta G}$'s influence, scaled by the inverse of the Hessian matrix at $\\theta_0$.", "C": "To derive the relationship \\(\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}\\) from the approximate linearization, we proceed with the following steps:\n\n### 1. **Approximate Linearization of the Loss Function**\nConsider a loss function \\(L(\\theta)\\) that depends on parameters \\(\\theta\\). Around a reference point \\(\\theta_0\\), the second-order Taylor approximation is:\n\\[\nL(\\theta) \\approx L(\\theta_0) + \\nabla L(\\theta_0)^T (\\theta - \\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H_{\\theta_0} (\\theta - \\theta_0),\n\\]\nwhere:\n- \\(\\nabla L(\\theta_0)\\) is the gradient of \\(L\\) at \\(\\theta_0\\),\n- \\(H_{\\theta_0}\\) is the Hessian matrix of \\(L\\) at \\(\\theta_0\\).\n\n### 2. **Perturbation Analysis with \\(\\epsilon = -1\\)**\nIntroduce a perturbation \\(\\Delta G\\) to the loss function, scaled by \\(\\epsilon\\):\n\\[\nL_{\\text{pert}}(\\theta) = L(\\theta) + \\epsilon \\cdot L_{\\Delta G}(\\theta).\n\\]\nFor \\(\\epsilon = -1\\), this becomes:\n\\[\nL_{\\text{pert}}(\\theta) = L(\\theta) - L_{\\Delta G}(\\theta).\n\\]\n\nThe optimal parameter \\(\\hat{\\theta}\\) under the perturbed loss satisfies \\(\\nabla L_{\\text{pert}}(\\hat{\\theta}) = 0\\). Linearizing around \\(\\theta_0\\):\n\\[\n\\nabla L(\\theta_0) + H_{\\theta_0} (\\hat{\\theta} - \\theta_0) - \\nabla L_{\\Delta G}(\\theta_0) \\approx 0.\n\\]\nAssuming \\(\\theta_0\\) is a critical point of \\(L\\) (i.e., \\(\\nabla L(\\theta_0) = 0\\)), this simplifies to:\n\\[\nH_{\\theta_0} (\\hat{\\theta} - \\theta_0) \\approx \\nabla L_{\\Delta G}(\\theta_0).\n\\]\n\n### 3. **Solving for the Parameter Shift**\nThe parameter shift is then:\n\\[\n\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla L_{\\Delta G}(\\theta_0).\n\\]\nHere, \\(\\nabla L_{\\Delta G}(\\theta_0)\\) represents the influence of the perturbation \\(\\Delta G\\) on the gradient, which we denote as \\(L_{\\Delta G}\\) for brevity. Thus:\n\\[\n\\hat{\\theta} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}.\n\\]\n\n### 4. **Special Case for \\(\\epsilon = -1\\)**\nWhen \\(\\epsilon = -1\\), the perturbed parameter \\(\\theta_{\\epsilon=-1}\\) coincides with \\(\\hat{\\theta}\\), yielding:\n\\[\n\\theta_{\\epsilon=-1} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}.\n\\]\nThis completes the derivation. \n\n### **Final Relationship**\n\\[\n\\boxed{\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}}\n\\]", "D": "To derive the relationship when $\\epsilon = -1$, we start by considering the approximate linearization of the parameter update. The key steps involve the Hessian matrix $H_{\\theta_{0}}$ and the gradient of the loss function $L_{\\Delta G}$ with respect to the parameters $\\theta_0$. The derivation proceeds as follows:\n\n1. Approximate Linearization: The change in parameters $\\hat{\\theta} - \\theta_0$ can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of the loss function and the inverse of the Hessian matrix.\n\n2. Setting $\\epsilon = -1$: This specific value of $\\epsilon$ implies that we are considering the effect of removing the influence of $L_{\\Delta G}$ on the parameter update.\n\n3. Resulting Relationship: Combining these elements, we arrive at the relationship\n   $$\\hat{\\theta} - \\theta_0 = \\theta_{\\epsilon=-1} - \\theta_0 \\approx H_{\\theta_0}^{-1}\\nabla_{\\theta_0} L_{\\Delta G},$$\n   which shows that the parameter update is dominated by the removal of $L_{\\Delta G}$'s influence, scaled by the inverse of the Hessian matrix at $\\theta_0$.", "ground-truth": "D"}
{"paper": "arxiv:2304.02835", "question": "How can we prove Theorem 5 regarding the Neumann series for an invertible matrix H? The theorem states that if the spectral radius of $(I - H)$ is less than 1, then $H^{-1} = \\sum_{l=0}^{\\infty}(I - H)^{l}$ and $\\lim_{t\\to\\infty} H_{t}^{-1} = H^{-1}$. Which conditions on H (positive definiteness, spectral radius) ensure the convergence of the series?", "A": "To prove Theorem 5 regarding the Neumann series for an invertible matrix \\( H \\), we proceed as follows:\n\n**Theorem 5**: If the spectral radius \\( \\rho(I - H) < 1 \\), then  \n1. \\( H^{-1} = \\sum_{l=0}^{\\infty} (I - H)^l \\), and  \n2. \\( \\lim_{t \\to \\infty} H_t^{-1} = H^{-1} \\), where \\( H_t^{-1} = \\sum_{l=0}^{t} (I - H)^l \\).\n\n**Proof**:\n\n1. **Neumann Series Convergence**:  \n   The Neumann series states that for a matrix \\( A \\), the series \\( \\sum_{l=0}^{\\infty} A^l \\) converges to \\( (I - A)^{-1} \\) if \\( \\rho(A) < 1 \\). Let \\( A = I - H \\). Then:  \n   \\[\n   \\sum_{l=0}^{\\infty} (I - H)^l = (I - (I - H))^{-1} = H^{-1},\n   \\]  \n   provided \\( \\rho(I - H) < 1 \\). This establishes the first part of the theorem.\n\n2. **Limit of Partial Sums**:  \n   Define \\( H_t^{-1} = \\sum_{l=0}^{t} (I - H)^l \\). As \\( t \\to \\infty \\), the partial sums \\( H_t^{-1} \\) converge to \\( \\sum_{l=0}^{\\infty} (I - H)^l \\), which equals \\( H^{-1} \\) under the spectral radius condition. Thus, \\( \\lim_{t \\to \\infty} H_t^{-1} = H^{-1} \\).\n\n**Conditions for Convergence**:  \nThe **key condition** for convergence is \\( \\rho(I - H) < 1 \\). This ensures the Neumann series converges absolutely. Additional remarks:\n- **Positive Definiteness**: If \\( H \\) is positive definite with eigenvalues in \\( (0, 2) \\), then \\( \\rho(I - H) < 1 \\), as \\( |1 - \\lambda| < 1 \\) for all eigenvalues \\( \\lambda \\) of \\( H \\). However, positive definiteness is **not necessary**; \\( \\rho(I - H) < 1 \\) can hold even if \\( H \\) is not positive definite.\n- **Spectral Radius**: The condition \\( \\rho(I - H) < 1 \\) is both necessary and sufficient for convergence.\n\n**Conclusion**: The theorem holds under \\( \\rho(I - H) < 1 \\), with positive definiteness serving as a potential (but non-unique) sufficient condition to satisfy the spectral radius requirement.", "B": "To prove Theorem 5 regarding the Neumann series for an invertible matrix H, we follow these steps:\n\n1. Neumann Series Expansion: Given that the spectral radius of $(I - H)$ is less than 1, we can express $H^{-1}$ as the infinite series $H^{-1} = \\sum_{l=0}^{\\infty}(I - H)^{l}$. This is because the condition on the spectral radius ensures the convergence of the series.\n\n2. Iterative Approximation: Consider the iterative process defined by $H_t^{-1} = (I - H) H_{t-1}^{-1} + I$. This process generates a sequence of approximations to $H^{-1}$.\n\n3. Eigenvalue Decomposition: Decompose $(I-H)$ as $T U T^{-1}$, where $T$ is a matrix whose columns are eigenvectors of $(I-H)$, and $U$ is a diagonal matrix of eigenvalues (i.e., $U = \\Lambda$). This allows us to express $(I - H)^n$ as $T U^n T^{-1}$.\n\n4. Convergence Analysis: Since the spectral radius of $(I - H)$ is less than 1, all eigenvalues of $(I - H)$ have magnitudes less than 1. Therefore, $\\lim_{n\\to\\infty} U^n = 0$, which implies $\\lim_{n\\to\\infty}(I - H)^n = 0$. Consequently, the iterative approximation $H_t^{-1}$ converges to $H^{-1}$ as $t$ approaches infinity.\n\nConditions for Convergence: The key condition ensuring the convergence of the Neumann series is that the spectral radius of $(I - H)$ must be less than 1. This condition is independent of whether H is positive definite or not. However, if H is positive definite, it can provide additional guarantees about the stability and rate of convergence of the series.", "C": "To prove Theorem 5 regarding the Neumann series for an invertible matrix H, we follow these steps:\n\n1. Neumann Series Expansion: Given that the spectral radius of $(I - H)$ is less than 1, we can express $H^{-1}$ as the infinite series $H^{-1} = \\sum_{l=0}^{\\infty}(I - H)^{l}$. This is because the condition on the spectral radius ensures the convergence of the series.\n\n2. Iterative Approximation: Consider the iterative process defined by $H_t^{-1} = (I - H) H_{t-1}^{-1} + I$. This process generates a sequence of approximations to $H^{-1}$.\n\n3. Eigenvalue Decomposition: Decompose $(I-H)$ as $T U T^{-1}$, where $T$ is a matrix whose columns are eigenvectors of $(I-H)$, and $U$ is a diagonal matrix of eigenvalues (i.e., $U = \\Lambda$). This allows us to express $(I - H)^n$ as $T U^n T^{-1}$.\n\n4. Convergence Analysis: Since the spectral radius of $(I - H)$ is less than 1, all eigenvalues of $(I - H)$ have magnitudes less than 1. Therefore, $\\lim_{n\\to\\infty} U^n = 0$, which implies $\\lim_{n\\to\\infty}(I - H)^n = 0$. Consequently, the iterative approximation $H_t^{-1}$ converges to $H^{-1}$ as $t$ approaches infinity.\n\nConditions for Convergence: The key condition ensuring the convergence of the Neumann series is that the spectral radius of $(I - H)$ must be less than 1. This condition is independent of whether H is positive definite or not. However, if H is positive definite, it can provide additional guarantees about the stability and rate of convergence of the series. Note that the series also converges if the spectral radius of $(I - H)$ equals 1, provided the eigenvalues on the unit circle are semi-simple.", "D": "To prove Theorem 5 regarding the Neumann series for an invertible matrix \\( H \\), we proceed as follows:\n\n### **Theorem Statement:**\nIf the spectral radius \\( \\rho(I - H) < 1 \\), then:\n1. The Neumann series converges, and \\( H^{-1} = \\sum_{l=0}^{\\infty} (I - H)^l \\).\n2. \\( \\lim_{t \\to \\infty} H_t^{-1} = H^{-1} \\), where \\( H_t \\) is an approximation of \\( H \\).\n\n### **Proof:**\n\n#### **1. Convergence of the Neumann Series:**\nThe Neumann series for \\( H^{-1} \\) is given by:\n\\[\nH^{-1} = \\sum_{l=0}^{\\infty} (I - H)^l.\n\\]\nThis series converges if and only if the spectral radius \\( \\rho(I - H) < 1 \\).\n\n- **Why?**  \n  The spectral radius \\( \\rho(A) \\) of a matrix \\( A \\) is the maximum absolute value of its eigenvalues. The Neumann series \\( \\sum_{l=0}^{\\infty} A^l \\) converges to \\( (I - A)^{-1} \\) if and only if \\( \\rho(A) < 1 \\).  \n  Here, \\( A = I - H \\), so the condition becomes \\( \\rho(I - H) < 1 \\).\n\n- **Explicit Form of \\( H^{-1} \\):**  \n  If \\( \\rho(I - H) < 1 \\), then:\n  \\[\n  \\sum_{l=0}^{\\infty} (I - H)^l = (I - (I - H))^{-1} = H^{-1}.\n  \\]\n\n#### **2. Conditions Ensuring Convergence:**\nThe convergence of the Neumann series depends on the spectral radius \\( \\rho(I - H) \\). Some sufficient conditions include:\n\n- **Positive Definiteness of \\( H \\):**  \n  If \\( H \\) is positive definite (i.e., all eigenvalues of \\( H \\) are positive), then \\( I - H \\) has eigenvalues \\( 1 - \\lambda_i \\), where \\( \\lambda_i > 0 \\).  \n  To ensure \\( \\rho(I - H) < 1 \\), we require \\( |1 - \\lambda_i| < 1 \\) for all \\( i \\). This holds if \\( 0 < \\lambda_i < 2 \\).  \n  Thus, if \\( H \\) is positive definite with eigenvalues in \\( (0, 2) \\), the Neumann series converges.\n\n- **Spectral Norm Condition:**  \n  If \\( \\| I - H \\| < 1 \\) for some matrix norm (e.g., the spectral norm \\( \\| \\cdot \\|_2 \\)), then \\( \\rho(I - H) \\leq \\| I - H \\| < 1 \\), ensuring convergence.\n\n#### **3. Limit of Approximate Inverses:**\nIf \\( H_t \\) is a sequence of matrices such that \\( \\| H_t - H \\| \\to 0 \\) as \\( t \\to \\infty \\), and \\( \\rho(I - H) < 1 \\), then:\n\\[\nH_t^{-1} = \\sum_{l=0}^{\\infty} (I - H_t)^l \\to \\sum_{l=0}^{\\infty} (I - H)^l = H^{-1}.\n\\]\nThis follows from the continuity of matrix inversion and the uniform convergence of the Neumann series under small perturbations.\n\n### **Conclusion:**\nThe Neumann series \\( \\sum_{l=0}^{\\infty} (I - H)^l \\) converges to \\( H^{-1} \\) if \\( \\rho(I - H) < 1 \\). Sufficient conditions include:\n- \\( H \\) being positive definite with eigenvalues in \\( (0, 2) \\), or\n- \\( \\| I - H \\| < 1 \\) for some matrix norm.\n\nUnder these conditions, the approximation \\( H_t^{-1} \\) also converges to \\( H^{-1} \\) as \\( t \\to \\infty \\).", "ground-truth": "B"}
{"paper": "arxiv:2309.04475", "question": "From the denoising diffusion probabilistic model, how do we obtain the reverse transition formula $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid \\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)?$$ In particular, how is it derived using the forward process $$q(L_t|L_0) = \\mathcal{N}\\bigl(L_t\\mid\\sqrt{\\bar{\\alpha}_t}L_0,\\,(1 - \\bar{\\alpha}_t)I\\bigr)$$ and the noise prediction term $\\hat{\\epsilon}_L(M_t, t)$?", "A": "To derive the reverse transition formula $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid \\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$ from the denoising diffusion probabilistic model, we start by considering the forward process given by $$q(L_t|L_0) = \\mathcal{N}\\bigl(L_t\\mid\\sqrt{\\bar{\\alpha}_t}L_0,\\,(1 - \\bar{\\alpha}_t)I\\bigr).$$ The key idea is to use the noise prediction term $\\hat{\\epsilon}_L(M_t, t)$ to model the reverse process. The derivation involves the following steps:\n\n1. Bayes' Rule Application: The reverse process can be related to the forward process through Bayes' rule, which allows us to express $p(L_{t-1}\\mid L_t)$ in terms of $q(L_t\\mid L_{t-1})$ and the prior $q(L_{t-1})$.\n\n2. Noise Prediction Term: The noise prediction term $\\hat{\\epsilon}_L(M_t, t)$ is used to estimate the noise that was added during the forward process. This estimation helps in reversing the diffusion process by subtracting the predicted noise from $L_t$ to obtain $L_{t-1}$.\n\n3. Parameterization of Mean and Variance: The mean $\\mu(M_t)$ and variance $\\sigma^2(M_t)$ of the reverse transition are parameterized based on the noise prediction term. Specifically, the mean is adjusted to account for the predicted noise, and the variance is set to ensure that the reverse process gradually denoises the data.\n\n4. Derivation of Reverse Transition: Combining these elements, we arrive at the reverse transition formula, which is a Gaussian distribution centered around a mean that depends on the current state $L_t$ and the predicted noise, with a variance that ensures the process moves towards the original data distribution.\n\nThis derivation showcases how the forward process and noise prediction term are utilized to model the reverse process, enabling the denoising of data through a series of learned transitions.", "B": "To derive the reverse transition formula $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid \\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$ from the denoising diffusion probabilistic model, we start by considering the forward process given by $$q(L_t|L_0) = \\mathcal{N}\\bigl(L_t\\mid\\sqrt{\\bar{\\alpha}_t}L_0,\\,(1 - \\bar{\\alpha}_t)I\\bigr).$$ The key idea is to use the noise prediction term $\\hat{\\epsilon}_L(M_t, t)$ to model the reverse process. The derivation involves the following steps:\n\n1. Bayes' Rule Application: The reverse process can be related to the forward process through Bayes' rule, which allows us to express $p(L_{t-1}\\mid L_t)$ in terms of $q(L_t\\mid L_{t-1})$ and the prior $q(L_{t-1})$.\n\n2. Noise Prediction Term: The noise prediction term $\\hat{\\epsilon}_L(M_t, t)$ is used to estimate the noise that was added during the forward process. This estimation helps in reversing the diffusion process by subtracting the predicted noise from $L_t$ to obtain $L_{t-1}$.\n\n3. Parameterization of Mean and Variance: The mean $\\mu(M_t)$ and variance $\\sigma^2(M_t)$ of the reverse transition are parameterized based on the noise prediction term. Specifically, the mean is adjusted to account for the predicted noise, and the variance is set to ensure that the reverse process gradually denoises the data.\n\n4. Derivation of Reverse Transition: Combining these elements, we arrive at the reverse transition formula, which is a Gaussian distribution centered around a mean that depends on the current state $L_t$ and the predicted noise, with a variance that ensures the process moves towards the original data distribution. However, the variance should actually be $\\sigma^2(M_t)I + (1 - \\bar{\\alpha}_t)I$ to account for the cumulative noise in the forward process. \n\nThis derivation showcases how the forward process and noise prediction term are utilized to model the reverse process, enabling the denoising of data through a series of learned transitions.", "C": "The reverse transition formula in denoising diffusion probabilistic models (DDPMs) is derived by leveraging the forward process, the noise prediction term, and variational inference. Here's the key derivation:\n\n---\n\n### **1. Forward Process and Reparameterization**\nThe forward process gradually adds Gaussian noise:\n$$\nq(L_t \\mid L_0) = \\mathcal{N}\\big(L_t; \\sqrt{\\bar{\\alpha}_t}L_0, (1 - \\bar{\\alpha}_t)I\\big),\n$$\nwhere $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$ and $\\alpha_t = 1 - \\beta_t$ ($\\beta_t$ is the noise schedule). Using reparameterization, $L_t$ can be expressed as:\n$$\nL_t = \\sqrt{\\bar{\\alpha}_t}L_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I).\n$$\n\n---\n\n### **2. Reverse Posterior via Bayes' Rule**\nTo derive $q(L_{t-1} \\mid L_t, L_0)$, apply Bayes' rule:\n$$\nq(L_{t-1} \\mid L_t, L_0) \\propto q(L_t \\mid L_{t-1}, L_0)q(L_{t-1} \\mid L_0).\n$$\nGiven the Markov property, $q(L_t \\mid L_{t-1}, L_0) = q(L_t \\mid L_{t-1}) = \\mathcal{N}\\big(L_t; \\sqrt{\\alpha_t}L_{t-1}, \\beta_t I\\big)$. Combining with $q(L_{t-1} \\mid L_0) = \\mathcal{N}\\big(L_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}}L_0, (1 - \\bar{\\alpha}_{t-1})I\\big)$, the posterior becomes:\n$$\nq(L_{t-1} \\mid L_t, L_0) = \\mathcal{N}\\big(L_{t-1}; \\mu_t(L_t, L_0), \\sigma_t^2 I\\big),\n$$\nwith parameters:\n$$\n\\mu_t(L_t, L_0) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}L_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}L_0, \\quad \\sigma_t^2 = \\frac{(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\beta_t.\n$$\n\n---\n\n### **3. Substituting $L_0$ via Noise Prediction**\nFrom the forward process, solve for $L_0$:\n$$\nL_0 = \\frac{L_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}}.\n$$\nReplace $\\epsilon_t$ with the learned noise prediction model $\\hat{\\epsilon}_L(M_t, t)$ (where $M_t \\equiv L_t$):\n$$\nL_0 \\approx \\frac{L_t - \\sqrt{1 - \\bar{\\alpha}_t}\\hat{\\epsilon}_L(L_t, t)}{\\sqrt{\\bar{\\alpha}_t}}.\n$$\n\n---\n\n### **4. Final Reverse Transition Formula**\nSubstitute $L_0$ into $\\mu_t(L_t, L_0)$:\n$$\n\\mu_t(L_t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\hat{\\epsilon}_L(L_t, t)\\right).\n$$\nThe variance $\\sigma_t^2$ is typically simplified to $\\beta_t$ (or a learned scalar). Thus:\n$$\np_\\theta(L_{t-1} \\mid L_t) = \\mathcal{N}\\big(L_{t-1}; \\mu_t(L_t), \\sigma_t^2 I\\big).\n$$\n\n---\n\n### **Key Steps Summary**\n1. **Forward Process**: Define the noise addition $q(L_t \\mid L_0)$.\n2. **Posterior Derivation**: Use Bayes' rule to express $q(L_{t-1} \\mid L_t, L_0)$.\n3. **Reparameterize $L_0$**: Express $L_0$ in terms of $L_t$ and the predicted noise $\\hat{\\epsilon}_L$.\n4. **Parameterize Reverse Process**: Substitute $L_0$ into the posterior mean and simplify.\n\nThis derivation aligns the reverse process with the forward process while conditioning on the noise prediction term $\\hat{\\epsilon}_L$, enabling iterative denoising.", "D": "To derive the reverse transition formula \\( p(L_{t-1} \\mid L_t) \\) in the denoising diffusion probabilistic model (DDPM), we start from the forward process and use the noise prediction term \\(\\hat{\\epsilon}_L(M_t, t)\\). Here's the step-by-step derivation:\n\n### 1. **Forward Process**\nThe forward process is defined as:\n\\[\nq(L_t \\mid L_0) = \\mathcal{N}\\bigl(L_t \\mid \\sqrt{\\bar{\\alpha}_t} L_0, (1 - \\bar{\\alpha}_t)I\\bigr),\n\\]\nwhere \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\) and \\(\\alpha_t = 1 - \\beta_t\\) (\\(\\beta_t\\) is the noise schedule).\n\nUsing the reparameterization trick, we can express \\(L_t\\) as:\n\\[\nL_t = \\sqrt{\\bar{\\alpha}_t} L_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon,\n\\]\nwhere \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).\n\n### 2. **Noise Prediction Term**\nThe noise prediction term \\(\\hat{\\epsilon}_L(M_t, t)\\) is learned by the model to predict the noise \\(\\epsilon\\) added at step \\(t\\). Given \\(L_t\\), the predicted \\(L_0\\) can be written as:\n\\[\n\\hat{L}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\bigl(L_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}_L(M_t, t)\\bigr).\n\\]\n\n### 3. **Reverse Process**\nThe reverse process \\(p(L_{t-1} \\mid L_t)\\) is derived using Bayes' rule and the Markov property. We start by considering the joint distribution \\(q(L_{t-1}, L_t \\mid L_0)\\):\n\\[\nq(L_{t-1}, L_t \\mid L_0) = q(L_t \\mid L_{t-1}) q(L_{t-1} \\mid L_0).\n\\]\nUsing the forward process definitions:\n\\[\nq(L_t \\mid L_{t-1}) = \\mathcal{N}\\bigl(L_t \\mid \\sqrt{\\alpha_t} L_{t-1}, (1 - \\alpha_t)I\\bigr),\n\\]\n\\[\nq(L_{t-1} \\mid L_0) = \\mathcal{N}\\bigl(L_{t-1} \\mid \\sqrt{\\bar{\\alpha}_{t-1}} L_0, (1 - \\bar{\\alpha}_{t-1})I\\bigr).\n\\]\n\nApplying Bayes' rule, the reverse transition is:\n\\[\nq(L_{t-1} \\mid L_t, L_0) = \\frac{q(L_t \\mid L_{t-1}, L_0) q(L_{t-1} \\mid L_0)}{q(L_t \\mid L_0)}.\n\\]\nSince \\(q(L_t \\mid L_{t-1}, L_0) = q(L_t \\mid L_{t-1})\\) (Markov property), we substitute the Gaussian densities and simplify to obtain:\n\\[\nq(L_{t-1} \\mid L_t, L_0) = \\mathcal{N}\\bigl(L_{t-1} \\mid \\tilde{\\mu}(L_t, L_0), \\tilde{\\beta}_t I\\bigr),\n\\]\nwhere:\n\\[\n\\tilde{\\mu}(L_t, L_0) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} L_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t)}{1 - \\bar{\\alpha}_t} L_0,\n\\]\n\\[\n\\tilde{\\beta}_t = \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}.\n\\]\n\n### 4. **Approximating \\(L_0\\) with \\(\\hat{L}_0\\)**\nSince \\(L_0\\) is unknown during the reverse process, we replace it with the predicted \\(\\hat{L}_0\\):\n\\[\n\\mu(M_t) = \\tilde{\\mu}(L_t, \\hat{L}_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\bigl(L_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t)\\bigr).\n\\]\nThe variance \\(\\sigma^2(M_t)\\) is typically set to \\(\\tilde{\\beta}_t\\) or a learned value.\n\n### 5. **Final Reverse Transition**\nThus, the reverse transition is:\n\\[\np(L_{t-1} \\mid L_t) = \\mathcal{N}\\bigl(L_{t-1} \\mid \\mu(M_t), \\sigma^2(M_t) I\\bigr),\n\\]\nwhere:\n\\[\n\\mu(M_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\bigl(L_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t)\\bigr),\n\\]\n\\[\n\\sigma^2(M_t) = \\tilde{\\beta}_t \\quad \\text{or a learned value}.\n\\]\n\nThis completes the derivation. The key steps involve using the forward process, noise prediction, and Bayes' rule to approximate the reverse transition.", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "Given the reverse transition $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$, derive the expressions for $$\\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t)$$ and $$\\sigma^2(M_t) = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$$. Also, explain how $\\hat{\\epsilon}_L(M_t, t)\\in\\mathbb{R}^{3\\times 3}$ is predicted by the denoising model $\\phi(L_t, F_t, A, t)$.", "A": "To derive the expressions for $\\mu(M_t)$ and $\\sigma^2(M_t)$, we start from the standard DDPM framework where the forward process is defined by $$q(L_t\\mid L_0) = \\mathcal{N}(L_t\\mid\\sqrt{\\bar\\alpha_t}\\,L_0,\\,(1-\\bar\\alpha_t)I).$$ The reverse transition aims to approximate the true denoising process, leading to $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr).$$ The mean $\\mu(M_t)$ is derived by considering the noise prediction $\\hat\\epsilon_L(M_t,t)$, which is the output of the denoising model $\\phi(L_t, F_t, A, t)$. This model predicts the noise added at each step, allowing us to express the mean as $$\\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar\\alpha_t}}\\; -\\;\\frac{1-\\bar\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\;\\hat\\epsilon_L(M_t,t).$$ The variance $\\sigma^2(M_t)$ is derived from the ratio of the noise schedules, resulting in $$\\sigma^2(M_t) = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\;\\beta_t.$$ The noise prediction $\\hat\\epsilon_L(M_t,t)\\in\\mathbb{R}^{3\\times 3}$ is obtained by the denoising model $\\phi$, which takes as input the current noisy sample $L_t$, the feature $F_t$, the attribute $A$, and the time step $t$, and outputs the predicted noise in the sample.", "B": "To derive the expressions for \\(\\mu(M_t)\\) and \\(\\sigma^2(M_t)\\) from the given reverse transition, we proceed step-by-step. The reverse transition is given by:\n\n\\[\np(L_{t-1} \\mid L_t) = \\mathcal{N}\\bigl(L_{t-1} \\mid \\mu(M_t),\\, \\sigma^2(M_t)I\\bigr),\n\\]\n\nwhere \\(M_t = (L_t, F_t, A, t)\\) represents the conditioning variables (latent representation \\(L_t\\), features \\(F_t\\), adjacency matrix \\(A\\), and timestep \\(t\\)).\n\n---\n\n### **1. Derivation of \\(\\mu(M_t)\\)**\n\nThe forward process in diffusion models typically follows:\n\n\\[\nL_t = \\sqrt{\\bar{\\alpha}_t} L_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon,\n\\]\n\nwhere \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) is noise, \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\), and \\(\\alpha_t = 1 - \\beta_t\\).\n\nTo reverse the process, we estimate the noise \\(\\epsilon\\) using a denoising model \\(\\phi\\):\n\n\\[\n\\hat{\\epsilon}_L(M_t, t) = \\phi(L_t, F_t, A, t).\n\\]\n\nThe posterior mean \\(\\mu(M_t)\\) is derived by rearranging the forward process to express \\(L_0\\) (the clean latent representation) in terms of \\(L_t\\) and \\(\\hat{\\epsilon}_L\\):\n\n\\[\nL_0 = \\frac{L_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}_L(M_t, t)}{\\sqrt{\\bar{\\alpha}_t}}.\n\\]\n\nThen, using the standard reverse process formulation (similar to DDPM), the mean \\(\\mu(M_t)\\) is a weighted combination of the predicted \\(L_0\\) and the current noisy \\(L_t\\):\n\n\\[\n\\mu(M_t) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} L_0 + \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} L_t.\n\\]\n\nSubstituting \\(L_0\\) from above and simplifying, we obtain:\n\n\\[\n\\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t).\n\\]\n\n---\n\n### **2. Derivation of \\(\\sigma^2(M_t)\\)**\n\nThe variance \\(\\sigma^2(M_t)\\) is derived from the reverse process variance in diffusion models. For timestep \\(t\\), the variance is:\n\n\\[\n\\sigma^2(M_t) = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t.\n\\]\n\nThis comes from the fact that the reverse process variance is a ratio of the noise schedules \\(\\bar{\\alpha}_{t-1}\\) and \\(\\bar{\\alpha}_t\\), scaled by \\(\\beta_t\\) (the noise at step \\(t\\)).\n\n---\n\n### **3. Explanation of \\(\\hat{\\epsilon}_L(M_t, t)\\)**\n\nThe term \\(\\hat{\\epsilon}_L(M_t, t) \\in \\mathbb{R}^{3 \\times 3}\\) is the predicted noise by the denoising model \\(\\phi\\). It takes as input:\n- The noisy latent representation \\(L_t\\),\n- The features \\(F_t\\),\n- The adjacency matrix \\(A\\),\n- The timestep \\(t\\).\n\nThe model \\(\\phi\\) is trained to predict the noise \\(\\epsilon\\) added during the forward process, conditioned on the current state \\(M_t\\). The output \\(\\hat{\\epsilon}_L(M_t, t)\\) is used to estimate the clean latent \\(L_0\\) and guide the reverse process.\n\n---\n\n### **Summary**\n- The mean \\(\\mu(M_t)\\) is derived by estimating the clean latent \\(L_0\\) and combining it with \\(L_t\\).\n- The variance \\(\\sigma^2(M_t)\\) is a function of the noise schedules \\(\\bar{\\alpha}_{t-1}\\), \\(\\bar{\\alpha}_t\\), and \\(\\beta_t\\).\n- The noise \\(\\hat{\\epsilon}_L(M_t, t)\\) is predicted by \\(\\phi\\) to reverse the diffusion process.\n\nThese derivations align with standard diffusion model theory, adapted for the latent space \\(L_t\\) and conditioning on \\(M_t\\).", "C": "To derive the expressions for $\\mu(M_t)$ and $\\sigma^2(M_t)$, we start from the forward process and apply Bayes' theorem. The forward process is defined as $q(L_t \\mid L_{t-1}) = \\mathcal{N}\\left(L_t; \\sqrt{1-\\beta_t}L_{t-1}, \\beta_t I\\right)$, where $\\bar{\\alpha}_t = \\prod_{s=1}^t (1-\\beta_s)$. The marginal distribution $q(L_t \\mid L_0)$ becomes $\\mathcal{N}\\left(L_t; \\sqrt{\\bar{\\alpha}_t}L_0, (1-\\bar{\\alpha}_t)I\\right)$.\n\nUsing Bayes' rule, the reverse posterior $q(L_{t-1} \\mid L_t, L_0)$ is Gaussian. After algebraic manipulation, its mean $\\mu(M_t)$ and variance $\\sigma^2(M_t)$ are derived as:\n\n1. **Mean ($\\mu(M_t)$):**\n   \\[\n   \\mu(M_t) = \\frac{1}{\\sqrt{\\alpha_t}}L_t - \\frac{\\beta_t}{\\sqrt{\\alpha_t(1-\\bar{\\alpha}_t)}}\\hat{\\epsilon}_L\n   \\]\n   Substituting $L_t = \\sqrt{\\bar{\\alpha}_t}L_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$ and expressing $L_0$ in terms of $L_t$ and predicted noise $\\hat{\\epsilon}_L$, we reparameterize to obtain the given form:\n   \\[\n   \\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L\n   \\]\n\n2. **Variance ($\\sigma^2(M_t)$):**\n   Derived directly from the ratio of coefficients in the posterior distribution:\n   \\[\n   \\sigma^2(M_t) = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t\n   \\]\n\n3. **Noise prediction:** The denoising model $\\phi(L_t, F_t, A, t)$ predicts $\\hat{\\epsilon}_L \\in \\mathbb{R}^{3\\times 3}$ by estimating the noise added during the forward process. This prediction is conditioned on $L_t$, spatiotemporal features $F_t$, adjacency matrix $A$, and timestep $t$ to capture both structural and temporal dependencies.", "D": "To derive the expressions for $\\mu(M_t)$ and $\\sigma^2(M_t)$, we start from the standard DDPM framework where the forward process is defined by $$q(L_t\\mid L_0) = \\mathcal{N}(L_t\\mid\\sqrt{\\bar\\alpha_t}\\,L_0,\\,(1-\\bar\\alpha_t)I).$$ The reverse transition aims to approximate the true denoising process, leading to $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr).$$ The mean $\\mu(M_t)$ is derived by considering the noise prediction $\\hat\\epsilon_L(M_t,t)$, which is the output of the denoising model $\\phi(L_t, F_t, A, t)$. This model predicts the noise added at each step, allowing us to express the mean as $$\\mu(M_t) = \\frac{L_t}{\\sqrt{\\bar\\alpha_t}}\\; -\\;\\frac{1-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_t}}\\;\\hat\\epsilon_L(M_t,t).$$ The variance $\\sigma^2(M_t)$ is derived from the ratio of the noise schedules, resulting in $$\\sigma^2(M_t) = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\;\\beta_t.$$ The noise prediction $\\hat\\epsilon_L(M_t,t)\\in\\mathbb{R}^{3\\times 3}$ is obtained by the denoising model $\\phi$, which takes as input the current noisy sample $L_t$, the feature $F_t$, the attribute $A$, and the time step $t$, and outputs the predicted noise in the sample.", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "Given the assumption of G-invariance, derive the chain of equalities leading from $p(gx_0)$ to $p(x_0)$. Explain the step where substituting $gx_t$ back to $x_t$ preserves the integral and results in $p(x_0)$.", "A": "Under the assumption of \\( G \\)-invariance, we aim to derive the chain of equalities leading from \\( p(gx_0) \\) to \\( p(x_0) \\). Here is the step-by-step derivation with explanations:\n\n1. **\\( G \\)-Invariance Assumption**:  \n   The probability density \\( p(x) \\) is \\( G \\)-invariant, meaning it satisfies:\n   \\[\n   p(gx) = p(x), \\quad \\forall g \\in G.\n   \\]\n   This implies that the density is unchanged under group actions.\n\n2. **Starting Point**:  \n   We begin with \\( p(gx_0) \\). By the \\( G \\)-invariance of \\( p \\), we immediately have:\n   \\[\n   p(gx_0) = p(x_0).\n   \\]\n   This is the simplest case where the equality holds directly due to the invariance property.\n\n3. **General Case with Integral Preservation**:  \n   Now, consider a scenario where \\( p(gx_0) \\) arises from an integral or expectation over group transformations. For example, suppose:\n   \\[\n   p(gx_0) = \\int p(gx_t) \\, d\\mu(t),\n   \\]\n   where \\( \\mu \\) is a measure over some parameter space (e.g., time or group elements). \n\n   - **Substitution Step**:  \n     Using the \\( G \\)-invariance \\( p(gx_t) = p(x_t) \\), we substitute inside the integral:\n     \\[\n     p(gx_0) = \\int p(gx_t) \\, d\\mu(t) = \\int p(x_t) \\, d\\mu(t).\n     \\]\n     The substitution preserves the integral because the measure \\( d\\mu(t) \\) is invariant under the group action (or the substitution is measure-preserving). This is a common assumption in such derivations, ensuring that the integral's value remains unchanged.\n\n   - **Resulting Equality**:  \n     If the integral \\( \\int p(x_t) \\, d\\mu(t) \\) evaluates to \\( p(x_0) \\) (e.g., when \\( x_t \\) is a path or process with \\( x_0 \\) as its initial or reference point), we obtain:\n     \\[\n     p(gx_0) = p(x_0).\n     \\]\n\n4. **Key Explanation**:  \n   The critical step is the substitution \\( p(gx_t) = p(x_t) \\) under the integral. This is valid because:\n   - The \\( G \\)-invariance of \\( p \\) ensures \\( p(gx_t) = p(x_t) \\) pointwise.\n   - The measure \\( d\\mu(t) \\) (or the underlying space) is assumed to be compatible with the group action, meaning the substitution does not introduce any Jacobian factors or alter the integral's value. For example, if \\( G \\) is a compact group with Haar measure \\( dg \\), then \\( \\int f(gx) \\, dg = \\int f(x) \\, dg \\) due to the invariance of the Haar measure.\n\nThus, under \\( G \\)-invariance and appropriate measure-preserving conditions, the chain of equalities simplifies \\( p(gx_0) \\) to \\( p(x_0) \\). The substitution step is justified by the invariance of both \\( p \\) and the measure under group actions.", "B": "To derive the chain of equalities under the assumption of G-invariance, we start with the expression for $p(gx_0)$ and systematically apply the properties of G-invariance and probability measures:\n\n1. Initial Expression: We begin with $p(gx_0)$, which represents the probability density at the transformed point $gx_0$.\n\n2. Application of G-invariance: Due to G-invariance, the probability density is unchanged under group action. This allows us to express $p(gx_0)$ in terms of $p(gx_T)$ and the conditional probabilities $p(gx_{t-1} \\mid gx_t)$ for $t = 1$ to $T$.\n\n3. Substitution and Integral Preservation: The key step involves substituting $gx_t$ back to $x_t$ within the integral. This substitution is valid because the probability measure is invariant under the group action, meaning the integral's value remains unchanged. This invariance ensures that the integral over the transformed variables is equivalent to the integral over the original variables.\n\n4. Final Equality: After substitution, the expression simplifies to $p(x_T)$ multiplied by the integral of the product of conditional probabilities $p(x_{t-1} \\mid x_t)$, which, by definition, equals $p(x_0)$. This final step relies on the consistency of the probability measure and the Markov property of the process.\n\nThe preservation of the integral when substituting $gx_t$ back to $x_t$ is a direct consequence of the measure's invariance under the group action, ensuring that the transformation does not alter the integral's value. This property is crucial for maintaining the equality throughout the chain and ultimately arriving at $p(x_0)$.", "C": "To derive the chain of equalities under the assumption of G-invariance, we start with the expression for $p(gx_0)$ and systematically apply the properties of G-invariance and probability measures:\n\n1. Initial Expression: We begin with $p(gx_0)$, which represents the probability density at the transformed point $gx_0$.\n\n2. Application of G-invariance: Due to G-invariance, the probability density is unchanged under group action. This allows us to express $p(gx_0)$ in terms of $p(gx_T)$ and the conditional probabilities $p(gx_{t-1} \\mid gx_t)$ for $t = 1$ to $T$.\n\n3. Substitution and Integral Preservation: The key step involves substituting $gx_t$ back to $x_t$ within the integral. This substitution is valid because the probability measure is invariant under the group action, meaning the integral's value remains unchanged. This invariance ensures that the integral over the transformed variables is equivalent to the integral over the original variables.\n\n4. Final Equality: After substitution, the expression simplifies to $p(x_T)$ multiplied by the integral of the product of conditional probabilities $p(x_{t-1} \\mid x_t)$, which, by definition, equals $p(x_T)$. This final step relies on the consistency of the probability measure and the Markov property of the process.\n\nThe preservation of the integral when substituting $gx_t$ back to $x_t$ is a direct consequence of the measure's invariance under the group action, ensuring that the transformation does not alter the integral's value. This property is crucial for maintaining the equality throughout the chain and ultimately arriving at $p(x_T)$.", "D": "Under the assumption of $G$-invariance, the probability density $p(x)$ satisfies $p(gx) = p(x)$ for all $g \\in G$. To derive $p(x_0)$ from $p(gx_0)$, consider the following steps:\n\n1. **Express $p(gx_0)$ via marginalization**:  \n   \\begin{align*}\n   p(gx_0) &= \\int p(gx_0 | x_t) p(x_t) \\, dx_t\n   \\end{align*}\n   Here, $x_t$ represents intermediate states (e.g., in a diffusion process or stochastic trajectory).\n\n2. **Exploit $G$-invariance of the conditional distribution**:  \n   By $G$-equivariance of the dynamics or symmetry of the transition kernel, we have  \n   \\begin{align*}\n   p(gx_0 | x_t) = p(x_0 | g^{-1}x_t).\n   \\end{align*}\n\n3. **Substitute $x_t \\to gx_t$ in the integral**:  \n   Change variables by letting $x'_t = g^{-1}x_t$. Since the measure $dx_t$ is $G$-invariant (e.g., under Haar measure), $dx'_t = dx_t$. The integral becomes:  \n   \\begin{align*}\n   p(gx_0) &= \\int p(x_0 | x'_t) \\cdot p(gx'_t) \\, dx'_t.\n   \\end{align*}\n\n4. **Apply $G$-invariance of $p(x_t)$**:  \n   By assumption, $p(gx'_t) = p(x'_t)$. Thus,  \n   \\begin{align*}\n   p(gx_0) &= \\int p(x_0 | x'_t) p(x'_t) \\, dx'_t.\n   \\end{align*}\n\n5. **Re-express as $p(x_0)$**:  \n   The right-hand side is the definition of $p(x_0)$ via marginalization:  \n   \\begin{align*}\n   p(gx_0) &= p(x_0).\n   \\end{align*}\n\n**Critical explanation for Step 3**:  \nThe substitution $x_t \\to gx_t$ preserves the integral because:  \n- The measure $dx_t$ is $G$-invariant, so $dx_t = d(gx_t)$.  \n- The conditional distribution $p(x_0 | x_t)$ transforms covariantly under $G$-action (equivariance), ensuring $p(gx_0 | x_t) = p(x_0 | g^{-1}x_t)$.  \n- The $G$-invariance of $p(x_t)$ ensures $p(gx_t) = p(x_t)$.  \n\nThese symmetries collectively guarantee that the integral evaluates to $p(x_0)$, independent of $g$. Thus, $p(gx_0) = p(x_0)$ holds universally under $G$-invariance.", "ground-truth": "B"}
{"paper": "arxiv:2309.04475", "question": "How can we prove Proposition 1: 'The marginal distribution $$p(L_0)$$ by Eq. (3) is O(3)-invariant if $$\\hat{\\epsilon}_L(M_t, t)$$ is O(3)-equivariant, i.e. $$\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t),\\;\\forall Q^TQ=I$$'? Show how the O(3)-equivariance of the denoising term guarantees that $$p(L_0)$$ remains invariant under any orthogonal transformation $Q$.", "A": "To prove Proposition 1, we start by understanding the implications of the O(3)-equivariance of the denoising term $$\\hat{\\epsilon}_L$$. The equivariance condition $$\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t)$$ for all orthogonal matrices $$Q$$ (i.e., $$Q^TQ = I$$) ensures that the denoising process transforms predictably under orthogonal transformations of the input. This property is crucial because it implies that the denoising operation commutes with the action of the orthogonal group O(3) on the input space.\n\nThe marginal distribution $$p(L_0)$$ is constructed through a process that involves the denoising term and a prior distribution, which is assumed to be O(3)-invariant (e.g., a standard normal distribution $$\\mathcal{N}(0, I)$$). The O(3)-invariance of the prior means that its probability density function remains unchanged under any orthogonal transformation $$Q$$ applied to its argument.\n\nGiven these two properties, the transition probability from $$L_t$$ to $$L_0$$, which involves the denoising term and the prior, will also be O(3)-invariant. This is because the transformation induced by $$Q$$ can be 'passed through' the denoising term due to its equivariance, and the prior's invariance ensures that the overall probability density remains unchanged. Therefore, the marginal distribution $$p(L_0)$$ inherits the O(3)-invariance from the denoising term and the prior, ensuring that it remains invariant under any orthogonal transformation $$Q$$.", "B": "To prove Proposition 1, we start by understanding the implications of the O(3)-equivariance of the denoising term $$\\hat{\\epsilon}_L$$. The equivariance condition $$\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t)$$ for all orthogonal matrices $$Q$$ (i.e., $$Q^TQ = I$$) ensures that the denoising process transforms predictably under orthogonal transformations of the input. This property is crucial because it implies that the denoising operation commutes with the action of the orthogonal group O(3) on the input space.\n\nThe marginal distribution $$p(L_0)$$ is constructed through a process that involves the denoising term and a prior distribution, which is assumed to be O(3)-invariant (e.g., a standard normal distribution $$\\mathcal{N}(0, I)$$). The O(3)-invariance of the prior means that its probability density function remains unchanged under any orthogonal transformation $$Q$$ applied to its argument.\n\nGiven these two properties, the transition probability from $$L_t$$ to $$L_0$$, which involves the denoising term and the prior, will also be O(3)-invariant. This is because the transformation induced by $$Q$$ can be 'passed through' the denoising term due to its equivariance, and the prior's invariance ensures that the overall probability density remains unchanged. However, this only holds if the prior is isotropic, which is not explicitly guaranteed in the given conditions. Therefore, the marginal distribution $$p(L_0)$$ may not inherit the O(3)-invariance from the denoising term and the prior, and it might not remain invariant under any orthogonal transformation $$Q$$.", "C": "To prove Proposition 1, we demonstrate that the O(3)-equivariance of the denoising term $\\hat{\\epsilon}_L$ ensures the marginal distribution $p(L_0)$ derived from the reverse diffusion process remains invariant under orthogonal transformations $Q$. Here's the step-by-step reasoning:\n\n**1. Reverse Process Structure**  \nAssume the reverse process is defined by a Markov chain with transition kernel:  \n$$\np_\\theta(L_{t-1} | L_t) = \\mathcal{N}\\left(L_{t-1}; \\mu_\\theta(L_t, t), \\sigma_t^2 I\\right)\n$$  \nwhere $\\mu_\\theta(L_t, t)$ depends on the O(3)-equivariant denoising term $\\hat{\\epsilon}_L(L_t, t)$. A typical parameterization is:  \n$$\n\\mu_\\theta(L_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, t)\\right)\n$$  \n\n**2. Equivariance of $\\mu_\\theta$ Under $Q$**  \nFor any $Q \\in \\text{O(3)}$, the O(3)-equivariance of $\\hat{\\epsilon}_L$ implies:  \n$$\n\\hat{\\epsilon}_L(Q L_t, t) = Q \\hat{\\epsilon}_L(L_t, t)\n$$  \nSubstituting into $\\mu_\\theta(Q L_t, t)$:  \n$$\n\\mu_\\theta(Q L_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(Q L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} Q \\hat{\\epsilon}_L(L_t, t)\\right) = Q \\mu_\\theta(L_t, t)\n$$  \nThus, $\\mu_\\theta$ is **equivariant** under $Q$.\n\n**3. Invariance of Transition Kernel**  \nThe transition kernel transforms as:  \n$$\np_\\theta(Q L_{t-1} | Q L_t) = \\mathcal{N}\\left(Q L_{t-1}; Q \\mu_\\theta(L_t, t), \\sigma_t^2 I\\right)\n$$  \nSince $\\mathcal{N}(Q x; Q \\mu, \\sigma^2 I) = \\mathcal{N}(x; \\mu, \\sigma^2 I)$ (orthogonal transformations preserve Gaussian distributions with isotropic covariance), we have:  \n$$\np_\\theta(Q L_{t-1} | Q L_t) = p_\\theta(L_{t-1} | L_t)\n$$  \n\n**4. Invariance of Initial Distribution**  \nAssume $p(L_T) = \\mathcal{N}(0, I)$, which is O(3)-invariant:  \n$$\np(Q L_T) = \\mathcal{N}(Q L_T; 0, I) = \\mathcal{N}(L_T; 0, I) = p(L_T)\n$$  \n\n**5. Invariance of Full Joint Distribution**  \nThe joint distribution over all timesteps is:  \n$$\np(L_{0:T}) = p(L_T) \\prod_{t=1}^T p_\\theta(L_{t-1} | L_t)\n$$  \nUnder transformation $Q$, Steps 3–4 ensure:  \n$$\np(Q L_{0:T}) = p(Q L_T) \\prod_{t=1}^T p_\\theta(Q L_{t-1} | Q L_t) = p(L_T) \\prod_{t=1}^T p_\\theta(L_{t-1} | L_t) = p(L_{0:T})\n$$  \n\n**6. Marginal Distribution Invariance**  \nMarginalizing over $L_{1:T}$:  \n$$\np(Q L_0) = \\int p(Q L_{0:T}) \\, d(Q L_{1:T}) = \\int p(L_{0:T}) \\, dL_{1:T} = p(L_0)\n$$  \nThus, $p(L_0)$ is O(3)-invariant.  \n\n**Conclusion**  \nThe O(3)-equivariance of $\\hat{\\epsilon}_L$ propagates through the reverse process, ensuring that $p(L_0)$ remains unchanged under orthogonal transformations. This symmetry arises from the equivariant parameterization of $\\mu_\\theta$ and the isotropic noise assumption in the diffusion process.", "D": "To prove Proposition 1, we need to show that the marginal distribution \\( p(L_0) \\) is \\( O(3) \\)-invariant given that the denoising term \\( \\hat{\\epsilon}_L(M_t, t) \\) is \\( O(3) \\)-equivariant. Here's the step-by-step proof:\n\n---\n\n### **1. Setup and Definitions**\n- Let \\( Q \\in O(3) \\) be an arbitrary orthogonal matrix (\\( Q^TQ = I \\)).\n- The denoising term \\( \\hat{\\epsilon}_L(M_t, t) \\) is \\( O(3) \\)-equivariant:\n  \\[\n  \\hat{\\epsilon}_L(Q L_t, F_t, A, t) = Q \\hat{\\epsilon}_L(L_t, F_t, A, t).\n  \\]\n- The marginal distribution \\( p(L_0) \\) is derived from a diffusion process governed by Eq. (3), which we assume to be of the form:\n  \\[\n  p(L_0) = \\int p(L_0 | L_t, F_t, A) p(L_t, F_t, A) \\, dL_t \\, dF_t \\, dA.\n  \\]\n  (Note: If Eq. (3) is a stochastic differential equation (SDE) or a probability flow ODE, the same logic applies by considering the transformation properties of the drift and diffusion terms.)\n\n---\n\n### **2. Key Idea: Transformation of the Diffusion Process**\nWe analyze how the diffusion process transforms under \\( Q \\):\n1. **Forward Process**: The noising process typically adds isotropic Gaussian noise to \\( L_t \\). If \\( L_t \\mapsto Q L_t \\), the noise distribution remains unchanged because Gaussian noise is \\( O(3) \\)-invariant.\n2. **Reverse Process**: The denoising term \\( \\hat{\\epsilon}_L \\) is used to reverse the noising process. By assumption, it satisfies:\n   \\[\n   \\hat{\\epsilon}_L(Q L_t, F_t, A, t) = Q \\hat{\\epsilon}_L(L_t, F_t, A, t).\n   \\]\n   This ensures that the reverse process commutes with orthogonal transformations.\n\n---\n\n### **3. Invariance of \\( p(L_0) \\)**\nTo show \\( p(L_0) \\) is \\( O(3) \\)-invariant, we must prove:\n\\[\np(Q L_0) = p(L_0), \\quad \\forall Q \\in O(3).\n\\]\n\n**Proof:**\n1. **Transformed Initial Distribution**: Apply \\( Q \\) to the entire system:\n   \\[\n   p(Q L_0) = \\int p(Q L_0 | Q L_t, F_t, A) p(Q L_t, F_t, A) \\, dL_t \\, dF_t \\, dA.\n   \\]\n2. **Equivariance of \\( p(Q L_0 | Q L_t, F_t, A) \\)**: The conditional distribution \\( p(L_0 | L_t, F_t, A) \\) depends on the denoising term \\( \\hat{\\epsilon}_L \\). Because \\( \\hat{\\epsilon}_L \\) is \\( O(3) \\)-equivariant, the reverse process transforms as:\n   \\[\n   p(Q L_0 | Q L_t, F_t, A) = p(L_0 | L_t, F_t, A).\n   \\]\n   (This follows from the fact that the drift/diffusion terms in the reverse SDE or ODE transform covariantly under \\( Q \\).)\n3. **Invariance of \\( p(Q L_t, F_t, A) \\)**: The joint distribution \\( p(L_t, F_t, A) \\) is \\( O(3) \\)-invariant because the noising process is isotropic (Gaussian noise is rotationally invariant). Thus:\n   \\[\n   p(Q L_t, F_t, A) = p(L_t, F_t, A).\n   \\]\n4. **Combining Results**: Substituting these into the expression for \\( p(Q L_0) \\):\n   \\[\n   p(Q L_0) = \\int p(L_0 | L_t, F_t, A) p(L_t, F_t, A) \\, dL_t \\, dF_t \\, dA = p(L_0).\n   \\]\n\n---\n\n### **4. Conclusion**\nBecause \\( p(Q L_0) = p(L_0) \\) for all \\( Q \\in O(3) \\), the marginal distribution \\( p(L_0) \\) is \\( O(3) \\)-invariant. This invariance arises directly from the \\( O(3) \\)-equivariance of the denoising term \\( \\hat{\\epsilon}_L \\), which ensures that the reverse process respects the symmetry of the forward process.\n\n\\[\n\\boxed{p(Q L_0) = p(L_0), \\quad \\forall Q \\in O(3).}\n\\]", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "Starting from the reverse diffusion transition $$p(L_{t-1}\\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$$ and substituting $$\\mu(M_t), \\sigma^2(M_t)$$ for the lattice, how do we arrive at $$p(L_{t-1}\\mid L_t, F_t, A) = \\mathcal{N}\\bigl(L_{t-1}\\mid a_t(L_t - b_t\\hat{\\epsilon}_L(L_t, F_t, A, t)),\\,\\sigma_t^2 I\\bigr)?$$ In your explanation, clarify how $a_t$ and $b_t$ are defined in terms of $\\bar{\\alpha}_t$ and why $\\hat{\\epsilon}_L(M_t,t)$ simplifies to $\\hat{\\epsilon}_L(L_t,F_t,A,t).", "A": "To derive the conditional reverse diffusion transition $p(L_{t-1} \\mid L_t, F_t, A)$ from the original form $p(L_{t-1} \\mid L_t) = \\mathcal{N}\\bigl(L_{t-1}\\mid\\mu(M_t),\\,\\sigma^2(M_t)I\\bigr)$, we proceed as follows:\n\n---\n\n### **1. Parameterizing $\\mu(M_t)$ with Noise Prediction**\nIn standard diffusion models, $\\mu(M_t)$ is reparameterized using a noise prediction network $\\hat{\\epsilon}_L$. For a lattice $L_t$:\n$$\n\\mu(M_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( L_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t) \\right)\n$$\nwhere $\\beta_t = 1 - \\alpha_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. This matches the form:\n$$\n\\mu(M_t) = a_t \\bigl( L_t - b_t \\hat{\\epsilon}_L(M_t, t) \\bigr)\n$$\nby defining:\n$$\na_t = \\frac{1}{\\sqrt{\\alpha_t}}, \\quad b_t = \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\n$$\n\n---\n\n### **2. Conditioning on $F_t$ and $A$**\nThe original noise predictor $\\hat{\\epsilon}_L(M_t, t)$ implicitly depends on all information in $M_t$. When conditioning on external variables $F_t$ (e.g., forces) and $A$ (e.g., atom types), we **explicitly** replace $M_t$ with $(L_t, F_t, A)$:\n$$\n\\hat{\\epsilon}_L(M_t, t) \\rightarrow \\hat{\\epsilon}_L(L_t, F_t, A, t)\n$$\nThis substitution arises because:\n- $M_t$ contains all contextual information up to step $t$, which is now decomposed into explicit dependencies on $L_t$, $F_t$, and $A$.\n- The noise predictor is trained to use these variables jointly.\n\n---\n\n### **3. Variance Simplification**\nThe variance $\\sigma^2(M_t)$ is simplified to a time-dependent constant $\\sigma_t^2$ (typically $\\beta_t$ or $\\tilde{\\beta}_t$ in standard diffusion), independent of $M_t$:\n$$\n\\sigma^2(M_t) \\rightarrow \\sigma_t^2\n$$\nThis is justified because:\n- For tractability, variance is often treated as non-learnable in practice.\n- $\\sigma_t^2$ is derived from the diffusion schedule $\\{\\bar{\\alpha}_t\\}$.\n\n---\n\n### **4. Final Expression**\nSubstituting $\\mu(M_t)$ and $\\sigma^2(M_t)$ with the above definitions, we get:\n$$\np(L_{t-1} \\mid L_t, F_t, A) = \\mathcal{N}\\bigl(L_{t-1}\\mid a_t(L_t - b_t\\hat{\\epsilon}_L(L_t, F_t, A, t)),\\,\\sigma_t^2 I\\bigr)\n$$\n\n---\n\n### **Key Notes**\n- **$a_t$ and $b_t$:** These are deterministic functions of $\\bar{\\alpha}_t$, ensuring consistency with the forward diffusion process.\n- **Noise Predictor Simplification:** $\\hat{\\epsilon}_L(M_t, t) \\rightarrow \\hat{\\epsilon}_L(L_t, F_t, A, t)$ reflects explicit conditioning on variables previously encapsulated in $M_t$.\n- **Variance:** $\\sigma_t^2$ is typically fixed to $\\beta_t$ or $\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t$, depending on the diffusion variant.\n\nThis derivation aligns with standard diffusion frameworks (e.g., DDPM) while extending noise prediction to include structured inputs $(L_t, F_t, A)$.", "B": "To derive the expression for $$p(L_{t-1}\\mid L_t, F_t, A)$$, we start with the given reverse diffusion transition and substitute the expressions for $$\\mu(M_t)$$ and $$\\sigma^2(M_t)$$. The key steps involve understanding the definitions of $$a_t$$ and $$b_t$$ in terms of $$\\bar{\\alpha}_t$$ and recognizing the simplification of $$\\hat{\\epsilon}_L(M_t,t)$$ to $$\\hat{\\epsilon}_L(L_t,F_t,A,t)$$. The parameters $$a_t$$ and $$b_t$$ are defined as $$a_t = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{\\sqrt{\\bar{\\alpha}_t}}$$ and $$b_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{\\sqrt{1 - \\bar{\\alpha}_{t-1}}}$$, respectively. The simplification occurs because $$\\hat{\\epsilon}_L$$ is a function that depends on the current state $$L_t$$, the features $$F_t$$, the adjacency matrix $$A$$, and the time step $$t$$, making $$M_t$$ redundant in this context. Thus, the expression simplifies to the given form, incorporating these definitions and simplifications directly into the reverse transition probability.", "C": "To derive the expression for \\( p(L_{t-1} \\mid L_t, F_t, A) \\) from the given reverse diffusion transition, we'll follow these steps:\n\n### 1. **Given Reverse Diffusion Transition**\nThe initial reverse transition is:\n\\[\np(L_{t-1} \\mid L_t) = \\mathcal{N}\\bigl(L_{t-1} \\mid \\mu(M_t), \\sigma^2(M_t)I\\bigr),\n\\]\nwhere \\( M_t \\) represents the state at time \\( t \\), which includes \\( L_t \\), \\( F_t \\), and \\( A \\).\n\n### 2. **Parameterization of \\(\\mu(M_t)\\) and \\(\\sigma^2(M_t)\\)**\nIn diffusion models, the mean \\(\\mu(M_t)\\) and variance \\(\\sigma^2(M_t)\\) are typically parameterized in terms of the noise prediction model \\(\\hat{\\epsilon}_L(M_t, t)\\). For the lattice \\( L_t \\), the mean is often expressed as:\n\\[\n\\mu(M_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(M_t, t) \\right),\n\\]\nwhere:\n- \\(\\alpha_t = 1 - \\beta_t\\) is the noise schedule,\n- \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\) is the cumulative product of \\(\\alpha_s\\),\n- \\(\\beta_t\\) is the variance schedule.\n\nThe variance \\(\\sigma^2(M_t)\\) is typically set to \\(\\sigma_t^2\\), a fixed or learned scalar.\n\n### 3. **Simplification of \\(\\hat{\\epsilon}_L(M_t, t)\\)**\nThe noise prediction model \\(\\hat{\\epsilon}_L(M_t, t)\\) depends on the full state \\( M_t \\), which includes \\( L_t \\), \\( F_t \\), and \\( A \\). Since \\( M_t \\) is fully determined by \\( (L_t, F_t, A) \\), we can write:\n\\[\n\\hat{\\epsilon}_L(M_t, t) = \\hat{\\epsilon}_L(L_t, F_t, A, t).\n\\]\nThis simplification reflects that the noise prediction is conditioned on the current lattice \\( L_t \\), the features \\( F_t \\), and the adjacency matrix \\( A \\).\n\n### 4. **Definition of \\( a_t \\) and \\( b_t \\)**\nLet \\( a_t \\) and \\( b_t \\) be defined in terms of \\(\\bar{\\alpha}_t\\) as follows:\n\\[\na_t = \\frac{1}{\\sqrt{\\alpha_t}}, \\quad b_t = \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}.\n\\]\nThese definitions arise from the noise schedule and ensure that the mean \\(\\mu(M_t)\\) can be rewritten as:\n\\[\n\\mu(M_t) = a_t (L_t - b_t \\hat{\\epsilon}_L(L_t, F_t, A, t)).\n\\]\n\n### 5. **Final Reverse Transition**\nSubstituting \\(\\mu(M_t)\\) and \\(\\sigma^2(M_t)\\) into the reverse transition, we obtain:\n\\[\np(L_{t-1} \\mid L_t, F_t, A) = \\mathcal{N}\\bigl(L_{t-1} \\mid a_t (L_t - b_t \\hat{\\epsilon}_L(L_t, F_t, A, t)), \\sigma_t^2 I\\bigr).\n\\]\n\n### Summary\n- The mean \\(\\mu(M_t)\\) is reparameterized using \\( a_t \\) and \\( b_t \\), which are derived from \\(\\bar{\\alpha}_t\\).\n- The noise prediction \\(\\hat{\\epsilon}_L(M_t, t)\\) simplifies to \\(\\hat{\\epsilon}_L(L_t, F_t, A, t)\\) because \\( M_t \\) is fully specified by \\( (L_t, F_t, A) \\).\n- The variance is set to \\(\\sigma_t^2 I\\), a common choice in diffusion models.\n\nThis yields the desired expression:\n\\[\np(L_{t-1} \\mid L_t, F_t, A) = \\mathcal{N}\\bigl(L_{t-1} \\mid a_t (L_t - b_t \\hat{\\epsilon}_L(L_t, F_t, A, t)), \\sigma_t^2 I\\bigr).\n\\]", "D": "To derive the expression for $$p(L_{t-1}\\mid L_t, F_t, A)$$, we start with the given reverse diffusion transition and substitute the expressions for $$\\mu(M_t)$$ and $$\\sigma^2(M_t)$$. The key steps involve understanding the definitions of $$a_t$$ and $$b_t$$ in terms of $$\\bar{\\alpha}_t$$ and recognizing the simplification of $$\\hat{\\epsilon}_L(M_t,t)$$ to $$\\hat{\\epsilon}_L(L_t,F_t,A,t)$$. The parameters $$a_t$$ and $$b_t$$ are defined as $$a_t = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{\\sqrt{\\bar{\\alpha}_t}}$$ and $$b_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{\\sqrt{1 - \\bar{\\alpha}_t}}$$, respectively. The simplification occurs because $$\\hat{\\epsilon}_L$$ is a function that depends on the current state $$L_t$$, the features $$F_t$$, the adjacency matrix $$A$$, and the time step $$t$$, making $$M_t$$ redundant in this context. Thus, the expression simplifies to the given form, incorporating these definitions and simplifications directly into the reverse transition probability.", "ground-truth": "B"}
{"paper": "arxiv:2309.04475", "question": "How can we demonstrate the sequence of equalities in the expression involving $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$ and its transformations? Specifically, how does the application of $Q$ and the property of O(3)-equivariance, expressed as $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$, ensure that the distribution for $L_{t-1}$ remains unchanged?", "A": "To understand the sequence of equalities, let's break down the process step by step:\n\n1. Initial Distribution: We start with the conditional distribution $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$, which describes the probability of $L_{t-1}$ given $L_t$, $F_t$, and $A$ under a normal distribution framework.\n\n2. Application of $Q$: By applying the orthogonal transformation $Q$ to both $L_{t-1}$ and $L_t$, we consider the transformed variables $QL_{t-1}$ and $QL_t$. The equality $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}(QL_{t-1}\\mid QL_t,F_t,A)$ holds because the normal distribution is invariant under orthogonal transformations. This means the probability density remains the same before and after the transformation.\n\n3. O(3)-Equivariance Property: The key to understanding the next equality lies in the O(3)-equivariance of $\\hat{\\epsilon}_L$, which is given by $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$. This property ensures that the transformation $Q$ commutes with the function $\\hat{\\epsilon}_L$, meaning that applying $Q$ to the input is equivalent to applying $Q$ to the output of $\\hat{\\epsilon}_L$.\n\n4. Recovery of Original Distribution: Due to the invariance of the normal distribution under orthogonal transformations and the O(3)-equivariance of $\\hat{\\epsilon}_L$, the distribution of $L_{t-1}$ conditioned on $L_t$, $F_t$, and $A$ remains unchanged. This leads us back to the original distribution $p(L_{t-1}\\mid L_t,F_t,A)$, completing the chain of equalities.\n\nIn summary, the combination of the normal distribution's invariance under orthogonal transformations and the O(3)-equivariance property of $\\hat{\\epsilon}_L$ ensures that the distribution for $L_{t-1}$ is preserved throughout the transformations, validating the sequence of equalities presented.", "B": "To understand the sequence of equalities, let's break down the process step by step:\n\n1. Initial Distribution: We start with the conditional distribution $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$, which describes the probability of $L_{t-1}$ given $L_t$, $F_t$, and $A$ under a normal distribution framework.\n\n2. Application of $Q$: By applying the orthogonal transformation $Q$ to both $L_{t-1}$ and $L_t$, we consider the transformed variables $QL_{t-1}$ and $QL_t$. The equality $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}(QL_{t-1}\\mid QL_t,F_t,A)$ holds because the normal distribution is invariant under orthogonal transformations. This means the probability density remains the same before and after the transformation.\n\n3. O(3)-Equivariance Property: The key to understanding the next equality lies in the O(3)-equivariance of $\\hat{\\epsilon}_L$, which is given by $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$. This property ensures that the transformation $Q$ commutes with the function $\\hat{\\epsilon}_L$, meaning that applying $Q$ to the input is equivalent to applying $Q$ to the output of $\\hat{\\epsilon}_L$.\n\n4. Recovery of Original Distribution: Due to the invariance of the normal distribution under orthogonal transformations and the O(3)-equivariance of $\\hat{\\epsilon}_L$, the distribution of $L_{t-1}$ conditioned on $L_t$, $F_t$, and $A$ remains unchanged. This leads us back to the original distribution $p(L_{t-1}\\mid L_t,F_t,A)$, completing the chain of equalities.\n\nIn summary, the combination of the normal distribution's invariance under orthogonal transformations and the O(3)-equivariance property of $\\hat{\\epsilon}_L$ ensures that the distribution for $L_{t-1}$ is preserved throughout the transformations, validating the sequence of equalities presented. However, this conclusion implicitly assumes that $Q$ is invertible, which is not guaranteed for all orthogonal transformations.", "C": "To demonstrate the sequence of equalities involving the distribution \\(\\mathcal{N}(L_{t-1} \\mid L_t, F_t, A)\\) and its transformations under the application of an orthogonal transformation \\(Q \\in O(3)\\), we will carefully analyze the equivariance property and its implications on the distribution.\n\n### **1. Initial Distribution and Transformation**\nThe distribution is given by:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(\\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right),\n\\]\nwhere \\(\\hat{\\epsilon}_L\\) is the noise predictor, and \\(\\alpha_t, \\beta_t, \\bar{\\alpha}_t\\) are diffusion process parameters.\n\n### **2. O(3)-Equivariance of \\(\\hat{\\epsilon}_L\\)**\nThe key property is the \\(O(3)\\)-equivariance of \\(\\hat{\\epsilon}_L\\):\n\\[\n\\hat{\\epsilon}_L(Q L_t, F_t, A, t) = Q \\hat{\\epsilon}_L(L_t, F_t, A, t).\n\\]\nThis means that rotating the input \\(L_t\\) by \\(Q\\) results in the same rotation applied to the output noise prediction.\n\n### **3. Distribution Under Transformation \\(Q\\)**\nNow, consider applying \\(Q\\) to \\(L_t\\) and examining the transformed distribution for \\(L_{t-1}\\). Let \\(L'_t = Q L_t\\). The distribution for \\(L_{t-1}\\) becomes:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(\\frac{1}{\\sqrt{\\alpha_t}} \\left(Q L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(Q L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right).\n\\]\nUsing the equivariance property:\n\\[\n\\hat{\\epsilon}_L(Q L_t, F_t, A, t) = Q \\hat{\\epsilon}_L(L_t, F_t, A, t),\n\\]\nwe substitute back:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(\\frac{1}{\\sqrt{\\alpha_t}} \\left(Q L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} Q \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right).\n\\]\nFactor out \\(Q\\) from the mean:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(Q \\cdot \\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right).\n\\]\n\n### **4. Rotational Invariance of the Distribution**\nThe covariance matrix \\(\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\) is isotropic (proportional to the identity matrix), and thus invariant under orthogonal transformations:\n\\[\nQ \\left(\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right) Q^T = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I.\n\\]\nTherefore, the distribution can be rewritten as:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(Q \\mu, \\Sigma\\right),\n\\]\nwhere \\(\\mu = \\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right)\\) and \\(\\Sigma = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\).\n\n### **5. Conclusion: Unchanged Distribution**\nSince the distribution of \\(L_{t-1}\\) is rotationally equivariant (i.e., applying \\(Q\\) to \\(L_t\\) results in \\(Q\\) being applied to the mean while the covariance remains unchanged), the overall distribution remains consistent under \\(O(3)\\) transformations. Thus, the distribution for \\(L_{t-1}\\) is unchanged up to the orthogonal transformation \\(Q\\), ensuring the desired invariance property.\n\n\\[\n\\boxed{\\mathcal{N}(L_{t-1} \\mid Q L_t, F_t, A) = \\mathcal{N}(Q L_{t-1} \\mid L_t, F_t, A)}\n\\]", "D": "To demonstrate the sequence of equalities and show that the distribution for $L_{t-1}$ remains unchanged under the application of $Q \\in \\text{O}(3)$, we leverage the equivariance property of $\\hat{\\epsilon}_L$ and the structure of the Gaussian distribution. Here's the step-by-step reasoning:\n\n---\n\n**1. Define the conditional distribution:**  \nAssume the reverse process is parameterized as:  \n$$\nL_{t-1} \\sim \\mathcal{N}\\left(\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right),\n$$  \nwhere the **mean** $\\mu(L_t, F_t, A, t)$ depends on the learned noise-prediction model $\\hat{\\epsilon}_L$ via:  \n$$\n\\mu(L_t, F_t, A, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(L_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\hat{\\epsilon}_L(L_t, F_t, A, t)\\right).\n$$  \nHere, $\\alpha_t$, $\\beta_t$, and $\\bar{\\alpha}_t$ are diffusion scheduling hyperparameters.\n\n---\n\n**2. Apply $Q \\in \\text{O}(3)$ to $L_t$:**  \nUnder the transformation $L_t \\to QL_t$, the equivariance property of $\\hat{\\epsilon}_L$ implies:  \n$$\n\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t).\n$$  \nSubstituting this into the mean $\\mu(QL_t, F_t, A, t)$:  \n$$\n\\mu(QL_t, F_t, A, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(QL_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}Q\\hat{\\epsilon}_L(L_t, F_t, A, t)\\right) = Q\\mu(L_t, F_t, A, t).\n$$  \nThus, **the mean transforms equivariantly** under $Q$.\n\n---\n\n**3. Analyze the covariance matrix:**  \nThe covariance $\\sigma_t^2 I$ is **isotropic** (invariant under $\\text{O}(3)$ transformations). For any $Q \\in \\text{O}(3)$:  \n$$\nQ\\left(\\sigma_t^2 I\\right)Q^\\top = \\sigma_t^2 QQ^\\top = \\sigma_t^2 I.\n$$  \nHence, the covariance structure is preserved.\n\n---\n\n**4. Show distributional equivalence:**  \nThe transformed distribution becomes:  \n$$\n\\mathcal{N}\\left(Q\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right).\n$$  \nThis is equivalent to applying $Q$ to samples from the original distribution:  \n$$\nQ\\mathcal{N}\\left(\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right) \\equiv \\mathcal{N}\\left(Q\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right).\n$$  \nThus, the distribution for $L_{t-1}$ under the transformation $L_t \\to QL_t$ is **identical** to applying $Q$ to $L_{t-1}$ directly.\n\n---\n\n**5. Conclusion:**  \nThe $\\text{O}(3)$-equivariance of $\\hat{\\epsilon}_L$ ensures the mean transforms as $\\mu(QL_t, \\cdot) = Q\\mu(L_t, \\cdot)$, while the isotropic covariance remains invariant. Consequently, the distribution of $L_{t-1}$ satisfies:  \n$$\n\\mathcal{N}(L_{t-1} \\mid QL_t, F_t, A) = Q \\cdot \\mathcal{N}(L_{t-1} \\mid L_t, F_t, A),\n$$  \nmeaning the distribution is **equivariant** under $Q$. If the prior distribution at the final timestep is $\\text{O}(3)$-invariant (e.g., standard Gaussian), the overall generative process becomes **invariant** to $\\text{O}(3)$ transformations.", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "How can we prove Lemma 3: 'If the denoising term $$\\hat{\\epsilon}_F(L_t,F_t,A,t)$$ is periodic translation invariant, and $$p(F_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}_w(F_{t-1}\\mid F_t + u_t\\hat{\\epsilon}_F(L_t,F_t,A,t),\\,v_t^2 I)$$, then the transition is periodic translation equivariant'? Please clarify why $\\hat{\\epsilon}_F$ and the wrapped normal distribution $\\mathcal{N}_w$ must both respect the crystal s periodicity.", "A": "To prove Lemma 3, we start by understanding the definitions and implications of periodic translation invariance and equivariance in the context of the given terms. The denoising term $\\hat{\\epsilon}_F(L_t, F_t, A, t)$ is periodic translation invariant, meaning its value does not change under periodic translations of its arguments within the crystal's lattice. This property ensures that the denoising process is consistent across equivalent positions in the crystal.\n\nThe transition probability is given by a wrapped normal distribution $\\mathcal{N}_w$, which is defined over a torus to respect the crystal's periodicity. This means that the distribution accounts for the periodic boundary conditions of the crystal, ensuring that transitions between states are physically meaningful within the crystal's framework.\n\nGiven these definitions, the proof proceeds as follows:\n1. Periodic Translation Invariance of $\\hat{\\epsilon}_F$: Since $\\hat{\\epsilon}_F$ is invariant under periodic translations, applying a periodic translation to $F_t$ and $L_t$ does not change the value of $\\hat{\\epsilon}_F(L_t, F_t, A, t)$. This implies that the mean of the distribution $F_t + u_t\\hat{\\epsilon}_F(L_t, F_t, A, t)$ is also invariant under the same translations.\n2. Wrapped Normal Distribution $\\mathcal{N}_w$: The use of $\\mathcal{N}_w$ ensures that the probability distribution respects the crystal's periodicity. This is crucial because it means that the transition probabilities are correctly computed modulo the crystal's lattice vectors, ensuring that the distribution 'wraps around' at the boundaries of the unit cell.\n3. Equivariance of the Transition: Combining the invariance of $\\hat{\\epsilon}_F$ and the periodicity-respecting property of $\\mathcal{N}_w$, the overall transition $p(F_{t-1}\\mid L_t, F_t, A)$ is equivariant under periodic translations. This means that translating the input $F_t$ by a lattice vector results in a corresponding translation of the output distribution $F_{t-1}$, preserving the crystal's symmetry.\n\nIn summary, both $\\hat{\\epsilon}_F$ and $\\mathcal{N}_w$ must respect the crystal's periodicity to ensure that the transition probabilities are physically meaningful and consistent with the crystal's symmetry. This leads to the periodic translation equivariance of the transition, as stated in Lemma 3.", "B": "To prove Lemma 3, we analyze how the periodic translation invariance of $\\hat{\\epsilon}_F$ and the wrapped normal distribution $\\mathcal{N}_w$ jointly ensure the transition's equivariance under crystal periodicity.\n\n---\n\n### **Proof of Periodic Translation Equivariance**\n\n**Definitions**:\n- Let $T_g$ denote a periodic translation by vector $g$ (aligned with the crystal lattice vectors). For coordinates $x$, $T_g(x) = (x + g) \\mod \\Omega$, where $\\Omega$ is the periodic unit cell.\n- A function $f$ is **periodic translation equivariant** if $T_g(f(x)) = f(T_g(x))$.\n- The wrapped normal distribution $\\mathcal{N}_w$ enforces $x \\in \\Omega$ by wrapping samples into the unit cell.\n\n---\n\n**Step 1: Equivariance of the Denoising Term $\\hat{\\epsilon}_F$**  \nBy assumption, $\\hat{\\epsilon}_F$ is periodic translation invariant. For $T_g$-translated inputs:\n$$\n\\hat{\\epsilon}_F\\big(T_g(L_t), T_g(F_t), A, t\\big) = T_g\\left(\\hat{\\epsilon}_F(L_t, F_t, A, t)\\right).\n$$\nThis ensures $\\hat{\\epsilon}_F$ respects crystal symmetry: translating the inputs $L_t, F_t$ by $g$ translates the output by the same $g$.\n\n---\n\n**Step 2: Equivariance of the Mean in $\\mathcal{N}_w$**  \nThe transition distribution is parameterized as:\n$$\n\\mu_{t-1} = F_t + u_t \\hat{\\epsilon}_F(L_t, F_t, A, t).\n$$\nUnder $T_g$, the translated mean becomes:\n$$\n\\mu_{t-1}' = T_g(F_t) + u_t \\hat{\\epsilon}_F\\big(T_g(L_t), T_g(F_t), A, t\\big) = T_g\\left(F_t + u_t \\hat{\\epsilon}_F(L_t, F_t, A, t)\\right) = T_g(\\mu_{t-1}).\n$$\nThus, $\\mu_{t-1}$ transforms equivariantly.\n\n---\n\n**Step 3: Equivariance of $\\mathcal{N}_w$**  \nThe wrapped normal distribution $\\mathcal{N}_w$ satisfies:\n$$\n\\mathcal{N}_w\\left(F_{t-1} \\,\\big|\\, \\mu_{t-1},\\, v_t^2I\\right) = \\mathcal{N}_w\\left(T_g(F_{t-1}) \\,\\big|\\, T_g(\\mu_{t-1}),\\, v_t^2I\\right).\n$$\nWrapping ensures $F_{t-1} \\in \\Omega$ and preserves the distribution under $T_g$. A non-wrapped normal distribution would violate periodicity by \"leaking\" probability mass outside $\\Omega$.\n\n---\n\n**Step 4: Combined Equivariance**  \nThe translated transition distribution is:\n$$\np\\left(T_g(F_{t-1}) \\,\\big|\\, T_g(L_t), T_g(F_t), A\\right) = \\mathcal{N}_w\\left(T_g(F_{t-1}) \\,\\big|\\, T_g(\\mu_{t-1}),\\, v_t^2I\\right).\n$$\nBy Steps 2–3, this equals $T_g\\big(p(F_{t-1} \\mid L_t, F_t, A)\\big)$. Hence, the transition is periodic translation equivariant.\n\n---\n\n### **Why Both $\\hat{\\epsilon}_F$ and $\\mathcal{N}_w$ Must Respect Periodicity**\n\n1. **$\\hat{\\epsilon}_F$ Requirement**:  \n   If $\\hat{\\epsilon}_F$ were not equivariant, the mean $\\mu_{t-1}$ would not transform correctly under $T_g$, breaking symmetry. For example, a non-equivariant $\\hat{\\epsilon}_F$ might output $0$ for translated inputs, collapsing $\\mu_{t-1}$ to $T_g(F_t)$ instead of $T_g(\\mu_{t-1})$.\n\n2. **$\\mathcal{N}_w$ Requirement**:  \n   A standard normal distribution $\\mathcal{N}$ (non-wrapped) would allow $F_{t-1}$ to drift outside $\\Omega$ under repeated translations, violating crystal periodicity. The wrapping in $\\mathcal{N}_w$ ensures $F_{t-1}$ stays within $\\Omega$, preserving equivariance.\n\n---\n\n### **Conclusion**\nBoth the **equivariant denoising term** $\\hat{\\epsilon}_F$ and the **wrapped normal distribution** $\\mathcal{N}_w$ are necessary to guarantee that the transition $p(F_{t-1} \\mid L_t, F_t, A)$ respects the crystal's periodicity. Omission of either component would break symmetry.", "C": "To prove Lemma 3, we start by understanding the definitions and implications of periodic translation invariance and equivariance in the context of the given terms. The denoising term $\\hat{\\epsilon}_F(L_t, F_t, A, t)$ is periodic translation invariant, meaning its value does not change under periodic translations of its arguments within the crystal's lattice. This property ensures that the denoising process is consistent across equivalent positions in the crystal.\n\nThe transition probability is given by a wrapped normal distribution $\\mathcal{N}_w$, which is defined over a torus to respect the crystal's periodicity. This means that the distribution accounts for the periodic boundary conditions of the crystal, ensuring that transitions between states are physically meaningful within the crystal's framework.\n\nGiven these definitions, the proof proceeds as follows:\n1. Periodic Translation Invariance of $\\hat{\\epsilon}_F$: Since $\\hat{\\epsilon}_F$ is invariant under periodic translations, applying a periodic translation to $F_t$ and $L_t$ does not change the value of $\\hat{\\epsilon}_F(L_t, F_t, A, t)$. This implies that the mean of the distribution $F_t + u_t\\hat{\\epsilon}_F(L_t, F_t, A, t)$ is also invariant under the same translations.\n2. Wrapped Normal Distribution $\\mathcal{N}_w$: The use of $\\mathcal{N}_w$ ensures that the probability distribution respects the crystal's periodicity. This is crucial because it means that the transition probabilities are correctly computed modulo the crystal's lattice vectors, ensuring that the distribution 'wraps around' at the boundaries of the unit cell.\n3. Equivariance of the Transition: Combining the invariance of $\\hat{\\epsilon}_F$ and the periodicity-respecting property of $\\mathcal{N}_w$, the overall transition $p(F_{t-1}\\mid L_t, F_t, A)$ is equivariant under periodic translations. This means that translating the input $F_t$ by a lattice vector results in a corresponding translation of the output distribution $F_{t-1}$, but the variance $v_t^2$ is not scaled by the translation, breaking the equivariance.\n\nIn summary, both $\\hat{\\epsilon}_F$ and $\\mathcal{N}_w$ must respect the crystal's periodicity to ensure that the transition probabilities are physically meaningful and consistent with the crystal's symmetry. This leads to the periodic translation equivariance of the transition, as stated in Lemma 3.", "D": "To prove Lemma 3, we need to show that the transition distribution \\( p(F_{t-1} \\mid L_t, F_t, A) \\) is **periodic translation equivariant** given the assumptions. Here's the step-by-step proof:\n\n---\n\n### **1. Definitions and Assumptions**\n- **Periodic Translation Invariance of \\(\\hat{\\epsilon}_F\\)**:  \n  The denoising term \\(\\hat{\\epsilon}_F(L_t, F_t, A, t)\\) is periodic translation invariant. This means that for any periodic translation \\(\\tau\\) (i.e., a translation by a lattice vector of the crystal),  \n  \\[\n  \\hat{\\epsilon}_F(L_t, F_t + \\tau, A, t) = \\hat{\\epsilon}_F(L_t, F_t, A, t).\n  \\]  \n  Here, \\(F_t\\) represents atomic positions, and \\(\\tau\\) is a vector in the crystal's lattice.\n\n- **Wrapped Normal Distribution \\(\\mathcal{N}_w\\)**:  \n  The transition is defined as:  \n  \\[\n  p(F_{t-1} \\mid L_t, F_t, A) = \\mathcal{N}_w(F_{t-1} \\mid F_t + u_t \\hat{\\epsilon}_F(L_t, F_t, A, t), v_t^2 I),\n  \\]  \n  where \\(\\mathcal{N}_w\\) is a **wrapped normal distribution** over the periodic domain (the crystal's unit cell). This ensures that the probability density respects the crystal's periodicity.\n\n---\n\n### **2. Proving Periodic Translation Equivariance**\nWe need to show that translating the input \\(F_t\\) by \\(\\tau\\) results in a corresponding translation of the output distribution \\(p(F_{t-1} \\mid L_t, F_t, A)\\) by \\(\\tau\\).  \n\n- **Step 1: Translate \\(F_t\\) by \\(\\tau\\)**  \n  Let \\(F_t' = F_t + \\tau\\). Then, the denoising term becomes:  \n  \\[\n  \\hat{\\epsilon}_F(L_t, F_t', A, t) = \\hat{\\epsilon}_F(L_t, F_t + \\tau, A, t) = \\hat{\\epsilon}_F(L_t, F_t, A, t),\n  \\]  \n  due to the periodic translation invariance of \\(\\hat{\\epsilon}_F\\).\n\n- **Step 2: Compute the Translated Transition Distribution**  \n  The transition for \\(F_t'\\) is:  \n  \\[\n  p(F_{t-1} \\mid L_t, F_t', A) = \\mathcal{N}_w(F_{t-1} \\mid F_t' + u_t \\hat{\\epsilon}_F(L_t, F_t', A, t), v_t^2 I).\n  \\]  \n  Substituting \\(F_t' = F_t + \\tau\\) and the invariance of \\(\\hat{\\epsilon}_F\\), we get:  \n  \\[\n  p(F_{t-1} \\mid L_t, F_t + \\tau, A) = \\mathcal{N}_w(F_{t-1} \\mid F_t + \\tau + u_t \\hat{\\epsilon}_F(L_t, F_t, A, t), v_t^2 I).\n  \\]  \n\n- **Step 3: Relate to Original Distribution**  \n  The wrapped normal distribution \\(\\mathcal{N}_w\\) respects periodicity, meaning:  \n  \\[\n  \\mathcal{N}_w(F_{t-1} \\mid F_t + \\tau + u_t \\hat{\\epsilon}_F, v_t^2 I) = \\mathcal{N}_w(F_{t-1} - \\tau \\mid F_t + u_t \\hat{\\epsilon}_F, v_t^2 I).\n  \\]  \n  Therefore,  \n  \\[\n  p(F_{t-1} \\mid L_t, F_t + \\tau, A) = p(F_{t-1} - \\tau \\mid L_t, F_t, A).\n  \\]  \n\nThis proves that the transition distribution is **periodic translation equivariant**, as translating the input \\(F_t\\) by \\(\\tau\\) results in the same translation of the output distribution.\n\n---\n\n### **3. Why \\(\\hat{\\epsilon}_F\\) and \\(\\mathcal{N}_w\\) Must Respect Periodicity**\n- **\\(\\hat{\\epsilon}_F\\) must be periodic translation invariant**:  \n  If \\(\\hat{\\epsilon}_F\\) were not invariant under periodic translations, the denoising direction would change arbitrarily when \\(F_t\\) is shifted by a lattice vector, breaking equivariance.  \n\n- **\\(\\mathcal{N}_w\\) must respect periodicity**:  \n  The wrapped normal distribution ensures that the noise added to \\(F_t\\) does not \"leak\" outside the unit cell. If a standard (non-wrapped) normal distribution were used, the transition could produce atomic positions outside the periodic boundary, violating crystal symmetry.  \n\nThus, **both conditions are necessary** to guarantee that the entire transition process respects the crystal's periodicity.\n\n---\n\n### **Conclusion**\nBy requiring \\(\\hat{\\epsilon}_F\\) to be periodic translation invariant and using a wrapped normal distribution \\(\\mathcal{N}_w\\), we ensure that the transition \\(p(F_{t-1} \\mid L_t, F_t, A)\\) is periodic translation equivariant. This property is crucial for maintaining physical consistency in crystal structure generation.", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "Given the definition of the wrapped normal distribution, how do we prove that $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$? Please illustrate how the index shift in the sum over $k$ preserves the same distribution.", "A": "To prove the equality $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$, we start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution is defined as a sum of normal distributions shifted by integer multiples of $2\\pi$, which can be written as: $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Now, consider the distribution of $x + t$ given $\\mu + t$ and $\\sigma^2$: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2).$$ By simplifying the argument of the normal distribution, we get: $$\\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2) = \\mathcal{N}(x\\mid\\mu + 2\\pi (k + 1), \\sigma^2).$$ Substituting this back into the sum, we obtain: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi (k + 1), \\sigma^2) = \\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$ This shows that shifting both the variable $x$ and the mean $\\mu$ by the same amount $t$ leaves the wrapped normal distribution unchanged. The key step is the index shift in the sum over $k$, which does not affect the infinite sum because it runs over all integers, ensuring that the distribution remains invariant under such shifts.", "B": "To prove the equality $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$, we start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution is defined as a sum of normal distributions shifted by integer multiples of $2\\pi$, which can be written as: $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Now, consider the distribution of $x + t$ given $\\mu + t$ and $\\sigma^2$: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2).$$ By simplifying the argument of the normal distribution, we get: $$\\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2) = \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Substituting this back into the sum, we obtain: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2) = \\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$ This shows that shifting both the variable $x$ and the mean $\\mu$ by the same amount $t$ leaves the wrapped normal distribution unchanged. The key step is the index shift in the sum over $k$, which does not affect the infinite sum because it runs over all integers, ensuring that the distribution remains invariant under such shifts.", "C": "To prove that the wrapped normal distribution satisfies $$\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2)$$ for any shift \\( t \\), we analyze its definition and the summation over periodic replicas.\n\n**Definition of Wrapped Normal Distribution:**\nThe wrapped normal density is constructed by summing over all \"wrapped\" (periodic) replicas of the normal distribution:\n$$\n\\mathcal{N}_w(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum_{k=-\\infty}^{\\infty} \\exp\\left(-\\frac{(x - \\mu + 2\\pi k)^2}{2\\sigma^2}\\right).\n$$\n\n**Left-Hand Side (LHS) Expansion:**\nSubstitute \\( x \\to x + t \\) and \\( \\mu \\to \\mu + t \\):\n$$\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum_{k=-\\infty}^{\\infty} \\exp\\left(-\\frac{(x + t - (\\mu + t) + 2\\pi k)^2}{2\\sigma^2}\\right).\n$$\nSimplify the numerator:\n$$\n(x + t) - (\\mu + t) = x - \\mu \\implies \\text{LHS} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum_{k=-\\infty}^{\\infty} \\exp\\left(-\\frac{(x - \\mu + 2\\pi k)^2}{2\\sigma^2}\\right).\n$$\n\n**Equivalence to Right-Hand Side (RHS):**\nThe simplified LHS matches the definition of \\( \\mathcal{N}_w(x \\mid \\mu, \\sigma^2) \\). Thus:\n$$\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n$$\n\n**Role of the Summation Index \\( k \\):**\nThe shift \\( t \\) cancels out in the exponent, leaving the summation over \\( k \\) unchanged. Since the sum includes **all integers** \\( k \\in \\mathbb{Z} \\), it inherently accounts for all periodic replicas of \\( x \\) and \\( \\mu \\). A shift \\( t \\) (even non-integer multiples of \\( 2\\pi \\)) does not affect the infinite summation’s coverage over the circle. Hence, the distribution remains invariant under translation.\n\n**Conclusion:**\nThe wrapped normal distribution is translationally invariant because the shift \\( t \\) cancels in the exponent, and the infinite sum over \\( k \\) ensures all periodic replicas are preserved.", "D": "To prove that the wrapped normal distribution satisfies the shift invariance property:\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2),\n\\]\nwe start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution \\(\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)\\) is defined as:\n\\[\n\\mathcal{N}_w(x \\mid \\mu, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2),\n\\]\nwhere \\(\\mathcal{N}(x \\mid \\mu, \\sigma^2)\\) is the standard normal distribution:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\n\\]\n\n### Proof of Shift Invariance:\nConsider the wrapped normal distribution evaluated at \\(x + t\\) with mean \\(\\mu + t\\):\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t + 2\\pi k \\mid \\mu + t, \\sigma^2).\n\\]\n\nSubstitute \\(y = x + t\\) and \\(\\nu = \\mu + t\\) into the standard normal distribution:\n\\[\n\\mathcal{N}(y + 2\\pi k \\mid \\nu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y + 2\\pi k - \\nu)^2}{2\\sigma^2}\\right).\n\\]\nSubstituting back \\(y = x + t\\) and \\(\\nu = \\mu + t\\):\n\\[\n\\mathcal{N}(x + t + 2\\pi k \\mid \\mu + t, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x + t + 2\\pi k - \\mu - t)^2}{2\\sigma^2}\\right) = \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2).\n\\]\n\nThus, the sum becomes:\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n\\]\n\n### Index Shift in the Sum:\nThe key observation is that the sum over \\(k\\) is invariant under any integer shift in the index. For example, let \\(k' = k + m\\) for some integer \\(m\\). Then:\n\\[\n\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2) = \\sum_{k'=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi (k' - m) \\mid \\mu, \\sigma^2) = \\sum_{k'=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k' \\mid \\mu, \\sigma^2),\n\\]\nsince the sum over all integers \\(k'\\) is identical to the sum over all integers \\(k\\). This demonstrates that the wrapped normal distribution is periodic and invariant under shifts \\(t\\). \n\n### Conclusion:\nThe wrapped normal distribution \\(\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)\\) is shift-invariant because the infinite sum over \\(k\\) ensures that any shift \\(t\\) in the argument \\(x\\) or the mean \\(\\mu\\) cancels out, leaving the distribution unchanged. Thus:\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n\\]", "ground-truth": "B"}
{"paper": "arxiv:2309.04475", "question": "Explain the sequence of equalities in the expression $$p\\bigl(w(F_{t-1}+t)\\bigr) = \\mathcal{N}_w(\\cdots) = p(F_{t-1}\\mid L_t,F_t,A)\\,?$$ Specifically, how does the periodic translation invariance of the wrapped normal distribution allow us to replace $w(F_t + t\\mathbf{1})$ with $F_t$ while maintaining the same distribution $p(F_{t-1}\\mid L_t,F_t,A$)?", "A": "The sequence of equalities can be explained through the periodic translation invariance of the wrapped normal distribution. Let’s break down the key steps:\n\n---\n\n### 1. **Wrapped Normal Distribution Basics**  \nThe wrapped normal distribution $\\mathcal{N}_w(\\mu, \\sigma^2)$ is defined over a periodic domain (e.g., $[0, 2\\pi)$). For any $x \\in \\mathbb{R}$, the wrapping operator $w(\\cdot)$ satisfies:  \n$$\nw(x + 2\\pi k) = w(x) \\quad \\text{for all integers } k.\n$$  \nThis implies that adding any multiple of the period (e.g., $2\\pi k$) to $x$ does not alter its wrapped value. Consequently, the probability density is preserved under periodic shifts:  \n$$\np\\bigl(w(x)\\bigr) = p\\bigl(w(x + 2\\pi k)\\bigr).\n$$\n\n---\n\n### 2. **Translational Invariance in the Model**  \nSuppose the state transition involves a periodic variable $F_t$ with dynamics:  \n$$\nF_t = w(F_{t-1} + t + \\epsilon_t), \\quad \\epsilon_t \\sim \\mathcal{N}_w(0, \\sigma^2),\n$$  \nwhere $w(\\cdot)$ wraps the result into $[0, 2\\pi)$. The term $t\\mathbf{1}$ (where $\\mathbf{1}$ is the period $2\\pi$) represents a deterministic periodic shift. Due to the wrapping operator:  \n$$\nw(F_t + t\\mathbf{1}) = w(F_t + 2\\pi t) = w(F_t),\n$$  \nsince $t\\mathbf{1} = 2\\pi t$ is an integer multiple of the period. Thus, $w(F_t + t\\mathbf{1})$ and $F_t$ are **distributionally equivalent** under $\\mathcal{N}_w$.\n\n---\n\n### 3. **Equality of Conditional Distributions**  \nThe equality $p\\bigl(w(F_{t-1} + t)\\bigr) = p(F_{t-1} \\mid L_t, F_t, A)$ arises from the Markov property and periodic invariance:  \n- The measurement likelihood $p(L_t \\mid F_t, A)$ depends only on the wrapped state $F_t$.  \n- The transition model $p(F_t \\mid F_{t-1})$ depends on $w(F_{t-1} + t + \\epsilon_t)$, which is invariant to shifts by $2\\pi k$.  \n- By Bayes’ rule:  \n  $$\n  p(F_{t-1} \\mid L_t, F_t, A) \\propto p(L_t \\mid F_t, A) \\cdot p(F_t \\mid F_{t-1}) \\cdot p(F_{t-1}).\n  $$  \n  Since $w(F_t + t\\mathbf{1}) = F_t$ in distribution, the term $p(F_t \\mid F_{t-1})$ remains unchanged when substituting $w(F_t + t\\mathbf{1})$ with $F_t$.\n\n---\n\n### 4. **Key Insight**  \nThe wrapped normal distribution’s translational invariance ensures that:  \n$$\n\\mathcal{N}_w(\\mu + 2\\pi k, \\sigma^2) \\equiv \\mathcal{N}_w(\\mu, \\sigma^2),\n$$  \nfor any integer $k$. Thus, adding $t\\mathbf{1}$ (a periodic shift) to $F_t$ does not alter the density $p(F_{t-1} \\mid L_t, F_t, A)$. This justifies replacing $w(F_t + t\\mathbf{1})$ with $F_t$ while preserving the conditional distribution.\n\n---\n\n### Summary  \nThe equality sequence holds because:  \n1. The wrapping operator $w(\\cdot)$ nullifies periodic shifts.  \n2. The wrapped normal distribution’s density is invariant to such shifts.  \n3. The substitution preserves the probabilistic structure of the transition model in $p(F_{t-1} \\mid L_t, F_t, A)$.  \n\nThis invariance is critical for simplifying temporal dependencies in periodic state-space models.", "B": "To explain the sequence of equalities in the expression:\n\n\\[\np\\bigl(w(F_{t-1}+t)\\bigr) = \\mathcal{N}_w(\\cdots) = p(F_{t-1}\\mid L_t,F_t,A),\n\\]\n\nwe need to carefully analyze the role of the wrapped normal distribution \\(\\mathcal{N}_w\\) and its periodic translation invariance property.\n\n### Step 1: Understanding the Wrapped Normal Distribution\nThe **wrapped normal distribution** \\(\\mathcal{N}_w\\) is a circular distribution obtained by \"wrapping\" a normal distribution around the unit circle. If \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then the wrapped version is \\(w(X) = X \\mod 2\\pi\\), where \\(w(\\cdot)\\) denotes the wrapping operation. \n\nA key property of \\(\\mathcal{N}_w\\) is its **periodic translation invariance**:\n\\[\nw(X + c) \\sim \\mathcal{N}_w(\\mu + c, \\sigma^2),\n\\]\nfor any constant \\(c\\). This means that adding a constant \\(c\\) before or after wrapping results in the same distribution, up to a shift in the mean.\n\n### Step 2: Analyzing the Given Expression\nThe expression involves:\n\\[\np\\bigl(w(F_{t-1}+t)\\bigr) = \\mathcal{N}_w(\\cdots) = p(F_{t-1}\\mid L_t,F_t,A).\n\\]\n\n1. **First Equality**: \\(p\\bigl(w(F_{t-1}+t)\\bigr) = \\mathcal{N}_w(\\cdots)\\)\n   - Here, \\(w(F_{t-1}+t)\\) represents the wrapped version of \\(F_{t-1}+t\\). The wrapped normal distribution \\(\\mathcal{N}_w(\\cdots)\\) is used to model this, where \\(\\cdots\\) encodes the mean and variance parameters conditioned on \\(L_t, F_t, A\\).\n\n2. **Second Equality**: \\(\\mathcal{N}_w(\\cdots) = p(F_{t-1}\\mid L_t,F_t,A)\\)\n   - This equality holds because the wrapped normal distribution \\(\\mathcal{N}_w(\\cdots)\\) is chosen to represent the conditional distribution \\(p(F_{t-1}\\mid L_t,F_t,A)\\).\n\n### Step 3: Role of Periodic Translation Invariance\nThe question asks how the periodic translation invariance allows replacing \\(w(F_t + t\\mathbf{1})\\) with \\(F_t\\) while preserving the distribution \\(p(F_{t-1}\\mid L_t,F_t,A)\\).\n\n1. **Translation Invariance in Wrapping**:\n   - Suppose \\(F_t\\) is modeled as \\(w(F_t + t\\mathbf{1})\\). Due to the periodic translation invariance of \\(\\mathcal{N}_w\\), adding \\(t\\mathbf{1}\\) before wrapping is equivalent to shifting the mean by \\(t\\mathbf{1}\\) after wrapping:\n     \\[\n     w(F_t + t\\mathbf{1}) \\sim \\mathcal{N}_w(\\mu + t\\mathbf{1}, \\Sigma).\n     \\]\n   - However, because the wrapping operation \\(w(\\cdot)\\) is periodic with period \\(2\\pi\\), the distribution of \\(w(F_t + t\\mathbf{1})\\) is the same as the distribution of \\(w(F_t)\\) up to a circular shift. Thus, for the purpose of computing the conditional distribution \\(p(F_{t-1}\\mid L_t,F_t,A)\\), we can equivalently use \\(F_t\\) instead of \\(w(F_t + t\\mathbf{1})\\), because the probabilistic structure (i.e., the shape of the distribution) remains unchanged under wrapping.\n\n2. **Conditional Distribution Preservation**:\n   - The conditional distribution \\(p(F_{t-1}\\mid L_t,F_t,A)\\) depends on the probabilistic relationship between \\(F_{t-1}\\) and \\(F_t\\), not on the specific representation (wrapped or unwrapped) as long as the wrapping is handled consistently. The periodic translation invariance ensures that:\n     \\[\n     p\\bigl(F_{t-1}\\mid w(F_t + t\\mathbf{1}), L_t, A\\bigr) = p(F_{t-1}\\mid F_t, L_t, A),\n     \\]\n     because \\(w(F_t + t\\mathbf{1})\\) and \\(F_t\\) describe the same angular distribution up to a shift.\n\n### Conclusion\nThe sequence of equalities holds because:\n1. The wrapped normal distribution \\(\\mathcal{N}_w\\) is used to model the wrapped variable \\(w(F_{t-1}+t)\\).\n2. The periodic translation invariance of \\(\\mathcal{N}_w\\) ensures that adding a constant \\(t\\mathbf{1}\\) before wrapping does not alter the distribution's essential properties, allowing the replacement of \\(w(F_t + t\\mathbf{1})\\) with \\(F_t\\) in the conditional distribution \\(p(F_{t-1}\\mid L_t,F_t,A)\\).\n\nThus, the equalities are justified by the properties of the wrapped normal distribution and its periodic behavior.", "C": "The sequence of equalities can be understood through the properties of the wrapped normal distribution and its invariance under periodic translations. The key steps are as follows:\n\n1. Initial Expression: The expression starts with $p\\bigl(w(F_{t-1}+t)\\bigr)$, which represents the probability distribution of the wrapped version of $F_{t-1}+t$.\n\n2. Wrapped Normal Distribution: This is then equated to $\\mathcal{N}_w\\bigl(w(F_{t-1}+t)\\mid w(F_t + t) + u_t\\hat{\\epsilon}_F(\\cdots), v_t^2I\\bigr)$, where $\\mathcal{N}_w$ denotes the wrapped normal distribution. The mean of this distribution is given by $w(F_t + t) + u_t\\hat{\\epsilon}_F(\\cdots)$, and the variance is $v_t^2I$.\n\n3. Invariance Under Shifts: The wrapped normal distribution is invariant under shifts by $t$. This means that adding $t$ to both $F_{t-1}$ and $F_t$ does not change the distribution. Therefore, $w(F_t + t)$ can be replaced by $F_{t-1}$ without altering the distribution.\n\n4. Final Equality: The final equality $p(F_{t-1}\\mid L_t,F_t,A)$ is achieved by recognizing that the transition probabilities remain unchanged under these operations, demonstrating the periodic translation invariance of the wrapped normal distribution.\n\nThis reasoning shows how the properties of the wrapped normal distribution allow for the substitution and ensure that the distribution $p(F_{t-1}\\mid L_t,F_t,A)$ remains consistent.", "D": "The sequence of equalities can be understood through the properties of the wrapped normal distribution and its invariance under periodic translations. The key steps are as follows:\n\n1. Initial Expression: The expression starts with $p\\bigl(w(F_{t-1}+t)\\bigr)$, which represents the probability distribution of the wrapped version of $F_{t-1}+t$.\n\n2. Wrapped Normal Distribution: This is then equated to $\\mathcal{N}_w\\bigl(w(F_{t-1}+t)\\mid w(F_t + t) + u_t\\hat{\\epsilon}_F(\\cdots), v_t^2I\\bigr)$, where $\\mathcal{N}_w$ denotes the wrapped normal distribution. The mean of this distribution is given by $w(F_t + t) + u_t\\hat{\\epsilon}_F(\\cdots)$, and the variance is $v_t^2I$.\n\n3. Invariance Under Shifts: The wrapped normal distribution is invariant under shifts by $t$. This means that adding $t$ to both $F_{t-1}$ and $F_t$ does not change the distribution. Therefore, $w(F_t + t)$ can be replaced by $F_t$ without altering the distribution.\n\n4. Final Equality: The final equality $p(F_{t-1}\\mid L_t,F_t,A)$ is achieved by recognizing that the transition probabilities remain unchanged under these operations, demonstrating the periodic translation invariance of the wrapped normal distribution.\n\nThis reasoning shows how the properties of the wrapped normal distribution allow for the substitution and ensure that the distribution $p(F_{t-1}\\mid L_t,F_t,A)$ remains consistent.", "ground-truth": "D"}
{"paper": "arxiv:2309.04475", "question": "How can we prove that the score $\\hat{\\epsilon}_L$ by Eq. (9) is O(3)-equivariant, and the score $\\hat{\\epsilon}_F$ from Eq. (10) is periodic translation invariant, thereby making the generated distribution periodic E(3) invariant? In particular, use:\n\n1. Eq. (9) for the lattice-denoising score:\n$$\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t),\\ \\forall\\ Q \\in \\mathbb{R}^{3\\times 3},\\ Q^T Q = I.$$\n\n2. Eq. (10) for the coordinate-denoising score $\\hat{\\epsilon}_F$, which is shown to be periodic translation invariant.\n\n3. The orthonormal-transformation identity:\n$$(QL)^T(QL) = L^T Q^T Q L = L^T I L = L^T L, \\quad \\text{for any orthogonal } Q \\text{ with } Q^T Q = I.$$\n\n4. The periodic-Fourier relation (used to show translation invariance) of the form:\n$$\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k].$$", "A": "To prove that the score $\\hat{\\epsilon}_L$ is O(3)-equivariant and $\\hat{\\epsilon}_F$ is periodic translation invariant, thereby ensuring the generated distribution is periodic E(3) invariant, we proceed as follows:\n\n---\n\n### **1. O(3)-Equivariance of $\\hat{\\epsilon}_L$ (Eq. 9)**  \nGiven the identity $(QL)^T(QL) = L^T L$ for any orthogonal $Q \\in \\mathbb{R}^{3 \\times 3}$ with $Q^T Q = I$, the lattice score $\\hat{\\epsilon}_L$ must depend on $L$ through invariants preserved under orthogonal transformations. Suppose $\\hat{\\epsilon}_L$ is derived from a function of $L^T L$ (e.g., via matrix decomposition or gradient descent). Since $L^T L$ remains unchanged under $Q$, the functional form of $\\hat{\\epsilon}_L$ inherits equivariance:  \n$$\n\\hat{\\epsilon}_L(QL, F, A, t) = Q \\cdot \\hat{\\epsilon}_L(L, F, A, t).\n$$  \nThis holds because $Q$ acts linearly on $L$, and $\\hat{\\epsilon}_L$ transforms covariantly with $L$ under $Q$. For instance, if $\\hat{\\epsilon}_L$ computes a gradient of a rotation-invariant energy function, the gradient direction rotates with $Q$.\n\n---\n\n### **2. Periodic Translation Invariance of $\\hat{\\epsilon}_F$ (Eq. 10)**  \nThe score $\\hat{\\epsilon}_F$ depends on periodicized coordinates via the Fourier relation:  \n$$\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k],\n$$  \nwhere $w$ is a periodic wrapping function (e.g., $w(x) = x - \\lfloor x/T \\rfloor T$). Translating all coordinates by $t$ modifies $f_j \\to f_j + t$, but the wrapped differences $w(f_j + t) - w(f_i + t)$ reduce to $f_j - f_i$ modulo periodic boundaries. The Fourier transform $\\mathcal{F}_T$ then eliminates phase shifts caused by $t$, leaving only translation-invariant spectral magnitudes. Thus, $\\hat{\\epsilon}_F$ depends solely on relative positions modulo $T$, making it invariant to periodic translations:  \n$$\n\\hat{\\epsilon}_F(L, F + t\\mathbf{1}^T, A, t) = \\hat{\\epsilon}_F(L, F, A, t).\n$$\n\n---\n\n### **3. Periodic E(3) Invariance of the Generated Distribution**  \nThe full denoising process combines $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$. For any $Q \\in \\text{O}(3)$ and periodic translation $t \\in \\mathbb{R}^3$, the scores satisfy:  \n- **Lattice**: $\\hat{\\epsilon}_L(Q L, Q F + t, A, t) = Q \\hat{\\epsilon}_L(L, F, A, t)$ (equivariance).  \n- **Coordinates**: $\\hat{\\epsilon}_F(Q L, Q F + t, A, t) = \\hat{\\epsilon}_F(L, F, A, t)$ (invariance to $Q$ due to O(3)-invariant input processing, and invariance to $t$ via periodicity).  \n\nUnder the diffusion process, the learned score $\\nabla \\log p(F, L)$ transforms as:  \n$$\n\\nabla \\log p(Q L + t, Q F + t) = Q \\nabla \\log p(L, F),\n$$  \nwhich matches the transformation of the noise terms. By the uniqueness of the score-based diffusion, the generated distribution inherits periodic E(3) invariance:  \n$$\np(Q L + t, Q F + t) = p(L, F) \\quad \\forall Q \\in \\text{O}(3), t \\in \\mathbb{R}^3.\n$$\n\n---\n\n### **Conclusion**  \nThe O(3)-equivariance of $\\hat{\\epsilon}_L$ and periodic translation invariance of $\\hat{\\epsilon}_F$ jointly ensure that the denoising process respects all transformations in the periodic E(3) group, resulting in a generated distribution invariant to periodic Euclidean symmetries.", "B": "To prove Proposition 3, we analyze the properties of the scores $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$ step by step.\n\n1. O(3)-equivariance of $\\hat{\\epsilon}_L$:\n   - Given the equation $\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t)$ for any orthogonal matrix $Q$, this directly shows that $\\hat{\\epsilon}_L$ is O(3)-equivariant. This means that applying an orthogonal transformation $Q$ to the lattice $L$ before computing the score is equivalent to applying $Q$ to the score computed from the original lattice $L$.\n   - The orthonormal-transformation identity $(QL)^T(QL) = L^T L$ further supports this by showing that the inner product structure of the lattice is preserved under orthogonal transformations, which is a key property for O(3)-equivariance.\n\n2. Periodic translation invariance of $\\hat{\\epsilon}_F$:\n   - The periodic-Fourier relation $\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]$ demonstrates that the Fourier transform of the differences between coordinates is invariant under periodic translations $t$. This implies that $\\hat{\\epsilon}_F$ is periodic translation invariant, as the score depends only on the relative positions of the coordinates, not on their absolute positions modulo the periodic boundary.\n\n3. Conclusion:\n   - Combining the O(3)-equivariance of $\\hat{\\epsilon}_L$ and the periodic translation invariance of $\\hat{\\epsilon}_F$, the generated distribution is invariant under the combined actions of orthogonal transformations and periodic translations, i.e., it is periodic E(3) invariant. However, this only holds when the lattice $L$ is invertible, which was not explicitly assumed in the proposition. This completes the proof of Proposition 3.", "C": "To prove that the generated distribution is periodic E(3) invariant, we need to show that the lattice-denoising score \\(\\hat{\\epsilon}_L\\) is O(3)-equivariant and the coordinate-denoising score \\(\\hat{\\epsilon}_F\\) is periodic translation invariant. Below is the step-by-step proof:\n\n---\n\n### 1. **O(3)-Equivariance of \\(\\hat{\\epsilon}_L\\) (Eq. 9)**\nGiven:\n\\[\n\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t), \\quad \\forall Q \\in \\mathbb{R}^{3 \\times 3}, Q^T Q = I.\n\\]\nThis is the definition of O(3)-equivariance for the lattice-denoising score. To verify this, we rely on the orthonormal-transformation identity:\n\\[\n(QL)^T(QL) = L^T Q^T Q L = L^T I L = L^T L.\n\\]\nThis shows that the Gram matrix \\(L^T L\\) (which encodes the lattice geometry) is invariant under orthogonal transformations. Since \\(\\hat{\\epsilon}_L\\) depends only on \\(L^T L\\) (and other O(3)-invariant quantities like \\(F\\) and \\(A\\)), the score must transform as:\n\\[\n\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t).\n\\]\nThus, \\(\\hat{\\epsilon}_L\\) is O(3)-equivariant.\n\n---\n\n### 2. **Periodic Translation Invariance of \\(\\hat{\\epsilon}_F\\) (Eq. 10)**\nThe coordinate-denoising score \\(\\hat{\\epsilon}_F\\) is constructed to be invariant under periodic translations. This is ensured by the Fourier-based formulation, where the score depends on relative positions in a periodic manner. \n\nGiven the periodic-Fourier relation:\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k],\n\\]\nwhere \\(w(\\cdot)\\) is a periodic wrapping function and \\(\\mathcal{F}_T\\) denotes the Fourier transform over the periodic domain. This shows that the relative positions \\(f_j - f_i\\) are preserved under translation \\(t\\), making the score invariant to periodic shifts:\n\\[\n\\hat{\\epsilon}_F(L, F + t, A, t) = \\hat{\\epsilon}_F(L, F, A, t).\n\\]\n\n---\n\n### 3. **Combined Periodic E(3) Invariance**\nThe full denoising process combines:\n- **O(3)-equivariance** in the lattice score \\(\\hat{\\epsilon}_L\\) (handling rotations/reflections).\n- **Periodic translation invariance** in the coordinate score \\(\\hat{\\epsilon}_F\\) (handling shifts modulo the lattice periodicity).\n\nSince E(3) is generated by orthogonal transformations (O(3)) and translations, and the periodic setting restricts translations to those modulo the lattice, the generated distribution is invariant under the **periodic E(3) group**.\n\n---\n\n### Summary:\n1. \\(\\hat{\\epsilon}_L\\) is O(3)-equivariant due to the invariance of \\(L^T L\\) under orthogonal transformations.\n2. \\(\\hat{\\epsilon}_F\\) is periodic translation invariant due to the Fourier-based preservation of relative positions.\n3. Together, they ensure the generated distribution is periodic E(3) invariant. \n\nThus, the proof is complete. \\(\\square\\)", "D": "To prove Proposition 3, we analyze the properties of the scores $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$ step by step.\n\n1. O(3)-equivariance of $\\hat{\\epsilon}_L$:\n   - Given the equation $\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t)$ for any orthogonal matrix $Q$, this directly shows that $\\hat{\\epsilon}_L$ is O(3)-equivariant. This means that applying an orthogonal transformation $Q$ to the lattice $L$ before computing the score is equivalent to applying $Q$ to the score computed from the original lattice $L$.\n   - The orthonormal-transformation identity $(QL)^T(QL) = L^T L$ further supports this by showing that the inner product structure of the lattice is preserved under orthogonal transformations, which is a key property for O(3)-equivariance.\n\n2. Periodic translation invariance of $\\hat{\\epsilon}_F$:\n   - The periodic-Fourier relation $\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]$ demonstrates that the Fourier transform of the differences between coordinates is invariant under periodic translations $t$. This implies that $\\hat{\\epsilon}_F$ is periodic translation invariant, as the score depends only on the relative positions of the coordinates, not on their absolute positions modulo the periodic boundary.\n\n3. Conclusion:\n   - Combining the O(3)-equivariance of $\\hat{\\epsilon}_L$ and the periodic translation invariance of $\\hat{\\epsilon}_F$, the generated distribution is invariant under the combined actions of orthogonal transformations and periodic translations, i.e., it is periodic E(3) invariant. This completes the proof of Proposition 3.", "ground-truth": "D"}
{"paper": "arxiv:2309.04475", "question": "Given an orthogonal transformation $Q$ such that $Q^T Q = I$, derive the identity: $$(QL)^T (QL) = L^T L$$ by showing step by step how $Q^T Q = I$ implies $(QL)^T(QL)$ simplifies to $L^T L$.", "A": "Certainly! Let's derive the identity step by step.\n\n**Given:**\n- \\( Q \\) is an orthogonal matrix, so \\( Q^T Q = I \\), where \\( I \\) is the identity matrix.\n- \\( L \\) is any matrix of compatible dimensions for multiplication with \\( Q \\).\n\n**To Show:**\n\\[\n(QL)^T (QL) = L^T L\n\\]\n\n**Proof:**\n\n1. Start with the left-hand side (LHS) of the identity:\n   \\[\n   (QL)^T (QL)\n   \\]\n\n2. Apply the property of matrix transposition that \\( (AB)^T = B^T A^T \\):\n   \\[\n   (QL)^T (QL) = L^T Q^T Q L\n   \\]\n\n3. Substitute the given condition \\( Q^T Q = I \\):\n   \\[\n   L^T Q^T Q L = L^T I L\n   \\]\n\n4. Simplify using the fact that multiplying by the identity matrix \\( I \\) leaves the matrix unchanged:\n   \\[\n   L^T I L = L^T L\n   \\]\n\n5. Thus, we have shown:\n   \\[\n   (QL)^T (QL) = L^T L\n   \\]\n\n**Conclusion:**\nThe identity \\( (QL)^T (QL) = L^T L \\) holds true due to the orthogonality of \\( Q \\) (\\( Q^T Q = I \\)). \n\n\\[\n\\boxed{(QL)^T (QL) = L^T L}\n\\]", "B": "To derive the identity $(QL)^T (QL) = L^T L$ using the property of an orthogonal matrix $Q$ where $Q^T Q = I$, follow these steps:\n\n1. Substitute $QL$ into the expression: Start by considering the expression $(QL)^T (QL)$. This represents the inner product of $QL$ with itself.\n\n2. Apply the transpose of a product: Recall that the transpose of a product of two matrices is the product of their transposes in reverse order. Therefore, $(QL)^T = L^T Q^T$.\n\n3. Substitute back into the expression: Replace $(QL)^T$ with $L^T Q^T$ in the original expression to get $L^T Q^T Q L$.\n\n4. Use the orthogonality condition: Since $Q$ is orthogonal, it satisfies $Q^T Q = I$. Substitute $I$ for $Q^T Q$ in the expression to obtain $L^T I L$.\n\n5. Simplify using the identity matrix: The identity matrix $I$ acts as a multiplicative identity in matrix multiplication, so $L^T I L = L^T L$.\n\nThus, we have shown that $(QL)^T (QL) = L^T L$, demonstrating that the inner product is preserved under the orthogonal transformation $Q$.", "C": "To derive the identity $(QL)^T (QL) = L^T L$ using the property of an orthogonal matrix $Q$ where $Q^T Q = I$, follow these steps:\n\n1. Substitute $QL$ into the expression: Start by considering the expression $(QL)^T (QL)$. This represents the inner product of $QL$ with itself.\n\n2. Apply the transpose of a product: Recall that the transpose of a product of two matrices is the product of their transposes in reverse order. Therefore, $(QL)^T = Q^T L^T$.\n\n3. Substitute back into the expression: Replace $(QL)^T$ with $Q^T L^T$ in the original expression to get $Q^T L^T Q L$.\n\n4. Use the orthogonality condition: Since $Q$ is orthogonal, it satisfies $Q^T Q = I$. Substitute $I$ for $Q^T Q$ in the expression to obtain $I L^T L$.\n\n5. Simplify using the identity matrix: The identity matrix $I$ acts as a multiplicative identity in matrix multiplication, so $I L^T L = L^T L$.\n\nThus, we have shown that $(QL)^T (QL) = L^T L$, demonstrating that the inner product is preserved under the orthogonal transformation $Q$.", "D": "To derive the identity \\((QL)^T (QL) = L^T L\\) given that \\(Q^T Q = I\\), follow these steps:\n\n1. **Expand the left-hand side (LHS):**  \n   \\[\n   (QL)^T (QL)\n   \\]\n\n2. **Apply the transpose property \\((AB)^T = B^T A^T\\):**  \n   \\[\n   (QL)^T = L^T Q^T\n   \\]\n   Substituting this back:\n   \\[\n   (QL)^T (QL) = L^T Q^T (QL)\n   \\]\n\n3. **Use associativity of matrix multiplication:**  \n   \\[\n   L^T Q^T Q L = L^T (Q^T Q) L\n   \\]\n\n4. **Substitute \\(Q^T Q = I\\) (orthogonality condition):**  \n   \\[\n   L^T (I) L = L^T I L\n   \\]\n\n5. **Simplify \\(L^T I L\\) using \\(I L = L\\):**  \n   \\[\n   L^T L\n   \\]\n\nThus, \\((QL)^T (QL) = L^T L\\) is proven.  \n**Conclusion:** Orthogonal transformations preserve the inner product structure \\(L^T L\\) when applied as \\(QL\\).", "ground-truth": "B"}
{"paper": "arxiv:2309.04475", "question": "How do we demonstrate the periodic-Fourier invariance expressed by the equation \\(\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]\\)? Using the substitution \\(\\sin(2\\pi m(w(f_{j,c} + t_c) - w(f_{i,c} + t_c))) = \\sin(2\\pi m(f_{j,c} - f_{i,c}))\\), explain how the wrapping function \\(w(\\cdot)\\) and the shift \\(t\\) cancel out in the sine argument, ensuring the Fourier transform remains the same for \\(f_j - f_i\\) and \\(w(f_j + t) - w(f_i + t)\\).", "A": "To demonstrate the periodic-Fourier invariance expressed by the equation:\n\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k],\n\\]\n\nwe analyze the substitution given:\n\n\\[\n\\sin\\left(2\\pi m\\left(w(f_{j,c} + t_c) - w(f_{i,c} + t_c)\\right)\\right) = \\sin\\left(2\\pi m\\left(f_{j,c} - f_{i,c}\\right)\\right).\n\\]\n\n### Key Observations:\n1. **Wrapping Function Property**:  \n   The wrapping function \\(w(\\cdot)\\) maps inputs to a periodic interval (e.g., \\([0, 1)\\)). For any \\(x\\), \\(w(x + n) \\equiv w(x) \\mod 1\\) where \\(n \\in \\mathbb{Z}\\). This implies:\n   \\[\n   w(f_{j,c} + t_c) - w(f_{i,c} + t_c) \\equiv (f_{j,c} - f_{i,c}) \\mod 1.\n   \\]\n   \n2. **Sine Function Periodicity**:  \n   The sine function has period \\(1/m\\) for \\(\\sin(2\\pi m \\cdot)\\). Hence, if two arguments differ by an integer multiple of \\(1/m\\), their sine values are equal:\n   \\[\n   \\sin(2\\pi m(a)) = \\sin(2\\pi m(a + k/m)) \\quad \\text{for } k \\in \\mathbb{Z}.\n   \\]\n   Here, \\(w(f_{j,c} + t_c) - w(f_{i,c} + t_c)\\) and \\(f_{j,c} - f_{i,c}\\) differ by an integer, ensuring:\n   \\[\n   \\sin(2\\pi m\\Delta_{\\text{wrapped}}) = \\sin(2\\pi m\\Delta_{\\text{original}}).\n   \\]\n\n### Proof Outline:\n1. **Wrapping Difference**:  \n   Let \\(f_{j,c} - f_{i,c} = d_{ji} + n_{ji}\\), where \\(d_{ji} \\in [0, 1)\\) and \\(n_{ji} \\in \\mathbb{Z}\\). The wrapping operation cancels \\(t_c\\) and integer offsets:\n   \\[\n   w(f_{j,c} + t_c) - w(f_{i,c} + t_c) = d_{ji} \\equiv (f_{j,c} - f_{i,c}) \\mod 1.\n   \\]\n\n2. **Fourier Transform Invariance**:  \n   The Fourier transform \\(\\mathcal{F}_T\\) involves sums/integrals over terms like \\(e^{-i2\\pi k \\Delta}\\). For **real-valued functions**, the coefficients depend on sine/cosine terms. Since:\n   \\[\n   \\sin(2\\pi m\\Delta_{\\text{wrapped}}) = \\sin(2\\pi m\\Delta_{\\text{original}}),\n   \\]\n   all trigonometric components in the Fourier series remain identical. Thus, the Fourier coefficients are preserved:\n   \\[\n   \\mathcal{F}_T[w(f_j + t) - w(f_i + t)] = \\mathcal{F}_T[f_j - f_i].\n   \\]\n\n### Conclusion:\nThe wrapping function \\(w(\\cdot)\\) and shift \\(t\\) cancel out in the sine argument due to periodicity, ensuring the Fourier transform depends only on \\(f_j - f_i\\). This invariance holds because modular arithmetic preserves the fractional part of differences, aligning the harmonic content of both expressions.", "B": "To demonstrate the periodic-Fourier invariance expressed by the equation:\n\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k],\n\\]\n\nwe need to show that the wrapping function \\(w(\\cdot)\\) and the shift \\(t\\) cancel out in the sine argument of the Fourier transform. Here's the step-by-step explanation:\n\n### 1. **Fourier Transform of a Periodic Function**\nThe Fourier transform \\(\\mathcal{F}_T\\) of a periodic function \\(g(t)\\) with period \\(T\\) is given by:\n\n\\[\n\\mathcal{F}_T(g(t))[c, k] = \\sum_{m} a_m \\sin(2\\pi m g(t_c)),\n\\]\n\nwhere \\(a_m\\) are the Fourier coefficients, and \\(t_c\\) is the evaluation point.\n\n### 2. **Apply to \\(w(f_j + t) - w(f_i + t)\\)**\nConsider the difference \\(w(f_j + t) - w(f_i + t)\\). The Fourier transform is:\n\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\sum_{m} a_m \\sin\\bigl(2\\pi m (w(f_{j,c} + t_c) - w(f_{i,c} + t_c))\\bigr).\n\\]\n\n### 3. **Key Substitution**\nThe problem provides the substitution:\n\n\\[\n\\sin\\bigl(2\\pi m (w(f_{j,c} + t_c) - w(f_{i,c} + t_c))\\bigr) = \\sin\\bigl(2\\pi m (f_{j,c} - f_{i,c})\\bigr).\n\\]\n\nThis equality holds because the wrapping function \\(w(\\cdot)\\) is periodic with period \\(T\\), and the shift \\(t_c\\) cancels out:\n\n- The wrapping function satisfies \\(w(x + T) = w(x)\\), so:\n  \\[\n  w(f_{j,c} + t_c) = w(f_{j,c} + t_c \\mod T), \\quad w(f_{i,c} + t_c) = w(f_{i,c} + t_c \\mod T).\n  \\]\n- The difference \\(w(f_{j,c} + t_c) - w(f_{i,c} + t_c)\\) is invariant under the shift \\(t_c\\) because:\n  \\[\n  w(f_{j,c} + t_c) - w(f_{i,c} + t_c) = w(f_{j,c}) - w(f_{i,c}) \\quad \\text{(mod } T\\text{)}.\n  \\]\n- Since \\(\\sin\\) is periodic with period \\(2\\pi\\), the shift \\(t_c\\) and the wrapping \\(w(\\cdot)\\) do not affect the sine argument:\n  \\[\n  \\sin\\bigl(2\\pi m (w(f_{j,c} + t_c) - w(f_{i,c} + t_c))\\bigr) = \\sin\\bigl(2\\pi m (f_{j,c} - f_{i,c})\\bigr).\n  \\]\n\n### 4. **Resulting Fourier Transform**\nSubstituting back, we get:\n\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\sum_{m} a_m \\sin\\bigl(2\\pi m (f_{j,c} - f_{i,c})\\bigr) = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k].\n\\]\n\n### 5. **Conclusion**\nThe wrapping function \\(w(\\cdot)\\) and the shift \\(t\\) cancel out in the sine argument due to the periodicity of \\(w(\\cdot)\\) and \\(\\sin\\). Thus, the Fourier transform remains invariant:\n\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k].\n\\]\n\nThis completes the proof.", "C": "To understand the periodic-Fourier invariance, let's break down the given equation step by step. The key lies in the properties of the wrapping function \\(w(\\cdot)\\) and how it interacts with the shift \\(t\\).\n\n1. Understanding the Wrapping Function: The wrapping function \\(w(\\cdot)\\) is periodic, meaning it maps any real number into a fixed interval, typically \\([0, T)\\) for some period \\(T\\). This property implies that \\(w(x + T) = w(x)\\) for any \\(x\\).\n\n2. Effect of the Shift \\(t\\): When we add a shift \\(t\\) inside the wrapping function, the periodicity ensures that \\(w(x + t) = w(w(x) + t)\\). This means that the shift \\(t\\) does not alter the fundamental behavior of \\(w(\\cdot)\\) due to its periodic nature.\n\n3. Substitution in the Sine Argument: The given substitution shows that \\(\\sin(2\\pi m(w(f_{j,c} + t_c) - w(f_{i,c} + t_c))) = \\sin(2\\pi m(f_{j,c} - f_{i,c}))\\). This equality holds because the differences inside the wrapping functions cancel out the shift \\(t_c\\), thanks to the periodicity of \\(w(\\cdot)\\). Essentially, the wrapping function and the shift \\(t\\) 'cancel out' in the context of the sine function's argument.\n\n4. Fourier Transform Invariance: Since the sine function's argument remains unchanged after the application of \\(w(\\cdot)\\) and the shift \\(t\\), the Fourier transform of the difference \\(w(f_j + t) - w(f_i + t)\\) is identical to that of \\(f_j - f_i\\). This demonstrates the periodic-Fourier invariance, showing that the Fourier transform is unaffected by the periodic wrapping and shifting operations, provided the frequencies \\(f_j\\) and \\(f_i\\) are integer multiples of the fundamental frequency.\n\nIn summary, the periodic nature of \\(w(\\cdot)\\) and the properties of the sine function ensure that the shift \\(t\\) and the wrapping operation do not alter the fundamental frequency components captured by the Fourier transform. This invariance is crucial for applications where periodic signals are analyzed, ensuring consistency in frequency domain representations regardless of shifts or wraps in the time domain.", "D": "To understand the periodic-Fourier invariance, let's break down the given equation step by step. The key lies in the properties of the wrapping function \\(w(\\cdot)\\) and how it interacts with the shift \\(t\\).\n\n1. Understanding the Wrapping Function: The wrapping function \\(w(\\cdot)\\) is periodic, meaning it maps any real number into a fixed interval, typically \\([0, T)\\) for some period \\(T\\). This property implies that \\(w(x + T) = w(x)\\) for any \\(x\\).\n\n2. Effect of the Shift \\(t\\): When we add a shift \\(t\\) inside the wrapping function, the periodicity ensures that \\(w(x + t) = w(w(x) + t)\\). This means that the shift \\(t\\) does not alter the fundamental behavior of \\(w(\\cdot)\\) due to its periodic nature.\n\n3. Substitution in the Sine Argument: The given substitution shows that \\(\\sin(2\\pi m(w(f_{j,c} + t_c) - w(f_{i,c} + t_c))) = \\sin(2\\pi m(f_{j,c} - f_{i,c}))\\). This equality holds because the differences inside the wrapping functions cancel out the shift \\(t_c\\), thanks to the periodicity of \\(w(\\cdot)\\). Essentially, the wrapping function and the shift \\(t\\) 'cancel out' in the context of the sine function's argument.\n\n4. Fourier Transform Invariance: Since the sine function's argument remains unchanged after the application of \\(w(\\cdot)\\) and the shift \\(t\\), the Fourier transform of the difference \\(w(f_j + t) - w(f_i + t)\\) is identical to that of \\(f_j - f_i\\). This demonstrates the periodic-Fourier invariance, showing that the Fourier transform is unaffected by the periodic wrapping and shifting operations.\n\nIn summary, the periodic nature of \\(w(\\cdot)\\) and the properties of the sine function ensure that the shift \\(t\\) and the wrapping operation do not alter the fundamental frequency components captured by the Fourier transform. This invariance is crucial for applications where periodic signals are analyzed, ensuring consistency in frequency domain representations regardless of shifts or wraps in the time domain.", "ground-truth": "D"}
{"paper": "arxiv:2309.04475", "question": "Explain the derivation of the mean formula for the backward process in a diffusion model, specifically focusing on how the model estimates the noise in $A_t$ and uses this estimate to compute the backward mean of $p(A_{t-1}\\mid M_t)$. The formula in question is $$\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}}\\; -\\; \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\,\\hat{\\epsilon}_A(M_t,t),$$ where $\\hat{\\epsilon}_A(M_t,t)$ is the model's estimate of the noise in $A_t$.", "A": "Certainly! Let's derive the mean formula for the backward process in a diffusion model step by step, focusing on how the model estimates the noise in \\( A_t \\) and uses this estimate to compute the backward mean of \\( p(A_{t-1} \\mid M_t) \\).\n\n---\n\n### **1. Forward Process (Diffusion)**\nIn the forward process, we gradually add Gaussian noise to the data \\( A_0 \\) over \\( T \\) steps. At each step \\( t \\), the noisy observation \\( A_t \\) is given by:\n\\[\nA_t = \\sqrt{\\alpha_t} A_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon_t,\n\\]\nwhere \\( \\epsilon_t \\sim \\mathcal{N}(0, I) \\) is the noise added at step \\( t \\), and \\( \\alpha_t \\) is a noise schedule parameter (typically \\( \\alpha_t \\in (0, 1) \\)).\n\nBy recursively applying this, we can express \\( A_t \\) directly in terms of \\( A_0 \\):\n\\[\nA_t = \\sqrt{\\bar{\\alpha}_t} A_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon,\n\\]\nwhere \\( \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s \\), and \\( \\epsilon \\sim \\mathcal{N}(0, I) \\).\n\n---\n\n### **2. Backward Process (Denoising)**\nThe backward process aims to reverse the diffusion by estimating \\( p(A_{t-1} \\mid A_t) \\). Under the assumption of a Gaussian transition, we have:\n\\[\np(A_{t-1} \\mid A_t) \\approx \\mathcal{N}(A_{t-1}; \\mu_A(A_t, t), \\Sigma_A(A_t, t)),\n\\]\nwhere \\( \\mu_A(A_t, t) \\) is the mean and \\( \\Sigma_A(A_t, t) \\) is the variance.\n\n---\n\n### **3. Estimating the Noise \\( \\epsilon \\)**\nFrom the forward process, we can solve for the noise \\( \\epsilon \\) in terms of \\( A_t \\) and \\( A_0 \\):\n\\[\n\\epsilon = \\frac{A_t - \\sqrt{\\bar{\\alpha}_t} A_0}{\\sqrt{1 - \\bar{\\alpha}_t}}.\n\\]\nHowever, since \\( A_0 \\) is unknown during the backward process, the model learns to estimate this noise. Let \\( \\hat{\\epsilon}_A(M_t, t) \\) be the model's estimate of \\( \\epsilon \\), where \\( M_t \\) represents the model's input (which may include \\( A_t \\) and other conditioning information).\n\n---\n\n### **4. Solving for \\( A_0 \\)**\nUsing the estimated noise \\( \\hat{\\epsilon}_A(M_t, t) \\), we can solve for \\( A_0 \\):\n\\[\nA_0 \\approx \\frac{A_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}_A(M_t, t)}{\\sqrt{\\bar{\\alpha}_t}}.\n\\]\n\n---\n\n### **5. Deriving the Backward Mean \\( \\mu_A(M_t) \\)**\nThe backward mean \\( \\mu_A(M_t) \\) is derived by substituting the estimated \\( A_0 \\) back into the forward process equation for \\( A_{t-1} \\). \n\nFrom the forward process, we have:\n\\[\nA_{t-1} = \\sqrt{\\alpha_t} A_0 + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}.\n\\]\nHowever, since we are reversing the process, we instead express \\( A_{t-1} \\) in terms of \\( A_t \\) and the estimated noise \\( \\hat{\\epsilon}_A(M_t, t) \\). \n\nSubstituting \\( A_0 \\) from step 4:\n\\[\nA_{t-1} = \\sqrt{\\alpha_t} \\left( \\frac{A_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}_A(M_t, t)}{\\sqrt{\\bar{\\alpha}_t}} \\right) + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}.\n\\]\nSimplifying:\n\\[\nA_{t-1} = \\frac{\\sqrt{\\alpha_t} A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_t)}}{\\sqrt{\\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t) + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}.\n\\]\nHere, the term \\( \\epsilon_{t-1} \\) is unknown, but in the backward process, we typically assume it is absorbed into the stochastic sampling step. Thus, the **deterministic mean** \\( \\mu_A(M_t) \\) is given by:\n\\[\n\\mu_A(M_t) = \\frac{\\sqrt{\\alpha_t} A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_t)}}{\\sqrt{\\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t).\n\\]\nFor simplicity, if we assume \\( \\alpha_t \\approx 1 \\) (as is common in diffusion models where \\( \\alpha_t \\) is close to 1 for most \\( t \\)), this simplifies to:\n\\[\n\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t).\n\\]\n\n---\n\n### **6. Final Formula**\nThus, the backward mean is:\n\\[\n\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t).\n\\]\n\nThis formula shows how the model uses its noise estimate \\( \\hat{\\epsilon}_A(M_t, t) \\) to compute the mean of \\( p(A_{t-1} \\mid M_t) \\), effectively reversing the diffusion process step by step.", "B": "The derivation of the mean formula for the backward process in a diffusion model involves understanding how the model estimates and removes noise from $A_t$ to recover a cleaner version of the data. The key steps are as follows:\n\n1. Noise Addition in Forward Process: The forward process gradually adds noise to the data $A_0$ over $t$ steps, resulting in $A_t$ as $$A_t = \\sqrt{\\bar{\\alpha}_t} A_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_A,$$ where $\\bar{\\alpha}_t$ is a product of scaling factors up to step $t$, and $\\epsilon_A$ is the noise added at step $t$.\n\n2. Noise Estimation: The model learns to estimate the noise $\\epsilon_A$ added at each step, denoted as $\\hat{\\epsilon}_A(M_t, t)$. This estimation is crucial for reversing the diffusion process.\n\n3. Backward Mean Calculation: The backward mean $\\mu_A(M_t)$ is computed by adjusting $A_t$ to remove the estimated noise. The formula $$\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{\\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t)$$ effectively scales $A_t$ back by $\\sqrt{\\bar{\\alpha}_t}$ and subtracts the scaled estimated noise, yielding a denoised version of $A_t$.\n\nThis process is central to the diffusion model's ability to generate data by iteratively denoising from a noisy initial state, leveraging the estimated noise at each step to guide the denoising direction.", "C": "The derivation of the mean formula for the backward process in a diffusion model involves understanding how the model estimates and removes noise from $A_t$ to recover a cleaner version of the data. The key steps are as follows:\n\n1. Noise Addition in Forward Process: The forward process gradually adds noise to the data $A_0$ over $t$ steps, resulting in $A_t$ as $$A_t = \\sqrt{\\bar{\\alpha}_t} A_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_A,$$ where $\\bar{\\alpha}_t$ is a product of scaling factors up to step $t$, and $\\epsilon_A$ is the noise added at step $t$.\n\n2. Noise Estimation: The model learns to estimate the noise $\\epsilon_A$ added at each step, denoted as $\\hat{\\epsilon}_A(M_t, t)$. This estimation is crucial for reversing the diffusion process.\n\n3. Backward Mean Calculation: The backward mean $\\mu_A(M_t)$ is computed by adjusting $A_t$ to remove the estimated noise. The formula $$\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t)$$ effectively scales $A_t$ back by $\\sqrt{\\bar{\\alpha}_t}$ and subtracts the scaled estimated noise, yielding a denoised version of $A_t$.\n\nThis process is central to the diffusion model's ability to generate data by iteratively denoising from a noisy initial state, leveraging the estimated noise at each step to guide the denoising direction.", "D": "The derivation of the mean formula for the backward process in diffusion models involves leveraging the relationship between the noisy observation $A_t$ and the model's estimate of the original noise $\\hat{\\epsilon}_A(M_t, t)$. Here's the step-by-step explanation:\n\n---\n\n### 1. **Forward Process Definition**\nThe forward process gradually adds noise to data $A_0$ over $t$ steps. At step $t$, the noisy observation $A_t$ is:\n$$\nA_t = \\sqrt{\\bar{\\alpha}_t} A_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t,\n$$\nwhere $\\epsilon_t \\sim \\mathcal{N}(0, I)$, and $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$ is the cumulative product of the noise schedule $\\alpha_s \\in (0, 1)$.\n\n---\n\n### 2. **Isolating the Original Data $A_0$**\nRearranging the forward process equation, $A_0$ can be expressed in terms of $A_t$ and the noise $\\epsilon_t$:\n$$\nA_0 = \\frac{A_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}}.\n$$\nIn the backward process, the model estimates the noise $\\epsilon_t$ as $\\hat{\\epsilon}_A(M_t, t)$, yielding an estimate of $A_0$:\n$$\n\\hat{A}_0 = \\frac{A_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}_A(M_t, t)}{\\sqrt{\\bar{\\alpha}_t}}.\n$$\n\n---\n\n### 3. **Backward Mean for $p(A_{t-1} \\mid A_t)$**\nThe backward process aims to compute $p(A_{t-1} \\mid A_t)$. Using the forward process’s Markov property and Bayes’ rule, the posterior $q(A_{t-1} \\mid A_t, A_0)$ is Gaussian. Its mean $\\tilde{\\mu}_t$ depends on $A_t$ and $A_0$:\n$$\n\\tilde{\\mu}_t = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} A_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} A_t,\n$$\nwhere $\\beta_t = 1 - \\alpha_t$. Substituting $\\hat{A}_0$ into this expression:\n$$\n\\tilde{\\mu}_t = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\left( \\frac{A_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}_A}{\\sqrt{\\bar{\\alpha}_t}} \\right) + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} A_t.\n$$\n\n---\n\n### 4. **Simplifying the Mean Expression**\nAfter algebraic manipulation (see appendix for details):\n1. **Combine terms involving $A_t$:**\n   - The coefficient of $A_t$ simplifies to $\\frac{1}{\\sqrt{\\alpha_t}}$ due to the identity $1 - \\bar{\\alpha}_t = \\beta_t + \\alpha_t(1 - \\bar{\\alpha}_{t-1})$.\n   \n2. **Simplify the noise term:**\n   - The noise term reduces to $-\\frac{\\beta_t}{\\sqrt{\\alpha_t} \\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A$.\n\nThe final mean becomes:\n$$\n\\mu_A(M_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( A_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t) \\right).\n$$\n\n---\n\n### 5. **Reparameterization to Match the Given Formula**\nThe expression above aligns with standard diffusion model derivations. To match the given formula:\n- Recognize that $\\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} = \\sqrt{1 - \\bar{\\alpha}_t}$ when $\\beta_t = 1 - \\alpha_t$ and $\\bar{\\alpha}_t = \\alpha_t \\bar{\\alpha}_{t-1}$.\n- Substitute $\\sqrt{\\alpha_t} = \\sqrt{\\bar{\\alpha}_t / \\bar{\\alpha}_{t-1}}$ and rearrange terms to express $\\mu_A(M_t)$ directly in terms of $\\bar{\\alpha}_t$:\n$$\n\\mu_A(M_t) = \\frac{A_t}{\\sqrt{\\bar{\\alpha}_t}} - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_A(M_t, t).\n$$\n\n---\n\n### Key Intuition\nThe backward mean subtracts the estimated noise component (scaled by the noise schedule) from the noisy observation $A_t$, effectively reversing the forward diffusion process. The term $\\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} = \\sqrt{1 - \\bar{\\alpha}_t}$ ensures proper scaling to match the Gaussian distribution of the backward process.", "ground-truth": "C"}
{"paper": "arxiv:2303.10876", "question": "How can we prove Theorem 1, which states the network s modules satisfy the listed equivariance and invariance properties for any translation vector $t$ and rotation (or reflection) matrix $R$? Specifically, how do Equations (1a) through (1e) ensure that $G^{(0)}R + t, H^{(0)} = F_{IL}(X_R + t)$, $\\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)})$, $G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\})$, $H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)})$, and $\\hat{Y}R + t = F_{EOL}(G^{(L)}R + t)$ collectively prove the claim?", "A": "To prove Theorem 1, we demonstrate that each module in the network adheres to the equivariance or invariance properties under translation \\( t \\) and rotation/reflection \\( R \\), as formalized by Equations (1a)–(1e). The proof follows by induction over the network layers, leveraging the structure of these equations.\n\n### **1. Initial Layer (\\( F_{IL} \\)): Invariance of \\( H^{(0)} \\)**\nEquation (1a): \\( H^{(0)} = F_{IL}(X_R + t) \\).  \n- **Input**: \\( X_R + t = XR + t \\) (transformed coordinates).  \n- **Output**: \\( H^{(0)} \\) must be invariant.  \n- **Mechanism**: \\( F_{IL} \\) computes invariant features (e.g., pairwise distances, angles, or mean-centered coordinates) that are unaffected by \\( R \\) and \\( t \\). For example, if \\( F_{IL} \\) subtracts the centroid \\( \\mu = \\frac{1}{N}\\sum_i (XR + t)_i = R\\mu_X + t \\), the centered coordinates become \\( (XR + t) - (R\\mu_X + t) = R(X - \\mu_X) \\). To ensure invariance, \\( F_{IL} \\) further processes these into rotation-invariant scalars (e.g., \\( \\|X_i - \\mu_X\\| \\)).  \n- **Result**: \\( H^{(0)} \\) is invariant: \\( H^{(0)}(XR + t) = H^{(0)}(X) \\).\n\n### **2. Interaction Relation Module (\\( F_{IRM} \\)): Invariance of \\( \\{c_{ij}\\} \\)**\nEquation (1b): \\( \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}) \\).  \n- **Inputs**:  \n  - Transformed geometric features \\( G^{(0)}R + t \\).  \n  - Invariant features \\( H^{(0)} \\).  \n- **Output**: \\( \\{c_{ij}\\} \\) must be invariant.  \n- **Mechanism**: \\( F_{IRM} \\) uses invariant operations on \\( G^{(0)}R + t \\), such as computing relative displacements \\( (G^{(0)}_iR + t) - (G^{(0)}_jR + t) = R(G^{(0)}_i - G^{(0)}_j) \\), and then extracting invariants (e.g., \\( \\|G^{(0)}_i - G^{(0)}_j\\| \\)) or using \\( H^{(0)} \\).  \n- **Result**: \\( \\{c_{ij}\\} \\) is invariant: \\( \\{c_{ij}\\}(G^{(0)}R + t, H^{(0)}) = \\{c_{ij}\\}(G^{(0)}, H^{(0)}) \\).\n\n### **3. Equivariant Geometric Feature Layer (\\( F_{EGFL}^{(l)} \\)): Equivariance of \\( G^{(l)} \\)**\nEquation (1c): \\( G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\}) \\).  \n- **Inductive Hypothesis**: Assume \\( G^{(l)}R + t \\) is equivariant at layer \\( l \\).  \n- **Mechanism**: \\( F_{EGFL}^{(l)} \\) uses equivariant operations (e.g., vector neurons, steerable convolutions) that commute with \\( R \\) and \\( t \\). For example, updating \\( G^{(l)} \\) via:  \n  \\[\n  G^{(l+1)}_i = \\sum_j c_{ij} \\cdot (G^{(l)}_j - G^{(l)}_i),\n  \\]  \n  where \\( c_{ij} \\) are invariant. Applying \\( R \\) and \\( t \\):  \n  \\[\n  G^{(l+1)}_iR + t = \\sum_j c_{ij} \\cdot \\left(R(G^{(l)}_j - G^{(l)}_i) + t - t\\right) = R\\left(\\sum_j c_{ij} \\cdot (G^{(l)}_j - G^{(l)}_i)\\right) + t.\n  \\]  \n- **Result**: \\( G^{(l+1)} \\) is equivariant: \\( G^{(l+1)}(G^{(l)}R + t) = G^{(l+1)}(G^{(l)})R + t \\).\n\n### **4. Invariant Pointwise Feature Layer (\\( F_{IPFL}^{(l)} \\)): Invariance of \\( H^{(l)} \\)**\nEquation (1d): \\( H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}) \\).  \n- **Inputs**:  \n  - Transformed geometric features \\( G^{(l)}R + t \\).  \n  - Invariant features \\( H^{(l)} \\).  \n- **Mechanism**: \\( F_{IPFL}^{(l)} \\) uses invariant operations on \\( G^{(l)}R + t \\), such as computing \\( \\|G^{(l)}_iR + t - G^{(l)}_jR + t\\| = \\|G^{(l)}_i - G^{(l)}_j\\| \\), or aggregating \\( H^{(l)} \\) with invariant functions (e.g., sum/max over neighbors).  \n- **Result**: \\( H^{(l+1)} \\) is invariant: \\( H^{(l+1)}(G^{(l)}R + t, H^{(l)}) = H^{(l+1)}(G^{(l)}, H^{(l)}) \\).\n\n### **5. Equivariant Output Layer (\\( F_{EOL} \\)): Equivariance of \\( \\hat{Y} \\)**\nEquation (1e): \\( \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t) \\).  \n- **Mechanism**: \\( F_{EOL} \\) maps the final equivariant features \\( G^{(L)}R + t \\) to the output space using equivariant operations. For example, a linear projection \\( \\hat{Y} = WG^{(L)} \\) satisfies:  \n  \\[\n  F_{EOL}(G^{(L)}R + t) = W(G^{(L)}R + t) = (WG^{(L)})R + Wt.\n  \\]  \n  If \\( Wt = t \\) (e.g., \\( W \\) is identity for translations), equivariance holds.  \n- **Result**: \\( \\hat{Y} \\) is equivariant: \\( \\hat{Y}(G^{(L)}R + t) = \\hat{Y}(G^{(L)})R + t \\).\n\n### **Conclusion**\nBy induction:  \n- **Base Case**: \\( H^{(0)} \\) is invariant (Equation 1a).  \n- **Inductive Step**: If \\( G^{(l)} \\) is equivariant and \\( H^{(l)} \\) is invariant at layer \\( l \\), then \\( G^{(l+1)} \\) remains equivariant (Equation 1c) and \\( H^{(l+1)} \\) remains invariant (Equation 1d).  \n- **Final Output**: \\( \\hat{Y} \\) is equivariant (Equation 1e).  \n\nThus, the network’s modules collectively ensure the claimed equivariance and invariance properties.", "B": "To prove Theorem 1, we systematically verify the equivariance and invariance properties of each module in the network under any translation vector $t$ and rotation (or reflection) matrix $R$. The proof is structured around the following key points:\n\n1. Initialization Layer ($F_{IL}(\\cdot)$): The initial geometric feature $G^{(0)}$ is designed to be equivariant, meaning that applying a transformation $R$ and translation $t$ to the input $X$ results in a corresponding transformation of $G^{(0)}$. The initial pattern feature $H^{(0)}$ is invariant, remaining unchanged under such transformations. This is encapsulated by the equation:\n   \\[ G^{(0)}R + t, \\quad H^{(0)} = F_{IL}(X_R + t). \\]\n\n2. Interaction Reasoning Module ($F_{IRM}(\\cdot)$): This module processes the transformed geometric features and invariant pattern features to produce interaction categorical vectors $\\{c_{ij}\\}$ that are invariant to transformations, as shown by:\n   \\[ \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}). \\]\n\n3. Geometric Feature Learning Layer ($F_{EGFL}^{(l)}(\\cdot)$): At each layer $l$, the geometric features are updated in a manner that preserves equivariance. The transformation properties are maintained through the layer's operations, ensuring that:\n   \\[ G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\}). \\]\n\n4. Pattern Feature Learning Layer ($F_{IPFL}^{(l)}(\\cdot)$): Similarly, the pattern features are updated in a way that maintains their invariance under transformations, as described by:\n   \\[ H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}). \\]\n\n5. Output Layer ($F_{EOL}(\\cdot)$): Finally, the output layer produces predictions that are equivariant to the input transformations, completing the proof with:\n   \\[ \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t). \\]\n\nBy demonstrating that each component of the network adheres to these transformation properties, we collectively establish the overall equivariance and invariance claims of Theorem 1. The detailed operations within each module, including attention mechanisms and non-linear functions, are carefully designed to preserve these properties, ensuring the network's behavior is consistent under Euclidean transformations.", "C": "To prove Theorem 1, which asserts that the network's modules satisfy the listed equivariance and invariance properties under translations \\( t \\) and rotations/reflections \\( R \\), we need to carefully analyze how each equation (1a) through (1e) contributes to the overall equivariance or invariance of the network's transformations. Below is a step-by-step proof:\n\n---\n\n### **Proof of Theorem 1**\n\nWe proceed by induction over the layers of the network, showing that each module preserves the claimed properties.\n\n#### **1. Base Case: Input Layer (\\( l = 0 \\))**\n- **Equation (1a):**  \n  The input layer \\( F_{IL} \\) processes the input point cloud \\( X \\). The claim states:\n  \\[\n  G^{(0)} R + t, H^{(0)} = F_{IL}(X R + t).\n  \\]\n  - If \\( F_{IL} \\) is **equivariant** to \\( R \\) and \\( t \\), then processing the transformed input \\( X R + t \\) yields transformed features \\( G^{(0)} R + t \\) (equivariant) and invariant features \\( H^{(0)} \\).  \n  - This ensures that the initial features respect the transformation.\n\n#### **2. Invariant Relation Module (\\( F_{IRM} \\))**\n- **Equation (1b):**  \n  The relation module computes pairwise interactions:\n  \\[\n  \\{c_{ij}\\} = F_{IRM}(G^{(0)} R + t, H^{(0)}).\n  \\]\n  - Since \\( F_{IRM} \\) takes \\( G^{(0)} R + t \\) (equivariant) and \\( H^{(0)} \\) (invariant) as inputs, and outputs \\(\\{c_{ij}\\}\\), these relations must be **invariant** to \\( R \\) and \\( t \\).  \n  - This is because \\( c_{ij} \\) typically depends on distances or angles, which are preserved under rigid transformations.\n\n#### **3. Inductive Step: Equivariant Graph Filter Layer (\\( F_{EGFL}^{(l)} \\))**\n- **Equation (1c):**  \n  The graph filter updates the equivariant features:\n  \\[\n  G^{(l+1)} R + t = F_{EGFL}^{(l)}(G^{(l)} R + t, H^{(l)}, \\{c_{ij}\\}).\n  \\]\n  - By the induction hypothesis, assume \\( G^{(l)} R + t \\) is equivariant and \\( H^{(l)} \\), \\(\\{c_{ij}\\}\\) are invariant.  \n  - If \\( F_{EGFL}^{(l)} \\) is designed to preserve equivariance (e.g., using linear transformations or message passing that respects \\( R \\) and \\( t \\)), then \\( G^{(l+1)} R + t \\) remains equivariant.\n\n#### **4. Invariant Pointwise Filter Layer (\\( F_{IPFL}^{(l)} \\))**\n- **Equation (1d):**  \n  The pointwise filter updates the invariant features:\n  \\[\n  H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)} R + t, H^{(l)}).\n  \\]\n  - Since \\( G^{(l)} R + t \\) is equivariant and \\( H^{(l)} \\) is invariant, \\( F_{IPFL}^{(l)} \\) must process them in a way that \\( H^{(l+1)} \\) remains **invariant**.  \n  - This can be achieved by only using invariant operations (e.g., norms, inner products, or MLPs on invariant inputs).\n\n#### **5. Equivariant Output Layer (\\( F_{EOL} \\))**\n- **Equation (1e):**  \n  The final output is computed as:\n  \\[\n  \\hat{Y} R + t = F_{EOL}(G^{(L)} R + t).\n  \\]\n  - Since \\( G^{(L)} R + t \\) is equivariant by the induction, \\( F_{EOL} \\) must preserve this property.  \n  - If \\( F_{EOL} \\) applies an equivariant transformation (e.g., a linear layer), then \\( \\hat{Y} R + t \\) is correctly transformed.\n\n---\n\n### **Conclusion**\nBy induction:\n1. The **equivariant features** (\\( G^{(l)} \\)) transform as \\( G^{(l)} R + t \\) at every layer.  \n2. The **invariant features** (\\( H^{(l)} \\)) and relations (\\( \\{c_{ij}\\} \\)) remain unchanged under \\( R \\) and \\( t \\).  \n3. The **final output** \\( \\hat{Y} \\) transforms equivariantly as \\( \\hat{Y} R + t \\).\n\nThus, the network satisfies the claimed equivariance and invariance properties under arbitrary translations \\( t \\) and rotations/reflections \\( R \\). \\(\\square\\)", "D": "To prove Theorem 1, we systematically verify the equivariance and invariance properties of each module in the network under any translation vector $t$ and rotation (or reflection) matrix $R$. The proof is structured around the following key points:\n\n1. Initialization Layer ($F_{IL}(\\cdot)$): The initial geometric feature $G^{(0)}$ is designed to be equivariant, meaning that applying a transformation $R$ and translation $t$ to the input $X$ results in a corresponding transformation of $G^{(0)}$. The initial pattern feature $H^{(0)}$ is invariant, remaining unchanged under such transformations. This is encapsulated by the equation:\n   \\[ G^{(0)}R + t, \\quad H^{(0)} = F_{IL}(X_R + t). \\]\n\n2. Interaction Reasoning Module ($F_{IRM}(\\cdot)$): This module processes the transformed geometric features and invariant pattern features to produce interaction categorical vectors $\\{c_{ij}\\}$ that are invariant to transformations, as shown by:\n   \\[ \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}). \\]\n\n3. Geometric Feature Learning Layer ($F_{EGFL}^{(l)}(\\cdot)$): At each layer $l$, the geometric features are updated in a manner that preserves equivariance. The transformation properties are maintained through the layer's operations, ensuring that:\n   \\[ G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R - t, H^{(l)}, \\{c_{ij}\\}). \\]\n\n4. Pattern Feature Learning Layer ($F_{IPFL}^{(l)}(\\cdot)$): Similarly, the pattern features are updated in a way that maintains their invariance under transformations, as described by:\n   \\[ H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}). \\]\n\n5. Output Layer ($F_{EOL}(\\cdot)$): Finally, the output layer produces predictions that are equivariant to the input transformations, completing the proof with:\n   \\[ \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t). \\]\n\nBy demonstrating that each component of the network adheres to these transformation properties, we collectively establish the overall equivariance and invariance claims of Theorem 1. The detailed operations within each module, including attention mechanisms and non-linear functions, are carefully designed to preserve these properties, ensuring the network's behavior is consistent under Euclidean transformations.", "ground-truth": "B"}
{"paper": "arxiv:2303.10876", "question": "Based on Theorem 1, how can we prove Corollary 1, which states that the entire EqMotion network $F_{pred}(\\cdot)$ satisfies $\\hat{Y}R + t = F_{pred}(X_R + t)$ for any $R \\in SO(n)$ and $t \\in \\mathbb{R}^n$? Please show how the result of Theorem 1 implies this corollary.", "A": "To prove Corollary 1, we start by recalling the result of Theorem 1, which establishes the equivariance of the network's operations under Euclidean transformations. Specifically, Theorem 1 shows that for any rotation matrix $R \\in SO(n)$ and translation vector $t \\in \\mathbb{R}^n$, the network's operations preserve the transformation properties of the input data. \n\nGiven this, we can derive Corollary 1 by considering the composition of all network operations. The network $F_{pred}(\\cdot)$ takes an input $X$ and applies a series of equivariant operations to produce an output $\\hat{Y}$. Due to the equivariance property established by Theorem 1, applying a rotation $R$ and translation $t$ to the input $X$ (resulting in $X_R + t$) and then passing it through the network yields an output that is equivalent to applying the same rotation and translation to the network's output for the original input $X$. Mathematically, this is expressed as:\n\n\\[ \\hat{Y}R + t = F_{pred}(X_R + t). \\]\n\nThis equation demonstrates that the entire network $F_{pred}(\\cdot)$ is equivariant under Euclidean transformations, thereby satisfying Corollary 1. The key insight is that the equivariance of individual operations, as guaranteed by Theorem 1, ensures the equivariance of the entire network when these operations are composed.", "B": "To prove Corollary 1, we start by recalling the result of Theorem 1, which establishes the equivariance of the network's operations under Euclidean transformations. Specifically, Theorem 1 shows that for any rotation matrix $R \\in SO(n)$ and translation vector $t \\in \\mathbb{R}^n$, the network's operations preserve the transformation properties of the input data. \n\nGiven this, we can derive Corollary 1 by considering the composition of all network operations. The network $F_{pred}(\\cdot)$ takes an input $X$ and applies a series of equivariant operations to produce an output $\\hat{Y}$. Due to the equivariance property established by Theorem 1, applying a rotation $R$ and translation $t$ to the input $X$ (resulting in $X_R + t$) and then passing it through the network yields an output that is equivalent to applying the same rotation and translation to the network's output for the original input $X$. Mathematically, this is expressed as:\n\n\\[ \\hat{Y}R + t = F_{pred}(X_R). \\]\n\nThis equation demonstrates that the entire network $F_{pred}(\\cdot)$ is equivariant under Euclidean transformations, thereby satisfying Corollary 1. The key insight is that the equivariance of individual operations, as guaranteed by Theorem 1, ensures the equivariance of the entire network when these operations are composed.", "C": "Based on **Theorem 1**, which likely establishes that the network $F_{pred}(\\cdot)$ is **equivariant** under rotations and **invariant/equivariant** under translations, we derive **Corollary 1** as follows:\n\n---\n\n**Corollary 1** (Transformation Equivariance):  \nFor any rotation $R \\in SO(n)$ and translation $t \\in \\mathbb{R}^n$, the EqMotion network satisfies:\n$$\n\\hat{Y}R + t = F_{pred}(XR + t),\n$$\nwhere $\\hat{Y} = F_{pred}(X)$ is the network’s output for input $X$.\n\n---\n\n**Proof Using Theorem 1:**\n\n1. **Assumption from Theorem 1**:  \n   Theorem 1 guarantees that $F_{pred}(\\cdot)$ is **equivariant** under the Euclidean group $SE(n)$ (rotations and translations). Specifically, for any input $X$, rotation $R$, and translation $t$, the network satisfies:\n   $$\n   F_{pred}(XR + t) = F_{pred}(X)R + t.\n   $$\n   *(This is the standard equivariance property for transformations in $SE(n)$.)*\n\n2. **Substitute $\\hat{Y} = F_{pred}(X)$**:  \n   By definition, the network’s output for the original input $X$ is $\\hat{Y} = F_{pred}(X)$. Substituting this into the right-hand side of Theorem 1:\n   $$\n   F_{pred}(XR + t) = \\hat{Y}R + t.\n   $$\n\n3. **Conclusion**:  \n   Rearranging terms, we directly obtain the statement of Corollary 1:\n   $$\n   \\hat{Y}R + t = F_{pred}(XR + t).\n   $$\n\n---\n\n**Key Steps:**  \n- Theorem 1 ensures that **transforming the input** ($XR + t$) results in an **equivariant transformation of the output** ($\\hat{Y}R + t$).  \n- This aligns with the intuition that the network’s predictions respect the **symmetries** of the input data under rigid motions (rotations/translations).  \n\nThus, Corollary 1 follows directly from the equivariance property stated in Theorem 1.", "D": "To prove Corollary 1 based on Theorem 1, let's first restate the relevant results clearly.\n\n**Theorem 1** (Assumed to be given):  \nThe network \\( F_{pred}(\\cdot) \\) is equivariant under rotations and translations. Specifically, for any rotation \\( R \\in SO(n) \\) and translation \\( t \\in \\mathbb{R}^n \\), the network satisfies:\n\\[\nF_{pred}(R X + t) = R F_{pred}(X) + t\n\\]\nwhere \\( X \\) is the input point cloud and \\( F_{pred}(X) \\) is the predicted output (e.g., motion or deformation).\n\n**Corollary 1**:  \nThe entire EqMotion network \\( F_{pred}(\\cdot) \\) satisfies:\n\\[\n\\hat{Y} R + t = F_{pred}(X R + t)\n\\]\nfor any \\( R \\in SO(n) \\) and \\( t \\in \\mathbb{R}^n \\), where \\( \\hat{Y} = F_{pred}(X) \\) is the predicted output for the input \\( X \\).\n\n---\n\n### Proof of Corollary 1 Using Theorem 1:\n\n1. **Start with the left-hand side (LHS) of Corollary 1**:\n   \\[\n   \\hat{Y} R + t = F_{pred}(X) R + t\n   \\]\n   Here, \\( \\hat{Y} = F_{pred}(X) \\) by definition.\n\n2. **Apply Theorem 1 to the right-hand side (RHS) of Corollary 1**:\n   We evaluate \\( F_{pred}(X R + t) \\). Let \\( X' = X R + t \\). Then:\n   \\[\n   F_{pred}(X') = F_{pred}(X R + t)\n   \\]\n   By Theorem 1, since \\( X' \\) is a rotated (by \\( R \\)) and translated (by \\( t \\)) version of \\( X \\), the output transforms equivariantly:\n   \\[\n   F_{pred}(X R + t) = F_{pred}(X) R + t\n   \\]\n\n3. **Equate the two results**:\n   From step 1 and step 2, we have:\n   \\[\n   F_{pred}(X) R + t = F_{pred}(X R + t)\n   \\]\n   Substituting \\( \\hat{Y} = F_{pred}(X) \\), this becomes:\n   \\[\n   \\hat{Y} R + t = F_{pred}(X R + t)\n   \\]\n   which is precisely the statement of Corollary 1.\n\n---\n\n### Conclusion:\nCorollary 1 follows directly from Theorem 1 by applying the equivariance property of \\( F_{pred}(\\cdot) \\) under rotations and translations. The key step is recognizing that the transformation \\( X \\rightarrow X R + t \\) induces the corresponding transformation \\( \\hat{Y} \\rightarrow \\hat{Y} R + t \\) in the output space. This is the essence of equivariance.", "ground-truth": "A"}
{"paper": "arxiv:2303.10876", "question": "Based on the formula: $G_i^{(l+1)} = \\phi_a(h_i^{(l)})\\cdot(G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}$, demonstrate how the expression $G_i^{(l+1)} R + t$ under rotation $R$ and translation $t$ can be shown to equal $\\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}})R + \\overline{G_i^{(l)}}R + t$, thereby proving its equivariance.", "A": "To establish the equivariance of the given formula under rotation $R$ and translation $t$, we start by applying the transformation to $G_i^{(l+1)}$:\n\n1. Apply the rotation and translation to $G_i^{(l+1)}$:\n   $$ G_i^{(l+1)} R + t = (\\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}) R + t $$\n\n2. Distribute the rotation $R$ and the translation $t$ across the terms:\n   $$ = \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) R + \\overline{G_i^{(l)}} R + t $$\n\n3. Recognize that $\\phi_a(h_i^{(l)})$ is a scalar and thus unaffected by the rotation $R$. The term $\\overline{G_i^{(l)}} R + t$ adjusts the coordinate frame to account for the rotation and translation.\n\nThis demonstrates that the formula maintains its structure under the transformation, confirming its equivariance with respect to rotation and translation.", "B": "To demonstrate the equivariance of the expression \\( G_i^{(l+1)} R + t \\) under rotation \\( R \\) and translation \\( t \\), we will start with the given formula and apply the transformation step-by-step.\n\n### Given Formula:\n\\[\nG_i^{(l+1)} = \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}\n\\]\n\n### Transformation Under Rotation \\( R \\) and Translation \\( t \\):\nWe want to compute \\( G_i^{(l+1)} R + t \\). Substituting the given formula:\n\n\\[\nG_i^{(l+1)} R + t = \\left[ \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}} \\right] R + t\n\\]\n\nDistribute \\( R \\) over the addition:\n\n\\[\n= \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) R + \\overline{G_i^{(l)}} R + t\n\\]\n\n### Equivariance Proof:\nThe key observation is that \\( \\phi_a(h_i^{(l)}) \\) is a scalar function (invariant under rotation and translation), and \\( \\overline{G_i^{(l)}} \\) is the mean of \\( G_i^{(l)} \\), which transforms as \\( \\overline{G_i^{(l)}} R + t \\) under the same transformation. However, in the expression \\( G_i^{(l)} - \\overline{G_i^{(l)}} \\), the translation \\( t \\) cancels out:\n\n\\[\nG_i^{(l)} R + t - (\\overline{G_i^{(l)}} R + t) = (G_i^{(l)} - \\overline{G_i^{(l)}}) R\n\\]\n\nThus, the term \\( \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) R \\) is equivariant under rotation, and the remaining terms \\( \\overline{G_i^{(l)}} R + t \\) account for the translation. \n\n### Final Expression:\n\\[\nG_i^{(l+1)} R + t = \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) R + \\overline{G_i^{(l)}} R + t\n\\]\n\nThis shows that the transformation of \\( G_i^{(l+1)} \\) under \\( R \\) and \\( t \\) is consistent with the equivariance property, as the output is a linear combination of rotated terms plus a translation. The scalar \\( \\phi_a(h_i^{(l)}) \\) remains unchanged, ensuring the equivariance of the entire expression.", "C": "To establish the equivariance of the given formula under rotation $R$ and translation $t$, we start by applying the transformation to $G_i^{(l+1)}$:\n\n1. Apply the rotation and translation to $G_i^{(l+1)}$:\n   $$ G_i^{(l+1)} R + t = (\\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}) R + t $$\n\n2. Distribute the rotation $R$ and the translation $t$ across the terms:\n   $$ = \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) R + \\overline{G_i^{(l)}} R $$\n\n3. Recognize that $\\phi_a(h_i^{(l)})$ is a scalar and thus unaffected by the rotation $R$. The term $\\overline{G_i^{(l)}} R + t$ adjusts the coordinate frame to account for the rotation and translation.\n\nThis demonstrates that the formula maintains its structure under the transformation, confirming its equivariance with respect to rotation and translation.", "D": "To demonstrate the equivariance of $G_i^{(l+1)} R + t$ under rotation $R$ and translation $t$, we start by substituting the given formula $G_i^{(l+1)} = \\phi_a(h_i^{(l)}) \\cdot (G_i^{(l)} - \\overline{G_i^{(l)}}) + \\overline{G_i^{(l)}}$ into the transformed expression:\n\n\\[\nG_i^{(l+1)} R + t = \\left[ \\phi_a(h_i^{(l)}) \\cdot \\left(G_i^{(l)} - \\overline{G_i^{(l)}}\\right) + \\overline{G_i^{(l)}} \\right] R + t.\n\\]\n\nExpanding this using linearity of rotation and translation:\n\n1. **Distribute $R$ over the terms inside the brackets**:\n   \\[\n   \\phi_a(h_i^{(l)}) \\cdot \\left(G_i^{(l)} - \\overline{G_i^{(l)}}\\right) R + \\overline{G_i^{(l)}} R.\n   \\]\n   Here, the scalar $\\phi_a(h_i^{(l)})$ commutes with $R$ (rotations do not affect scalar multipliers).\n\n2. **Add the translation $t$**:\n   \\[\n   \\phi_a(h_i^{(l)}) \\cdot \\left(G_i^{(l)} - \\overline{G_i^{(l)}}\\right) R + \\overline{G_i^{(l)}} R + t.\n   \\]\n\nThis matches the target expression. To prove equivariance, we must ensure that applying $R$ and $t$ **after** computing $G_i^{(l+1)}$ (left-hand side) is equivalent to computing $G_i^{(l+1)}$ **after** applying $R$ and $t$ to the input $G_i^{(l)}$ (right-hand side). \n\n### Key Observations:\n1. **Mean invariance under rotation and translation**:  \n   If $G_i^{(l)}$ is transformed as $G_i^{(l)} R + t$, the mean becomes:\n   \\[\n   \\overline{G_i^{(l)}}' = \\overline{G_i^{(l)}} R + t.\n   \\]\n   The term $t$ cancels in the centered component:\n   \\[\n   G_i^{(l)} R + t - \\overline{G_i^{(l)}}' = \\left(G_i^{(l)} - \\overline{G_i^{(l)}}\\right) R.\n   \\]\n   This ensures the **relative geometry** (centered term) is rotation-equivariant but translation-invariant.\n\n2. **Equivariance of the full expression**:  \n   When $G_i^{(l)}$ is transformed, the update rule for $G_i^{(l+1)'}$ (using transformed inputs) becomes:\n   \\[\n   G_i^{(l+1)'} = \\phi_a(h_i^{(l)}) \\cdot \\left(G_i^{(l)} - \\overline{G_i^{(l)}}\\right) R + \\overline{G_i^{(l)}} R + t.\n   \\]\n   This matches $G_i^{(l+1)} R + t$, proving equivariance. The scalar $\\phi_a(h_i^{(l)})$ is unaffected by $R$ and $t$ if $h_i^{(l)}$ depends only on **invariant features** (e.g., distances or angles).\n\n### Conclusion:\nThe equality $G_i^{(l+1)} R + t = \\phi_a(h_i^{(l)}) \\cdot \\left(G_i^{(l)} - \\overline{G_i^{(l)}}\\right) R + \\overline{G_i^{(l)}} R + t$ holds, confirming that the update rule preserves equivariance under rotation and translation.", "ground-truth": "A"}
{"paper": "arxiv:2303.10876", "question": "Starting from the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features, how do we obtain $G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})R + t$? Show how the transformation $R$ and translation $t$ commute with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$ to yield $G_i^{(l+1)} R + t$ again.", "A": "To understand how $G_i^{(l+1)} R + t$ is obtained from the given equation, let's break down the process step by step, focusing on how the transformation $R$ and translation $t$ interact with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$. Here's the reasoning:\n\n1. Initial Definition: We start with the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features:\n   $$G_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})$$\n\n2. Apply Transformation and Translation: We then apply the transformation $R$ and translation $t$ to $G_i^{(l+1)}$:\n   $$G_i^{(l+1)} R + t = \\left(G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\\right) R + t$$\n\n3. Distribute $R$ and $t$: Distribute the transformation $R$ and the translation $t$ inside the parentheses:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R + t$$\n   Here, we see that $R$ is distributed over the difference $(G_j^{(l)} - G_i^{(l)})$, and $t$ is added to each term.\n\n4. Simplify the Expression: Notice that the translation $t$ appears twice in the equation. However, since $t$ is a constant translation vector, adding it once is sufficient. Thus, the equation simplifies to:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This simplification shows that the transformation $R$ commutes with the difference operation, and the translation $t$ is applied once to the entire expression.\n\n5. Final Form: The final form demonstrates that the transformation and translation preserve the structure of the neighbor aggregation, leading to the equivariance property:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This confirms that $G_i^{(l+1)} R + t$ is indeed obtained as described, with $R$ and $t$ commuting appropriately with the operations involved.", "B": "Starting from the definition of the updated node feature $G_i^{(l+1)}$ as a neighborhood aggregation:\n\n$$\nG_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)\n$$\n\nWe aim to show how applying the transformation $R$ (rotation) and translation $t$ preserves the structure when commuting with the aggregation. Specifically, we derive:\n\n$$\nG_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R + t\n$$\n\n**Step-by-Step Derivation:**\n\n1. **Apply $R$ and $t$ to $G_i^{(l+1)}$:**\n   $$\n   G_i^{(l+1)} R + t = \\left[G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)\\right] R + t\n   $$\n\n2. **Distribute $R$ over the summation:**\n   $$\n   = G_i^{(l)} R + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R + t\n   $$\n\n3. **Handle the translation $t$:**\n   - The translation $t$ is additive and commutes with linear operations. Since $t$ is applied *after* the rotation $R$, it remains outside the summation:\n   $$\n   = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R\n   $$\n\n4. **Final alignment with the target expression:**\n   - The last term $\\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R$ inherently excludes $t$ because:\n     - The difference $\\left(G_j^{(l)} - G_i^{(l)}\\right)$ cancels any shared translation between neighbors.\n     - The rotation $R$ acts linearly on the difference, preserving relative geometric relationships.\n\n**Key Observations:**\n- **Linear transformation:** $R$ distributes over the summation and differences because it is a linear operator.\n- **Translation invariance:** The subtraction $\\left(G_j^{(l)} - G_i^{(l)}\\right)$ removes absolute positional information (invariant to $t$), leaving only relative differences to be scaled by $e_{ij}^{(l)}$ and rotated by $R$.\n- **Final composition:** The term $G_i^{(l)} R + t$ represents the transformed \"base\" feature, while the summation term accounts for transformed neighborhood differences.\n\nThus, the equality holds as the transformation $R$ and translation $t$ commute with the aggregation mechanics. The apparent extra $t$ in the original statement arises from emphasizing the composition of transformations, but it is absorbed into the final additive structure consistently.", "C": "Let's carefully derive the given expression step by step, showing how the transformation \\( R \\) and translation \\( t \\) commute with the aggregation operation.\n\n### Starting Definition\nThe feature \\( G_i^{(l+1)} \\) is defined as an aggregation of neighbor features:\n\\[\nG_i^{(l+1)} = G_i^{(l)} + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\n\\]\nwhere:\n- \\( G_i^{(l)} \\) is the feature of node \\( i \\) at layer \\( l \\),\n- \\( N_i \\) is the set of neighbors of node \\( i \\),\n- \\( e_{ij}^{(l)} \\) is the attention coefficient or edge weight between nodes \\( i \\) and \\( j \\) at layer \\( l \\).\n\n### Applying Transformation \\( R \\) and Translation \\( t \\)\nWe want to compute \\( G_i^{(l+1)} R + t \\). Substituting the definition of \\( G_i^{(l+1)} \\):\n\\[\nG_i^{(l+1)} R + t = \\left( G_i^{(l)} + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) \\right) R + t\n\\]\n\n### Distributing \\( R \\) and \\( t \\)\nDistribute the transformation \\( R \\) over the summation:\n\\[\n= G_i^{(l)} R + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R + t\n\\]\n\nNow, observe that the translation \\( t \\) is added only once at the end, not inside the summation. This is because translations are additive and commute with linear transformations (like \\( R \\)) in affine spaces. Thus:\n\\[\n= G_i^{(l)} R + t + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R\n\\]\n\n### Simplifying the Difference Inside the Sum\nThe term \\( (G_j^{(l)} - G_i^{(l)}) R \\) can be expanded using the linearity of \\( R \\):\n\\[\n(G_j^{(l)} - G_i^{(l)}) R = G_j^{(l)} R - G_i^{(l)} R\n\\]\nThus, the summation becomes:\n\\[\n\\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)\n\\]\n\n### Final Expression\nSubstituting back, we get:\n\\[\nG_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)\n\\]\nThis matches the given expression:\n\\[\nG_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R\n\\]\n\n### Key Observations\n1. **Linearity of \\( R \\)**: The transformation \\( R \\) distributes over the difference \\( (G_j^{(l)} - G_i^{(l)}) \\), allowing us to write \\( (G_j^{(l)} - G_i^{(l)}) R = G_j^{(l)} R - G_i^{(l)} R \\).\n2. **Translation \\( t \\)**: The translation is added only once at the end because it is a global shift applied after the linear transformation and aggregation. The difference \\( (G_j^{(l)} - G_i^{(l)}) \\) cancels out any translations, as \\( t - t = 0 \\).\n\nThus, the transformation \\( R \\) and translation \\( t \\) commute with the aggregation and difference operations, preserving the structure of the update rule.", "D": "To understand how $G_i^{(l+1)} R + t$ is obtained from the given equation, let's break down the process step by step, focusing on how the transformation $R$ and translation $t$ interact with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$. Here's the reasoning:\n\n1. Initial Definition: We start with the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features:\n   $$G_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})$$\n\n2. Apply Transformation and Translation: We then apply the transformation $R$ and translation $t$ to $G_i^{(l+1)}$:\n   $$G_i^{(l+1)} R + t = \\left(G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\\right) R + t$$\n\n3. Distribute $R$ and $t$: Distribute the transformation $R$ and the translation $t$ inside the parentheses:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R$$\n   Here, we see that $R$ is distributed over the difference $(G_j^{(l)} - G_i^{(l)})$, but the translation $t$ is only added to the first term.\n\n4. Simplify the Expression: Notice that the translation $t$ appears only once in the equation. However, since $t$ is a constant translation vector, it should be added to the entire expression. Thus, the equation should be:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This simplification shows that the transformation $R$ commutes with the difference operation, but the translation $t$ is not properly distributed.\n\n5. Final Form: The final form demonstrates that the transformation and translation preserve the structure of the neighbor aggregation, leading to the equivariance property:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This confirms that $G_i^{(l+1)} R + t$ is indeed obtained as described, with $R$ commuting appropriately but $t$ not being fully distributed.", "ground-truth": "A"}
{"paper": "arxiv:2303.10876", "question": "Given the formulas $Q_i^{(l)} = W_Q^{(l)}(G_i^{(l)} - \\overline{G_i^{(l)}})$ and $K_i^{(l)} = W_K^{(l)}(G_i^{(l)} - \\overline{G_i^{(l)}})$, and knowing that $R R^T = I$, explain how we can derive the equality $Q_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} (K_i^{(l)})^T$. Show the reasoning behind why multiplying by $R$ and $R^T$ cancels out in the inner product of queries and keys.", "A": "We start with the left-hand side expression \\( Q_i^{(l)} R (K_i^{(l)} R)^T \\) and aim to show it equals \\( Q_i^{(l)} (K_i^{(l)})^T \\).  \n\n**Step 1:** Expand the transpose in the second term.  \nBy the property of matrix transpose, \\( (K_i^{(l)} R)^T = R^T (K_i^{(l)})^T \\). Substituting this gives:  \n\\[\nQ_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} R \\cdot R^T (K_i^{(l)})^T\n\\]\n\n**Step 2:** Use associativity of matrix multiplication.  \nGroup \\( R \\cdot R^T \\) first:  \n\\[\nQ_i^{(l)} (R R^T) (K_i^{(l)})^T\n\\]\n\n**Step 3:** Apply the orthogonality condition \\( R R^T = I \\).  \nSince \\( R \\) is orthogonal, substituting \\( R R^T = I \\) simplifies the expression:  \n\\[\nQ_i^{(l)} I (K_i^{(l)})^T = Q_i^{(l)} (K_i^{(l)})^T\n\\]\n\n**Conclusion:**  \nThe equality \\( Q_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} (K_i^{(l)})^T \\) holds because the orthogonal rotation matrix \\( R \\) and its transpose \\( R^T \\) cancel out via \\( R R^T = I \\). This demonstrates that the inner product of \\( Q_i^{(l)} \\) and \\( K_i^{(l)} \\) is invariant under orthogonal transformations, preserving the geometric structure of the original queries and keys.", "B": "To understand why multiplying by $R$ and $R^T$ cancels out in the inner product of queries and keys, let's break down the derivation step by step. Starting with the expression $Q_i^{(l)} R (K_i^{(l)} R)^T$, we can expand it using the properties of matrix multiplication and transposition:\n\n1. First, apply the transpose to the product $(K_i^{(l)} R)^T$, which gives $R^T (K_i^{(l)})^T$ due to the property $(AB)^T = B^T A^T$.\n\n2. Now, the expression becomes $Q_i^{(l)} R R^T (K_i^{(l)})^T$.\n\n3. Given that $R R^T = I$, where $I$ is the identity matrix, we can substitute $I$ into the expression, resulting in $Q_i^{(l)} I (K_i^{(l)})^T$.\n\n4. Multiplying by the identity matrix $I$ leaves the original matrix unchanged, so the expression simplifies to $Q_i^{(l)} (K_i^{(l)})^T$.\n\nThis derivation shows that the attention score $Q_i^{(l)} (K_i^{(l)})^T$ remains unchanged when both the query and key are multiplied by $R$ and $R^T$, respectively, because the product $R R^T$ cancels out to the identity matrix. This property is crucial for maintaining the integrity of the attention mechanism when applying orthogonal transformations like $R$.", "C": "To understand why multiplying by $R$ and $R^T$ cancels out in the inner product of queries and keys, let's break down the derivation step by step. Starting with the expression $Q_i^{(l)} R (K_i^{(l)} R)^T$, we can expand it using the properties of matrix multiplication and transposition:\n\n1. First, apply the transpose to the product $(K_i^{(l)} R)^T$, which gives $R^T (K_i^{(l)})^T$ due to the property $(AB)^T = B^T A^T$.\n\n2. Now, the expression becomes $Q_i^{(l)} R R^T (K_i^{(l)})^T$.\n\n3. Given that $R R^T = I$, where $I$ is the identity matrix, we can substitute $I$ into the expression, resulting in $Q_i^{(l)} I (K_i^{(l)})^T$.\n\n4. Multiplying by the identity matrix $I$ leaves the original matrix unchanged, so the expression simplifies to $Q_i^{(l)} (K_i^{(l)})^T$.\n\nThis derivation shows that the attention score $Q_i^{(l)} (K_i^{(l)})^T$ remains unchanged when both the query and key are multiplied by $R$ and $R^T$, respectively, because the product $R R^T$ cancels out to the identity matrix. However, this assumes that $R$ is square, which may not hold in all cases where $R$ is a rectangular matrix. This property is crucial for maintaining the integrity of the attention mechanism when applying orthogonal transformations like $R$.", "D": "Certainly! Let's carefully derive the equality \\( Q_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} (K_i^{(l)})^T \\) step by step, using the given formulas and properties.\n\n### Given:\n1. The query and key vectors are defined as:\n   \\[\n   Q_i^{(l)} = W_Q^{(l)}(G_i^{(l)} - \\overline{G_i^{(l)}})\n   \\]\n   \\[\n   K_i^{(l)} = W_K^{(l)}(G_i^{(l)} - \\overline{G_i^{(l)}})\n   \\]\n   where \\( W_Q^{(l)} \\) and \\( W_K^{(l)} \\) are weight matrices, and \\( G_i^{(l)} - \\overline{G_i^{(l)}} \\) represents the centered input features.\n\n2. The matrix \\( R \\) satisfies the orthogonality condition:\n   \\[\n   R R^T = I\n   \\]\n   where \\( I \\) is the identity matrix. This implies that \\( R \\) is an orthogonal matrix.\n\n### Derivation:\nWe start by expanding \\( Q_i^{(l)} R (K_i^{(l)} R)^T \\):\n\n1. First, compute \\( (K_i^{(l)} R)^T \\):\n   \\[\n   (K_i^{(l)} R)^T = R^T (K_i^{(l)})^T\n   \\]\n   (using the property that \\( (AB)^T = B^T A^T \\) for any matrices \\( A \\) and \\( B \\)).\n\n2. Substitute this back into the original expression:\n   \\[\n   Q_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} R R^T (K_i^{(l)})^T\n   \\]\n\n3. Now, use the orthogonality condition \\( R R^T = I \\):\n   \\[\n   Q_i^{(l)} R R^T (K_i^{(l)})^T = Q_i^{(l)} I (K_i^{(l)})^T = Q_i^{(l)} (K_i^{(l)})^T\n   \\]\n\nThus, we have shown that:\n\\[\nQ_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} (K_i^{(l)})^T\n\\]\n\n### Intuition Behind the Cancellation:\nThe key insight here is that multiplying by an orthogonal matrix \\( R \\) and its transpose \\( R^T \\) preserves the inner product structure. Here's why:\n\n- The inner product of the rotated queries and keys is:\n  \\[\n  \\langle Q_i^{(l)} R, K_i^{(l)} R \\rangle = (Q_i^{(l)} R) (K_i^{(l)} R)^T = Q_i^{(l)} R R^T (K_i^{(l)})^T\n  \\]\n- Because \\( R R^T = I \\), the \\( R \\) and \\( R^T \\) effectively \"cancel out\" in the inner product, leaving the original inner product \\( Q_i^{(l)} (K_i^{(l)})^T \\).\n\nThis property is fundamental in many applications, such as attention mechanisms in transformers, where orthogonal transformations can be applied without altering the attention scores (inner products). The invariance arises because orthogonal matrices preserve lengths and angles, and thus the inner product remains unchanged.\n\n### Final Answer:\n\\[\nQ_i^{(l)} R (K_i^{(l)} R)^T = Q_i^{(l)} (K_i^{(l)})^T\n\\]\nThis equality holds because \\( R R^T = I \\), and the orthogonal transformation \\( R \\) cancels out in the inner product computation.", "ground-truth": "B"}
{"paper": "A Connection Between Score Matching and Denoising Autoencoders", "question": "Step 1 (Preconditions)\n- This shows a key relationship between the Denoising Score Matching objective \\(J_{DSM_{q_\\sigma}}\\) and the DAE reconstruction error \\(J_{DAE_\\sigma}(\\theta)\\).\n- It depends on Formula 3 for \\(J_{DSM_{q_\\sigma}}(\\theta)\\), the gradient formula 4 for \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x})\\), and the specific form of \\(\\psi(\\tilde{x}; \\theta)\\).\n- \\(J_{DAE_\\sigma}(\\theta)\\) is the mean squared reconstruction error for the denoising autoencoder with noise level \\(\\sigma\\).\n\nStep 2 (Derived Question)\n\"Based on Formula 3: \\(J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\|\\psi(\\tilde{x}; \\theta) - \\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x})\\|^2\\right]\\), Formula 4: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x}) = \\frac{1}{\\sigma^2}(x - \\tilde{x})\\), and the definition of the DAE reconstruction cost \\(J_{DAE_\\sigma}(\\theta)\\), how do we derive \\(J_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta)\\)?\"", "A": "To derive the relationship \\( J_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta) \\), we proceed as follows:\n\n1. **Substitute Formula 4 into Formula 3**:  \n   Given \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x}) = \\frac{1}{\\sigma^2}(x - \\tilde{x})\\), substitute this into \\(J_{DSM_{q_\\sigma}}\\):  \n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\left\\|\\psi(\\tilde{x}; \\theta) - \\frac{1}{\\sigma^2}(x - \\tilde{x})\\right\\|^2\\right].\n   \\]\n\n2. **Relate \\(\\psi(\\tilde{x}; \\theta)\\) to the DAE reconstruction function**:  \n   The DAE minimizes the reconstruction error \\(J_{DAE_\\sigma}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\|x - \\psi(\\tilde{x}; \\theta)\\|^2\\right]\\). Assume the optimal \\(\\psi(\\tilde{x}; \\theta)\\) is parameterized to approximate \\(x\\) as:  \n   \\[\n   \\psi(\\tilde{x}; \\theta) = \\frac{1}{\\sigma^2}\\left(f(\\tilde{x}; \\theta) - \\tilde{x}\\right),\n   \\]  \n   where \\(f(\\tilde{x}; \\theta)\\) is the DAE's reconstruction of \\(x\\) from \\(\\tilde{x}\\). This parameterization aligns \\(\\psi\\) with the score term.\n\n3. **Expand the squared norm in \\(J_{DSM_{q_\\sigma}}\\)**:  \n   Substitute \\(\\psi(\\tilde{x}; \\theta) = \\frac{1}{\\sigma^2}\\left(f(\\tilde{x}; \\theta) - \\tilde{x}\\right)\\) into the DSM objective:  \n   \\[\n   \\left\\|\\frac{1}{\\sigma^2}\\left(f(\\tilde{x}; \\theta) - \\tilde{x}\\right) - \\frac{1}{\\sigma^2}(x - \\tilde{x})\\right\\|^2 = \\frac{1}{\\sigma^4}\\left\\|f(\\tilde{x}; \\theta) - x\\right\\|^2.\n   \\]\n\n4. **Express \\(J_{DSM_{q_\\sigma}}\\) in terms of \\(J_{DAE_\\sigma}\\)**:  \n   Substitute the expanded norm back into \\(J_{DSM_{q_\\sigma}}\\):  \n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2\\sigma^4}\\|f(\\tilde{x}; \\theta) - x\\|^2\\right] = \\frac{1}{2\\sigma^4} \\underbrace{\\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\|x - f(\\tilde{x}; \\theta)\\|^2\\right]}_{J_{DAE_\\sigma}(\\theta)}.\n   \\]  \n\nThus, we conclude:  \n\\[\n\\boxed{J_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta)}.\n\\]", "B": "Substituting the given equations in the expression for \\(J_{DSM_{q_\\sigma}}\\), we get, for this choice of Parzen kernel and density model, \n$$J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x,\\tilde{x})}\\Bigl[\\tfrac{1}{2}\\bigl\\|\\psi(\\tilde{x};\\theta) - \\tfrac{1}{\\sigma^2}(x-\\tilde{x})\\bigr\\|^2\\Bigr]\\;=\\;\\frac{1}{2\\sigma^4}\\,J_{DAE_\\sigma}(\\theta).$$\nThis shows that \\(J_{DSM_{q_\\sigma}} = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}\\).", "C": "Substituting the given equations in the expression for \\(J_{DSM_{q_\\sigma}}\\), we get, for this choice of Parzen kernel and density model, \n$$J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x,\\tilde{x})}\\Bigl[\\tfrac{1}{2}\\bigl\\|\\psi(\\tilde{x};\\theta) - \\tfrac{1}{\\sigma^2}(x-\\tilde{x})\\bigr\\|^2\\Bigr]\\;=\\;\\frac{1}{2\\sigma^2}\\,J_{DAE_\\sigma}(\\theta).$$\nThis shows that \\(J_{DSM_{q_\\sigma}} = \\frac{1}{2\\sigma^2} J_{DAE_\\sigma}\\).", "D": "To derive the relationship \\( J_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta) \\), let's carefully analyze the given formulas and definitions.\n\n### Given:\n1. **Denoising Score Matching (DSM) Objective** (Formula 3):\n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\|\\psi(\\tilde{x}; \\theta) - \\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x})\\|^2\\right]\n   \\]\n2. **Gradient of the Conditional Log-Likelihood** (Formula 4):\n   \\[\n   \\nabla_{\\tilde{x}} \\log q_\\sigma(x|\\tilde{x}) = \\frac{1}{\\sigma^2}(x - \\tilde{x})\n   \\]\n3. **DAE Reconstruction Cost**:\n   The DAE reconstruction error \\( J_{DAE_\\sigma}(\\theta) \\) is typically defined as the mean squared error between the original data \\( x \\) and the reconstructed output \\( r(\\tilde{x}; \\theta) \\):\n   \\[\n   J_{DAE_\\sigma}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\|x - r(\\tilde{x}; \\theta)\\|^2\\right]\n   \\]\n   Here, \\( r(\\tilde{x}; \\theta) \\) is the reconstruction function of the DAE. For simplicity, we assume \\( \\psi(\\tilde{x}; \\theta) \\) is the output of the DAE's score network, and the reconstruction function can be expressed as:\n   \\[\n   r(\\tilde{x}; \\theta) = \\tilde{x} + \\sigma^2 \\psi(\\tilde{x}; \\theta)\n   \\]\n   This is because the DAE is trained to minimize the reconstruction error, and the optimal reconstruction under Gaussian noise is given by \\( r(\\tilde{x}; \\theta) = \\tilde{x} + \\sigma^2 \\psi(\\tilde{x}; \\theta) \\).\n\n### Derivation:\n1. Substitute Formula 4 into the DSM objective:\n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\left\\|\\psi(\\tilde{x}; \\theta) - \\frac{1}{\\sigma^2}(x - \\tilde{x})\\right\\|^2\\right]\n   \\]\n2. Expand the squared norm:\n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\left(\\|\\psi(\\tilde{x}; \\theta)\\|^2 - \\frac{2}{\\sigma^2} \\psi(\\tilde{x}; \\theta)^T (x - \\tilde{x}) + \\frac{1}{\\sigma^4}\\|x - \\tilde{x}\\|^2\\right)\\right]\n   \\]\n3. The term \\( \\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2] \\) is independent of \\( x \\), and the cross-term \\( \\mathbb{E}[\\psi(\\tilde{x}; \\theta)^T (x - \\tilde{x})] \\) can be rewritten using the reconstruction function:\n   \\[\n   \\mathbb{E}[\\psi(\\tilde{x}; \\theta)^T (x - \\tilde{x})] = \\mathbb{E}[\\psi(\\tilde{x}; \\theta)^T (x - r(\\tilde{x}; \\theta) + r(\\tilde{x}; \\theta) - \\tilde{x})]\n   \\]\n   Since \\( r(\\tilde{x}; \\theta) - \\tilde{x} = \\sigma^2 \\psi(\\tilde{x}; \\theta) \\), this simplifies to:\n   \\[\n   \\mathbb{E}[\\psi(\\tilde{x}; \\theta)^T (x - r(\\tilde{x}; \\theta))] + \\sigma^2 \\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2]\n   \\]\n   The first term \\( \\mathbb{E}[\\psi(\\tilde{x}; \\theta)^T (x - r(\\tilde{x}; \\theta))] \\) is zero because the DAE is trained to make \\( r(\\tilde{x}; \\theta) \\) the conditional mean of \\( x \\) given \\( \\tilde{x} \\), so \\( x - r(\\tilde{x}; \\theta) \\) is uncorrelated with \\( \\psi(\\tilde{x}; \\theta) \\).\n\n4. Thus, the DSM objective simplifies to:\n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\left(\\|\\psi(\\tilde{x}; \\theta)\\|^2 - \\frac{2}{\\sigma^2} \\cdot \\sigma^2 \\|\\psi(\\tilde{x}; \\theta)\\|^2 + \\frac{1}{\\sigma^4}\\|x - \\tilde{x}\\|^2\\right)\\right]\n   \\]\n   \\[\n   = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\frac{1}{2}\\left(-\\|\\psi(\\tilde{x}; \\theta)\\|^2 + \\frac{1}{\\sigma^4}\\|x - \\tilde{x}\\|^2\\right)\\right]\n   \\]\n5. The term \\( \\|x - \\tilde{x}\\|^2 \\) can be rewritten using the reconstruction error:\n   \\[\n   \\|x - \\tilde{x}\\|^2 = \\|x - r(\\tilde{x}; \\theta) + r(\\tilde{x}; \\theta) - \\tilde{x}\\|^2\n   \\]\n   \\[\n   = \\|x - r(\\tilde{x}; \\theta)\\|^2 + \\|r(\\tilde{x}; \\theta) - \\tilde{x}\\|^2 + 2 (x - r(\\tilde{x}; \\theta))^T (r(\\tilde{x}; \\theta) - \\tilde{x})\n   \\]\n   Again, the cross-term vanishes in expectation, so:\n   \\[\n   \\mathbb{E}[\\|x - \\tilde{x}\\|^2] = \\mathbb{E}[\\|x - r(\\tilde{x}; \\theta)\\|^2] + \\mathbb{E}[\\|r(\\tilde{x}; \\theta) - \\tilde{x}\\|^2]\n   \\]\n   \\[\n   = J_{DAE_\\sigma}(\\theta) + \\sigma^4 \\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2]\n   \\]\n6. Substituting back into the DSM objective:\n   \\[\n   J_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2}\\left(-\\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2] + \\frac{1}{\\sigma^4} \\left(J_{DAE_\\sigma}(\\theta) + \\sigma^4 \\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2]\\right)\\right)\n   \\]\n   \\[\n   = \\frac{1}{2}\\left(-\\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2] + \\frac{1}{\\sigma^4} J_{DAE_\\sigma}(\\theta) + \\mathbb{E}[\\|\\psi(\\tilde{x}; \\theta)\\|^2]\\right)\n   \\]\n   \\[\n   = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta)\n   \\]\n\n### Final Result:\n\\[\nJ_{DSM_{q_\\sigma}}(\\theta) = \\frac{1}{2\\sigma^4} J_{DAE_\\sigma}(\\theta)\n\\]", "ground-truth": "B"}
{"paper": "A Connection Between Score Matching and Denoising Autoencoders", "question": "Starting from the expression:\n\\[ S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right], \\]\nhow can we rewrite it using the definition \n\\[ q_\\sigma(x) = \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}? \\]", "A": "Starting from the original expression:\n\\[\nS(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right],\n\\]\nwe substitute the definition \\( q_\\sigma(x) = \\int q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\). The gradient term becomes:\n\\[\n\\partial_x q_\\sigma(x) = \\int \\partial_x q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}.\n\\]\nSubstituting this into \\( S(\\theta) \\), we have:\n\\[\nS(\\theta) = \\int \\left\\langle \\psi(x;\\theta), \\int \\partial_x q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx.\n\\]\nBy Fubini's theorem, we interchange the order of integration:\n\\[\nS(\\theta) = \\int q(\\tilde{x}) \\left( \\int \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle dx \\right) d\\tilde{x}.\n\\]\nRecognizing that \\( \\partial_x q_\\sigma(x|\\tilde{x}) = q_\\sigma(x|\\tilde{x}) \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\), the inner integral becomes an expectation over \\( q_\\sigma(x|\\tilde{x}) \\):\n\\[\n\\int \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle dx = \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\rangle \\right].\n\\]\nThus, the final expression for \\( S(\\theta) \\) is:\n\\[\nS(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right].\n\\]\nThis represents a nested expectation over \\( \\tilde{x} \\) and \\( x \\), utilizing the score function \\( \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\).", "B": "To rewrite the expression for \\( S(\\theta) \\), we start by substituting the definition of \\( q_\\sigma(x) \\) into the original equation. This gives us:\n\\[ S(\\theta) = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} q_\\sigma(x) \\right\\rangle dx = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx. \\]\nHere, we have interchanged the derivative and integral operations, assuming the conditions for doing so are met. This step is crucial for moving forward with the derivation, as it allows us to work directly with the conditional probability density function \\( q_\\sigma(x|\\tilde{x}) \\) and the marginal density \\( q(\\tilde{x}) \\).", "C": "To rewrite the expression for \\( S(\\theta) \\), we start by substituting the definition of \\( q_\\sigma(x) \\) into the original equation. This gives us:\n\\[ S(\\theta) = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} q_\\sigma(x) \\right\\rangle dx = \\int_x \\left\\langle \\psi(x;\\theta), \\int_{\\tilde{x}} \\frac{\\partial}{\\partial x} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx. \\]\nHere, we have interchanged the derivative and integral operations, assuming the conditions for doing so are met. This step is crucial for moving forward with the derivation, as it allows us to work directly with the conditional probability density function \\( q_\\sigma(x|\\tilde{x}) \\) and the marginal density \\( q(\\tilde{x}) \\).", "D": "To rewrite the expression \\( S(\\theta) \\) using the definition of \\( q_\\sigma(x) \\), we proceed as follows:\n\n1. **Given Definitions:**\n   \\[\n   S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right],\n   \\]\n   \\[\n   q_\\sigma(x) = \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}.\n   \\]\n\n2. **Compute the Gradient \\( \\partial_x q_\\sigma(x) \\):**\n   Since \\( q_\\sigma(x) \\) is defined as an integral over \\( \\tilde{x} \\), we can bring the gradient inside the integral:\n   \\[\n   \\partial_x q_\\sigma(x) = \\partial_x \\left( \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right) = \\int_{\\tilde{x}} \\partial_x q_\\sigma(x|\\tilde{x}) \\, q(\\tilde{x}) \\, d\\tilde{x}.\n   \\]\n\n3. **Substitute into \\( S(\\theta) \\):**\n   Replace \\( \\partial_x q_\\sigma(x) \\) in \\( S(\\theta) \\):\n   \\[\n   S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\int_{\\tilde{x}} \\partial_x q_\\sigma(x|\\tilde{x}) \\, q(\\tilde{x}) \\, d\\tilde{x} \\rangle \\right].\n   \\]\n\n4. **Interchange Expectation and Integral:**\n   The expectation \\( \\mathbb{E}_{q_\\sigma(x)} \\) is an integral over \\( x \\), so we can interchange the order of integration (assuming Fubini's theorem applies):\n   \\[\n   S(\\theta) = \\int_x \\langle \\psi(x;\\theta), \\int_{\\tilde{x}} \\partial_x q_\\sigma(x|\\tilde{x}) \\, q(\\tilde{x}) \\, d\\tilde{x} \\rangle \\, q_\\sigma(x) \\, dx.\n   \\]\n   This can be written as:\n   \\[\n   S(\\theta) = \\int_{\\tilde{x}} \\left( \\int_x \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle \\, q_\\sigma(x) \\, dx \\right) q(\\tilde{x}) \\, d\\tilde{x}.\n   \\]\n\n5. **Final Expression:**\n   The inner integral is an expectation over \\( q_\\sigma(x) \\), so we can write:\n   \\[\n   S(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right].\n   \\]\n\n   Alternatively, if we recognize that \\( q_\\sigma(x) \\) is the marginal distribution, we can also express it as:\n   \\[\n   S(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\log q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right],\n   \\]\n   where we used the identity \\( \\partial_x q_\\sigma(x|\\tilde{x}) = q_\\sigma(x|\\tilde{x}) \\partial_x \\log q_\\sigma(x|\\tilde{x}) \\).\n\n**Final Answer:**\n\\[\nS(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\log q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right].\n\\]", "ground-truth": "B"}
{"paper": "A Connection Between Score Matching and Denoising Autoencoders", "question": "Step 1 (Preconditions): Convert \\(q(x|\\tilde{x})\\) into \\(q(x|\\tilde{x})\\nabla_x q(x|\\tilde{x})\\). Use the identity \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\). Also introduce \\(q(x, \\tilde{x}) = q(x|\\tilde{x}) q(\\tilde{x})\\). Step 2 (Derived Question): From the given identity, how do we use \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\) to obtain the formula for \\(S(\\theta)\\): \\(S(\\theta) = \\int_x \\int_{\\tilde{x}} q(x|\\tilde{x}) \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(\\tilde{x}) d\\tilde{x} dx\\)?", "A": "To derive the formula for \\( S(\\theta) \\), we start with the given identity and apply substitutions involving the joint distribution \\( q(x, \\tilde{x}) \\). Here's the step-by-step process:\n\n1. **Apply the Identity**:  \n   Given the identity:  \n   \\[\n   \\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x}) \\nabla_x \\log q(x|\\tilde{x}),\n   \\]  \n   substitute \\( \\nabla_x q(x|\\tilde{x}) \\) in the original expression (not explicitly written but implied to involve \\( \\nabla_x q(x|\\tilde{x}) \\)) to replace the gradient term. For instance, assume the original integrand includes \\( \\langle \\psi(x;\\theta), \\nabla_x q(x|\\tilde{x}) \\rangle \\). Using the identity, this becomes:  \n   \\[\n   \\langle \\psi(x;\\theta), q(x|\\tilde{x}) \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle.\n   \\]\n\n2. **Incorporate the Joint Distribution**:  \n   The joint distribution \\( q(x, \\tilde{x}) = q(x|\\tilde{x}) q(\\tilde{x}) \\) allows rewriting the double integral over \\( x \\) and \\( \\tilde{x} \\). After substitution, the integrand contains \\( q(x|\\tilde{x}) \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\) multiplied by \\( q(\\tilde{x}) \\). Combining these terms explicitly preserves the structure of the integral as:  \n   \\[\n   \\int_x \\int_{\\tilde{x}} q(x|\\tilde{x}) \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(\\tilde{x}) \\, d\\tilde{x} \\, dx.\n   \\]\n\n3. **Final Expression**:  \n   The above steps directly yield the formula for \\( S(\\theta) \\):  \n   \\[\n   S(\\theta) = \\int_x \\int_{\\tilde{x}} q(x|\\tilde{x}) \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(\\tilde{x}) \\, d\\tilde{x} \\, dx.\n   \\]\n\n**Key Insight**: The identity converts the gradient of the conditional distribution into a product of the distribution itself and its log-gradient, naturally introducing the \\( q(x|\\tilde{x}) \\) factor. The joint distribution \\( q(x, \\tilde{x}) = q(x|\\tilde{x}) q(\\tilde{x}) \\) ensures the integral accounts for both \\( x \\) and \\( \\tilde{x} \\), leading to the final expression for \\( S(\\theta) \\).", "B": "Starting with the identity \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\), we can express the score function \\(S(\\theta)\\) as follows: \\[ S(\\theta) = \\int_x \\int_{\\tilde{x}} \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q_\\sigma(\\tilde{x}|x) q(\\tilde{x}) d\\tilde{x} dx. \\] This can be rewritten in terms of expectation over the joint distribution \\(q_\\sigma(x, \\tilde{x})\\) as: \\[ S(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle\\right]. \\] This derivation shows how the given identity is used to transition from the gradient of the conditional probability to the expectation form of the score function, incorporating the joint distribution \\(q_\\sigma(x, \\tilde{x})\\).", "C": "Starting with the identity \\(\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x})\\nabla_x \\log q(x|\\tilde{x})\\), we can express the score function \\(S(\\theta)\\) as follows: \\[ S(\\theta) = \\int_x \\int_{\\tilde{x}} \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) d\\tilde{x} dx. \\] This can be rewritten in terms of expectation over the joint distribution \\(q_\\sigma(x, \\tilde{x})\\) as: \\[ S(\\theta) = \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\\left[\\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle\\right]. \\] This derivation shows how the given identity is used to transition from the gradient of the conditional probability to the expectation form of the score function, incorporating the joint distribution \\(q_\\sigma(x, \\tilde{x})\\).", "D": "Let's proceed step-by-step to derive the formula for \\(S(\\theta)\\) using the given identity and preconditions.\n\n### Step 1: Rewrite \\(\\nabla_x q(x|\\tilde{x})\\) using the identity\nGiven the identity:\n\\[\n\\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x}) \\nabla_x \\log q(x|\\tilde{x})\n\\]\nWe can express \\(q(x|\\tilde{x}) \\nabla_x q(x|\\tilde{x})\\) as:\n\\[\nq(x|\\tilde{x}) \\nabla_x q(x|\\tilde{x}) = q(x|\\tilde{x}) \\cdot \\left( q(x|\\tilde{x}) \\nabla_x \\log q(x|\\tilde{x}) \\right) = q(x|\\tilde{x})^2 \\nabla_x \\log q(x|\\tilde{x})\n\\]\n\nHowever, this intermediate step isn't directly necessary for deriving \\(S(\\theta)\\). Instead, we'll use the identity to substitute \\(\\nabla_x \\log q(x|\\tilde{x})\\) in the expression for \\(S(\\theta)\\).\n\n### Step 2: Express \\(S(\\theta)\\) using the joint distribution\nGiven the precondition \\(q(x, \\tilde{x}) = q(x|\\tilde{x}) q(\\tilde{x})\\), we can rewrite integrals involving the joint distribution. The formula for \\(S(\\theta)\\) is:\n\\[\nS(\\theta) = \\int_x \\int_{\\tilde{x}} q(x|\\tilde{x}) \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(\\tilde{x}) \\, d\\tilde{x} \\, dx\n\\]\nUsing the joint distribution, this becomes:\n\\[\nS(\\theta) = \\int_x \\int_{\\tilde{x}} \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(x, \\tilde{x}) \\, d\\tilde{x} \\, dx\n\\]\n\n### Step 3: Substitute the identity into \\(S(\\theta)\\)\nFrom the identity \\(\\nabla_x \\log q(x|\\tilde{x}) = \\frac{\\nabla_x q(x|\\tilde{x})}{q(x|\\tilde{x})}\\), we substitute:\n\\[\nS(\\theta) = \\int_x \\int_{\\tilde{x}} \\langle \\psi(x;\\theta), \\frac{\\nabla_x q(x|\\tilde{x})}{q(x|\\tilde{x})} \\rangle q(x, \\tilde{x}) \\, d\\tilde{x} \\, dx\n\\]\nSimplify using \\(q(x, \\tilde{x}) = q(x|\\tilde{x}) q(\\tilde{x})\\):\n\\[\nS(\\theta) = \\int_x \\int_{\\tilde{x}} \\langle \\psi(x;\\theta), \\nabla_x q(x|\\tilde{x}) \\rangle q(\\tilde{x}) \\, d\\tilde{x} \\, dx\n\\]\n\n### Step 4: Final expression\nThis shows how the given identity is used to express \\(S(\\theta)\\) in terms of \\(\\nabla_x q(x|\\tilde{x})\\). The final formula for \\(S(\\theta)\\) is:\n\\[\nS(\\theta) = \\int_x \\int_{\\tilde{x}} q(x|\\tilde{x}) \\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle q(\\tilde{x}) \\, d\\tilde{x} \\, dx\n\\]\n\n### Key Observations:\n1. The identity \\(\\nabla_x \\log q(x|\\tilde{x}) = \\frac{\\nabla_x q(x|\\tilde{x})}{q(x|\\tilde{x})}\\) allows us to rewrite the score function (gradient of the log-density) in terms of \\(\\nabla_x q(x|\\tilde{x})\\).\n2. The joint distribution \\(q(x, \\tilde{x})\\) simplifies the integration by factoring into \\(q(x|\\tilde{x}) q(\\tilde{x})\\).\n3. The inner product \\(\\langle \\psi(x;\\theta), \\nabla_x \\log q_\\sigma(x|\\tilde{x}) \\rangle\\) captures the interaction between the model \\(\\psi(x;\\theta)\\) and the score of the conditional distribution.\n\nThus, the derivation relies on substituting the identity and using the joint distribution to express \\(S(\\theta)\\) in the desired form.", "ground-truth": "C"}
{"paper": "arxiv:2309.04475", "question": "How can we demonstrate that the wrapped normal distribution $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ simplifies to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ through index shifting in the infinite sum? Provide the detailed reindexing process that leads to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$", "A": "To demonstrate that the wrapped normal distribution $$\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2)$$ simplifies to $$\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)$$ through index shifting in the infinite sum, follow the detailed reindexing process below:\n\n---\n\n**Step 1: Expand the Wrapped Normal Distribution**\n\nThe wrapped normal distribution is defined as an infinite sum of Gaussian densities over periodic intervals. Explicitly, the given distribution expands to:\n$$\n\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + k' + 2\\pi m \\mid \\mu + k'', \\sigma^2).\n$$\n\n**Step 2: Rewrite the Gaussian Density Terms**\n\nEach term in the sum is a Gaussian density:\n$$\n\\mathcal{N}(x + k' + 2\\pi m \\mid \\mu + k'', \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x + k' + 2\\pi m - \\mu - k'')^2}{2\\sigma^2}}.\n$$\n\n**Step 3: Simplify the Exponent**\n\nFactor the terms in the exponent:\n$$\n(x + k' + 2\\pi m - \\mu - k'')^2 = \\left(x - \\mu + (k' - k'') + 2\\pi m\\right)^2.\n$$\n\n**Step 4: Reindex the Summation**\n\nLet \\( n = m + \\frac{k' - k''}{2\\pi} \\). However, since \\( k' \\) and \\( k'' \\) are integers (or integer multiples of \\( 2\\pi \\) in angular terms), we can instead redefine the summation index as:\n$$\nn = m + \\Delta,\n$$\nwhere \\( \\Delta = \\frac{k' - k''}{2\\pi} \\), ensuring \\( n \\) remains an integer. This substitution shifts the index \\( m \\) to absorb the constants \\( k' \\) and \\( k'' \\).\n\n**Step 5: Substitute and Simplify**\n\nSubstitute \\( m = n - \\Delta \\) into the exponent:\n$$\nx - \\mu + (k' - k'') + 2\\pi(n - \\Delta) = x - \\mu + 2\\pi n.\n$$\nThe Gaussian term becomes:\n$$\n\\mathcal{N}(x - \\mu + 2\\pi n \\mid 0, \\sigma^2) = \\mathcal{N}(x + 2\\pi n \\mid \\mu, \\sigma^2).\n$$\n\n**Step 6: Rewrite the Sum**\n\nThe original sum now transforms to:\n$$\n\\sum_{n=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi n \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n$$\n\n---\n\n**Conclusion**  \nBy reindexing \\( n = m + \\Delta \\), the constants \\( k' \\) and \\( k'' \\) are absorbed into the summation index, leaving the sum invariant. This confirms that:\n$$\n\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n$$  \nThe wrapped normal distribution retains its form under integer shifts of the variable and mean due to its inherent periodicity.", "B": "To demonstrate that the wrapped normal distribution \\(\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2)\\) simplifies to \\(\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)\\) through index shifting in the infinite sum, we will carefully reindex the summation. Here is the detailed derivation:\n\n### Definition of the Wrapped Normal Distribution\nThe wrapped normal distribution is given by:\n\\[\n\\mathcal{N}_w(x \\mid \\mu, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2),\n\\]\nwhere \\(\\mathcal{N}(x \\mid \\mu, \\sigma^2)\\) is the standard normal density function:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\n\\]\n\n### Starting Point\nWe begin with the expression:\n\\[\n\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + k' + 2\\pi m \\mid \\mu + k'', \\sigma^2).\n\\]\nHere, \\(k'\\) and \\(k''\\) are arbitrary shifts (they can be integer multiples of \\(2\\pi\\) or other values, but for the wrapped normal distribution, shifts by \\(2\\pi\\) are periodic).\n\n### Reindexing Process\nTo simplify, let \\(k' = 2\\pi n\\) and \\(k'' = 2\\pi p\\), where \\(n, p \\in \\mathbb{Z}\\). This is because the wrapped normal distribution is periodic with period \\(2\\pi\\). Now, substitute these into the sum:\n\\[\n\\mathcal{N}_w(x + 2\\pi n \\mid \\mu + 2\\pi p, \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi n + 2\\pi m \\mid \\mu + 2\\pi p, \\sigma^2).\n\\]\n\nCombine the terms in the argument:\n\\[\n= \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi(m + n) \\mid \\mu + 2\\pi p, \\sigma^2).\n\\]\n\nNow, perform a change of variables in the summation index. Let \\(k = m + n - p\\). Then \\(m = k - n + p\\), and the sum becomes:\n\\[\n= \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi(k - p + n) \\mid \\mu + 2\\pi p, \\sigma^2).\n\\]\n\nSimplify the argument:\n\\[\nx + 2\\pi(k - p + n) = x + 2\\pi k - 2\\pi p + 2\\pi n.\n\\]\nThe mean is \\(\\mu + 2\\pi p\\), so the normal density becomes:\n\\[\n\\mathcal{N}(x + 2\\pi k - 2\\pi p + 2\\pi n \\mid \\mu + 2\\pi p, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x + 2\\pi k + 2\\pi n - 2\\pi p - \\mu - 2\\pi p)^2}{2\\sigma^2}\\right).\n\\]\nSimplify the exponent:\n\\[\n(x + 2\\pi k + 2\\pi n - 2\\pi p - \\mu - 2\\pi p) = (x - \\mu) + 2\\pi k + 2\\pi n - 4\\pi p.\n\\]\nHowever, this seems overly complicated. Instead, let's reconsider the reindexing step.\n\n### Correct Reindexing\nLet’s reindex more carefully. Start from:\n\\[\n\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi n + 2\\pi m \\mid \\mu + 2\\pi p, \\sigma^2).\n\\]\nLet \\(k = m + n - p\\). Then \\(m = k - n + p\\), and the sum becomes:\n\\[\n\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi(k - n + p) + 2\\pi n \\mid \\mu + 2\\pi p, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k + 2\\pi p \\mid \\mu + 2\\pi p, \\sigma^2).\n\\]\nNow, the argument of the normal distribution is \\(x + 2\\pi k + 2\\pi p\\), and the mean is \\(\\mu + 2\\pi p\\). The term \\(2\\pi p\\) cancels out in the difference:\n\\[\n(x + 2\\pi k + 2\\pi p) - (\\mu + 2\\pi p) = x - \\mu + 2\\pi k.\n\\]\nThus, the density becomes:\n\\[\n\\mathcal{N}(x + 2\\pi k + 2\\pi p \\mid \\mu + 2\\pi p, \\sigma^2) = \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2).\n\\]\nSubstituting back, the sum is:\n\\[\n\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n\\]\n\n### Conclusion\nThrough this reindexing, we have shown that:\n\\[\n\\mathcal{N}_w(x + 2\\pi n \\mid \\mu + 2\\pi p, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n\\]\nThis demonstrates the periodicity and shift invariance of the wrapped normal distribution. For arbitrary shifts \\(k'\\) and \\(k''\\) that are integer multiples of \\(2\\pi\\), the distribution simplifies to the original form \\(\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)\\). \n\nThus, the wrapped normal distribution is invariant under shifts of \\(2\\pi\\) in both the variable \\(x\\) and the mean \\(\\mu\\). \n\n\\[\n\\boxed{\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2) \\quad \\text{for } k', k'' \\in 2\\pi\\mathbb{Z}.}\n\\]", "C": "To show that the wrapped normal distribution $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ simplifies to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ by shifting indices in the infinite sum, follow these steps:\n\n1. Start with the definition of the wrapped normal distribution:\n   \\[\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + k' + k \\mid \\mu + k'', \\sigma^2).\\]\n\n2. Perform a change of variables by setting \\( m = k + k' - k'' \\). This reindexes the sum as:\n   \\[\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + k' + k \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m + k'' \\mid \\mu + k'', \\sigma^2).\\]\n\n3. Utilize the property of the normal distribution to simplify the expression inside the sum:\n   \\[\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m + k'' \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu, \\sigma^2).\\]\n\n4. Recognize that the resulting sum is the definition of the wrapped normal distribution for \\( x \\):\n   \\[\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\\]\n\nThus, through careful reindexing and simplification, we've shown that $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ indeed collapses to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$.", "D": "To show that the wrapped normal distribution $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ simplifies to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ by shifting indices in the infinite sum, follow these steps:\n\n1. Start with the definition of the wrapped normal distribution:\n   \\[\\mathcal{N}_w(x + k' \\mid \\mu + k'', \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + k' + k \\mid \\mu + k'', \\sigma^2).\\]\n\n2. Perform a change of variables by setting \\( m = k + k' \\). This reindexes the sum as:\n   \\[\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + k' + k \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu + k'', \\sigma^2).\\]\n\n3. Utilize the property of the normal distribution to simplify the expression inside the sum:\n   \\[\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu + k'', \\sigma^2) = \\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu, \\sigma^2).\\]\n\n4. Recognize that the resulting sum is the definition of the wrapped normal distribution for \\( x \\):\n   \\[\\sum_{m=-\\infty}^{\\infty} \\mathcal{N}(x + m \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\\]\n\nThus, through careful reindexing and simplification, we've shown that $$\\mathcal{N}_w(x + k'\\mid\\mu + k'',\\sigma^2)$$ indeed collapses to $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$.", "ground-truth": "C"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}[Wasserstein Bound]\nAssume $\\Omega$ is convex and the velocity model $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$ (uniformly on $t$). \nLet $p_{\\bm{\\theta},t}(\\bm{x})$ be the probability path of the reflected CNF induced by $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ starting from the same prior distribution $p_0(\\bm{x})$.\nThen the squared Wasserstein-2 distance between $p_{1}(\\bm{x})$ and $p_{\\bm{\\theta},1}(\\bm{x})$ is bounded by\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\n\\end{theorem}", "A": "We will prove the Wasserstein bound theorem step-by-step. The key idea is to relate the Wasserstein distance between the true and learned distributions at time $t=1$ to the RFM (Reflected Flow Matching) loss $\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})$.\n\n### Step 1: Flow Representation\nLet $\\phi_t(\\bm{x})$ and $\\phi_{\\bm{\\theta},t}(\\bm{x})$ be the true and learned flow maps respectively, which satisfy:\n\\[\n\\frac{d}{dt}\\phi_t(\\bm{x}) = \\bm{v}_t(\\phi_t(\\bm{x})), \\quad \\phi_0(\\bm{x}) = \\bm{x}\n\\]\n\\[\n\\frac{d}{dt}\\phi_{\\bm{\\theta},t}(\\bm{x}) = \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t), \\quad \\phi_{\\bm{\\theta},0}(\\bm{x}) = \\bm{x}\n\\]\n\n### Step 2: Difference Between Flows\nDefine the difference between trajectories:\n\\[\n\\bm{u}_t(\\bm{x}) = \\phi_t(\\bm{x}) - \\phi_{\\bm{\\theta},t}(\\bm{x})\n\\]\nIts time derivative is:\n\\[\n\\frac{d}{dt}\\bm{u}_t(\\bm{x}) = \\bm{v}_t(\\phi_t(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\n\\]\n\n### Step 3: Decompose the Velocity Difference\nAdd and subtract $\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x}))$:\n\\[\n\\frac{d}{dt}\\bm{u}_t(\\bm{x}) = [\\bm{v}_t(\\phi_t(\\bm{x})) - \\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x}))] + [\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)]\n\\]\n\n### Step 4: Apply Lipschitz Condition\nSince $\\bm{v}_{\\bm{\\theta}}$ is $M$-Lipschitz in $\\bm{x}$ and $\\Omega$ is convex, we have:\n\\[\n\\|\\bm{v}_t(\\phi_t(\\bm{x})) - \\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x}))\\| \\leq M\\|\\bm{u}_t(\\bm{x})\\|\n\\]\nThus:\n\\[\n\\frac{d}{dt}\\|\\bm{u}_t(\\bm{x})\\| \\leq M\\|\\bm{u}_t(\\bm{x})\\| + \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\|\n\\]\n\n### Step 5: Apply Grönwall's Inequality\nThis differential inequality implies:\n\\[\n\\|\\bm{u}_1(\\bm{x})\\| \\leq e^M \\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\| dt\n\\]\n\n### Step 6: Relate to Wasserstein Distance\nThe squared Wasserstein-2 distance is bounded by:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq \\mathbb{E}_{p_0(\\bm{x})}[\\|\\phi_1(\\bm{x}) - \\phi_{\\bm{\\theta},1}(\\bm{x})\\|^2] = \\mathbb{E}_{p_0(\\bm{x})}[\\|\\bm{u}_1(\\bm{x})\\|^2]\n\\]\nUsing the inequality from Step 5:\n\\[\n\\|\\bm{u}_1(\\bm{x})\\|^2 \\leq e^{2M} \\left(\\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\| dt\\right)^2\n\\]\nBy Jensen's inequality:\n\\[\n\\|\\bm{u}_1(\\bm{x})\\|^2 \\leq e^{2M} \\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\|^2 dt\n\\]\n\n### Step 7: Connect to RFM Loss\nThe RFM loss is defined as:\n\\[\n\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\mathbb{E}_{t, p_t(\\bm{x})}[\\|\\bm{v}_t(\\bm{x}) - \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2]\n\\]\nUsing the change of variables $\\bm{x} = \\phi_{\\bm{\\theta},t}(\\bm{x}_0)$:\n\\[\n\\mathbb{E}_{p_0(\\bm{x})}\\left[\\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\|^2 dt\\right] = \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\n\\]\n\n### Step 8: Final Bound\nCombining all results:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq e^{2M} \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) \\int_0^1 dt = e^{2M} \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\n\\]\nThe constant can be tightened to $e^{1+2M}$ by more careful analysis of the integration constant, giving the final bound:\n\\[\nW_2^2(p_1(\\bm{x}), p_{\\bm{\\theta},1}(\\bm{x})) \\leq e^{1+2M} \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\n\\]\n\nThis completes the proof. The key was tracking the difference between flow trajectories and relating it to the velocity matching loss through Grönwall's inequality.", "B": "To prove the Wasserstein Bound theorem, we analyze the squared Wasserstein-2 distance between the true distribution \\(p_1(\\bm{x})\\) and the model distribution \\(p_{\\bm{\\theta},1}(\\bm{x})\\) induced by the velocity field \\(\\bm{v}_{\\bm{\\theta}}\\). The key steps involve comparing the particle trajectories governed by the true and model velocity fields, leveraging Gronwall's inequality, and bounding the accumulated error via the RFM loss.\n\n**Step 1: Trajectory Difference Dynamics**\n\nLet \\(\\bm{\\phi}_t(\\bm{x}_0)\\) and \\(\\bm{\\psi}_t(\\bm{x}_0)\\) denote the true and model trajectories starting from \\(\\bm{x}_0 \\sim p_0\\), satisfying:\n\\[\n\\frac{d\\bm{\\phi}_t}{dt} = \\bm{v}^*(\\bm{\\phi}_t,t), \\quad \\frac{d\\bm{\\psi}_t}{dt} = \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t),\n\\]\nwith \\(\\bm{\\phi}_0 = \\bm{\\psi}_0 = \\bm{x}_0\\). Define the displacement \\(\\bm{\\delta}_t = \\bm{\\phi}_t - \\bm{\\psi}_t\\). Its dynamics are:\n\\[\n\\frac{d\\bm{\\delta}_t}{dt} = \\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t).\n\\]\n\n**Step 2: Differential Inequality for \\(\\|\\bm{\\delta}_t\\|^2\\)**\n\nCompute the time derivative of \\(\\|\\bm{\\delta}_t\\|^2\\):\n\\[\n\\frac{d}{dt}\\|\\bm{\\delta}_t\\|^2 = 2\\bm{\\delta}_t \\cdot \\left[\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t)\\right].\n\\]\nDecompose the velocity difference:\n\\[\n\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t) = \\underbrace{\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t)}_{\\text{Model error}} + \\underbrace{\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t)}_{\\text{Lipschitz term}}.\n\\]\nUsing the \\(M\\)-Lipschitz condition of \\(\\bm{v}_{\\bm{\\theta}}\\) and Cauchy-Schwarz:\n\\[\n\\left\\|\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t)\\right\\| \\leq M\\|\\bm{\\delta}_t\\|, \\quad \\bm{\\delta}_t \\cdot \\left(\\bm{v}^* - \\bm{v}_{\\bm{\\theta}}\\right) \\leq \\|\\bm{\\delta}_t\\|\\|\\bm{v}^* - \\bm{v}_{\\bm{\\theta}}\\|.\n\\]\nThus:\n\\[\n\\frac{d}{dt}\\|\\bm{\\delta}_t\\|^2 \\leq 2\\|\\bm{\\delta}_t\\|\\|\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t)\\| + 2M\\|\\bm{\\delta}_t\\|^2.\n\\]\nApply the inequality \\(2ab \\leq a^2 + b^2\\) (with \\(a = \\|\\bm{\\delta}_t\\|\\), \\(b = \\|\\bm{v}^* - \\bm{v}_{\\bm{\\theta}}\\|\\)):\n\\[\n\\frac{d}{dt}\\|\\bm{\\delta}_t\\|^2 \\leq (1 + 2M)\\|\\bm{\\delta}_t\\|^2 + \\|\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t)\\|^2.\n\\]\n\n**Step 3: Gronwall's Inequality**\n\nLet \\(y(t) = \\mathbb{E}[\\|\\bm{\\delta}_t\\|^2]\\). Taking expectations and applying Gronwall's inequality:\n\\[\ny(t) \\leq e^{(1 + 2M)t} \\int_0^t e^{-(1 + 2M)s} \\mathbb{E}\\left[\\|\\bm{v}^*(\\bm{\\phi}_s,s) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_s,s)\\|^2\\right] ds.\n\\]\nAt \\(t = 1\\):\n\\[\ny(1) \\leq e^{1 + 2M} \\int_0^1 \\mathbb{E}\\left[\\|\\bm{v}^*(\\bm{\\phi}_s,s) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_s,s)\\|^2\\right] ds = e^{1 + 2M} \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}).\n\\]\n\n**Step 4: Relating to Wasserstein Distance**\n\nThe squared Wasserstein-2 distance is bounded by the expectation of the squared displacement:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq \\mathbb{E}[\\|\\bm{\\phi}_1 - \\bm{\\psi}_1\\|^2] = y(1).\n\\]\nCombining results:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq e^{1 + 2M} \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}).\n\\]\nThis completes the proof.", "C": "We first give an alternative expression of the reflected ODE\n\\begin{subequations}\n\\arraycolsep=1.8pt\n\\def\\arraystretch{1.5}\n\\begin{align}\n\\mathrm{d}\\bm{\\phi}_t(\\bm{x}) &= \\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t,\\\\\n\\bm{\\phi}_0(\\bm{x}) &= \\bm{x},\n\\end{align}\n\\end{subequations}\nwhere the initial point $\\bm{x}\\sim p_0(\\bm{x})$, $\\bm{n}(\\bm{x})$ is the inward unit normal vector at $\\bm{x}$ on $\\partial\\Omega$, and $l_t$ is non-decreasing in $t$ with $l_0=0$.\nMoreover, $l_t$ satisfies $\\int_{0}^t \\mathbb{I}(\\bm{\\phi}_s(\\bm{x})\\notin\\partial\\Omega)\\dif l_s=0$ for $t>0$, i.e., $\\dif l_t$ vanishes in the interior and will push the velocity along $\\bm{n}(\\bm{x})$ when the trajectory hits the boundary.\n\nLet $\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})$ (with a reflection term $\\bar{l}_t$) be the solution to the reflected ODE with velocity field $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$.\nBy the definition of Wasserstein-2 distance, we have\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq \\int_{\\Omega}\\|\\bm{\\phi}_1(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},1}(\\bm{x})\\|^2 p_0(\\bm{x})\\dif \\bm{x}:= \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=1}.\n\\end{equation}\nTake the derivative of $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)$ w.r.t. $t$ gives\n\\begin{equation}\n\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\n=2\\int_{\\Omega}\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right) p_0(\\bm{x})\\dif \\bm{x}\n\\end{equation}\nNote that\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left[(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t-\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\right]\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t \\\\\n&\\quad\\quad\\quad\\quad\\quad+ \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t - \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t,\n\\end{align*}\nThe first term can be bounded by\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n=&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t))+\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n\\leq & \\frac{1}{2}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2+\\frac{1}{2}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2 + M\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\n\\end{align*}\nwhere in the last inequality we use the mean inequality and the fact that $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$.\nTo handle the second term, note that $dl_t>0$ only if $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$. When $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$, we know\n$\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\leq 0$ due to the convexity of $\\Omega$.\nTherefore, the second term satisfies\n\\[\n\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t\\leq 0.\n\\]\nUsing the same argument, we know that the third term satisfies\n\\[\n-\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\leq 0\n\\]\nTherefore, we have the following bound for $\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t$\n\\begin{align*}\n&\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t\\\\\n\\leq & \\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2p_0(\\bm{x})\\dif \\bm{x}+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x} + 2M\\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\np_0(\\bm{x})\\dif \\bm{x}\\\\\n=&(1+2M)\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}.\n\\end{align*}\nBy Gronwall's inequality and $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=0}=0$, we have\n\\begin{equation}\n\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right) \\leq e^{1+2M}\\int_0^1\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}\\dif t=e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\nBy combining the above equations, we finally conclude\n\\[\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nThis finishes the proof.", "D": "We first give an alternative expression of the reflected ODE\n\\begin{subequations}\n\\arraycolsep=1.8pt\n\\def\\arraystretch{1.5}\n\\begin{align}\n\\mathrm{d}\\bm{\\phi}_t(\\bm{x}) &= \\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t,\\\\\n\\bm{\\phi}_0(\\bm{x}) &= \\bm{x},\n\\end{align}\n\\end{subequations}\nwhere the initial point $\\bm{x}\\sim p_0(\\bm{x})$, $\\bm{n}(\\bm{x})$ is the inward unit normal vector at $\\bm{x}$ on $\\partial\\Omega$, and $l_t$ is non-decreasing in $t$ with $l_0=0$.\nMoreover, $l_t$ satisfies $\\int_{0}^t \\mathbb{I}(\\bm{\\phi}_s(\\bm{x})\\notin\\partial\\Omega)\\dif l_s=0$ for $t>0$, i.e., $\\dif l_t$ vanishes in the interior and will push the velocity along $\\bm{n}(\\bm{x})$ when the trajectory hits the boundary.\n\nLet $\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})$ (with a reflection term $\\bar{l}_t$) be the solution to the reflected ODE with velocity field $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$.\nBy the definition of Wasserstein-2 distance, we have\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq \\int_{\\Omega}\\|\\bm{\\phi}_1(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},1}(\\bm{x})\\|^2 p_0(\\bm{x})\\dif \\bm{x}:= \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=1}.\n\\end{equation}\nTake the derivative of $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)$ w.r.t. $t$ gives\n\\begin{equation}\n\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\n=2\\int_{\\Omega}\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right) p_0(\\bm{x})\\dif \\bm{x}\n\\end{equation}\nNote that\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left[(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t-\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\right]\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t \\\\\n&\\quad\\quad\\quad\\quad\\quad+ \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t - \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t,\n\\end{align*}\nThe first term can be bounded by\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n=&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t))+\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n\\leq & \\frac{1}{2}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2+\\frac{1}{2}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2 + M\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\n\\end{align*}\nwhere in the last inequality we use the mean inequality and the fact that $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$.\nTo handle the second term, note that $dl_t>0$ only if $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$. When $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$, we know\n$\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\leq 0$ due to the convexity of $\\Omega$.\nTherefore, the second term satisfies\n\\[\n\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t\\leq 0.\n\\]\nUsing the same argument, we know that the third term satisfies\n\\[\n-\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\leq 0\n\\]\nTherefore, we have the following bound for $\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t$\n\\begin{align*}\n&\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t\\\\\n\\leq & \\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2p_0(\\bm{x})\\dif \\bm{x}+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x} + 2M\\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\np_0(\\bm{x})\\dif \\bm{x}\\\\\n=&(1+2M)\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}.\n\\end{align*}\nBy Gronwall's inequality and $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=0}=0$, we have\n\\begin{equation}\n\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right) \\leq e^{2+2M}\\int_0^1\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}\\dif t=e^{2+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\nBy combining the above equations, we finally conclude\n\\[\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{2+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nThis finishes the proof.", "ground-truth": "C"}
{"paper": "arxiv:2309.04475", "question": "How can we prove the following lemma: 'Consider the generation Markov process defined by \\( p(x_0) = \\int p(x_{0:T-1}\\mid x_T)\\,dx_{1:T} \\). If the prior distribution \\( p(x_T) \\) is G-invariant and the Markov transitions \\( p(x_{t-1}\\mid x_t) \\) for \\( 0 < t \\le T \\) are G-equivariant, then the marginal distribution \\( p(x_0) \\) is also G-invariant.'? Please outline why the property of G-invariance propagates backward through the Markov chain.", "A": "To prove the lemma, we demonstrate that the G-invariance of \\( p(x_T) \\) and G-equivariance of the Markov transitions propagate invariance backward through the chain, ultimately making \\( p(x_0) \\) G-invariant. Here's the step-by-step reasoning:\n\n---\n\n**Definitions**:\n- **G-invariance**: For \\( g \\in G \\), \\( p(g \\cdot x) = p(x) \\).\n- **G-equivariance**: For \\( g \\in G \\), \\( p(g \\cdot x_{t-1} \\mid g \\cdot x_t) = p(x_{t-1} \\mid x_t) \\).\n\n---\n\n**Proof**:\n1. **Start with the transformed marginal**:\n   For any \\( g \\in G \\), consider the transformed distribution \\( p(g \\cdot x_0) \\):\n   \\[\n   p(g \\cdot x_0) = \\int p(g \\cdot x_0, x_{1:T}) \\, dx_{1:T}.\n   \\]\n   Using the generative process \\( p(x_{0:T}) = p(x_T) \\prod_{t=1}^T p(x_{t-1} \\mid x_t) \\), substitute \\( x_t \\to g \\cdot x_t \\):\n   \\[\n   p(g \\cdot x_0) = \\int p(g \\cdot x_T) \\prod_{t=1}^T p(g \\cdot x_{t-1} \\mid g \\cdot x_t) \\, dx_{1:T}.\n   \\]\n\n2. **Apply G-invariance of \\( p(x_T) \\)**:\n   Since \\( p(x_T) \\) is G-invariant, \\( p(g \\cdot x_T) = p(x_T) \\).\n\n3. **Apply G-equivariance of transitions**:\n   For each transition \\( p(x_{t-1} \\mid x_t) \\), G-equivariance implies:\n   \\[\n   p(g \\cdot x_{t-1} \\mid g \\cdot x_t) = p(x_{t-1} \\mid x_t).\n   \\]\n   Thus, the product term becomes:\n   \\[\n   \\prod_{t=1}^T p(g \\cdot x_{t-1} \\mid g \\cdot x_t) = \\prod_{t=1}^T p(x_{t-1} \\mid x_t).\n   \\]\n\n4. **Reconstruct the original integral**:\n   Substitute these results back into the expression for \\( p(g \\cdot x_0) \\):\n   \\[\n   p(g \\cdot x_0) = \\int p(x_T) \\prod_{t=1}^T p(x_{t-1} \\mid x_t) \\, dx_{1:T} = p(x_0).\n   \\]\n\n5. **Conclusion**:\n   Since \\( p(g \\cdot x_0) = p(x_0) \\) holds for all \\( g \\in G \\), \\( p(x_0) \\) is G-invariant.\n\n---\n\n**Key Intuition**:\n- G-equivariant transitions ensure that transformations propagate coherently through the chain.\n- G-invariant priors ensure symmetry is preserved at the terminal state \\( x_T \\).\n- Combining these properties, symmetry propagates backward to \\( x_0 \\), making \\( p(x_0) \\) invariant.", "B": "To prove that the marginal distribution \\( p(x_0) \\) is G-invariant under the given conditions, we can follow these steps:\n\n1. Understand G-invariance and G-equivariance: A distribution \\( p(x) \\) is G-invariant if for any group action \\( g \\in G \\), \\( p(gx) = p(x) \\). A Markov transition \\( p(x_{t-1}\\mid x_t) \\) is G-equivariant if \\( p(gx_{t-1}\\mid gx_t) = p(x_{t-1}\\mid x_t) \\) for all \\( g \\in G \\).\n\n2. Start with the prior distribution: Given that \\( p(x_T) \\) is G-invariant, we have \\( p(gx_T) = p(x_T) \\) for any \\( g \\in G \\).\n\n3. Propagate G-invariance through the Markov chain: For each transition \\( p(x_{t-1}\\mid x_t) \\), because it is G-equivariant, applying a group action \\( g \\) to \\( x_t \\) and \\( x_{t-1} \\) leaves the transition probability unchanged. This means that the G-invariance property is preserved at each step of the Markov chain.\n\n4. Integrate over the Markov chain: The marginal distribution \\( p(x_0) \\) is obtained by integrating over all possible paths \\( x_{1:T} \\). Since each component in the integrand (the prior and the transitions) preserves G-invariance, the resulting marginal distribution \\( p(x_0) \\) must also be G-invariant.\n\n5. Conclusion: Therefore, under the conditions that the prior distribution is G-invariant and the Markov transitions are G-equivariant, the marginal distribution \\( p(x_0) \\) inherits the G-invariance property, completing the proof.", "C": "To prove the lemma, we need to show that the marginal distribution \\( p(x_0) \\) is \\( G \\)-invariant given the assumptions that the prior \\( p(x_T) \\) is \\( G \\)-invariant and the Markov transitions \\( p(x_{t-1} \\mid x_t) \\) are \\( G \\)-equivariant for \\( 0 < t \\le T \\). Here is the step-by-step reasoning:\n\n### Definitions:\n1. **\\( G \\)-invariance**: A distribution \\( p(x) \\) is \\( G \\)-invariant if for any group action \\( g \\in G \\), \\( p(g \\cdot x) = p(x) \\).\n2. **\\( G \\)-equivariance**: A conditional distribution \\( p(x_{t-1} \\mid x_t) \\) is \\( G \\)-equivariant if for any \\( g \\in G \\), \\( p(g \\cdot x_{t-1} \\mid g \\cdot x_t) = p(x_{t-1} \\mid x_t) \\).\n\n### Proof Outline:\n1. **Express the marginal \\( p(x_0) \\)**:\n   \\[\n   p(x_0) = \\int p(x_{0:T-1} \\mid x_T) p(x_T) \\, dx_{1:T}.\n   \\]\n   This can be expanded using the Markov property:\n   \\[\n   p(x_0) = \\int \\left( \\prod_{t=1}^T p(x_{t-1} \\mid x_t) \\right) p(x_T) \\, dx_{1:T}.\n   \\]\n\n2. **Apply the group action \\( g \\cdot x_0 \\)**:\n   We need to show \\( p(g \\cdot x_0) = p(x_0) \\). Substitute \\( g \\cdot x_0 \\) into the integral:\n   \\[\n   p(g \\cdot x_0) = \\int \\left( \\prod_{t=1}^T p(g \\cdot x_{t-1} \\mid x_t) \\right) p(x_T) \\, dx_{1:T}.\n   \\]\n   However, this substitution is not directly valid because \\( g \\) acts on the entire sequence. Instead, we perform a change of variables \\( x_t' = g \\cdot x_t \\) for all \\( t \\). The Jacobian determinant is 1 (since group actions are measure-preserving), so:\n   \\[\n   p(g \\cdot x_0) = \\int \\left( \\prod_{t=1}^T p(g \\cdot x_{t-1} \\mid g \\cdot x_t) \\right) p(g \\cdot x_T) \\, dx_{1:T}'.\n   \\]\n\n3. **Use \\( G \\)-invariance of \\( p(x_T) \\) and \\( G \\)-equivariance of transitions**:\n   - By \\( G \\)-invariance of \\( p(x_T) \\), \\( p(g \\cdot x_T) = p(x_T) \\).\n   - By \\( G \\)-equivariance of \\( p(x_{t-1} \\mid x_t) \\), \\( p(g \\cdot x_{t-1} \\mid g \\cdot x_t) = p(x_{t-1} \\mid x_t) \\).\n   Substituting these properties:\n   \\[\n   p(g \\cdot x_0) = \\int \\left( \\prod_{t=1}^T p(x_{t-1} \\mid x_t) \\right) p(x_T) \\, dx_{1:T}'.\n   \\]\n   Since \\( dx_{1:T}' = dx_{1:T} \\) (measure-preserving action), this simplifies to:\n   \\[\n   p(g \\cdot x_0) = \\int \\left( \\prod_{t=1}^T p(x_{t-1} \\mid x_t) \\right) p(x_T) \\, dx_{1:T} = p(x_0).\n   \\]\n\n### Conclusion:\nThe marginal distribution \\( p(x_0) \\) inherits \\( G \\)-invariance from the prior \\( p(x_T) \\) and the \\( G \\)-equivariant transitions \\( p(x_{t-1} \\mid x_t) \\). The property propagates backward through the Markov chain because the group action commutes with the transitions and the prior, preserving the invariance at each step.", "D": "To prove that the marginal distribution \\( p(x_0) \\) is G-invariant under the given conditions, we can follow these steps:\n\n1. Understand G-invariance and G-equivariance: A distribution \\( p(x) \\) is G-invariant if for any group action \\( g \\in G \\), \\( p(gx) = p(x) \\). A Markov transition \\( p(x_{t-1}\\mid x_t) \\) is G-equivariant if \\( p(gx_{t-1}\\mid gx_t) = p(x_{t-1}\\mid x_t) \\) for all \\( g \\in G \\).\n\n2. Start with the prior distribution: Given that \\( p(x_T) \\) is G-invariant, we have \\( p(gx_T) = p(x_T) \\) for any \\( g \\in G \\).\n\n3. Propagate G-invariance through the Markov chain: For each transition \\( p(x_{t-1}\\mid x_t) \\), because it is G-equivariant, applying a group action \\( g \\) to \\( x_t \\) and \\( x_{t-1} \\) leaves the transition probability unchanged. This means that the G-invariance property is preserved at each step of the Markov chain.\n\n4. Integrate over the Markov chain: The marginal distribution \\( p(x_0) \\) is obtained by integrating over all possible paths \\( x_{1:T} \\). Since each component in the integrand (the prior and the transitions) preserves G-invariance, the resulting marginal distribution \\( p(x_0) \\) must also be G-invariant.\n\n5. Conclusion: Therefore, under the conditions that the prior distribution is G-invariant and the Markov transitions are G-equivariant, the marginal distribution \\( p(x_0) \\) inherits the G-invariance property, provided the group action \\( g \\) is invertible. \n\n\\boxed{}", "ground-truth": "B"}
{"paper": "arxiv:2303.10876", "question": "Given that $R$ is an orthonormal rotation matrix and $t$ is a translation vector, demonstrate how the columnwise $\\ell_2$-distance $||G_i^{(0)} - G_j^{(0)}||_{2,col}$ remains invariant under Euclidean transformations by showing $||g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)||_2 = ||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$.", "A": "Certainly! Let's demonstrate the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations. \n\nGiven:\n- $R$ is an orthonormal rotation matrix, meaning $R^T R = I$ (where $I$ is the identity matrix).\n- $t$ is a translation vector.\n- $g_{i,c}^{(0)}$ and $g_{j,c}^{(0)}$ are column vectors representing points in the original space.\n\nWe want to show that:\n\\[\n\\|g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)\\|_2 = \\|g_{i,c}^{(0)} - g_{j,c}^{(0)}\\|_2.\n\\]\n\n**Proof:**\n\n1. Simplify the left-hand side (LHS):\n   \\[\n   \\|g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)\\|_2 = \\|g_{i,c}^{(0)} R - g_{j,c}^{(0)} R\\|_2.\n   \\]\n   The translation vectors $t$ cancel out.\n\n2. Factor out $R$:\n   \\[\n   \\|g_{i,c}^{(0)} R - g_{j,c}^{(0)} R\\|_2 = \\|(g_{i,c}^{(0)} - g_{j,c}^{(0)}) R\\|_2.\n   \\]\n\n3. Use the property of the $\\ell_2$-norm under orthonormal transformations:\n   For any vector $v$ and orthonormal matrix $R$, $\\|v R\\|_2 = \\|v\\|_2$. This is because:\n   \\[\n   \\|v R\\|_2^2 = (v R)^T (v R) = v^T R^T R v = v^T I v = v^T v = \\|v\\|_2^2.\n   \\]\n   Taking square roots gives $\\|v R\\|_2 = \\|v\\|_2$.\n\n4. Apply this property to our expression:\n   \\[\n   \\|(g_{i,c}^{(0)} - g_{j,c}^{(0)}) R\\|_2 = \\|g_{i,c}^{(0)} - g_{j,c}^{(0)}\\|_2.\n   \\]\n\nThus, we have shown:\n\\[\n\\|g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)\\|_2 = \\|g_{i,c}^{(0)} - g_{j,c}^{(0)}\\|_2.\n\\]\n\nThis proves that the columnwise $\\ell_2$-distance is invariant under Euclidean transformations (rotation and translation).", "B": "To demonstrate the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations, consider two points $g_{i,c}^{(0)}$ and $g_{j,c}^{(0)}$ under the transformation $g \\mapsto gR + t$, where $R$ is an orthonormal rotation matrix and $t$ is a translation vector. The transformed points are $g_{i,c}^{(0)}R + t$ and $g_{j,c}^{(0)}R + t$. The distance between them is computed as:\n\n\\[\n\\begin{aligned}\n||(g_{i,c}^{(0)}R + t) - (g_{j,c}^{(0)}R + t)||_2 &= ||(g_{i,c}^{(0)} - g_{j,c}^{(0)})R + (t - t)||_2 \\\\\n&= ||(g_{i,c}^{(0)} - g_{j,c}^{(0)})R||_2.\n\\end{aligned}\n\\]\n\nSince $R$ is orthonormal ($R^\\top R = I$), it preserves the $\\ell_2$-norm of any vector. For a vector $v = g_{i,c}^{(0)} - g_{j,c}^{(0)}$, we have:\n\n\\[\n||vR||_2 = \\sqrt{(vR)(vR)^\\top} = \\sqrt{vRR^\\top v^\\top} = \\sqrt{vIv^\\top} = \\sqrt{vv^\\top} = ||v||_2.\n\\]\n\nSubstituting $v = g_{i,c}^{(0)} - g_{j,c}^{(0)}$, it follows that:\n\n\\[\n||(g_{i,c}^{(0)} - g_{j,c}^{(0)})R||_2 = ||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2.\n\\]\n\nThus, the columnwise $\\ell_2$-distance is invariant under Euclidean transformations.", "C": "To prove the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations, consider the transformation of the c-th column of matrices $G_i^{(0)}$ and $G_j^{(0)}$ by an orthonormal rotation matrix $R$ and a translation vector $t$. The calculation proceeds as follows:\n\n1. Apply the Euclidean Transformation: Each column is transformed as $g_{i,c}^{(0)} \\rightarrow g_{i,c}^{(0)} R + t$ and similarly for $g_{j,c}^{(0)}$.\n\n2. Compute the Difference: The difference between the transformed columns is $(g_{i,c}^{(0)} R + t) - (g_{j,c}^{(0)} R + t) = (g_{i,c}^{(0)} - g_{j,c}^{(0)}) R$.\n\n3. Calculate the Norm: The $\\ell_2$-norm of the difference is $||(g_{i,c}^{(0)} - g_{j,c}^{(0)}) R||_2$. Due to the orthonormality of $R$, this simplifies to $||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$, because multiplying by an orthonormal matrix preserves the norm.\n\nThus, we have shown that $||g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)||_2 = ||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$, proving the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations.", "D": "To prove the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations, consider the transformation of the c-th column of matrices $G_i^{(0)}$ and $G_j^{(0)}$ by an orthonormal rotation matrix $R$ and a translation vector $t$. The calculation proceeds as follows:\n\n1. Apply the Euclidean Transformation: Each column is transformed as $g_{i,c}^{(0)} \\rightarrow g_{i,c}^{(0)} R + t$ and similarly for $g_{j,c}^{(0)}$.\n\n2. Compute the Difference: The difference between the transformed columns is $(g_{i,c}^{(0)} R + t) - (g_{j,c}^{(0)} R + t) = (g_{i,c}^{(0)} - g_{j,c}^{(0)}) R$.\n\n3. Calculate the Norm: The $\\ell_2$-norm of the difference is $||(g_{i,c}^{(0)} - g_{j,c}^{(0)}) R||_2$. Due to the orthonormality of $R$, this simplifies to $||g_{i,c}^{(0)} - g_{j,c}^{(0)}||_2$, because multiplying by an orthonormal matrix preserves the norm.\n\nThus, we have shown that $||g_{i,c}^{(0)} R + t - (g_{j,c}^{(0)} R + t)||_2 = ||g_{i,c}^{(0)} + g_{j,c}^{(0)}||_2$, proving the invariance of the columnwise $\\ell_2$-distance under Euclidean transformations.", "ground-truth": "C"}
{"paper": "arxiv:2407.10490", "question": "In our analysis of learning dynamics for neural network finetuning, we consider a model with logits \\(z = h_{\\theta}(x)\\) and a probability distribution defined by \\(\\pi = \\mathrm{Softmax}(z)\\). The model parameters are updated via gradient descent on a loss function \\(L(x,y)\\) with an update \\(\\Delta\\theta = -\\eta\\,\\nabla_{\\theta} L(x_u,y_u)\\), where \\((x_u,y_u)\\) is the updating example. For an observing example \\(x_o\\), the one-step change in the log-probability is given by \\(\\Delta\\log\\pi_t(y \\mid x_o)\\).\n\nProposition 1 states that\n\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = -\\eta\\, A_t(x_o)\\, K_t(x_o,x_u)\\, G_t(x_u,y_u) + O\\Big(\\eta^2\\,\\|\\nabla_{\\theta}z(x_u)\\|^2_{op}\\Big),\n\\]\n\nwhere\n\n- \\(A_t(x_o) = \\nabla_z \\log \\pi_{\\theta_t}(x_o) = I - 1\\,\\pi_{\\theta_t}^\\top(x_o)\\),\n- \\(K_t(x_o,x_u) = \\Big(\\nabla_{\\theta}z(x_o)\\Big|_{\\theta_t}\\Big)\\Big(\\nabla_{\\theta}z(x_u)\\Big|_{\\theta_t}\\Big)^\\top\\) is the empirical neural tangent kernel, and\n- \\(G_t(x_u,y_u) = \\nabla_z L(x_u,y_u)\\big|_{z_t}\\).\n\nCould you provide a complete and detailed proof of Proposition 1?", "A": "Certainly! Below is a detailed proof of **Proposition 1**, which describes the one-step change in the log-probability \\(\\Delta \\log \\pi_t(y \\mid x_o)\\) under gradient descent updates.\n\n---\n\n### **Proof of Proposition 1**\n\nWe analyze the change in the log-probability \\(\\log \\pi(y \\mid x_o)\\) after a single gradient descent step. \n\n#### **1. Setup and Notation**\n- Let the logits be \\(z = h_{\\theta}(x)\\), and the probability distribution be \\(\\pi = \\mathrm{Softmax}(z)\\).\n- The loss function is \\(L(x_u, y_u)\\), and the gradient descent update is:\n  \\[\n  \\Delta \\theta = -\\eta \\nabla_{\\theta} L(x_u, y_u).\n  \\]\n- The one-step change in the log-probability is:\n  \\[\n  \\Delta \\log \\pi_t(y \\mid x_o) = \\log \\pi_{\\theta_t + \\Delta \\theta}(y \\mid x_o) - \\log \\pi_{\\theta_t}(y \\mid x_o).\n  \\]\n\n#### **2. First-Order Taylor Expansion**\nWe approximate the change using a first-order Taylor expansion:\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) \\approx \\nabla_{\\theta} \\log \\pi_{\\theta_t}(y \\mid x_o)^\\top \\Delta \\theta + O(\\eta^2 \\|\\nabla_{\\theta} z(x_u)\\|^2_{op}).\n\\]\nSubstituting \\(\\Delta \\theta\\):\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) \\approx -\\eta \\nabla_{\\theta} \\log \\pi_{\\theta_t}(y \\mid x_o)^\\top \\nabla_{\\theta} L(x_u, y_u) + O(\\eta^2 \\|\\nabla_{\\theta} z(x_u)\\|^2_{op}).\n\\]\n\n#### **3. Chain Rule for \\(\\nabla_{\\theta} \\log \\pi_{\\theta_t}(y \\mid x_o)\\)**\nThe gradient of the log-probability with respect to \\(\\theta\\) is:\n\\[\n\\nabla_{\\theta} \\log \\pi_{\\theta_t}(y \\mid x_o) = \\nabla_z \\log \\pi_{\\theta_t}(y \\mid x_o) \\cdot \\nabla_{\\theta} z(x_o).\n\\]\nFor the softmax function, the gradient of the log-probability with respect to the logits is:\n\\[\n\\nabla_z \\log \\pi_{\\theta_t}(y \\mid x_o) = A_t(x_o) = I - \\mathbf{1} \\pi_{\\theta_t}^\\top(x_o),\n\\]\nwhere \\(I\\) is the identity matrix, and \\(\\mathbf{1}\\) is a vector of ones. This follows because:\n\\[\n\\frac{\\partial \\log \\pi(y \\mid x_o)}{\\partial z_j} = \\delta_{y j} - \\pi(j \\mid x_o).\n\\]\n\n#### **4. Chain Rule for \\(\\nabla_{\\theta} L(x_u, y_u)\\)**\nThe gradient of the loss with respect to \\(\\theta\\) is:\n\\[\n\\nabla_{\\theta} L(x_u, y_u) = \\nabla_z L(x_u, y_u) \\cdot \\nabla_{\\theta} z(x_u).\n\\]\nLet \\(G_t(x_u, y_u) = \\nabla_z L(x_u, y_u)|_{z_t}\\).\n\n#### **5. Combining the Terms**\nSubstituting these into the expression for \\(\\Delta \\log \\pi_t(y \\mid x_o)\\):\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) \\approx -\\eta \\left( A_t(x_o) \\nabla_{\\theta} z(x_o) \\right)^\\top \\left( G_t(x_u, y_u) \\nabla_{\\theta} z(x_u) \\right) + O(\\eta^2 \\|\\nabla_{\\theta} z(x_u)\\|^2_{op}).\n\\]\nRearranging:\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) \\approx -\\eta A_t(x_o) \\left( \\nabla_{\\theta} z(x_o) \\nabla_{\\theta} z(x_u)^\\top \\right) G_t(x_u, y_u) + O(\\eta^2 \\|\\nabla_{\\theta} z(x_u)\\|^2_{op}).\n\\]\nThe term \\(\\nabla_{\\theta} z(x_o) \\nabla_{\\theta} z(x_u)^\\top\\) is the **empirical neural tangent kernel (NTK)**:\n\\[\nK_t(x_o, x_u) = \\nabla_{\\theta} z(x_o) \\nabla_{\\theta} z(x_u)^\\top.\n\\]\n\n#### **6. Final Expression**\nThus, the one-step change in the log-probability is:\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = -\\eta A_t(x_o) K_t(x_o, x_u) G_t(x_u, y_u) + O(\\eta^2 \\|\\nabla_{\\theta} z(x_u)\\|^2_{op}).\n\\]\n\nThis completes the proof of **Proposition 1**.\n\n---\n\n### **Summary of Key Steps**\n1. **Taylor expansion** of \\(\\log \\pi(y \\mid x_o)\\) around \\(\\theta_t\\).\n2. **Chain rule** to express gradients of \\(\\log \\pi\\) and \\(L\\) in terms of \\(\\nabla_{\\theta} z\\).\n3. **Softmax gradient** identity for \\(A_t(x_o)\\).\n4. **Neural Tangent Kernel (NTK)** emerges as \\(\\nabla_{\\theta} z(x_o) \\nabla_{\\theta} z(x_u)^\\top\\).\n5. **Final assembly** of terms, accounting for higher-order errors.\n\nThis proof rigorously connects gradient descent updates to changes in the model's predictive probabilities via the NTK.", "B": "Suppose we want to observe the model's prediction on an \"observing example\" \\(x_o\\). Starting from Equation (2), we first approximate \\(\\log \\pi_{t+1}(y \\mid x_o)\\) using first-order Taylor expansion (we use \\(\\pi_t\\) to represent \\(\\pi_{\\theta_t}\\) interchangeably for notation conciseness):\n\n\\[\n\\log \\pi_{t+1}(y \\mid x_o) = \\log \\pi_t(y \\mid x_o) + \\langle \\nabla \\log \\pi_t(y \\mid x_o), \\theta_{t+1} - \\theta_t \\rangle + O(\\|\\theta_{t+1} - \\theta_t\\|^2),\n\\]\n\nThen, assuming the model updates its parameters using SGD calculated by an \"updating example\" \\((x_u,y_u)\\), we can rearrange the terms in the above equation to get the following expression:\n\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = \\log \\pi_{t+1}(y \\mid x_o) - \\log \\pi_t(y \\mid x_o) = \\nabla_\\theta \\log \\pi_t(y \\mid x_o)\\big|_{\\theta_t} (\\theta_{t+1} - \\theta_t) + O(\\|\\theta_{t+1} - \\theta_t\\|^2),\n\\]\n\nwhere \\(d\\) is the number of parameters of the model. To evaluate the leading term, we plug in the definition of SGD and repeatedly use the chain rule:\n\n\\[\n\\begin{aligned}\n\\nabla_\\theta \\log \\pi_t(y \\mid x_o)\\big|_{\\theta_t} (\\theta_{t+1} - \\theta_t) & = \\Big( \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\Big) \\Big( -\\eta \\nabla_\\theta L(x_u)\\big|_{\\theta_t} \\Big)^\\top \\\\\n& = \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\Big( -\\eta \\nabla_z L(x_u)\\big|_{z_t} \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\Big) \\\\\n& = -\\eta\\, \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\Big[ \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\Big] \\; \\Big( \\nabla_z L(x_u)\\big|_{z_t} \\Big)^\\top \\\\\n& = -\\eta\\, A_t(x_o)\\, K_t(x_o,x_u)\\, G_t(x_u,y_u).\n\\end{aligned}\n\\]\n\nFor the higher-order term, using as above that\n\n\\[\n\\theta_{t+1} - \\theta_t = -\\eta \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\; G_t(x_u,y_u),\n\\]\n\nand noting that, since the residual term \\(G_t\\) is usually bounded (and the practical algorithms will also use gradient clipping to avoid too large gradients), we have that\n\n\\[\nO(\\|\\theta_{t+1} - \\theta_t\\|^2) = O\\Big(\\eta^2 \\; \\big\\|\\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top\\big\\|^2_{\\text{op}} \\; \\|G_t(x_u,y_u)\\|^2_{\\text{op}}\\Big) = O\\Big(\\eta^2 \\; \\|\\nabla_\\theta z(x_u)\\|^2_{\\text{op}}\\Big).\n\\]\n", "C": "To prove Proposition 1, we analyze the first-order Taylor expansion of the log-probability change under gradient descent parameter updates. Here is the detailed derivation:\n\n---\n\n**Step 1: Parameter Update via Gradient Descent**\n\nThe parameters are updated as:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(x_u, y_u)\n\\]\nwhere the gradient of the loss is computed via the chain rule:\n\\[\n\\nabla_\\theta L(x_u, y_u) = \\underbrace{\\nabla_\\theta z(x_u)}_{\\text{Jacobian}} \\cdot \\underbrace{\\nabla_z L(x_u, y_u)}_{G_t(x_u, y_u)}.\n\\]\n\n**Step 2: First-Order Change in Log-Probability**\n\nThe change in log-probability for observation \\(x_o\\) is approximated by:\n\\[\n\\Delta \\log \\pi(y \\mid x_o) \\approx \\nabla_\\theta \\log \\pi(y \\mid x_o)^\\top \\Delta\\theta + \\mathcal{O}(\\eta^2 \\|\\Delta\\theta\\|^2).\n\\]\nSubstituting \\(\\Delta\\theta = -\\eta \\nabla_\\theta L(x_u, y_u)\\):\n\\[\n\\Delta \\log \\pi(y \\mid x_o) = -\\eta \\cdot \\nabla_\\theta \\log \\pi(y \\mid x_o)^\\top \\nabla_\\theta L(x_u, y_u) + \\mathcal{O}(\\eta^2 \\|\\nabla_\\theta z(x_u)\\|_{\\text{op}}^2).\n\\]\n\n**Step 3: Decompose Gradients Using Chain Rule**\n\n1. **Gradient of \\(\\log \\pi(y \\mid x_o)\\):**\n   \\[\n   \\nabla_\\theta \\log \\pi(y \\mid x_o) = \\underbrace{\\nabla_z \\log \\pi(y \\mid x_o)}_{A_t(x_o)} \\cdot \\underbrace{\\nabla_\\theta z(x_o)}_{\\text{Jacobian}}.\n   \\]\n   For the Softmax derivative:\n   \\[\n   \\nabla_z \\log \\pi(y \\mid x) = I - \\mathbf{1} \\pi^\\top(x), \\quad \\text{where } A_t(x_o) = I - \\mathbf{1} \\pi_{\\theta_t}^\\top(x_o).\n   \\]\n\n2. **Gradient of \\(L(x_u, y_u)\\):**\n   \\[\n   \\nabla_\\theta L(x_u, y_u) = \\underbrace{\\nabla_\\theta z(x_u)}_{\\text{Jacobian}} \\cdot \\underbrace{\\nabla_z L(x_u, y_u)}_{G_t(x_u, y_u)}.\n   \\]\n\n**Step 4: Combine Terms and Identify NTK**\n\nSubstitute the gradients into \\(\\Delta \\log \\pi(y \\mid x_o)\\):\n\\[\n\\Delta \\log \\pi(y \\mid x_o) = -\\eta \\cdot \\underbrace{\\nabla_z \\log \\pi(y \\mid x_o)}_{A_t(x_o)} \\cdot \\underbrace{\\nabla_\\theta z(x_o) \\nabla_\\theta z(x_u)^\\top}_{K_t(x_o, x_u)} \\cdot \\underbrace{\\nabla_z L(x_u, y_u)}_{G_t(x_u, y_u)} + \\mathcal{O}(\\eta^2 \\|\\nabla_\\theta z(x_u)\\|_{\\text{op}}^2).\n\\]\nThis matches the form in Proposition 1:\n\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = -\\eta \\, A_t(x_o) K_t(x_o, x_u) G_t(x_u, y_u) + \\mathcal{O}(\\eta^2 \\|\\nabla_\\theta z(x_u)\\|_{\\text{op}}^2).\n\\]\n\n---\n\n**Key Components:**\n1. **Jacobian Alignment**: The term \\(\\nabla_\\theta z(x_o) \\nabla_\\theta z(x_u)^\\top\\) defines the empirical Neural Tangent Kernel \\(K_t(x_o, x_u)\\), measuring parameter-space similarity between \\(x_o\\) and \\(x_u\\).\n2. **Log-Probability Gradient**: \\(A_t(x_o)\\) captures the sensitivity of \\(\\log \\pi(y \\mid x_o)\\) to changes in \\(z\\), structured by the Softmax derivative.\n3. **Loss Gradient**: \\(G_t(x_u, y_u)\\) propagates the loss gradient through the logits.\n\nThe \\(\\mathcal{O}(\\eta^2)\\) residual term arises from higher-order Taylor terms, dominated by the operator norm of the Jacobian \\(\\nabla_\\theta z(x_u)\\).", "D": "\\[\n\\Delta \\log \\pi_t(y \\mid x_o) = \\log \\pi_{t+1}(y \\mid x_o) - \\log \\pi_t(y \\mid x_o) = \\nabla_\\theta \\log \\pi_t(y \\mid x_o)\\big|_{\\theta_t} (\\theta_{t+1} - \\theta_t) + O(\\|\\theta_{t+1} - \\theta_t\\|^2),\n\\]\n\nwhere \\(d\\) is the number of parameters of the model. To evaluate the leading term, we plug in the definition of SGD and repeatedly use the chain rule:\n\n\\[\n\\begin{aligned}\n\\nabla_\\theta \\log \\pi_t(y \\mid x_o)\\big|_{\\theta_t} (\\theta_{t+1} - \\theta_t) & = \\Big( \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\Big) \\Big( -\\eta \\nabla_\\theta L(x_u)\\big|_{\\theta_t} \\Big)^\\top \\\\\n& = \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\Big( -\\eta \\nabla_z L(x_u)\\big|_{z_t} \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\Big) \\\\\n& = -\\eta\\, \\nabla_z \\log \\pi_t(x_o)\\big|_{z_t} \\; \\Big[ \\nabla_\\theta z_t(x_o)\\big|_{\\theta_t} \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\Big] \\; \\Big( \\nabla_z L(x_u)\\big|_{z_t} \\Big) \\\\\n& = -\\eta\\, A_t(x_o)\\, K_t(x_o,x_u)\\, G_t(x_u,y_u).\n\\end{aligned}\n\\]\n\nFor the higher-order term, using as above that\n\n\\[\n\\theta_{t+1} - \\theta_t = -\\eta \\; \\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top \\; G_t(x_u,y_u),\n\\]\n\nand noting that, since the residual term \\(G_t\\) is usually bounded (and the practical algorithms will also use gradient clipping to avoid too large gradients), we have that\n\n\\[\nO(\\|\\theta_{t+1} - \\theta_t\\|^2) = O\\Big(\\eta^2 \\; \\big\\|\\big(\\nabla_\\theta z_t(x_u)\\big|_{\\theta_t}\\big)^\\top\\big\\|^2_{\\text{op}} \\; \\|G_t(x_u,y_u)\\|^2_{\\text{op}}\\Big) = O\\Big(\\eta^2 \\; \\|\\nabla_\\theta z(x_u)\\|^2_{\\text{op}}\\Big).\n\\]", "ground-truth": "B"}
{"paper": "arxiv:2407.10490", "question": "Consider a large language model (LLM) being fine-tuned using gradient descent with a small learning rate \\(\\eta\\). The model outputs logits \\(z = h_{\\theta}(\\chi)\\) given an input \\(\\chi\\) (which may represent a concatenation of a prompt and a response), and the output probability is computed via the softmax function as \\(\\pi_{\\theta}(y\\mid\\chi) = \\mathrm{Softmax}(z)\\). In this setting, the per-step change in the log-probability of an output token when performing an update on an updating sample \\((x_u, y_u)\\) (or more generally \\(\\chi_u\\)) is approximated by the decomposition\n\\[\n\\Delta \\log \\pi_t(y \\mid \\chi_o) = -\\eta\\, A_t(\\chi_o)\\, K_t(\\chi_o, \\chi_u)\\, G_t(\\chi_u) + O(\\eta^2),\n\\]\nwhere\n- \\(A_t(\\chi_o) = \\nabla_z \\log \\pi_{\\theta}(\\chi_o)\\) is the sensitivity matrix at an observation point \\(\\chi_o\\),\n- \\(K_t(\\chi_o, \\chi_u) = \\big(\\nabla_{\\theta} z(\\chi_o)\\big)\\big(\\nabla_{\\theta} z(\\chi_u)\\big)^\\top\\) is the empirical neural tangent kernel (eNTK), and\n- \\(G_t(\\chi_u) = \\nabla_z L(\\chi_u)\\big|_{z_t}\\) is the residual term corresponding to the gradient of the loss with respect to the logits, evaluated at \\(\\chi_u\\).\n\nFor supervised fine-tuning (SFT), the loss is defined as the negative log-likelihood (NLL) over the chosen response \\(y_u^+\\), i.e.,\n\\[\nL_{\\mathrm{SFT}}(\\chi_u) = -\\sum_{l=1}^{L} \\log \\pi_{\\theta}(y_l^+ \\mid \\chi_u),\n\\]\nwhile for Direct Preference Optimization (DPO) the loss involves comparing the chosen response \\(y_u^+\\) and a rejected response \\(y_u^-\\) (with a margin parameter \\(\\beta\\)) via terms such as\n\\[\nL_{\\mathrm{DPO}}(\\theta) = -\\mathbb{E}\\Big[\\log \\sigma\\Big(\\beta\\,\\Big(\\log \\frac{\\pi_{\\theta}(y_u^+\\mid \\chi_u^+)}{\\pi_{\\theta}(y_u^-\\mid \\chi_u^-)} - \\log \\frac{\\pi_{\\mathrm{ref}}(y_u^+\\mid \\chi_u^+)}{\\pi_{\\mathrm{ref}}(y_u^-\\mid \\chi_u^-)}\\Big)\\Big)\\Big],\n\\]\nwith the teacher forcing mechanism applied in both cases so that the target sequence is given.\n\nCould you derive the explicit expression for the residual term \\(G_t(\\chi_u)\\) for these LLM fine-tuning algorithms (SFT and DPO), including a clear statement of all symbols, definitions, and necessary assumptions?", "A": "Certainly! Below, I derive the explicit expressions for the residual term \\( G_t(\\chi_u) \\) for both **Supervised Fine-Tuning (SFT)** and **Direct Preference Optimization (DPO)**. \n\n---\n\n### **1. Supervised Fine-Tuning (SFT)**\n\n#### **Definitions and Setup**\n- **Input:** \\(\\chi_u\\) (a sequence of tokens, typically a prompt concatenated with a response).\n- **Target:** \\(y_u^+ = (y_1^+, \\dots, y_L^+)\\) (the desired response sequence).\n- **Loss Function (NLL):**\n  \\[\n  L_{\\mathrm{SFT}}(\\chi_u) = -\\sum_{l=1}^L \\log \\pi_{\\theta}(y_l^+ \\mid \\chi_u).\n  \\]\n- **Logits:** \\(z = h_{\\theta}(\\chi_u)\\) (model output before softmax).\n- **Softmax Probability:**\n  \\[\n  \\pi_{\\theta}(y_l^+ \\mid \\chi_u) = \\frac{e^{z_{y_l^+}}}{\\sum_{y'} e^{z_{y'}}}.\n  \\]\n\n#### **Derivation of \\(G_t(\\chi_u)\\)**\nThe residual term \\(G_t(\\chi_u)\\) is the gradient of the loss with respect to the logits \\(z\\):\n\\[\nG_t(\\chi_u) = \\nabla_z L_{\\mathrm{SFT}}(\\chi_u).\n\\]\n\nCompute the gradient for a single token \\(y_l^+\\):\n\\[\n\\nabla_z \\log \\pi_{\\theta}(y_l^+ \\mid \\chi_u) = \\nabla_z \\left( z_{y_l^+} - \\log \\sum_{y'} e^{z_{y'}} \\right).\n\\]\n\\[\n= \\mathbf{1}_{y_l^+} - \\pi_{\\theta}(y \\mid \\chi_u),\n\\]\nwhere \\(\\mathbf{1}_{y_l^+}\\) is a one-hot vector with \\(1\\) at the position of \\(y_l^+\\) and \\(0\\) elsewhere.\n\nSumming over all tokens in the sequence:\n\\[\nG_t(\\chi_u) = \\nabla_z L_{\\mathrm{SFT}}(\\chi_u) = -\\sum_{l=1}^L \\left( \\mathbf{1}_{y_l^+} - \\pi_{\\theta}(y \\mid \\chi_u) \\right).\n\\]\n\n#### **Final Expression for SFT**\n\\[\nG_t(\\chi_u) = \\sum_{l=1}^L \\left( \\pi_{\\theta}(y \\mid \\chi_u) - \\mathbf{1}_{y_l^+} \\right).\n\\]\n\n---\n\n### **2. Direct Preference Optimization (DPO)**\n\n#### **Definitions and Setup**\n- **Input:** \\(\\chi_u^+\\) (prompt + chosen response) and \\(\\chi_u^-\\) (prompt + rejected response).\n- **Loss Function (DPO):**\n  \\[\n  L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\beta \\left( \\log \\frac{\\pi_{\\theta}(y_u^+ \\mid \\chi_u^+)}{\\pi_{\\theta}(y_u^- \\mid \\chi_u^-)} - \\log \\frac{\\pi_{\\mathrm{ref}}(y_u^+ \\mid \\chi_u^+)}{\\pi_{\\mathrm{ref}}(y_u^- \\mid \\chi_u^-)} \\right) \\right),\n  \\]\n  where \\(\\sigma\\) is the sigmoid function, \\(\\beta\\) is a temperature parameter, and \\(\\pi_{\\mathrm{ref}}\\) is a reference model.\n\n- **Logits:** \\(z^+ = h_{\\theta}(\\chi_u^+)\\), \\(z^- = h_{\\theta}(\\chi_u^-)\\).\n- **Probabilities:** \\(\\pi_{\\theta}(y_u^+ \\mid \\chi_u^+)\\), \\(\\pi_{\\theta}(y_u^- \\mid \\chi_u^-)\\).\n\n#### **Derivation of \\(G_t(\\chi_u)\\)**\nFirst, define the **log-ratio**:\n\\[\nr(\\theta) = \\log \\frac{\\pi_{\\theta}(y_u^+ \\mid \\chi_u^+)}{\\pi_{\\theta}(y_u^- \\mid \\chi_u^-)} - \\log \\frac{\\pi_{\\mathrm{ref}}(y_u^+ \\mid \\chi_u^+)}{\\pi_{\\mathrm{ref}}(y_u^- \\mid \\chi_u^-)}.\n\\]\n\nThe DPO loss becomes:\n\\[\nL_{\\mathrm{DPO}} = -\\log \\sigma(\\beta r(\\theta)).\n\\]\n\nThe gradient with respect to the logits is:\n\\[\nG_t(\\chi_u) = \\nabla_z L_{\\mathrm{DPO}} = -\\nabla_z \\log \\sigma(\\beta r(\\theta)).\n\\]\n\nUsing the chain rule:\n\\[\n\\nabla_z \\log \\sigma(\\beta r(\\theta)) = \\beta \\sigma(-\\beta r(\\theta)) \\nabla_z r(\\theta).\n\\]\n\nNow, compute \\(\\nabla_z r(\\theta)\\):\n\\[\nr(\\theta) = \\log \\pi_{\\theta}(y_u^+ \\mid \\chi_u^+) - \\log \\pi_{\\theta}(y_u^- \\mid \\chi_u^-) + \\text{(reference terms, constant w.r.t. } \\theta).\n\\]\n\nThe gradient of \\(\\log \\pi_{\\theta}(y \\mid \\chi)\\) with respect to \\(z\\) is:\n\\[\n\\nabla_z \\log \\pi_{\\theta}(y \\mid \\chi) = \\mathbf{1}_y - \\pi_{\\theta}(y \\mid \\chi).\n\\]\n\nThus:\n\\[\n\\nabla_z r(\\theta) = \\left( \\mathbf{1}_{y_u^+} - \\pi_{\\theta}(y \\mid \\chi_u^+) \\right) - \\left( \\mathbf{1}_{y_u^-} - \\pi_{\\theta}(y \\mid \\chi_u^-) \\right).\n\\]\n\n#### **Final Expression for DPO**\n\\[\nG_t(\\chi_u) = -\\beta \\sigma(-\\beta r(\\theta)) \\left[ \\left( \\mathbf{1}_{y_u^+} - \\pi_{\\theta}(y \\mid \\chi_u^+) \\right) - \\left( \\mathbf{1}_{y_u^-} - \\pi_{\\theta}(y \\mid \\chi_u^-) \\right) \\right].\n\\]\n\n---\n\n### **Summary**\n- **SFT:**  \n  \\[\n  G_t(\\chi_u) = \\sum_{l=1}^L \\left( \\pi_{\\theta}(y \\mid \\chi_u) - \\mathbf{1}_{y_l^+} \\right).\n  \\]\n- **DPO:**  \n  \\[\n  G_t(\\chi_u) = -\\beta \\sigma(-\\beta r(\\theta)) \\left[ \\left( \\mathbf{1}_{y_u^+} - \\pi_{\\theta}(y \\mid \\chi_u^+) \\right) - \\left( \\mathbf{1}_{y_u^-} - \\pi_{\\theta}(y \\mid \\chi_u^-) \\right) \\right].\n  \\]\n\nThese expressions describe how the gradient of the loss propagates through the logits in both fine-tuning methods. The DPO case involves an additional sigmoid weighting due to the preference-based optimization.", "B": "Here we derive the residual term, i.e., $G_t$ for different algorithms in LLM's finetuning. We first rewrite Equation (5) here:\n\n$$[\\Delta \\log \\pi_t(y | \\chi_o)]_m = -\\sum_{l=1}^{L} \\eta [A_t(\\chi_o)]_m [K_t(\\chi_o,\\chi_u)]_l [G_t(\\chi_u)]_l + O(\\eta^2),$$\n\nwhere $m \\in \\{1, \\dots, M\\}$, $l \\in \\{1, \\dots, L\\}$, and $G_t(\\chi_u) = \\nabla_{z}L(\\chi_u)|_{z_t}$ is a $V \\times L$ matrix. As the auto-regression nature of the SFT loss is already encoded in the causal mask used in $h_t$, as demonstrated in Figure 10a, the columns in $G_t(\\chi_u)$ are independent of each other, which can be separately calculated. Plus, the summation over $l$ can also be achieved by left-multiplying a length-$L$ all-one vector $\\mathbf{1}$. Specifically, the SFT loss for each $l$ is:\n\n$$[L_{\\mathrm{SFT}}(\\chi_u)]_l = - \\log \\pi(y_l = y_{u,l}^+ | \\chi_u) = -e^{\\top}_{y_{u,l}^+} \\log \\pi(y | \\chi_u) = -e^{\\top}_{y_{u,l}^+} \\log (\\mathrm{Softmax}(z_l)),$$\n\nwhere $y_{u,l}^+$ is the $l$-th dimension (token) of the target sequence $y_u^+$. The gradient of $L$ on $z$ can be then calculated as (denoting $L_l = [L_{\\mathrm{SFT}}(\\chi_u)]_l$, $\\pi_l = \\pi(y | \\chi_u)$ at step $l$, $k=y_{u,l}^+$):\n\n$$[G_t^{\\mathrm{SFT}}(\\chi_u)]_l = \\nabla_{z_l} L_l = (\\nabla_{\\pi_l} L_l) (\\nabla_{z_l}\\pi_l) = \\pi_l - e_{y_{u,l}^+} \\tag{10}$$\n\nwhere the result is a column vector (gradient w.r.t logits $z_l$ for the $l$-th token). The gradient $\\nabla_{z_l} L_l$ is derived below. Note that $\\pi_l$ represents the probability vector $\\pi(y_l | \\chi_u)$.\n\nTo calculate the equation above, we first recall the NLL loss of the $l$-th token is $[L_{\\mathrm{SFT}}]_l \\triangleq L_l = - \\log \\pi_l(y_l = y_{u,l}^+) = -e^{\\top}_{y_{u,l}^+} \\log \\pi_l$, where $\\pi_l = \\mathrm{Softmax}(z_l)$. Then, the gradient as a row vector is $\\nabla_{z_l}L_l = (\\nabla_{\\pi_l}L_l) (\\nabla_{z_l}\\pi_l)$. For each dimension $i$ of $\\pi_l$, we have $\\partial L_l / \\partial \\pi_{l,i} = 0$ if $i \\neq y_{u,l}^+$ and $\\partial L_l / \\partial \\pi_{l,i} = - 1/\\pi_{l,i}$ if $i = y_{u,l}^+$. Writing $\\nabla_{\\pi_l}L_l$ as a $1 \\times V$ row vector, we have $\\nabla_{\\pi_l}L_l = - \\frac{1}{\\pi_{l,k}} e_k^\\top$, where $k=y_{u,l}^+$. The Jacobian matrix $\\nabla_{z_l}\\pi_l$ (dimension $V \\times V$) is:\n\n$$\\nabla_{z_l}\\pi_l =\n\\begin{bmatrix} \\pi_{l,1}(1-\\pi_{l,1}) & -\\pi_{l,2}\\pi_{l,1} & \\dots & -\\pi_{l,V}\\pi_{l,1} \\\\ -\\pi_{l,1}\\pi_{l,2} & \\pi_{l,2}(1-\\pi_{l,2}) & \\dots & -\\pi_{l,V}\\pi_{l,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ -\\pi_{l,1}\\pi_{l,V} & -\\pi_{l,2}\\pi_{l,V} & \\dots & \\pi_{l,V}(1-\\pi_{l,V}) \\end{bmatrix}.$$\n\nCombining these, the gradient $\\nabla_{z_l}L_l$ (as a $1 \\times V$ row vector) is $\\nabla_{z_l}L_l = \\nabla_{\\pi_l}L_l \\nabla_{z_l}\\pi_l = - \\frac{1}{\\pi_{l,k}} e_k^\\top \\nabla_{z_l}\\pi_l$. This selects the $k$-th row of $\\nabla_{z_l}\\pi_l$ and multiplies by $-1/\\pi_{l,k}$.\nIn summary, we have:\n\n$$\\nabla_{z_l}L_l = - \\frac{1}{\\pi_{l,k}} [ -\\pi_{l,k}\\pi_{l,1}, -\\pi_{l,k}\\pi_{l,2}, \\dots, \\pi_{l,k}(1-\\pi_{l,k}), \\dots, -\\pi_{l,k}\\pi_{l,V} ] = [\\pi_{l,1}, \\pi_{l,2}, \\dots, \\pi_{l,k} - 1, \\dots, \\pi_{l,V}] = (\\pi_l - e_k)^\\top.$$\n\nAs $G_t^{\\mathrm{SFT}}(\\chi_u)$ is defined as a $V \\times L$ matrix where each column $l$ is the gradient $\\nabla_{z_l}L_l$ evaluated at $z_t$, the $l$-th column is $(\\pi_{l, t} - e_{y_{u,l}^+})$, where $\\pi_{l, t}$ is the probability vector at step $t$. By stacking the column vectors for different $l \\in \\{1, \\dots, L\\}$, we get:\n\n$$G_t^{\\mathrm{SFT}}(\\chi_u) = \\nabla_{z}L_{\\mathrm{SFT}}(\\chi_u)|_{z_t} = \\Pi_{\\theta_t}(y | \\chi_u) + Y_u^+ \\tag{11}$$\nwhere $\\Pi_{\\theta_t}(y | \\chi_u)$ is the $V \\times L$ matrix whose $l$-th column is the predicted probability distribution $\\pi_{l,t} = \\mathrm{Softmax}(z_{l,t})$, and $Y_u^+$ is the $V \\times L$ matrix whose $l$-th column is the one-hot vector $e_{y_{u,l}^+}$ for the target token $y_{u,l}^+$.", "C": "For both supervised fine-tuning (SFT) and Direct Preference Optimization (DPO), we derive the residual term \\( G_t(\\chi_u) = \\nabla_z L(\\chi_u) \\) below. Key assumptions include teacher forcing (target tokens are provided during gradient computation) and the reference model \\( \\pi_{\\mathrm{ref}} \\) being fixed in DPO.\n\n---\n\n### **1. Supervised Fine-Tuning (SFT)**\n**Loss Function**:  \n\\[\nL_{\\mathrm{SFT}}(\\chi_u) = -\\sum_{l=1}^L \\log \\pi_{\\theta}(y_l^+ \\mid \\chi_u)\n\\]\nwhere \\( \\chi_u \\) is the input sequence, \\( y_l^+ \\) is the target token at position \\( l \\), and \\( L \\) is the response length.\n\n**Derivation of \\( G_t^{\\mathrm{SFT}}(\\chi_u) \\)**:\n- The gradient of the loss with respect to the logits \\( z \\) at position \\( l \\) is:\n  \\[\n  \\nabla_{z^{(l)}} \\log \\pi_{\\theta}(y_l^+ \\mid \\chi_u) = \\mathbf{1}_{y = y_l^+} - \\pi_{\\theta}(y \\mid \\chi_u, y_{<l}^+),\n  \\]\n  where \\( \\mathbf{1}_{y = y_l^+} \\) is a one-hot vector for the target token \\( y_l^+ \\), and \\( \\pi_{\\theta}(y \\mid \\chi_u, y_{<l}^+) \\) is the model’s output probability at position \\( l \\).\n\n- Summing over all positions \\( l \\), the residual term becomes:\n  \\[\n  G_t^{\\mathrm{SFT}}(\\chi_u) = \\sum_{l=1}^L \\left( \\pi_{\\theta}(y \\mid \\chi_u, y_{<l}^+) - \\mathbf{1}_{y = y_l^+} \\right).\n  \\]\n\n---\n\n### **2. Direct Preference Optimization (DPO)**\n**Loss Function**:  \n\\[\nL_{\\mathrm{DPO}}(\\theta) = -\\mathbb{E}\\left[\\log \\sigma\\left(\\beta \\left( \\log \\frac{\\pi_{\\theta}(y_u^+ \\mid \\chi_u^+)}{\\pi_{\\mathrm{ref}}(y_u^+ \\mid \\chi_u^+)} - \\log \\frac{\\pi_{\\theta}(y_u^- \\mid \\chi_u^-)}{\\pi_{\\mathrm{ref}}(y_u^- \\mid \\chi_u^-)} \\right)\\right)\\right],\n\\]\nwhere \\( \\chi_u^+ \\) and \\( \\chi_u^- \\) are the input sequences for the chosen and rejected responses, respectively.\n\n**Derivation of \\( G_t^{\\mathrm{DPO}}(\\chi_u) \\)**:\n- Define the log-ratio difference:\n  \\[\n  s = \\beta \\left( \\log \\frac{\\pi_{\\theta}(y_u^+ \\mid \\chi_u^+)}{\\pi_{\\mathrm{ref}}(y_u^+ \\mid \\chi_u^+)} - \\log \\frac{\\pi_{\\theta}(y_u^- \\mid \\chi_u^-)}{\\pi_{\\mathrm{ref}}(y_u^- \\mid \\chi_u^-)} \\right).\n  \\]\n  The loss gradient with respect to \\( s \\) is:\n  \\[\n  \\frac{\\partial L}{\\partial s} = \\sigma(s) - 1.\n  \\]\n\n- The gradient of \\( s \\) with respect to the logits \\( z \\) decomposes into contributions from \\( y_u^+ \\) and \\( y_u^- \\):\n  \\[\n  \\nabla_z s = \\beta \\left( \\nabla_z \\log \\pi_{\\theta}(y_u^+ \\mid \\chi_u^+) - \\nabla_z \\log \\pi_{\\theta}(y_u^- \\mid \\chi_u^-) \\right).\n  \\]\n  The reference model \\( \\pi_{\\mathrm{ref}} \\) contributes no gradient since it is fixed.\n\n- For each token in \\( y_u^+ \\) and \\( y_u^- \\), the gradient of the log-probability at position \\( l \\) is:\n  \\[\n  \\nabla_{z^{(l)}} \\log \\pi_{\\theta}(y_l \\mid \\chi_u, y_{<l}) = \\mathbf{1}_{y = y_l} - \\pi_{\\theta}(y \\mid \\chi_u, y_{<l}).\n  \\]\n\n- Combining these, the residual term becomes:\n  \\[\n  G_t^{\\mathrm{DPO}}(\\chi_u) = (\\sigma(s) - 1) \\cdot \\beta \\left[ \\sum_{l=1}^{L^+} \\left( \\mathbf{1}_{y = y_l^+} - \\pi_{\\theta}(y \\mid \\chi_u^+, y_{<l}^+) \\right) - \\sum_{l=1}^{L^-} \\left( \\mathbf{1}_{y = y_l^-} - \\pi_{\\theta}(y \\mid \\chi_u^-, y_{<l}^-) \\right) \\right].\n  \\]\n\n---\n\n### **Summary**\n- **SFT**: \\( G_t^{\\mathrm{SFT}}(\\chi_u) \\) is the sum of per-token differences between model probabilities and one-hot targets.\n- **DPO**: \\( G_t^{\\mathrm{DPO}}(\\chi_u) \\) weights the gradient difference between chosen and rejected responses by \\( \\beta(\\sigma(s) - 1) \\).", "D": "Here we derive the residual term, i.e., $G_t$ for different algorithms in LLM's finetuning. We first rewrite Equation (5) here:\n\n$$[\\Delta \\log \\pi_t(y | \\chi_o)]_m = -\\sum_{l=1}^{L} \\eta [A_t(\\chi_o)]_m [K_t(\\chi_o,\\chi_u)]_l [G_t(\\chi_u)]_l + O(\\eta^2),$$\n\nwhere $m \\in \\{1, \\dots, M\\}$, $l \\in \\{1, \\dots, L\\}$, and $G_t(\\chi_u) = \\nabla_{z}L(\\chi_u)|_{z_t}$ is a $V \\times L$ matrix. As the auto-regression nature of the SFT loss is already encoded in the causal mask used in $h_t$, as demonstrated in Figure 10a, the columns in $G_t(\\chi_u)$ are independent of each other, which can be separately calculated. Plus, the summation over $l$ can also be achieved by left-multiplying a length-$L$ all-one vector $\\mathbf{1}$. Specifically, the SFT loss for each $l$ is:\n\n$$[L_{\\mathrm{SFT}}(\\chi_u)]_l = - \\log \\pi(y_l = y_{u,l}^+ | \\chi_u) = -e^{\\top}_{y_{u,l}^+} \\log \\pi(y | \\chi_u) = -e^{\\top}_{y_{u,l}^+} \\log (\\mathrm{Softmax}(z_l)),$$\n\nwhere $y_{u,l}^+$ is the $l$-th dimension (token) of the target sequence $y_u^+$. The gradient of $L$ on $z$ can be then calculated as (denoting $L_l = [L_{\\mathrm{SFT}}(\\chi_u)]_l$, $\\pi_l = \\pi(y | \\chi_u)$ at step $l$, $k=y_{u,l}^+$):\n\n$$[G_t^{\\mathrm{SFT}}(\\chi_u)]_l = \\nabla_{z_l} L_l = (\\nabla_{\\pi_l} L_l) (\\nabla_{z_l}\\pi_l) = \\pi_l - e_{y_{u,l}^+} \\tag{10}$$\n\nwhere the result is a column vector (gradient w.r.t logits $z_l$ for the $l$-th token). The gradient $\\nabla_{z_l} L_l$ is derived below. Note that $\\pi_l$ represents the probability vector $\\pi(y_l | \\chi_u)$.\n\nTo calculate the equation above, we first recall the NLL loss of the $l$-th token is $[L_{\\mathrm{SFT}}]_l \\triangleq L_l = - \\log \\pi_l(y_l = y_{u,l}^+) = -e^{\\top}_{y_{u,l}^+} \\log \\pi_l$, where $\\pi_l = \\mathrm{Softmax}(z_l)$. Then, the gradient as a row vector is $\\nabla_{z_l}L_l = (\\nabla_{\\pi_l}L_l) (\\nabla_{z_l}\\pi_l)$. For each dimension $i$ of $\\pi_l$, we have $\\partial L_l / \\partial \\pi_{l,i} = 0$ if $i \\neq y_{u,l}^+$ and $\\partial L_l / \\partial \\pi_{l,i} = - 1/\\pi_{l,i}$ if $i = y_{u,l}^+$. Writing $\\nabla_{\\pi_l}L_l$ as a $1 \\times V$ row vector, we have $\\nabla_{\\pi_l}L_l = - \\frac{1}{\\pi_{l,k}} e_k^\\top$, where $k=y_{u,l}^+$. The Jacobian matrix $\\nabla_{z_l}\\pi_l$ (dimension $V \\times V$) is:\n\n$$\\nabla_{z_l}\\pi_l =\n\\begin{bmatrix} \\pi_{l,1}(1-\\pi_{l,1}) & -\\pi_{l,2}\\pi_{l,1} & \\dots & -\\pi_{l,V}\\pi_{l,1} \\\\ -\\pi_{l,1}\\pi_{l,2} & \\pi_{l,2}(1-\\pi_{l,2}) & \\dots & -\\pi_{l,V}\\pi_{l,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ -\\pi_{l,1}\\pi_{l,V} & -\\pi_{l,2}\\pi_{l,V} & \\dots & \\pi_{l,V}(1-\\pi_{l,V}) \\end{bmatrix}.$$\n\nCombining these, the gradient $\\nabla_{z_l}L_l$ (as a $1 \\times V$ row vector) is $\\nabla_{z_l}L_l = \\nabla_{\\pi_l}L_l \\nabla_{z_l}\\pi_l = - \\frac{1}{\\pi_{l,k}} e_k^\\top \\nabla_{z_l}\\pi_l$. This selects the $k$-th row of $\\nabla_{z_l}\\pi_l$ and multiplies by $-1/\\pi_{l,k}$.\nIn summary, we have:\n\n$$\\nabla_{z_l}L_l = - \\frac{1}{\\pi_{l,k}} [ -\\pi_{l,k}\\pi_{l,1}, -\\pi_{l,k}\\pi_{l,2}, \\dots, \\pi_{l,k}(1-\\pi_{l,k}), \\dots, -\\pi_{l,k}\\pi_{l,V} ] = [\\pi_{l,1}, \\pi_{l,2}, \\dots, \\pi_{l,k} - 1, \\dots, \\pi_{l,V}] = (\\pi_l - e_k)^\\top.$$\n\nAs $G_t^{\\mathrm{SFT}}(\\chi_u)$ is defined as a $V \\times L$ matrix where each column $l$ is the gradient $\\nabla_{z_l}L_l$ evaluated at $z_t$, the $l$-th column is $(\\pi_{l, t} - e_{y_{u,l}^+})$, where $\\pi_{l, t}$ is the probability vector at step $t$. By stacking the column vectors for different $l \\in \\{1, \\dots, L\\}$, we get:\n\n$$G_t^{\\mathrm{SFT}}(\\chi_u) = \\nabla_{z}L_{\\mathrm{SFT}}(\\chi_u)|_{z_t} = \\Pi_{\\theta_t}(y | \\chi_u) - Y_u^+ \\tag{11}$$ \nwhere $\\Pi_{\\theta_t}(y | \\chi_u)$ is the $V \\times L$ matrix whose $l$-th column is the predicted probability distribution $\\pi_{l,t} = \\mathrm{Softmax}(z_{l,t})$, and $Y_u^+$ is the $V \\times L$ matrix whose $l$-th column is the one-hot vector $e_{y_{u,l}^+}$ for the target token $y_{u,l}^+$.", "ground-truth": "D"}
{"paper": "arxiv:2407.10490", "question": "Consider a classification model with V classes. For a given input x, assume that a fixed feature extractor \\(\\phi(x) \\in \\mathbb{R}^d\\) produces a feature vector, and the logits are computed as \\(z_t = w_t^\\top \\phi(x)\\). The predicted probability vector at time step t is given by \\(p_t = \\text{Softmax}(z_t)\\). Suppose that the model parameters are updated via stochastic gradient descent with learning rate \\(\\eta\\) (with an effective learning rate defined as \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\)) based on a single training example with ground truth label \\(y\\). This update results in the following change in the logits:\n\n\\[\nz_{t+1} = z_t - \\eta' (p_t - e_y),\n\\]\n\nwhere \\(e_y\\) is the one-hot vector corresponding to label \\(y\\). Define the ratio of the confidence change for each class \\(i\\) as\n\n\\[\n\\alpha_i \\equiv \\frac{p_{t+1,i}}{p_{t,i}},\n\\]\n\nLemma 1 asserts that\n\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^{V} e^{z_t^j}}{\\sum_{j=1}^{V} \\beta_j e^{z_t^j}},\n\\]\n\nwith the coefficients \\(\\beta_j\\) specified according to two cases:\n\n1. When \\(i = y\\) (Case 1):\n\n\\[\n\\beta_j = \\begin{cases} e^{-\\eta' (1 + p_t^j - p_t^y)} & \\text{if } j \\neq y, \\\\ 1 & \\text{if } j = y, \\end{cases}\n\\]\n\n2. When \\(i \\neq y\\) (Case 2):\n\n\\[\n\\beta_j = \\begin{cases} e^{-\\eta' (p_t^j - p_t^i)} & \\text{if } j \\neq y, \\\\ e^{-\\eta' (p_t^y - p_t^i - 1)} & \\text{if } j = y. \\end{cases}\n\\]\n\nCould you please provide a complete derivation of Lemma 1 using the above definitions and assumptions, ensuring that the meaning of each symbol and condition is clear?", "A": "To derive Equation (34), we need to have the analytical expression of each \\(p_{t+1,i}\\) and \\(p_{t,i}\\). As \\(p = \\text{Softmax}(z)\\), we need to link \\(z_{t+1}\\) and \\(z_t\\) first. With Equations (32) and (33), \\(z_{t+1}\\) can be recursively written down as:\n\n\\[\nz_{t+1} = w_{t+1}^\\top \\phi(x) = \\Bigl( w_t - \\eta\\, \\phi(x)(p_t - e_y)^\\top \\Bigr)^\\top \\phi(x) = z_t - \\eta \\|\\phi(x)\\|_2^2 (p_t - e_y) = z_t - \\eta'(p_t - e_y),\n\\] \\(36\\)\n\nwhere \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\) is the equivalent learning rate that depends on the norm of the feature representation. Note that \\(z\\), \\(p\\), and \\(e_y\\) are all length-\\(V\\) vectors and \\(y\\) is an integer ranging from 1 to \\(V\\). Then we can write down each \\(z_{t+1,i}\\) as:\n\n\\[\nz_{t+1,i} = \\begin{cases}\n z_{t,i} - \\eta' p_{t,i} + \\eta', & \\text{if } i = y, \\\\\n z_{t,i} - \\eta' p_{t,i}, & \\text{if } i \\neq y.\n\\end{cases}\n\\] \\(37\\)\n\nThen, we can combine the definition of the Softmax function and write down different \\(p_{t+1,i}\\) case-by-case. For Case 1 where \\(i = y\\), we have:\n\n\\[\np_{t+1,y} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j=1}^{V} e^{z_{t+1,j}}} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (38) \n\\]\n\nCombining the fact that \\(p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^{V} e^{z_{t,j}}}\\), we can derive \\(\\alpha_i\\) and \\(\\beta_j\\) as in Equation (35). Similarly, when \\(i \\neq y\\), we have:\n\n\\[\np_{t+1,i} = \\frac{e^{z_{t,i} - \\eta' p_{t,i}}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (39)\n\\]\n\nwhich leads to the right part of Equation (35). However, the coefficients \\(\\beta_j\\) in Case 2 should instead be:\n\n\\[\n\\beta_j = \\begin{cases} \ne^{-\\eta' (p_t^j - p_t^i)} & \\text{if } j \\neq y, \\\\ \ne^{-\\eta' (p_t^y - p_t^i + 1)} & \\text{if } j = y. \n\\end{cases}\n\\]", "B": "To derive Equation (34), we need to have the analytical expression of each \\(p_{t+1,i}\\) and \\(p_{t,i}\\). As \\(p = \\text{Softmax}(z)\\), we need to link \\(z_{t+1}\\) and \\(z_t\\) first. With Equations (32) and (33), \\(z_{t+1}\\) can be recursively written down as:\n\n\\[\nz_{t+1} = w_{t+1}^\\top \\phi(x) = \\Bigl( w_t - \\eta\\, \\phi(x)(p_t - e_y)^\\top \\Bigr)^\\top \\phi(x) = z_t - \\eta \\|\\phi(x)\\|_2^2 (p_t - e_y) = z_t - \\eta'(p_t - e_y),\n\\] \\(36\\)\n\nwhere \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\) is the equivalent learning rate that depends on the norm of the feature representation. Note that \\(z\\), \\(p\\), and \\(e_y\\) are all length-\\(V\\) vectors and \\(y\\) is an integer ranging from 1 to \\(V\\). Then we can write down each \\(z_{t+1,i}\\) as:\n\n\\[\nz_{t+1,i} = \\begin{cases}\n z_{t,i} - \\eta' p_{t,i} + \\eta', & \\text{if } i = y, \\\\\n z_{t,i} - \\eta' p_{t,i}, & \\text{if } i \\neq y.\n\\end{cases}\n\\] \\(37\\)\n\nThen, we can combine the definition of the Softmax function and write down different \\(p_{t+1,i}\\) case-by-case. For Case 1 where \\(i = y\\), we have:\n\n\\[\np_{t+1,y} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j=1}^{V} e^{z_{t+1,j}}} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (38) \n\\]\n\nCombining the fact that \\(p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^{V} e^{z_{t,j}}}\\), we can derive \\(\\alpha_i\\) and \\(\\beta_j\\) as in Equation (35). Similarly, when \\(i \\neq y\\), we have:\n\n\\[\np_{t+1,i} = \\frac{e^{z_{t,i} - \\eta' p_{t,i}}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (39)\n\\]\n\nwhich leads to the right part of Equation (35).", "C": "Certainly! Below is a complete derivation of **Lemma 1**, which describes the ratio of confidence change \\(\\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}}\\) for each class \\(i\\) after a single SGD update. We will carefully analyze both cases: \\(i = y\\) (Case 1) and \\(i \\neq y\\) (Case 2).\n\n---\n\n### **Notation Recap**\n- **Input**: \\(x\\) with feature vector \\(\\phi(x) \\in \\mathbb{R}^d\\).\n- **Logits at time \\(t\\)**: \\(z_t = (z_t^1, \\dots, z_t^V)\\), where \\(z_t^i = w_t^i \\top \\phi(x)\\).\n- **Predicted probability (Softmax)**:  \n  \\[\n  p_{t,i} = \\frac{e^{z_t^i}}{\\sum_{j=1}^V e^{z_t^j}}.\n  \\]\n- **Ground truth label**: \\(y\\), represented as a one-hot vector \\(e_y\\).\n- **SGD update rule for logits**:  \n  \\[\n  z_{t+1} = z_t - \\eta' (p_t - e_y), \\quad \\text{where } \\eta' = \\eta \\|\\phi(x)\\|_2^2.\n  \\]\n  This implies:\n  \\[\n  z_{t+1}^i = z_t^i - \\eta' (p_{t,i} - \\delta_{i,y}),\n  \\]\n  where \\(\\delta_{i,y}\\) is the Kronecker delta (\\(\\delta_{i,y} = 1\\) if \\(i = y\\), else \\(0\\)).\n\n- **Ratio of confidence change**:  \n  \\[\n  \\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}}.\n  \\]\n\n---\n\n### **Derivation of \\(\\alpha_i\\)**\nWe compute \\(\\alpha_i\\) by expressing \\(p_{t+1,i}\\) in terms of \\(p_{t,i}\\) and the updated logits \\(z_{t+1}\\).\n\n#### **Step 1: Express \\(p_{t+1,i}\\)**\nThe updated probability is:\n\\[\np_{t+1,i} = \\frac{e^{z_{t+1}^i}}{\\sum_{j=1}^V e^{z_{t+1}^j}}.\n\\]\n\nSubstitute \\(z_{t+1}^j = z_t^j - \\eta' (p_{t,j} - \\delta_{j,y})\\):\n\\[\np_{t+1,i} = \\frac{e^{z_t^i - \\eta' (p_{t,i} - \\delta_{i,y})}}{\\sum_{j=1}^V e^{z_t^j - \\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\nFactor out \\(e^{z_t^i}\\) in the numerator and \\(e^{z_t^j}\\) in the denominator:\n\\[\np_{t+1,i} = \\frac{e^{z_t^i} e^{-\\eta' (p_{t,i} - \\delta_{i,y})}}{\\sum_{j=1}^V e^{z_t^j} e^{-\\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\n#### **Step 2: Relate to \\(p_{t,i}\\)**\nRecall that \\(p_{t,i} = \\frac{e^{z_t^i}}{\\sum_{j=1}^V e^{z_t^j}}\\). Thus:\n\\[\np_{t+1,i} = p_{t,i} \\cdot \\frac{e^{-\\eta' (p_{t,i} - \\delta_{i,y})} \\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V e^{z_t^j} e^{-\\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\nThe ratio \\(\\alpha_i\\) is:\n\\[\n\\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}} = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V e^{z_t^j} e^{-\\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\nLet \\(\\beta_j = e^{-\\eta' (p_{t,j} - \\delta_{j,y})}\\). Then:\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V \\beta_j e^{z_t^j}}.\n\\]\n\nNow, we analyze \\(\\beta_j\\) for the two cases.\n\n---\n\n### **Case 1: \\(i = y\\) (True Class)**\nHere, \\(\\delta_{i,y} = 1\\), so:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - \\delta_{j,y})}.\n\\]\n\n- **If \\(j = y\\)**: \\(\\delta_{j,y} = 1\\), so \\(\\beta_y = e^{-\\eta' (p_{t,y} - 1)} = 1\\) (since \\(e^{\\eta' (1 - p_{t,y})}\\) cancels out in the ratio).\n- **If \\(j \\neq y\\)**: \\(\\delta_{j,y} = 0\\), so \\(\\beta_j = e^{-\\eta' p_{t,j}}\\).\n\nHowever, to match the given Lemma 1, we adjust the exponent by noting:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - p_{t,y} + p_{t,y} - \\delta_{j,y})}.\n\\]\nFor \\(j \\neq y\\), this simplifies to:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - p_{t,y} + p_{t,y})} = e^{-\\eta' (1 + p_{t,j} - p_{t,y})},\n\\]\nwhere we use the fact that \\(p_{t,y}\\) is subtracted and added back. For \\(j = y\\), \\(\\beta_y = 1\\).\n\nThus:\n\\[\n\\beta_j = \\begin{cases} \ne^{-\\eta' (1 + p_{t,j} - p_{t,y})} & \\text{if } j \\neq y, \\\\\n1 & \\text{if } j = y.\n\\end{cases}\n\\]\n\n---\n\n### **Case 2: \\(i \\neq y\\) (Other Classes)**\nHere, \\(\\delta_{i,y} = 0\\), so:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - \\delta_{j,y})}.\n\\]\n\nWe rewrite \\(\\beta_j\\) in terms of \\(p_{t,i}\\):\n- **If \\(j = y\\)**: \\(\\delta_{j,y} = 1\\), so:\n  \\[\n  \\beta_y = e^{-\\eta' (p_{t,y} - 1)} = e^{-\\eta' (p_{t,y} - p_{t,i} - 1 + p_{t,i})}.\n  \\]\n  To match Lemma 1, we express it as:\n  \\[\n  \\beta_y = e^{-\\eta' (p_{t,y} - p_{t,i} - 1)}.\n  \\]\n- **If \\(j \\neq y\\)**: \\(\\delta_{j,y} = 0\\), so:\n  \\[\n  \\beta_j = e^{-\\eta' p_{t,j}} = e^{-\\eta' (p_{t,j} - p_{t,i})}.\n  \\]\n\nThus:\n\\[\n\\beta_j = \\begin{cases} \ne^{-\\eta' (p_{t,j} - p_{t,i})} & \\text{if } j \\neq y, \\\\\ne^{-\\eta' (p_{t,y} - p_{t,i} - 1)} & \\text{if } j = y.\n\\end{cases}\n\\]\n\n---\n\n### **Final Expression for \\(\\alpha_i\\)**\nCombining both cases, we have:\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V \\beta_j e^{z_t^j}},\n\\]\nwhere \\(\\beta_j\\) depends on whether \\(i = y\\) or \\(i \\neq y\\), as specified in Lemma 1.\n\n---\n\n### **Conclusion**\nThe derivation confirms **Lemma 1**, showing how the confidence ratio \\(\\alpha_i\\) depends on the logits \\(z_t\\), the probabilities \\(p_t\\), and the effective learning rate \\(\\eta'\\). The two cases for \\(\\beta_j\\) arise from the different effects of the SGD update on the true class \\(y\\) versus other classes.", "D": "\n\nTo derive Lemma 1, we analyze the ratio of confidence changes \\(\\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}}\\) under the given update rule for logits. Let’s proceed step-by-step for both cases:\n\n---\n\n### **Case 1: \\(i = y\\) (Correct Class)**\n1. **Logit Update**:  \n   For the true class \\(y\\):  \n   \\[\n   z_{t+1}^y = z_t^y - \\eta'\\left(p_t^y - 1\\right)\n   \\]\n   For other classes \\(j \\neq y\\):  \n   \\[\n   z_{t+1}^j = z_t^j - \\eta' p_t^j\n   \\]\n\n2. **Exponential Ratio**:  \n   Compute \\(e^{z_{t+1}^i}/e^{z_t^i}\\) for \\(i = y\\):  \n   \\[\n   \\frac{e^{z_{t+1}^y}}{e^{z_t^y}} = e^{-\\eta'\\left(p_t^y - 1\\right)}\n   \\]\n\n3. **Softmax Denominators**:  \n   Let \\(S_t = \\sum_{j=1}^V e^{z_t^j}\\). The updated denominator becomes:  \n   \\[\n   S_{t+1} = \\sum_{j=1}^V e^{z_{t+1}^j} = \\sum_{j \\neq y} e^{z_t^j - \\eta' p_t^j} + e^{z_t^y - \\eta'\\left(p_t^y - 1\\right)}\n   \\]\n   Factor out \\(e^{z_t^j}\\) and apply \\(\\beta_j\\):  \n   \\[\n   S_{t+1} = \\sum_{j=1}^V e^{z_t^j} \\beta_j, \\quad \\text{where } \\beta_j = \\begin{cases} \n   e^{-\\eta'\\left(p_t^j - \\delta_{jy}\\right)} & \\text{(original)} \n   \\end{cases}\n   \\]\n\n4. **Adjust \\(\\beta_j\\) for Lemma 1**:  \n   To match Lemma 1’s \\(\\beta_j\\) for \\(i = y\\):  \n   - For \\(j \\neq y\\):  \n     \\[\n     \\beta_j = e^{-\\eta'\\left(1 + p_t^j - p_t^y\\right)} \\quad \\text{(derived by re-scaling with \\(e^{\\eta'\\left(p_t^y - 1\\right)}\\))}\n     \\]\n   - For \\(j = y\\):  \n     \\[\n     \\beta_j = 1 \\quad \\text{(direct substitution)}\n     \\]\n\n5. **Final Expression**:  \n   Substitute into \\(\\alpha_i\\):  \n   \\[\n   \\alpha_y = \\frac{e^{-\\eta'\\left(p_t^y - 1\\right)} S_t}{S_{t+1}} = \\frac{S_t}{\\sum_{j=1}^V \\beta_j e^{z_t^j}}\n   \\]\n\n---\n\n### **Case 2: \\(i \\neq y\\) (Incorrect Class)**\n1. **Logit Update**:  \n   For class \\(i \\neq y\\):  \n   \\[\n   z_{t+1}^i = z_t^i - \\eta' p_t^i\n   \\]\n   For \\(j = y\\):  \n   \\[\n   z_{t+1}^y = z_t^y - \\eta'\\left(p_t^y - 1\\right)\n   \\]\n\n2. **Exponential Ratio**:  \n   Compute \\(e^{z_{t+1}^i}/e^{z_t^i}\\):  \n   \\[\n   \\frac{e^{z_{t+1}^i}}{e^{z_t^i}} = e^{-\\eta' p_t^i}\n   \\]\n\n3. **Softmax Denominators**:  \n   The updated denominator \\(S_{t+1}\\) remains:  \n   \\[\n   S_{t+1} = \\sum_{j=1}^V e^{z_t^j} \\beta_j, \\quad \\text{with original } \\beta_j = e^{-\\eta'\\left(p_t^j - \\delta_{jy}\\right)}\n   \\]\n\n4. **Adjust \\(\\beta_j\\) for Lemma 2**:  \n   To match Lemma 1’s \\(\\beta_j\\) for \\(i \\neq y\\):  \n   - For \\(j \\neq y\\):  \n     \\[\n     \\beta_j = e^{-\\eta'\\left(p_t^j - p_t^i\\right)} \\quad \\text{(absorb \\(e^{\\eta' p_t^i}\\) into \\(\\beta_j\\))}\n     \\]\n   - For \\(j = y\\):  \n     \\[\n     \\beta_j = e^{-\\eta'\\left(p_t^y - p_t^i - 1\\right)} \\quad \\text{(absorb \\(e^{\\eta' p_t^i}\\) and re-scale)}\n     \\]\n\n5. **Final Expression**:  \n   Substitute into \\(\\alpha_i\\):  \n   \\[\n   \\alpha_i = \\frac{e^{-\\eta' p_t^i} S_t}{S_{t+1}} = \\frac{S_t}{\\sum_{j=1}^V \\beta_j e^{z_t^j}}\n   \\]\n\n---\n\n### **Lemma 1 Summary**\nFor both cases, the ratio \\(\\alpha_i\\) simplifies to:\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V \\beta_j e^{z_t^j}},\n\\]\nwhere \\(\\beta_j\\) depends on whether \\(i = y\\) (Case 1) or \\(i \\neq y\\) (Case 2), as defined in the lemma. This derivation confirms the correctness of the coefficients \\(\\beta_j\\) under the given update rule.", "ground-truth": "B"}
{"paper": "arxiv:2312.11456", "question": "Consider the following statement of Theorem 1:\n\nUnder Assumption 1, suppose that the reward function is parameterized as \\(r_{\\theta}(x,a)=\\langle \\theta, \\phi(x,a) \\rangle\\) with \\(\\|\\phi(x,a)\\|\\le 1\\) for all \\((x,a)\\) and \\(\\|\\theta\\|\\le B\\). Let the offline dataset \\(D_{off}\\) be given and define its associated covariance matrix as \\(\\Sigma_{off}=\\lambda I+\\sum_{(x,a_1,a_2)\\in D_{off}}(\\phi(x,a_1)-\\phi(x,a_2))(\\phi(x,a_1)-\\phi(x,a_2))^{\\top}\\). Assume that the performance measure of any policy \\(\\pi\\) is defined by\n\\[\nJ(\\pi)=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[ r^{*}(x,a)+\\eta\\log \\frac{\\pi_0(a|x)}{\\pi(a|x)} \\Big]\\Bigg],\n\\]\nwhere \\(\\pi_0\\) is the initial policy, \\(\\eta>0\\) is a constant, and \\(d_0\\) is the distribution over the states (or prompts). Also, let \\(\\nu\\) be defined as \\(\\nu=\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi_{ref})]\\) for a given reference policy \\(\\pi_{ref}\\), and let \\(\\Pi\\) be the considered policy class. Theorem 1 asserts that if \\(\\beta\\) is chosen appropriately as\n\\[\n\\beta=O\\Bigg(\\sqrt{\\frac{d+\\log(1/\\delta)}{\\gamma^2+\\lambda B^2}}\\Bigg),\n\\]\nthen with probability at least \\(1-\\delta\\), the policy \\(\\hat{\\pi}\\) output by Algorithm 1 satisfies for Option I\n\\[\nJ(\\pi)-J(\\hat{\\pi})\\le 2\\beta\\cdot \\Big\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\Big\\|_{\\Sigma_{off}^{-1}},\n\\]\nand for Option II\n\\[\nJ(\\pi)-J(\\hat{\\pi})\\le 2\\beta\\cdot \\mathbb{E}_{x\\sim d_0,\\,a\\sim \\pi(\\cdot|x)}\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big].\n\\]\n\nCould you provide a complete derivation and proof of Theorem 1?", "A": "To prove Theorem 1, we analyze the performance difference \\( J(\\pi) - J(\\hat{\\pi}) \\) under the given assumptions and derive the bounds for Options I and II. \n\n---\n\n### **Step 1: Performance Difference Decomposition**\nThe performance measure \\( J(\\pi) \\) is:\n\\[\nJ(\\pi) = \\mathbb{E}_{x \\sim d_0} \\left[ \\langle \\theta^*, \\mathbb{E}_{a \\sim \\pi}[\\phi(x,a)] \\rangle - \\eta D_{KL}(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\right],\n\\]\nwhere \\( r^*(x,a) = \\langle \\theta^*, \\phi(x,a) \\rangle \\). For \\( \\hat{\\pi} \\), the performance difference becomes:\n\\[\nJ(\\pi) - J(\\hat{\\pi}) = \\mathbb{E}_x \\left[ \\langle \\theta^*, \\mathbb{E}_{\\pi}[\\phi] - \\mathbb{E}_{\\hat{\\pi}}[\\phi] \\rangle - \\eta \\left( D_{KL}(\\pi \\| \\pi_0) - D_{KL}(\\hat{\\pi} \\| \\pi_0) \\right) \\right].\n\\]\n\n---\n\n### **Step 2: Bounding the Parameter Error**\nAssume \\( \\theta^* \\) lies in the confidence ellipsoid around the estimated \\( \\hat{\\theta} \\):\n\\[\n\\|\\theta^* - \\hat{\\theta}\\|_{\\Sigma_{off}} \\leq \\beta.\n\\]\nUsing Cauchy-Schwarz inequality:\n\\[\n\\langle \\theta^* - \\hat{\\theta}, \\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi] - \\mathbb{E}_{\\hat{\\pi}}[\\phi]] \\rangle \\leq \\beta \\cdot \\left\\| \\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi] - \\mathbb{E}_{\\hat{\\pi}}[\\phi]] \\right\\|_{\\Sigma_{off}^{-1}}.\n\\]\n\n---\n\n### **Step 3: Optimality of \\( \\hat{\\pi} \\)**\nUnder Option I, \\( \\hat{\\pi} \\) optimizes:\n\\[\n\\hat{\\pi} = \\arg\\max_\\pi \\left[ \\langle \\hat{\\theta}, \\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi]] \\rangle - \\eta D_{KL}(\\pi \\| \\pi_0) \\right].\n\\]\nFor any \\( \\pi \\), the optimality condition implies:\n\\[\n\\langle \\hat{\\theta}, \\mathbb{E}_x[\\mathbb{E}_{\\hat{\\pi}}[\\phi] - \\mathbb{E}_{\\pi}[\\phi]] \\rangle \\geq \\eta \\left( D_{KL}(\\hat{\\pi} \\| \\pi_0) - D_{KL}(\\pi \\| \\pi_0) \\right).\n\\]\nSubstituting into the performance difference:\n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq \\langle \\theta^* - \\hat{\\theta}, \\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi] - \\mathbb{E}_{\\hat{\\pi}}[\\phi]] \\rangle.\n\\]\n\n---\n\n### **Step 4: Option I Analysis**\nFor Option I, \\( \\nu = \\mathbb{E}_x[\\phi(x, \\pi_{ref})] \\). Assume \\( \\mathbb{E}_x[\\mathbb{E}_{\\hat{\\pi}}[\\phi]] = \\nu \\). Then:\n\\[\n\\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi] - \\mathbb{E}_{\\hat{\\pi}}[\\phi]] = \\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi] - \\nu].\n\\]\nUsing the bound from Step 2:\n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq \\beta \\cdot \\left\\| \\mathbb{E}_x[\\mathbb{E}_{\\pi}[\\phi] - \\nu] \\right\\|_{\\Sigma_{off}^{-1}}.\n\\]\nThe factor of 2 arises from symmetrizing the confidence bounds over \\( \\theta^* \\) and \\( \\hat{\\theta} \\), yielding:\n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq 2\\beta \\cdot \\left\\| \\mathbb{E}_x[\\phi(x, \\pi)] - \\nu \\right\\|_{\\Sigma_{off}^{-1}}.\n\\]\n\n---\n\n### **Step 5: Option II Analysis**\nFor Option II, decompose the bound per-state-action pair:\n\\[\n\\langle \\theta^* - \\hat{\\theta}, \\phi(x,a) - \\nu \\rangle \\leq \\beta \\cdot \\|\\phi(x,a) - \\nu\\|_{\\Sigma_{off}^{-1}}.\n\\]\nTaking expectations over \\( x \\sim d_0 \\) and \\( a \\sim \\pi(\\cdot|x) \\):\n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq \\beta \\cdot \\mathbb{E}_{x,a} \\|\\phi(x,a) - \\nu\\|_{\\Sigma_{off}^{-1}} + \\beta \\cdot \\mathbb{E}_x \\|\\mathbb{E}_{\\pi}[\\phi] - \\nu\\|_{\\Sigma_{off}^{-1}} - \\eta \\cdot \\mathbb{E}_x D_{KL}(\\pi \\| \\hat{\\pi}).\n\\]\nThe second term is absorbed into the first via Jensen’s inequality, yielding:\n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq 2\\beta \\cdot \\mathbb{E}_{x,a} \\|\\phi(x,a) - \\nu\\|_{\\Sigma_{off}^{-1}} - \\eta \\cdot \\mathbb{E}_x D_{KL}(\\pi \\| \\hat{\\pi}).\n\\]\n\n---\n\n### **Conclusion**\nBy selecting \\( \\beta = \\mathcal{O}\\left( \\sqrt{\\frac{d + \\log(1/\\delta)}{\\gamma^2 + \\lambda B^2}} \\right) \\), the bounds hold with probability \\( 1-\\delta \\), completing the proof.", "B": "Certainly! Below is a detailed derivation and proof of **Theorem 1**, structured to clarify the key steps and assumptions.\n\n---\n\n### **Preliminaries and Assumptions**\n\n1. **Reward Parameterization**:  \n   The reward function is linear in features:  \n   \\[\n   r_{\\theta}(x,a) = \\langle \\theta, \\phi(x,a) \\rangle,\n   \\]  \n   where \\(\\|\\phi(x,a)\\| \\leq 1\\) for all \\((x,a)\\), and \\(\\|\\theta\\| \\leq B\\).\n\n2. **Offline Dataset and Covariance Matrix**:  \n   Given the offline dataset \\(D_{off}\\), the covariance matrix is:  \n   \\[\n   \\Sigma_{off} = \\lambda I + \\sum_{(x,a_1,a_2) \\in D_{off}} (\\phi(x,a_1) - \\phi(x,a_2))(\\phi(x,a_1) - \\phi(x,a_2))^{\\top}.\n   \\]  \n   Here, \\(\\lambda > 0\\) ensures \\(\\Sigma_{off}\\) is positive definite.\n\n3. **Performance Measure**:  \n   The performance of a policy \\(\\pi\\) is:  \n   \\[\n   J(\\pi) = \\mathbb{E}_{x \\sim d_0} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|x)} \\left[ r^*(x,a) + \\eta \\log \\frac{\\pi_0(a|x)}{\\pi(a|x)} \\right] \\right],\n   \\]  \n   where:\n   - \\(\\pi_0\\) is the initial policy,\n   - \\(\\eta > 0\\) is a regularization coefficient,\n   - \\(d_0\\) is the state distribution.\n\n4. **Reference Policy and Feature Expectation**:  \n   Let \\(\\nu = \\mathbb{E}_{x \\sim d_0} [\\phi(x, \\pi_{ref})]\\) for a reference policy \\(\\pi_{ref}\\).\n\n5. **Policy Class \\(\\Pi\\)**:  \n   The considered policy class \\(\\Pi\\) is assumed to be sufficiently expressive.\n\n---\n\n### **Key Steps in the Proof**\n\n#### **Step 1: Concentration of \\(\\theta\\)**\nUnder **Assumption 1**, the true reward parameter \\(\\theta^*\\) satisfies \\(\\|\\theta^*\\| \\leq B\\). The estimated \\(\\hat{\\theta}\\) (from Algorithm 1) is obtained via regularized regression:  \n\\[\n\\hat{\\theta} = \\arg\\min_{\\|\\theta\\| \\leq B} \\left[ \\sum_{(x,a_1,a_2) \\in D_{off}} \\left( \\langle \\theta, \\phi(x,a_1) - \\phi(x,a_2) \\rangle - \\text{observed preference} \\right)^2 + \\lambda \\|\\theta\\|^2 \\right].\n\\]  \nUsing standard concentration arguments (e.g., self-normalized martingale bounds [1]), with probability \\(1-\\delta\\),  \n\\[\n\\|\\hat{\\theta} - \\theta^*\\|_{\\Sigma_{off}} \\leq \\beta,\n\\]  \nwhere \\(\\beta = O\\left( \\sqrt{\\frac{d + \\log(1/\\delta)}{\\gamma^2 + \\lambda B^2}} \\right)\\), and \\(\\gamma\\) is a lower bound on the eigenvalue of \\(\\Sigma_{off}\\).\n\n#### **Step 2: Performance Difference Decomposition**\nFor any policy \\(\\pi\\), the performance difference \\(J(\\pi) - J(\\hat{\\pi})\\) can be decomposed as:  \n\\[\nJ(\\pi) - J(\\hat{\\pi}) = \\mathbb{E}_{x \\sim d_0} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|x)} [r^*(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}(\\cdot|x)} [r^*(x,a)] \\right] + \\eta \\mathbb{E}_{x \\sim d_0} \\left[ D_{KL}(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)) - D_{KL}(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\right].\n\\]  \nUsing the linearity of \\(r^*\\), the first term becomes:  \n\\[\n\\langle \\theta^*, \\mathbb{E}_{x \\sim d_0} [\\phi(x,\\pi)] - \\mathbb{E}_{x \\sim d_0} [\\phi(x,\\hat{\\pi})] \\rangle.\n\\]  \nBy the Cauchy-Schwarz inequality and the concentration bound on \\(\\|\\hat{\\theta} - \\theta^*\\|_{\\Sigma_{off}}\\),  \n\\[\n\\langle \\theta^* - \\hat{\\theta}, \\mathbb{E}_{x \\sim d_0} [\\phi(x,\\pi)] - \\nu \\rangle \\leq \\beta \\cdot \\|\\mathbb{E}_{x \\sim d_0} [\\phi(x,\\pi)] - \\nu\\|_{\\Sigma_{off}^{-1}}.\n\\]  \nThis leads to the bound for **Option I**:  \n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq 2\\beta \\cdot \\|\\mathbb{E}_{x \\sim d_0} [\\phi(x,\\pi)] - \\nu\\|_{\\Sigma_{off}^{-1}}.\n\\]\n\n#### **Step 3: Handling Option II**\nFor **Option II**, we consider the per-state deviation:  \n\\[\nJ(\\pi) - J(\\hat{\\pi}) \\leq 2\\beta \\cdot \\mathbb{E}_{x \\sim d_0, a \\sim \\pi(\\cdot|x)} \\|\\phi(x,a) - \\nu\\|_{\\Sigma_{off}^{-1}} - \\eta \\mathbb{E}_{x \\sim d_0} \\left[ D_{KL}(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)) \\right].\n\\]  \nThe first term bounds the reward estimation error, while the second term accounts for the KL divergence penalty.\n\n#### **Step 4: Finalizing the Bound**\nCombining the above, and ensuring \\(\\beta\\) is chosen as specified, we obtain the high-probability bounds for both options.\n\n---\n\n### **Conclusion**\nThe proof relies on:\n1. **Concentration of \\(\\hat{\\theta}\\)** around \\(\\theta^*\\),\n2. **Decomposition of \\(J(\\pi) - J(\\hat{\\pi})\\)** into reward and KL terms,\n3. **Application of Cauchy-Schwarz** and the feature norm bounds.\n\nThe derived bounds ensure that Algorithm 1 produces a policy \\(\\hat{\\pi}\\) with suboptimality scaling as \\(O(\\beta)\\), where \\(\\beta\\) depends on the dataset coverage (\\(\\Sigma_{off}\\)) and regularization.\n\n---\n\n### **References**\n[1] Abbasi-Yadkori, Y., Pál, D., & Szepesvári, C. (2011). *Improved algorithms for linear stochastic bandits.* NeurIPS.  \n[2] Agarwal, A., Kakade, S., & Yang, L. F. (2020). *Model-based offline RL with theoretical guarantees.* COLT.  \n\nThis completes the proof of **Theorem 1**. Let me know if you'd like any part expanded or clarified!", "C": "We start with Option I. If we set \\(\\hat{r}(x,a)=\\langle \\theta_{MLE},\\phi(x,a)\\rangle\\), and take the policy by\n\n\\[\n\\hat{\\pi}=\\arg\\max_{\\pi\\in \\Pi}\\Big[\\langle \\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]\\rangle-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\big[D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big]\\Big]\n\\]\n\nThen, we have\n\n\\[\n\\langle \\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]\\rangle+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x))-D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))\\Big]\\le 0. \\tag{20}\n\\]\n\nFor simplicity, we denote the LHS of Equation (20) as (($\\star$)). We plugging this into the estimation of \\(J(\\pi)-J(\\hat{\\pi})\\):\n\n\\[\n\\begin{aligned}\nJ(\\pi)-J(\\hat{\\pi})&=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[r^*(x,a)+\\eta\\log\\frac{\\pi_0(a|x)}{\\pi(a|x)}\\Big]\\\\\n&\\qquad\\qquad-\\mathbb{E}_{a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[r^*(x,a)+\\eta\\log\\frac{\\pi_0(a|x)}{\\hat{\\pi}(a|x)}\\Big]\\Bigg]\\\\\n&=(\\star)+\\langle \\theta^*-\\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]\\rangle+\\langle \\theta_{MLE}-\\theta^*,\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]\\rangle\\\\\n&\\qquad\\qquad-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}\\\\\n&\\le \\langle \\theta^*-\\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\rangle+\\langle \\theta_{MLE}-\\theta^*,\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\rangle\\\\\n&\\qquad\\qquad-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}\\\\\n&\\le \\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}.\n\\end{aligned}\n\\]\n\nFor Option II, we use the point-wise pessimism:\n\n\\[\n\\hat{r}(x,a)=r_{MLE}(x,a)-\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}.\n\\]\n\nThen, we call Oracle 1 with \\(\\hat{r}\\) to get \\(\\hat{\\pi}\\). By Lemma 1, we have\n\n\\[\n\\begin{aligned}\nJ(\\pi)-J(\\hat{\\pi})&=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[r^*(x,a)-\\hat{r}(x,a)\\Big]+\\mathbb{E}_{a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[\\hat{r}(x,a)-r^*(x,a)\\Big]\\\\\n&\\qquad\\qquad-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big]\\Bigg]\\\\\n&=\\mathbb{E}_{x\\sim d_0,a\\sim \\pi(\\cdot|x)}\\Big[\\langle \\theta^*-\\theta_{MLE},\\phi(x,a)-\\nu\\rangle+\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}\\Big]\\\\\n&\\qquad+\\mathbb{E}_{x\\sim d_0,a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[\\langle \\theta_{MLE}-\\theta^*,\\phi(x,a)-\\nu\\rangle-\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}\\Big]\\\\\n&\\qquad-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big]\\\\\n&\\le \\beta\\cdot\\mathbb{E}_{x\\sim d_0,a\\sim \\pi(\\cdot|x)}\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big].\n\\end{aligned}\n\\]", "D": "We start with Option I. If we set \\(\\hat{r}(x,a)=\\langle \\theta_{MLE},\\phi(x,a)\\rangle\\), and take the policy by\n\n\\[\n\\hat{\\pi}=\\arg\\max_{\\pi\\in \\Pi}\\Big[\\langle \\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]\\rangle-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\big[D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big]\\Big]\\]\n\nThen, we have\n\n\\[\n\\langle \\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]\\rangle+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x))-D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))\\Big]\\le 0. \\tag{20}\n\\]\n\nFor simplicity, we denote the LHS of Equation (20) as (($\\star$)). We plugging this into the estimation of \\(J(\\pi)-J(\\hat{\\pi})\\):\n\n\\[\n\\begin{aligned}\nJ(\\pi)-J(\\hat{\\pi})&=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[r^*(x,a)+\\eta\\log\\frac{\\pi_0(a|x)}{\\pi(a|x)}\\Big]\\\\\n&\\qquad\\qquad-\\mathbb{E}_{a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[r^*(x,a)+\\eta\\log\\frac{\\pi_0(a|x)}{\\hat{\\pi}(a|x)}\\Big]\\Bigg]\\\\\n&=(\\star)+\\langle \\theta^*-\\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]\\rangle+\\langle \\theta_{MLE}-\\theta^*,\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]\\rangle\\\\\n&\\qquad\\qquad-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}\\\\\n&\\le \\langle \\theta^*-\\theta_{MLE},\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\rangle+\\langle \\theta_{MLE}-\\theta^*,\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\rangle\\\\\n&\\qquad\\qquad-\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\hat{\\pi})]-\\nu\\|_{\\Sigma_{off}^{-1}}+\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}\\\\\n&\\le 2\\beta\\cdot\\|\\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi)]-\\nu\\|_{\\Sigma_{off}^{-1}}.\n\\end{aligned}\n\\]\n\nFor Option II, we use the point-wise pessimism:\n\n\\[\n\\hat{r}(x,a)=r_{MLE}(x,a)-\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}.\n\\]\n\nThen, we call Oracle 1 with \\(\\hat{r}\\) to get \\(\\hat{\\pi}\\). By Lemma 1, we have\n\n\\[\n\\begin{aligned}\nJ(\\pi)-J(\\hat{\\pi})&=\\mathbb{E}_{x\\sim d_0}\\Bigg[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\Big[r^*(x,a)-\\hat{r}(x,a)\\Big]+\\mathbb{E}_{a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[\\hat{r}(x,a)-r^*(x,a)\\Big]\\\\\n&\\qquad\\qquad-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big]\\Bigg]\\\\\n&=\\mathbb{E}_{x\\sim d_0,a\\sim \\pi(\\cdot|x)}\\Big[\\langle \\theta^*-\\theta_{MLE},\\phi(x,a)-\\nu\\rangle+\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}\\Big]\\\\\n&\\qquad+\\mathbb{E}_{x\\sim d_0,a\\sim \\hat{\\pi}(\\cdot|x)}\\Big[\\langle \\theta_{MLE}-\\theta^*,\\phi(x,a)-\\nu\\rangle-\\beta\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}\\Big]\\\\\n&\\qquad-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big]\\\\\n&\\le 2\\beta\\cdot\\mathbb{E}_{x\\sim d_0,a\\sim \\pi(\\cdot|x)}\\|\\phi(x,a)-\\nu\\|_{\\Sigma_{off}^{-1}}-\\eta\\cdot\\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\Big(\\pi(\\cdot|x)\\Big\\|\\hat{\\pi}(\\cdot|x)\\Big)\\Big].\n\\end{aligned}\n\\]", "ground-truth": "D"}
{"paper": "arxiv:2312.11456", "question": "Consider the following setting: Let \\(\\pi_0\\) be the initial policy and \\(\\pi^*\\) the optimal policy, with the KL penalty coefficient \\(\\eta > 0\\). Under Assumption 1, the reward function is parameterized as \\(r_\\theta(x,a)=\\langle\\theta,\\phi(x,a)\\rangle\\) for all \\((x,a)\\) with \\(\\|\\phi(x,a)\\| \\le 1\\) and \\(\\|\\theta\\| \\le B\\), and \\(\\gamma = 1/(2+\\exp(-B)+\\exp(B))\\). Moreover, assume that we have an offline dataset \\(D_{\\mathrm{off}}\\) with covariance matrix \\(\\Sigma_{\\mathrm{off}}\\) and a reference policy \\(\\pi_{\\mathrm{ref}}\\). Let \\(T = \\min\\{n \\in \\mathbb{N}_+ : n \\ge d\\log(n)\\}\\) and \\(\\beta = O\\big(\\sqrt{d\\log(T/\\delta)}/\\gamma^2\\big)\\). Then, Theorem 2 asserts that with probability at least \\(1-3\\delta\\), there exists an iteration \\(t_0 \\in [T]\\) such that Algorithm 2 with Option I satisfies\n\\[\nJ(\\pi^*) - J(\\pi_{t_0}) \\lesssim \\sqrt{\\frac{d}{\\gamma^2 m}} + \\beta \\cdot \\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\mathrm{ref}})\\big]\\Big\\|_{\\Sigma_{\\mathrm{off}}^{-1}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{\\mathrm{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\,.\n\\]\nFurthermore, if Assumption 2 holds, a similar bound---with an adjusted second term---is obtained. Could you provide a detailed derivation and proof of Theorem 2 under these settings, clearly explaining the meaning of each symbol and the underlying assumptions?", "A": "Certainly! Below is a detailed derivation and proof of **Theorem 2** under the given settings. We will break it down into key steps, explain the notation, and justify the assumptions.\n\n---\n\n### **Notation and Assumptions**\n1. **Policies**:\n   - \\(\\pi_0\\): Initial policy.\n   - \\(\\pi^*\\): Optimal policy (maximizes expected reward).\n   - \\(\\pi_{\\mathrm{ref}}\\): Reference policy (e.g., behavior policy used for data collection).\n   - \\(\\pi_t\\): Policy at iteration \\(t\\) of Algorithm 2.\n\n2. **Reward Function**:\n   - \\(r_\\theta(x,a) = \\langle \\theta, \\phi(x,a) \\rangle\\), where:\n     - \\(\\phi(x,a)\\): Feature vector for state-action pair \\((x,a)\\), with \\(\\|\\phi(x,a)\\| \\le 1\\).\n     - \\(\\theta\\): Reward parameter, with \\(\\|\\theta\\| \\le B\\).\n\n3. **KL Penalty**:\n   - \\(\\eta > 0\\): Coefficient for the KL divergence penalty (controls deviation from \\(\\pi_0\\)).\n\n4. **Offline Dataset**:\n   - \\(D_{\\mathrm{off}}\\): Dataset with covariance matrix \\(\\Sigma_{\\mathrm{off}} = \\mathbb{E}_{(x,a)\\sim D_{\\mathrm{off}}}[\\phi(x,a)\\phi(x,a)^T]\\).\n   - \\(m\\): Number of samples in \\(D_{\\mathrm{off}}\\).\n\n5. **Parameters**:\n   - \\(\\gamma = \\frac{1}{2 + \\exp(-B) + \\exp(B)}\\): A normalization factor ensuring bounded rewards.\n   - \\(T = \\min\\{n \\in \\mathbb{N}_+ : n \\ge d \\log n\\}\\): Number of iterations (depends on feature dimension \\(d\\)).\n   - \\(\\beta = O\\left(\\sqrt{d \\log(T/\\delta)} / \\gamma^2\\right)\\): Confidence parameter (scales with \\(d\\) and \\(\\log(1/\\delta)\\)).\n\n6. **Performance Measures**:\n   - \\(J(\\pi) = \\mathbb{E}_{x \\sim d_0, a \\sim \\pi(\\cdot|x)}[r_\\theta(x,a)]\\): Expected reward of policy \\(\\pi\\).\n   - \\(D_{\\mathrm{KL}}(\\pi^* \\| \\pi_t)\\): KL divergence between \\(\\pi^*\\) and \\(\\pi_t\\).\n\n7. **Assumptions**:\n   - **Assumption 1**: The reward is linear in \\(\\phi(x,a)\\), and \\(\\|\\phi(x,a)\\| \\le 1\\), \\(\\|\\theta\\| \\le B\\).\n   - **Assumption 2**: The offline dataset \\(D_{\\mathrm{off}}\\) provides sufficient coverage of \\(\\pi^*\\) (formalized later).\n\n---\n\n### **Proof of Theorem 2**\nThe theorem bounds the suboptimality gap \\(J(\\pi^*) - J(\\pi_{t_0})\\) for some \\(t_0 \\in [T]\\). We proceed step-by-step.\n\n#### **Step 1: Policy Improvement with KL Penalty**\nAlgorithm 2 (Option I) iteratively updates \\(\\pi_t\\) by solving:\n\\[\n\\pi_{t+1} = \\arg\\max_\\pi \\mathbb{E}_{x \\sim d_0, a \\sim \\pi(\\cdot|x)} \\left[ r_\\theta(x,a) - \\eta D_{\\mathrm{KL}}(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\right].\n\\]\nThis ensures that \\(\\pi_{t+1}\\) improves the reward while staying close to \\(\\pi_0\\).\n\n#### **Step 2: Performance Difference Lemma**\nUsing the performance difference lemma (Kakade & Langford, 2002), we have:\n\\[\nJ(\\pi^*) - J(\\pi_t) = \\mathbb{E}_{x \\sim d_0} \\left[ \\langle \\theta, \\phi(x,\\pi^*) - \\phi(x,\\pi_t) \\rangle \\right].\n\\]\nThis decomposes the suboptimality gap into a difference in expected feature vectors.\n\n#### **Step 3: Bounding the Feature Difference**\nThe key term is \\(\\|\\mathbb{E}_{x \\sim d_0}[\\phi(x,\\pi^*) - \\phi(x,\\pi_{\\mathrm{ref}})]\\|_{\\Sigma_{\\mathrm{off}}^{-1}}\\). Under **Assumption 2**, the offline dataset \\(D_{\\mathrm{off}}\\) ensures:\n\\[\n\\Sigma_{\\mathrm{off}} \\succeq \\lambda I \\quad \\text{(for some \\(\\lambda > 0\\))},\n\\]\nmeaning the data covers \\(\\pi^*\\) sufficiently. Thus:\n\\[\n\\|\\mathbb{E}_{x \\sim d_0}[\\phi(x,\\pi^*) - \\phi(x,\\pi_{\\mathrm{ref}})]\\|_{\\Sigma_{\\mathrm{off}}^{-1}} \\le \\sqrt{\\mathbb{E}[\\|\\phi(x,\\pi^*) - \\phi(x,\\pi_{\\mathrm{ref}})\\|^2] / \\lambda}.\n\\]\n\n#### **Step 4: Confidence Bound on \\(\\theta\\)**\nUsing standard least-squares regression analysis, with probability \\(1-\\delta\\):\n\\[\n\\|\\theta - \\hat{\\theta}\\|_{\\Sigma_{\\mathrm{off}}} \\le \\beta,\n\\]\nwhere \\(\\hat{\\theta}\\) is the estimated reward parameter. This implies:\n\\[\n|\\langle \\theta - \\hat{\\theta}, \\phi(x,\\pi^*) - \\phi(x,\\pi_t) \\rangle| \\le \\beta \\|\\phi(x,\\pi^*) - \\phi(x,\\pi_t)\\|_{\\Sigma_{\\mathrm{off}}^{-1}}.\n\\]\n\n#### **Step 5: Combining Terms**\nSubstituting into the performance difference:\n\\[\nJ(\\pi^*) - J(\\pi_t) \\le \\langle \\hat{\\theta}, \\phi(x,\\pi^*) - \\phi(x,\\pi_t) \\rangle + \\beta \\|\\phi(x,\\pi^*) - \\phi(x,\\pi_t)\\|_{\\Sigma_{\\mathrm{off}}^{-1}}.\n\\]\nThe first term is controlled by the KL penalty, and the second term is bounded by the offline data coverage.\n\n#### **Step 6: Final Bound**\nAfter \\(T\\) iterations, with probability \\(1-3\\delta\\), there exists \\(t_0 \\in [T]\\) such that:\n\\[\nJ(\\pi^*) - J(\\pi_{t_0}) \\lesssim \\sqrt{\\frac{d}{\\gamma^2 m}} + \\beta \\|\\mathbb{E}_{x \\sim d_0}[\\phi(x,\\pi^*) - \\phi(x,\\pi_{\\mathrm{ref}})]\\|_{\\Sigma_{\\mathrm{off}}^{-1}} - \\eta \\mathbb{E}_{x \\sim d_0}[D_{\\mathrm{KL}}(\\pi^* \\| \\pi_{t_0})].\n\\]\nHere:\n- \\(\\sqrt{d/(\\gamma^2 m)}\\) comes from statistical estimation error.\n- The \\(\\beta\\) term accounts for uncertainty in \\(\\theta\\).\n- The KL term ensures \\(\\pi_{t_0}\\) does not deviate too far from \\(\\pi^*\\).\n\n#### **Under Assumption 2**\nIf the offline data covers \\(\\pi^*\\) well, the second term improves to:\n\\[\n\\beta \\cdot \\text{(smaller coverage-dependent term)}.\n\\]\n\n---\n\n### **Conclusion**\nThe proof combines:\n1. **Policy optimization with KL regularization**,\n2. **Concentration bounds for least-squares regression**,\n3. **Coverage assumptions on offline data**.\n\nThe final bound balances **statistical error**, **uncertainty in \\(\\theta\\)**, and **policy deviation penalties**, ensuring near-optimal performance with high probability.\n\nLet me know if you'd like any part expanded or clarified!", "B": "We recall the value decomposition\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})=\\mathbb{E}_{x\\sim d_0}\\left[\\langle \\theta^*-\\theta_{t_0},\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\rangle\\right]\n+\\mathbb{E}_{x\\sim d_0}\\left[\\langle \\theta_{t_0}-\\theta^*,\\phi(x,\\pi_{t_0})-\\phi(x,\\pi_{\\text{ref}})\\rangle\\right]\n-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\left[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\right].\n\\]\nFollowing the proof of batch online learning (Theorem 3), we can control the exploration error \\(P'_2\\) as in Equation (12) by fixing \\(\\pi^t_2=\\pi_{\\text{ref}}\\). We notice that since \\(\\pi_{\\text{ref}}\\) is directly available to the agent and is used to collect data, we do not need to add additional optimism to relate its uncertainty to the data. Therefore, we only need to handle the suboptimality source \\(P'_1\\), which satisfies\n\\[\nP'_1\\le \\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}+D_{1:t_0}}}.\n\\]\nIt follows that\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})\\le \\Big( \\sqrt{\\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1+2\\sqrt{\\frac{\\log(2/\\delta)}{2m}}}\\;\\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}}-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\Big)\n+\\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}+D_{1:t_0}}}.\n\\]\nBy choosing \\(T\\) satisfying \\(T\\ge d\\log(T)\\) and \\(\\lambda=\\Theta\\big(\\frac{d\\log(T/\\delta)}{m\\gamma^2}\\big)\\), we have\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})=\\widetilde{O}\\Big(\\sqrt{\\frac{d}{\\gamma^2 m}}+\\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}+D_{1:t_0}}}-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\Big).\n\\]\nThis concludes the proof.", "C": "Here, we provide a detailed derivation of Theorem 2 under the given settings. The proof leverages techniques from offline reinforcement learning and inverse covariance weighting, incorporating KL regularization.\n\n---\n\n**Symbol Definitions & Assumptions**\n\n1. **Policies**:  \n   - \\(\\pi_0\\): Initial policy  \n   - \\(\\pi^*\\): Optimal policy maximizing expected return  \n   - \\(\\pi_{\\mathrm{ref}}\\): Reference policy (e.g., behavior policy for offline data)  \n\n2. **Reward Function**:  \n   - \\(r_\\theta(x,a) = \\langle \\theta, \\phi(x,a) \\rangle\\): Linear reward parameterization  \n   - \\(\\|\\phi(x,a)\\| \\leq 1\\): Bounded feature vectors  \n   - \\(\\|\\theta\\| \\leq B\\): Bounded reward parameter  \n\n3. **Constants**:  \n   - \\(\\eta > 0\\): KL penalty coefficient  \n   - \\(\\gamma = \\frac{1}{2 + e^{-B} + e^{B}}\\): Normalization factor ensuring \\(r_\\theta(x,a) \\in [-1,1]\\)  \n   - \\(T = \\min\\{n \\in \\mathbb{N}_+ : n \\ge d\\log(n\\}\\): Horizon for iterative algorithm  \n   - \\(\\beta = O\\left(\\sqrt{d\\log(T/\\delta)}/\\gamma^2\\right)\\): Confidence radius for parameter estimation  \n\n4. **Data & Covariance**:  \n   - \\(D_{\\mathrm{off}}\\): Offline dataset with covariance \\(\\Sigma_{\\mathrm{off}} = \\mathbb{E}_{D_{\\mathrm{off}}}[\\phi(x,a)\\phi(x,a)^\\top]\\)  \n   - \\(m\\): Number of offline samples  \n\n5. **Assumption 1**:  \n   - Linear realizability of rewards and bounded features.  \n   - The optimal policy \\(\\pi^*\\) is covered by the offline data in feature space:  \n     \\(\\exists \\epsilon > 0\\) s.t. \\(\\Sigma_{\\mathrm{off}} \\succeq \\epsilon \\cdot \\mathbb{E}_{x\\sim d_0}[\\phi(x,\\pi^*)\\phi(x,\\pi^*)^\\top]\\).  \n\n---\n\n**Proof Sketch**\n\n**Step 1: Reward Parameter Estimation**  \nUsing offline data \\(D_{\\mathrm{off}}\\), estimate \\(\\theta\\) via regularized least squares:  \n\\[\n\\hat{\\theta} = \\arg\\min_{\\|\\theta\\| \\leq B} \\frac{1}{m}\\sum_{(x,a,r)\\in D_{\\mathrm{off}}} \\left(r - \\langle \\theta, \\phi(x,a) \\rangle\\right)^2 + \\lambda\\|\\theta\\|^2\n\\]  \nBy standard ridge regression analysis, with probability \\(1-\\delta\\):  \n\\[\n\\|\\hat{\\theta} - \\theta^*\\|_{\\Sigma_{\\mathrm{off}}} \\leq \\beta = O\\left(\\sqrt{\\frac{d\\log(1/\\delta)}{m}}\\right)\n\\]  \nwhere \\(\\theta^*\\) is the true reward parameter.\n\n---\n\n**Step 2: Performance Difference Lemma**  \nThe policy value difference satisfies:  \n\\[\nJ(\\pi^*) - J(\\pi_t) = \\mathbb{E}_{x\\sim d_0}\\left[ Q^{\\pi_t}(x,\\pi^*) - Q^{\\pi_t}(x,\\pi_t) \\right]\n\\]  \nBy the Performance Difference Lemma, this can be bounded using advantage functions \\(A^{\\pi_t}\\).\n\n---\n\n**Step 3: Incorporating KL Regularization**  \nAlgorithm 2 (Option I) updates policies with KL penalty:  \n\\[\n\\pi_{t+1} = \\arg\\max_\\pi \\mathbb{E}_{\\pi}\\left[\\hat{Q}_t(x,a)\\right] - \\eta \\mathbb{E}_{x\\sim d_0}\\left[D_{\\mathrm{KL}}(\\pi(\\cdot|x)\\|\\pi_t(\\cdot|x))\\right]\n\\]  \nUsing Pinsker's inequality and the smoothness of KL divergence, the KL term introduces a negative definite term:  \n\\[\n-\\eta \\mathbb{E}_{x\\sim d_0}\\left[D_{\\mathrm{KL}}(\\pi^*\\|\\pi_t)\\right] \\leq -\\frac{\\eta}{2} \\mathbb{E}_{x\\sim d_0}\\left[\\|\\pi^*(\\cdot|x) - \\pi_t(\\cdot|x)\\|_1^2\\right]\n\\]  \n\n---\n\n**Step 4: Bounding Feature Discrepancy**  \nUnder Assumption 1, the suboptimality gap from feature mismatch is:  \n\\[\n\\left\\|\\mathbb{E}_{x\\sim d_0}\\left[\\phi(x,\\pi^*) - \\phi(x,\\pi_{\\mathrm{ref}})\\right]\\right\\|_{\\Sigma_{\\mathrm{off}}^{-1}} \\leq \\sqrt{\\mathbb{E}\\left[\\|\\phi(x,\\pi^*)\\|_{\\Sigma_{\\mathrm{off}}^{-1}}^2\\right]} + \\text{const}\n\\]  \nThis term quantifies the distribution shift between \\(\\pi^*\\) and the offline data.\n\n---\n\n**Step 5: Combining All Terms**  \nUsing the Bellman equation and Cauchy-Schwarz inequality:  \n\\[\nJ(\\pi^*) - J(\\pi_{t_0}) \\leq \\underbrace{\\sqrt{\\frac{d}{\\gamma^2 m}}}_{\\text{Estimation error}} + \\underbrace{\\beta \\cdot \\left\\|\\mathbb{E}[\\phi(x,\\pi^*) - \\phi(x,\\pi_{\\mathrm{ref}})]\\right\\|_{\\Sigma_{\\mathrm{off}}^{-1}}}_{\\text{Distribution shift}} - \\underbrace{\\eta \\mathbb{E}[D_{\\mathrm{KL}}(\\pi^*\\|\\pi_{t_0})]}_{\\text{Regularization}}\n\\]  \nThe first term arises from statistical estimation error (\\(O(1/\\sqrt{m})\\)), the second from limited coverage of \\(\\pi^*\\), and the third from KL regularization.\n\n---\n\n**Adjustment Under Assumption 2**  \nIf Assumption 2 (e.g., concentrability) holds, the second term is replaced by:  \n\\[\n\\beta \\cdot \\sqrt{\\mathbb{E}_{x\\sim d_0}\\left[\\|\\phi(x,\\pi^*)\\|_{\\Sigma_{\\mathrm{off}}^{-1}}^2\\right]}\n\\]  \nwhich tightens the bound when \\(\\pi_{\\mathrm{ref}}\\) has sufficient coverage of \\(\\pi^*\\).\n\n---\n\n**Probability Union Bound**  \nFailure probabilities come from three sources:  \n1. Reward estimation (\\(1-\\delta\\))  \n2. Policy optimization (\\(1-\\delta\\))  \n3. Covariance matrix concentration (\\(1-\\delta\\))  \nTaking a union bound gives total probability \\(1-3\\delta\\).\n\n---\n\n**Conclusion**  \nThe derived bound balances estimation error, distribution shift, and regularization. The KL term ensures policy updates remain conservative, while the inverse covariance norm quantifies the difficulty of recovering \\(\\pi^*\\) from offline data. The normalization \\(\\gamma\\) ensures numerical stability given the bounded reward structure.", "D": "We recall the value decomposition\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})=\\mathbb{E}_{x\\sim d_0}\\left[\\langle \\theta^*-\\theta_{t_0},\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\rangle\\right]\n+\\mathbb{E}_{x\\sim d_0}\\left[\\langle \\theta_{t_0}-\\theta^*,\\phi(x,\\pi_{t_0})-\\phi(x,\\pi_{\\text{ref}})\\rangle\\right]\n-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\left[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\right].\n\\]\nFollowing the proof of batch online learning (Theorem 3), we can control the exploration error \\(P'_2\\) as in Equation (12) by fixing \\(\\pi^t_2=\\pi_{\\text{ref}}\\). We notice that since \\(\\pi_{\\text{ref}}\\) is directly available to the agent and is used to collect data, we do not need to add additional optimism to relate its uncertainty to the data. Therefore, we only need to handle the suboptimality source \\(P'_1\\), which satisfies\n\\[\nP'_1\\le \\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}}}.\n\\]\nIt follows that\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})\\le \\Big( \\sqrt{\\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1+2\\sqrt{\\frac{\\log(2/\\delta)}{2m}}}\\;\\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}}-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\Big)\n+\\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}}}.\n\\]\nBy choosing \\(T\\) satisfying \\(T\\ge d\\log(T)\\) and \\(\\lambda=\\Theta\\big(\\frac{d\\log(T/\\delta)}{m\\gamma^2}\\big)\\), we have\n\\[\nJ(\\pi^*)-J(\\pi_{t_0})=\\widetilde{O}\\Big(\\sqrt{\\frac{d}{\\gamma^2 m}}+\\beta\\,\\Big\\|\\mathbb{E}_{x\\sim d_0}\\big[\\phi(x,\\pi^*)-\\phi(x,\\pi_{\\text{ref}})\\big]\\Big\\|_{\\Sigma^{-1}_{\\text{off}}}-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[ D_{\\text{KL}}\\big(\\pi^*(\\cdot|x)\\|\\pi_{t_0}(\\cdot|x)\\big)\\Big]\\Big).\n\\]", "ground-truth": "B"}
{"paper": "arxiv:2312.11456", "question": "Consider the following theorem (Theorem 3). Assume that the reward function is parameterized linearly as \\(r_\\theta(x,a)=\\langle \\theta, \\phi(x,a)\\rangle\\) with \\(\\|\\phi(x,a)\\|\\le 1\\) for all \\((x,a)\\), and that the true reward is given by \\(r^*(x,a)=\\langle \\theta^*, \\phi(x,a)\\rangle\\) where \\(\\|\\theta^*\\|\\le B\\) for some constant \\(B>0\\). Let the KL-regularized objective be defined as\n\\[\nJ(\\pi)=\\mathbb{E}_{x\\sim d_0}\\Big[\\mathbb{E}_{a\\sim \\pi(\\cdot|x)}\\big[r^*(x,a)\\big]-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\parallel \\pi_0(\\cdot|x))\\Big],\n\\]\nwith a given reference policy \\(\\pi_0\\) and a penalty coefficient \\(\\eta>0\\). Define the batch size as \\(m=\\frac{d}{\\gamma^2\\epsilon^2}\\), where \\(\\gamma=\\frac{1}{2+e^{-B}+e^{B}}\\) and \\(\\epsilon>0\\) is a desired accuracy level. Also, let \\(\\beta=O\\Big(\\sqrt{d}\\frac{\\log(T/\\delta)}{\\gamma^2m}\\Big)\\) and \\(\\lambda=\\Theta\\Big(\\frac{d\\log(T/\\delta)}{m\\gamma^2B^2}\\Big)\\) for a confidence parameter \\(\\delta\\). Suppose that Algorithm 2 with Option II is run for \\(T=\\min\\{n\\in\\mathbb{N}^+: n\\ge d\\log(n)\\}\\) iterations. Theorem 3 then asserts that with probability at least \\(1-3\\delta\\) there exists an iteration \\(t_0\\in[T]\\) such that\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0})\\lesssim\\epsilon-\\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel \\pi_1^{t_0}(\\cdot|x))\\Big],\n\\]\nand that the total number of collected samples is at most \\(mT=\\tilde{O}\\Big(\\frac{d^2}{\\gamma^2\\epsilon^2}\\Big)\\). Here, \\(\\pi^*\\) denotes the optimal policy and \\(\\pi_1^{t}\\) is the policy of the main agent at iteration \\(t\\) (as defined in Algorithm 2 with Option II), while \\(D_{KL}(\\cdot\\parallel\\cdot)\\) represents the Kullback-Leibler divergence. Could you provide a detailed derivation of Theorem 3 including all necessary intermediate steps and justifications", "A": "To derive Theorem 3, we proceed with the following steps:\n\n---\n\n### **Step 1: Optimal Policy Characterization**\nThe KL-regularized objective is:\n\\[\nJ(\\pi) = \\mathbb{E}_{x \\sim d_0}\\left[\\mathbb{E}_{a \\sim \\pi(\\cdot|x)}[r^*(x,a)] - \\eta D_{KL}(\\pi(\\cdot|x) \\parallel \\pi_0(\\cdot|x))\\right].\n\\]\nFor each state \\(x\\), the optimal policy \\(\\pi^*\\) maximizes the per-state objective:\n\\[\n\\pi^*(\\cdot|x) \\propto \\pi_0(\\cdot|x) \\exp\\left(\\frac{r^*(x,a)}{\\eta}\\right).\n\\]\nThe normalization factor is \\(Z(x) = \\mathbb{E}_{a \\sim \\pi_0(\\cdot|x)}[\\exp(r^*(x,a)/\\eta)]\\).\n\n---\n\n### **Step 2: Parameter Estimation Error**\nThe reward function is linear: \\(r_\\theta(x,a) = \\langle \\theta, \\phi(x,a) \\rangle\\). Using \\(m\\) samples, we estimate \\(\\theta^*\\) via ridge regression:\n\\[\n\\hat{\\theta} = \\arg\\min_\\theta \\frac{1}{m} \\sum_{i=1}^m \\left(\\langle \\theta, \\phi(x_i,a_i) \\rangle - r^*(x_i,a_i)\\right)^2 + \\lambda \\|\\theta\\|^2.\n\\]\nThe estimation error is bounded with high probability (using matrix concentration and regularization):\n\\[\n\\|\\hat{\\theta} - \\theta^*\\| \\leq \\beta = \\mathcal{O}\\left(\\sqrt{\\frac{d \\log(T/\\delta)}{\\gamma^2 m}}\\right),\n\\]\nwhere \\(\\gamma = \\frac{1}{2 + e^{-B} + e^{B}}\\) ensures the feature covariance matrix has a minimum eigenvalue \\(\\geq \\gamma^2\\).\n\n---\n\n### **Step 3: Policy Suboptimality Gap**\nLet \\(\\pi_1^t\\) be the policy using the estimated \\(\\theta_t\\). The performance gap is:\n\\[\nJ(\\pi^*) - J(\\pi_1^t) = \\mathbb{E}_{x \\sim d_0}\\left[\\langle \\theta^* - \\theta_t, \\mathbb{E}_{\\pi^*}\\phi(x,a) - \\mathbb{E}_{\\pi_1^t}\\phi(x,a) \\rangle\\right] - \\eta \\mathbb{E}_{x \\sim d_0}[D_{KL}(\\pi^* \\parallel \\pi_1^t)].\n\\]\nUsing Cauchy-Schwarz and \\(\\|\\phi(x,a)\\| \\leq 1\\):\n\\[\n\\left|\\mathbb{E}_{x \\sim d_0}[\\langle \\theta^* - \\theta_t, \\mathbb{E}_{\\pi^*}\\phi - \\mathbb{E}_{\\pi_1^t}\\phi \\rangle]\\right| \\leq \\|\\theta^* - \\theta_t\\| \\cdot \\mathbb{E}_x \\|\\mathbb{E}_{\\pi^*}\\phi - \\mathbb{E}_{\\pi_1^t}\\phi\\| \\leq 2\\beta.\n\\]\nThus:\n\\[\nJ(\\pi^*) - J(\\pi_1^t) \\leq 2\\beta - \\eta \\mathbb{E}_{x \\sim d_0}[D_{KL}(\\pi^* \\parallel \\pi_1^t)].\n\\]\n\n---\n\n### **Step 4: Batch Size and Iteration Complexity**\nTo ensure \\(2\\beta \\lesssim \\epsilon\\), set:\n\\[\n\\beta = \\mathcal{O}(\\epsilon) \\implies m = \\Theta\\left(\\frac{d \\log(T/\\delta)}{\\gamma^2 \\epsilon^2}\\right).\n\\]\nWith \\(T = \\min\\{n \\in \\mathbb{N}^+ : n \\geq d \\log n\\}\\), the total sample complexity is:\n\\[\nmT = \\tilde{\\mathcal{O}}\\left(\\frac{d^2}{\\gamma^2 \\epsilon^2}\\right).\n\\]\n\n---\n\n### **Step 5: Existence of Good Iteration**\nBy the probabilistic method, with probability \\(\\geq 1 - 3\\delta\\), there exists \\(t_0 \\in [T]\\) where:\n\\[\nJ(\\pi^*) - J(\\pi_1^{t_0}) \\lesssim \\epsilon - \\eta \\mathbb{E}_{x \\sim d_0}[D_{KL}(\\pi^* \\parallel \\pi_1^{t_0})].\n\\]\nThis follows from union bounds over estimation errors in \\(T\\) iterations.\n\n---\n\n### **Final Result**\nCombining all steps, Theorem 3 holds with:\n- **Suboptimality Gap**: \\(J(\\pi^*) - J(\\pi_1^{t_0}) \\lesssim \\epsilon - \\eta \\mathbb{E}_{x \\sim d_0}[D_{KL}(\\pi^* \\parallel \\pi_1^{t_0})]\\).\n- **Sample Complexity**: \\(\\tilde{\\mathcal{O}}\\left(\\frac{d^2}{\\gamma^2 \\epsilon^2}\\right)\\).\n\nKey factors include the linearity of rewards, regularization via KL-divergence, and concentration of measure for parameter estimation. The proof hinges on balancing estimation error (\\(\\beta\\)) and policy optimization via the KL penalty.", "B": "Recall the definition of the covariance matrix:\n\n\\(\\Sigma_{t,m} = \\lambda I + \\frac{1}{m}\\sum_{i=1}^{t-1}\\sum_{j=1}^{m} (\\phi(x_{i,j}, a_{1\\,i,j}) - \\phi(x_{i,j}, a_{2\\,i,j}))(\\phi(x_{i,j}, a_{1\\,i,j}) - \\phi(x_{i,j}, a_{2\\,i,j}))^\\top\\).\\n\\nThen, by invoking Lemma 8 for \\(\\theta_t\\) with \\(\\Sigma_D = m\\Sigma_{t,m}\\) and \\(\\lambda' = m\\lambda\\), we have with probability at least \\(1-\\delta\\), for any \\(t \\in [T]\\),\n\\[\n\\|\\theta_t - \\theta^*\\|_{\\Sigma_{t,m}} = \\frac{1}{\\sqrt{m}} \\|\\theta_t - \\theta^*\\|_{\\Sigma_D} \\le C\\sqrt{\\frac{d+\\log(T/\\delta)}{\\gamma^2 m + \\lambda B^2}}. \\tag{10}\n\\]\n\nLet\n\\[\n\\tilde{\\Sigma}_t = \\lambda I + \\sum_{i=1}^{t-1} \\mathbb{E}_{x\\sim d_0,\\, a_1\\sim\\pi_1^i,\\, a_2\\sim\\pi_2^i}\\Big[(\\phi(x,a_1)-\\phi(x,a_2))(\\phi(x,a_1)-\\phi(x,a_2))^\\top\\Big].\n\\]\n\nNow, by the elliptical potential lemma (Lemma 9), we have\n\\[\n\\sum_{t=1}^{T} \\log\\Big(1+\\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^t)-\\phi(x,\\pi_2^t)\\|^2_{\\tilde{\\Sigma}_t^{-1}}\\Big) \\le \\log\\frac{\\det(\\tilde{\\Sigma}_T)}{\\det(\\lambda I)} \\le d\\log\\Big(1+\\frac{TL^2}{\\lambda d}\\Big) := \\gamma_T(\\lambda).\n\\]\n\nSince each term on the left-hand side is positive, we know that there exists at least a \\(t_0 \\in [T]\\) such that\n\\[\n\\log\\Big(1+\\psi_{t_0}^2\\Big) \\le \\frac{1}{T}\\gamma_T(\\lambda),\n\\]\nwhere we use the shorthand notation \\(\\psi_t = \\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^t)-\\phi(x,\\pi_2^t)\\|_{\\tilde{\\Sigma}_t^{-1}}\\). It is equivalent to\n\\[\n\\psi_{t_0}^2 \\le \\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1.\n\\]\n\nWe now consider the suboptimality at iteration \\(t_0\\):\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0}) = \\mathbb{E}_{x\\sim d_0}\\Big[\\langle\\theta_{t_0}-\\theta^*,\\, \\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\rangle\\Big] \\;\\; - \\;\\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\tag{11}\n\\]\nwhere the inequality uses the Cauchy-Schwarz inequality (Lemma 7). Then, since the samples \\(\\{x_{t,i}\\}_{i=1}^m\\) are i.i.d. and for any \\(x \\in X\\)\n\\[\n\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} \\le 2\\sqrt{\\lambda},\n\\]\nwe can use Chernoff's bound (Theorem 2.16 of Zhang (2023)) to obtain that with probability at least \\(1-\\delta/2\\),\n\\[\n\\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} \\le \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}}.\n\\]\nSimilarly, we also get with probability at least \\(1-\\delta/2\\),\n\\[\n\\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} \\le \\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}}.\n\\]\n\nWe take the two inequalities above back into Equation (11) to derive with probability at least \\(1-3\\delta\\),\n\\[\n\\begin{aligned}\nJ(\\pi^*)-J(\\pi_1^{t_0}) \\le & \\Bigg( \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\sqrt{3}\\;\\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi_2^{t_0})\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\sqrt{\\mathbb{E}_{x\\sim d_0}\\Big[\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}}^2\\Big] + 2\\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & C\\Big(\\sqrt{\\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1 + 2\\sqrt{\\frac{\\log(2/\\delta)}{2m}}\\Big) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big]\\Big), \\tag{12}\n\\end{aligned}\n\\]\nwhere the second inequality applies Lemma 10 with \\(\\lambda = \\Omega\\Big(\\frac{d\\log(T/\\delta)}{m}\\Big)\\), and the last inequality uses Equation (10). By choosing \\(T\\) satisfying that \\(T \\ge d\\log(T)\\) and \\(\\lambda = \\Theta\\Big(\\frac{d\\log(T/\\delta)}{m\\gamma}\\Big)\\), we have\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0}) = \\tilde{O}\\Big(\\sqrt{\\frac{d}{\\gamma^2 m}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big]\\Big),\n\\]\nwhich concludes the proof.", "C": "Certainly! Below is a detailed derivation of **Theorem 3**, including all necessary intermediate steps and justifications. The proof follows a structured approach, leveraging tools from statistical learning theory, optimization, and reinforcement learning.\n\n---\n\n### **Preliminaries and Setup**\n1. **Reward Parameterization**:\n   - The reward function is linear in features:  \n     \\[\n     r_\\theta(x,a) = \\langle \\theta, \\phi(x,a) \\rangle,\n     \\]\n     where \\(\\|\\phi(x,a)\\| \\leq 1\\) for all \\((x,a)\\).\n   - The true reward is \\(r^*(x,a) = \\langle \\theta^*, \\phi(x,a) \\rangle\\), with \\(\\|\\theta^*\\| \\leq B\\).\n\n2. **KL-Regularized Objective**:\n   \\[\n   J(\\pi) = \\mathbb{E}_{x \\sim d_0} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|x)} [r^*(x,a)] - \\eta D_{KL}(\\pi(\\cdot|x) \\parallel \\pi_0(\\cdot|x)) \\right],\n   \\]\n   where:\n   - \\(\\pi_0\\) is a reference policy,\n   - \\(\\eta > 0\\) is a regularization coefficient.\n\n3. **Algorithm 2 with Option II**:\n   - The algorithm iteratively updates the policy \\(\\pi_1^t\\) using collected samples.\n   - The batch size is \\(m = \\frac{d}{\\gamma^2 \\epsilon^2}\\), where \\(\\gamma = \\frac{1}{2 + e^{-B} + e^B}\\).\n   - The confidence parameters are:\n     \\[\n     \\beta = O\\left( \\sqrt{d} \\frac{\\log(T/\\delta)}{\\gamma^2 m} \\right), \\quad \\lambda = \\Theta\\left( \\frac{d \\log(T/\\delta)}{m \\gamma^2 B^2} \\right).\n     \\]\n   - The number of iterations is \\(T = \\min\\{n \\in \\mathbb{N}^+ : n \\geq d \\log n\\}\\).\n\n---\n\n### **Key Steps in the Proof**\n\n#### **Step 1: Concentration of the Empirical Reward Estimator**\n- The empirical reward estimate \\(\\hat{\\theta}\\) is obtained via ridge regression:\n  \\[\n  \\hat{\\theta} = \\arg \\min_{\\theta} \\left[ \\sum_{i=1}^m \\left( r^*(x_i, a_i) - \\langle \\theta, \\phi(x_i, a_i) \\rangle \\right)^2 + \\lambda \\|\\theta\\|^2 \\right].\n  \\]\n- Using standard least-squares concentration bounds (e.g., Abbasi-Yadkori et al., 2011), with probability \\(1-\\delta\\):\n  \\[\n  \\|\\hat{\\theta} - \\theta^*\\|_{\\Sigma} \\leq \\beta,\n  \\]\n  where \\(\\Sigma = \\sum_{i=1}^m \\phi(x_i, a_i) \\phi(x_i, a_i)^T + \\lambda I\\).\n\n#### **Step 2: Bounding the Suboptimality Gap**\n- The suboptimality gap for \\(\\pi_1^t\\) is:\n  \\[\n  J(\\pi^*) - J(\\pi_1^t) = \\mathbb{E}_{x \\sim d_0} \\left[ \\langle \\theta^*, \\mathbb{E}_{a \\sim \\pi^*(\\cdot|x)} [\\phi(x,a)] - \\mathbb{E}_{a \\sim \\pi_1^t(\\cdot|x)} [\\phi(x,a)] \\rangle - \\eta D_{KL}(\\pi^* \\parallel \\pi_1^t) \\right].\n  \\]\n- Using the empirical reward estimate \\(\\hat{\\theta}\\), we decompose the error:\n  \\[\n  \\langle \\theta^*, \\phi_{\\pi^*} - \\phi_{\\pi_1^t} \\rangle = \\langle \\hat{\\theta}, \\phi_{\\pi^*} - \\phi_{\\pi_1^t} \\rangle + \\langle \\theta^* - \\hat{\\theta}, \\phi_{\\pi^*} - \\phi_{\\pi_1^t} \\rangle.\n  \\]\n- The second term is bounded via Cauchy-Schwarz and the concentration result:\n  \\[\n  \\langle \\theta^* - \\hat{\\theta}, \\phi_{\\pi^*} - \\phi_{\\pi_1^t} \\rangle \\leq \\|\\theta^* - \\hat{\\theta}\\|_{\\Sigma} \\cdot \\|\\phi_{\\pi^*} - \\phi_{\\pi_1^t}\\|_{\\Sigma^{-1}} \\leq \\beta \\cdot \\sqrt{2/m},\n  \\]\n  where we use \\(\\|\\phi_{\\pi^*} - \\phi_{\\pi_1^t}\\|_{\\Sigma^{-1}} \\leq \\sqrt{2/m}\\) due to feature normalization.\n\n#### **Step 3: Policy Improvement Guarantee**\n- The KL-regularized objective ensures that at each iteration, the policy \\(\\pi_1^t\\) improves upon \\(\\pi_0\\):\n  \\[\n  J(\\pi_1^{t+1}) \\geq J(\\pi_1^t) + \\eta \\mathbb{E}_{x \\sim d_0} [D_{KL}(\\pi_1^t \\parallel \\pi_1^{t+1})].\n  \\]\n- By telescoping, the total improvement over \\(T\\) iterations is:\n  \\[\n  J(\\pi^*) - J(\\pi_1^{t_0}) \\leq \\frac{\\beta \\sqrt{2/m} + \\epsilon}{T} + \\eta \\mathbb{E}_{x \\sim d_0} [D_{KL}(\\pi^* \\parallel \\pi_1^{t_0})].\n  \\]\n\n#### **Step 4: Final Bound and Sample Complexity**\n- Substituting \\(\\beta = O\\left( \\sqrt{d} \\frac{\\log(T/\\delta)}{\\gamma^2 m} \\right)\\) and \\(m = \\frac{d}{\\gamma^2 \\epsilon^2}\\), we obtain:\n  \\[\n  J(\\pi^*) - J(\\pi_1^{t_0}) \\lesssim \\epsilon - \\eta \\mathbb{E}_{x \\sim d_0} [D_{KL}(\\pi^* \\parallel \\pi_1^{t_0})].\n  \\]\n- The total number of samples is \\(mT = \\tilde{O}\\left( \\frac{d^2}{\\gamma^2 \\epsilon^2} \\right)\\), since \\(T = \\tilde{O}(d)\\).\n\n---\n\n### **Conclusion**\nThe proof combines:\n1. **Concentration of empirical reward estimates** (via ridge regression),\n2. **Policy improvement guarantees** (via KL-regularized optimization),\n3. **Feature normalization and suboptimality decomposition**.\n\nThe final result holds with probability \\(\\geq 1-3\\delta\\), accounting for errors in reward estimation, policy updates, and concentration bounds. The sample complexity is dominated by the need to estimate \\(\\theta^*\\) accurately in high dimensions (\\(d\\)), while the KL penalty ensures stable policy updates.\n\n---\n\n### **References**\n- Abbasi-Yadkori, Pál, and Szepesvári (2011). *Improved Algorithms for Linear Stochastic Bandits*.  \n- Kakade and Langford (2002). *Approximately Optimal Approximate Reinforcement Learning*.  \n- Modern RL theory tools (e.g., Agarwal et al., 2020).  \n\nThis completes the derivation of **Theorem 3**. Let me know if any steps need further clarification!", "D": "Recall the definition of the covariance matrix:\n\n\\(\\Sigma_{t,m} = \\lambda I + \\frac{1}{m}\\sum_{i=1}^{t-1}\\sum_{j=1}^{m} (\\phi(x_{i,j}, a_{1\\,i,j}) - \\phi(x_{i,j}, a_{2\\,i,j}))(\\phi(x_{i,j}, a_{1\\,i,j}) - \\phi(x_{i,j}, a_{2\\,i,j}))^\\top\\).\\n\\nThen, by invoking Lemma 8 for \\(\\theta_t\\) with \\(\\Sigma_D = m\\Sigma_{t,m}\\) and \\(\\lambda' = m\\lambda\\), we have with probability at least \\(1-\\delta\\), for any \\(t \\in [T]\\),\n\\[\n\\|\\theta_t - \\theta^*\\|_{\\Sigma_{t,m}} = \\frac{1}{\\sqrt{m}} \\|\\theta_t - \\theta^*\\|_{\\Sigma_D} \\le C\\sqrt{\\frac{d+\\log(T/\\delta)}{\\gamma^2 m + \\lambda B^2}}. \\tag{10}\n\\]\n\nLet\n\\[\n\\tilde{\\Sigma}_t = \\lambda I + \\sum_{i=1}^{t-1} \\mathbb{E}_{x\\sim d_0,\\, a_1\\sim\\pi_1^i,\\, a_2\\sim\\pi_2^i}\\Big[(\\phi(x,a_1)-\\phi(x,a_2))(\\phi(x,a_1)-\\phi(x,a_2))^\\top\\Big].\n\\]\n\nNow, by the elliptical potential lemma (Lemma 9), we have\n\\[\n\\sum_{t=1}^{T} \\log\\Big(1+\\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^t)-\\phi(x,\\pi_2^t)\\|^2_{\\tilde{\\Sigma}_t^{-1}}\\Big) \\le \\log\\frac{\\det(\\tilde{\\Sigma}_T)}{\\det(\\lambda I)} \\le d\\log\\Big(1+\\frac{TL^2}{\\lambda d}\\Big) := \\gamma_T(\\lambda).\n\\]\n\nSince each term on the left-hand side is positive, we know that there exists at least a \\(t_0 \\in [T]\\) such that\n\\[\n\\log\\Big(1+\\psi_{t_0}^2\\Big) \\le \\frac{1}{T}\\gamma_T(\\lambda),\n\\]\nwhere we use the shorthand notation \\(\\psi_t = \\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^t)-\\phi(x,\\pi_2^t)\\|_{\\tilde{\\Sigma}_t^{-1}}\\). It is equivalent to\n\\[\n\\psi_{t_0}^2 \\le \\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1.\n\\]\n\nWe now consider the suboptimality at iteration \\(t_0\\):\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0}) = \\mathbb{E}_{x\\sim d_0}\\Big[\\langle\\theta_{t_0}-\\theta^*,\\, \\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\rangle\\Big] \\;\\; - \\;\\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\tag{11}\n\\]\nwhere the inequality uses the Cauchy-Schwarz inequality (Lemma 7). Then, since the samples \\(\\{x_{t,i}\\}_{i=1}^m\\) are i.i.d. and for any \\(x \\in X\\)\n\\[\n\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} \\le 2\\sqrt{\\lambda},\n\\]\nwe can use Chernoff's bound (Theorem 2.16 of Zhang (2023)) to obtain that with probability at least \\(1-\\delta/2\\),\n\\[\n\\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} \\le \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}}.\n\\]\nSimilarly, we also get with probability at least \\(1-\\delta/2\\),\n\\[\n\\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} \\le \\mathbb{E}_{x\\sim d_0}\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}}.\n\\]\n\nWe take the two inequalities above back into Equation (11) to derive with probability at least \\(1-3\\delta\\),\n\\[\n\\begin{aligned}\nJ(\\pi^*)-J(\\pi_1^{t_0}) \\le & \\Bigg( \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi^*)\\|_{\\Sigma_{t_0,m}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\sqrt{3}\\;\\frac{1}{m}\\sum_{i=1}^{m}\\|\\phi(x_{t_0,i},\\pi_1^{t_0})-\\phi(x_{t_0,i},\\pi_2^{t_0})\\|_{\\tilde{\\Sigma}_{t_0}^{-1}} + \\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & \\Bigg( \\sqrt{\\mathbb{E}_{x\\sim d_0}\\Big[\\|\\phi(x,\\pi_1^{t_0})-\\phi(x,\\pi^*)\\|_{\\tilde{\\Sigma}_{t_0}^{-1}}^2\\Big] + 2\\sqrt{\\frac{\\log(2/\\delta)}{2m}} \\Bigg) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} \\\\ & \\quad - \\; \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big] \\\\ \\le & C\\Big(\\sqrt{\\exp\\Big(\\frac{\\gamma_T(\\lambda)}{T}\\Big)-1 + 2\\sqrt{\\frac{\\log(2/\\delta)}{2m}}\\Big) \\cdot \\|\\theta_{t_0}-\\theta^*\\|_{\\Sigma_{t_0,m}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big]\\Big), \\tag{12}\n\\end{aligned}\n\\]\nwhere the second inequality applies Lemma 10 with \\(\\lambda = \\Omega\\Big(\\frac{d\\log(T/\\delta)}{m}\\Big)\\), and the last inequality uses Equation (10). By choosing \\(T\\) satisfying that \\(T \\ge d\\log(T)\\) and \\(\\lambda = \\Theta\\Big(\\frac{d\\log(T/\\delta)}{m\\gamma^2}\\Big)\\), we have\n\\[\nJ(\\pi^*)-J(\\pi_1^{t_0}) = \\tilde{O}\\Big(\\sqrt{\\frac{d}{\\gamma^2 m}} - \\eta\\,\\mathbb{E}_{x\\sim d_0}\\Big[D_{KL}(\\pi^*(\\cdot|x)\\parallel\\pi_1^{t_0}(\\cdot|x))\\Big]\\Big),\n\\]\nwhich concludes the proof.", "ground-truth": "D"}
{"paper": "arxiv:2312.11456", "question": "Let \\(\\Pi\\) be a policy class and let \\(\\pi, \\hat{\\pi} \\in \\Pi\\) be two policies that, along with a reference policy \\(\\pi_0\\), share the same support. Let \\(d_0\\) denote the distribution over states (or prompts), and let \\(\\hat{r}: X \\times A \\to \\mathbb{R}\\) be an estimated reward function. Suppose that \\(\\hat{\\pi}\\) is generated by applying Oracle 1 with \\(\\hat{r}\\). Under these conditions, Lemma 2 states that\n\n\\[\n\\mathbb{E}_{x\\sim d_0}\\Big[\\mathbb{E}_{a\\sim \\pi}\\big[\\hat{r}(x,a)\\big] - \\mathbb{E}_{a\\sim \\hat{\\pi}}\\big[\\hat{r}(x,a)\\big] + \\eta\\, D_{KL}\\big(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big) - \\eta\\, D_{KL}\\big(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big)\\Big] = -\\eta\\, \\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\big(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x)\\big)\\Big].\n\\]\n\nCould you provide a complete derivation or proof of this lemma?", "A": "Certainly! Below is the detailed derivation of Lemma 2, which connects the expected reward difference between policies \\(\\pi\\) and \\(\\hat{\\pi}\\) with their KL divergences relative to \\(\\pi_0\\) and each other.\n\n---\n\n### **Derivation of Lemma 2**\n\nGiven:\n- Policies \\(\\pi, \\hat{\\pi}, \\pi_0\\) share the same support.\n- \\(\\hat{\\pi}\\) is obtained by applying Oracle 1 with the estimated reward \\(\\hat{r}\\).\n- Oracle 1 solves the following optimization problem:\n  \\[\n  \\hat{\\pi} = \\arg\\max_{\\pi' \\in \\Pi} \\mathbb{E}_{x \\sim d_0, a \\sim \\pi'}\\big[\\hat{r}(x,a)\\big] - \\eta D_{KL}\\big(\\pi'(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big).\n  \\]\n  The solution to this is known to be:\n  \\[\n  \\hat{\\pi}(a|x) = \\frac{\\pi_0(a|x) e^{\\hat{r}(x,a)/\\eta}}{Z(x)},\n  \\]\n  where \\(Z(x) = \\mathbb{E}_{a \\sim \\pi_0}\\big[e^{\\hat{r}(x,a)/\\eta}\\big]\\) is the partition function.\n\n---\n\n#### **Step 1: Express \\(\\hat{r}(x,a)\\) in terms of \\(\\hat{\\pi}\\) and \\(\\pi_0\\)**\nFrom the optimal form of \\(\\hat{\\pi}\\), we have:\n\\[\n\\hat{r}(x,a) = \\eta \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) + \\eta \\log Z(x).\n\\]\n\n---\n\n#### **Step 2: Compute \\(\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)]\\) and \\(\\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)]\\)**\nUsing the expression for \\(\\hat{r}(x,a)\\):\n\\[\n\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] = \\eta \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] + \\eta \\log Z(x),\n\\]\n\\[\n\\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] = \\eta \\mathbb{E}_{a \\sim \\hat{\\pi}}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] + \\eta \\log Z(x).\n\\]\n\nSubtracting the two expectations:\n\\[\n\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] = \\eta \\left( \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] - \\mathbb{E}_{a \\sim \\hat{\\pi}}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] \\right).\n\\]\n\n---\n\n#### **Step 3: Relate to KL Divergences**\nRecall that the KL divergence is defined as:\n\\[\nD_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) = \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\pi(a|x)}{\\hat{\\pi}(a|x)} \\right) \\right],\n\\]\n\\[\nD_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) = \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\pi(a|x)}{\\pi_0(a|x)} \\right) \\right],\n\\]\n\\[\nD_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) = \\mathbb{E}_{a \\sim \\hat{\\pi}}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right].\n\\]\n\nThus, we can rewrite the difference as:\n\\[\n\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] = \\eta \\left( D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) - D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\right).\n\\]\n\n---\n\n#### **Step 4: Rearrange Terms**\nSubstituting back into the original expression:\n\\[\n\\mathbb{E}_{x \\sim d_0} \\Big[ \\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] + \\eta D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - \\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\Big]\n\\]\n\\[\n= \\mathbb{E}_{x \\sim d_0} \\Big[ \\eta \\left( D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) - D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\right) + \\eta D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - \\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\Big]\n\\]\n\\[\n= \\mathbb{E}_{x \\sim d_0} \\Big[ -\\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) \\Big].\n\\]\n\n---\n\n### **Final Result**\nThus, we have:\n\\[\n\\mathbb{E}_{x \\sim d_0} \\Big[ \\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] + \\eta D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - \\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\Big] = -\\eta \\mathbb{E}_{x \\sim d_0} \\Big[ D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) \\Big].\n\\]\n\nThis completes the proof. The key steps involve:\n1. Expressing \\(\\hat{r}(x,a)\\) in terms of \\(\\hat{\\pi}\\) and \\(\\pi_0\\),\n2. Relating expectations to KL divergences,\n3. Simplifying the expression by cancellation.\n\nThe result shows that the difference in expected rewards, adjusted by KL penalties, reduces to the negative KL divergence between \\(\\pi\\) and \\(\\hat{\\pi}\\).", "B": "Since \\(\\hat{\\pi}\\) is induced by calling Oracle 1 with \\(\\hat{r}\\), we know that for any \\(x \\in X\\),\n\n\\[\n\\hat{\\pi}(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\, \\hat{r}(x,a)\\Big),\n\\]\n\nwhere \\(Z(x)=\\sum_{a \\in A}\\pi_0(a|x)\\exp\\Big(\\frac{1}{\\eta}\\hat{r}(x,a)\\Big)\\) is the normalization constant. We can rewrite the reward function as\n\n\\[\n\\hat{r}(x,a)=\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}+\\eta\\log Z(x).\n\\]\n\nPlugging this reward reparameterization into the policy optimization error under \\(\\hat{r}\\), we have\n\n\\[\n\\mathbb{E}_{\\pi}[\\hat{r}(x,a)]-\\mathbb{E}_{\\hat{\\pi}}[\\hat{r}(x,a)] = \\mathbb{E}_{\\pi}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big] - \\mathbb{E}_{\\hat{\\pi}}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big]\n\\]\n\n\\[\n= \\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x))-\\eta\\,D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)).\n\\]\n\nPlugging the above equality into the left-hand side of Lemma 2 completes the proof.", "C": "Since $\\hat{\\pi}$ is induced by calling Oracle 1 with $\\hat{r}$, we know that for any $x \\in X$,\n\n\\[\n\\hat{\\pi}(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\, \\hat{r}(x,a)\\Big),\n\\]\n\nwhere $Z(x)=\\sum_{a \\in A}\\pi_0(a|x)\\exp\\Big(\\frac{1}{\\eta}\\hat{r}(x,a)\\Big)$ is the normalization constant. We can rewrite the reward function as\n\n\\[\n\\hat{r}(x,a)=\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}+\\eta\\log Z(x).\n\\]\n\nPlugging this reward reparameterization into the policy optimization error under $\\hat{r}$, we have\n\n\\[\n\\mathbb{E}_{\\pi}[\\hat{r}(x,a)]-\\mathbb{E}_{\\hat{\\pi}}[\\hat{r}(x,a)] = \\mathbb{E}_{\\pi}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big] - \\mathbb{E}_{\\hat{\\pi}}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big]\n\\]\n\n\\[\n= \\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x))+\\eta\\,D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)).\n\\]\n\nPlugging the above equality into the left-hand side of Lemma 2 completes the proof.", "D": "To prove Lemma 2, we leverage the optimality condition of \\(\\hat{\\pi}\\) derived from Oracle 1 and simplify the given expression using properties of KL divergence. Here's the complete derivation:\n\n---\n\n**Step 1: Optimal Policy \\(\\hat{\\pi}\\) via Oracle 1**  \nOracle 1 generates \\(\\hat{\\pi}\\) by solving the regularized optimization problem for each \\(x\\):  \n\\[\n\\hat{\\pi}(\\cdot|x) = \\arg\\max_{\\pi'} \\Big[ \\mathbb{E}_{a\\sim\\pi'}\\big[\\hat{r}(x,a)\\big] - \\eta D_{KL}\\big(\\pi'(\\cdot|x) \\,\\|\\, \\pi_0(\\cdot|x)\\big) \\Big].\n\\]\nThe solution takes the form:  \n\\[\n\\hat{\\pi}(a|x) = \\frac{\\pi_0(a|x) \\exp\\left(\\frac{\\hat{r}(x,a)}{\\eta}\\right)}{Z(x)}, \\quad Z(x) = \\mathbb{E}_{a\\sim\\pi_0}\\left[\\exp\\left(\\frac{\\hat{r}(x,a)}{\\eta}\\right)\\right].\n\\]\n\n**Step 2: Express \\(\\hat{r}(x,a)\\) via Optimality Condition**  \nTaking logarithms of \\(\\hat{\\pi}(a|x)\\):  \n\\[\n\\log \\hat{\\pi}(a|x) = \\log \\pi_0(a|x) + \\frac{\\hat{r}(x,a)}{\\eta} - \\log Z(x).\n\\]\nRearranging yields:  \n\\[\n\\hat{r}(x,a) = \\eta \\left( \\log \\hat{\\pi}(a|x) - \\log \\pi_0(a|x) + \\log Z(x) \\right).\n\\]\n\n**Step 3: Substitute \\(\\hat{r}(x,a)\\) into the LHS**  \nThe LHS of the lemma becomes:  \n\\[\n\\mathbb{E}_{x\\sim d_0} \\Bigg[ \\underbrace{\\mathbb{E}_{a\\sim\\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a\\sim\\hat{\\pi}}[\\hat{r}(x,a)]}_{\\text{Term 1}} + \\eta \\underbrace{D_{KL}\\big(\\hat{\\pi} \\,\\|\\, \\pi_0\\big)}_{\\text{Term 2}} - \\eta \\underbrace{D_{KL}\\big(\\pi \\,\\|\\, \\pi_0\\big)}_{\\text{Term 3}} \\Bigg].\n\\]\n\n**Step 4: Expand Term 1**  \nSubstitute \\(\\hat{r}(x,a)\\) into Term 1:  \n\\[\n\\text{Term 1} = \\eta \\mathbb{E}_{a\\sim\\pi}\\left[ \\log \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} + \\log Z(x) \\right] - \\eta \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[ \\log \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} + \\log Z(x) \\right].\n\\]\nSimplify using \\(\\mathbb{E}_{a\\sim p}[\\log Z(x)] = \\log Z(x)\\):  \n\\[\n\\text{Term 1} = \\eta \\left( \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] \\right).\n\\]\n\n**Step 5: Expand Terms 2 and 3**  \nTerm 2:  \n\\[\nD_{KL}\\big(\\hat{\\pi} \\,\\|\\, \\pi_0\\big) = \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[ \\log \\frac{\\hat{\\pi}}{\\pi_0} \\right].\n\\]\nTerm 3:  \n\\[\nD_{KL}\\big(\\pi \\,\\|\\, \\pi_0\\big) = \\mathbb{E}_{a\\sim\\pi}\\left[ \\log \\frac{\\pi}{\\pi_0} \\right].\n\\]\n\n**Step 6: Combine All Terms**  \nSubstitute Terms 1-3 into the LHS:  \n\\[\n\\eta \\mathbb{E}_{x\\sim d_0} \\Bigg[ \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] + \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\pi}{\\pi_0}\\right] \\Bigg].\n\\]\nSimplify by cancellation:  \n\\[\n\\eta \\mathbb{E}_{x\\sim d_0} \\Bigg[ \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\pi}{\\pi_0}\\right] \\Bigg].\n\\]\nFactor out \\(\\mathbb{E}_{a\\sim\\pi}\\):  \n\\[\n\\eta \\mathbb{E}_{x\\sim d_0} \\mathbb{E}_{a\\sim\\pi} \\left[ \\log \\frac{\\hat{\\pi}}{\\pi_0} - \\log \\frac{\\pi}{\\pi_0} \\right] = \\eta \\mathbb{E}_{x\\sim d_0} \\mathbb{E}_{a\\sim\\pi} \\left[ \\log \\frac{\\hat{\\pi}}{\\pi} \\right].\n\\]\n\n**Step 7: Relate to \\(D_{KL}(\\pi \\,\\|\\, \\hat{\\pi})\\)**  \nRecognize that:  \n\\[\n\\mathbb{E}_{a\\sim\\pi} \\left[ \\log \\frac{\\hat{\\pi}}{\\pi} \\right] = -D_{KL}\\big(\\pi \\,\\|\\, \\hat{\\pi}\\big).\n\\]\nThus:  \n\\[\n\\text{LHS} = -\\eta \\mathbb{E}_{x\\sim d_0} \\Big[ D_{KL}\\big(\\pi(\\cdot|x) \\,\\|\\, \\hat{\\pi}(\\cdot|x)\\big) \\Big].\n\\]\n\n---\n\nThis completes the proof of Lemma 2. The key steps involve substituting the optimal policy's form, leveraging the relationship between \\(\\hat{r}\\) and \\(\\hat{\\pi}\\), and simplifying using KL divergence properties.", "ground-truth": "B"}
{"paper": "arxiv:2312.11456", "question": "Given a preference dataset \\(D_{off}\\) consisting of tuples \\((x, a_w, a_l)\\) where for each prompt \\(x\\) the action \\(a_w\\) is preferred over \\(a_l\\), a reference policy \\(\\pi_0(a|x)\\) and a learned policy \\(\\pi_{\\theta}(a|x)\\), along with a positive scaling parameter \\(\\eta\\) and an uncertainty estimator \\(\\Gamma(x,a)\\) that quantifies the uncertainty of taking action \\(a\\) under context \\(x\\), Proposition 1 states that one can implement Option II of Algorithm 1 by minimizing the loss function\n\\[\nL_{D_{off}}(\\theta, \\pi_0) = \\sum_{(x, a_w, a_l) \\in D_{off}} \\log \\sigma \\Big( \\eta \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} + \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big)\\]\nwhere the term \\(\\Gamma(x,a_w) - \\Gamma(x,a_l)\\) serves as an adaptive margin based on the uncertainty difference between the two actions. Could you provide a complete derivation or proof of Proposition 1 that explains how this loss function is obtained?", "A": "For notation simplicity, we denote the uncertainty bonus as \\(\\Gamma(x, a)\\). We first recall that in Algorithm 1, we optimize the following KL-regularized target:\n\n\\[\n\\hat{\\pi} = \\arg\\max_{\\pi} \\mathbb{E}_{x \\sim d_0, a \\sim \\pi(\\cdot|x)}\\Big[ r_{MLE}(x,a) - \\Gamma(x,a) - \\eta \\log \\frac{\\pi(a|x)}{\\pi_0(a|x)} \\Big]\n\\]  \\(21\\)\n\nwhere \\(r_{MLE}\\) is the MLE of the BT model on the offline preference dataset \\(D\\) obtained via\n\n\\[\nr_{MLE} = \\arg\\max_r \\sum_{(x,a_w,a_l) \\in D_{off}} \\log \\sigma \\Big( r(x,a_w) - r(x,a_l) \\Big)\n\\]  \\(22\\)\n\nAccording to Lemma 11, for any fixed \\(r\\), we have the following closed-form policy for Equation (21):\n\n\\[\n\\tilde{\\pi}_r(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\big( r(x,a) - \\Gamma(x,a) \\big) \\Big)\n\\]  \\(23\\)\n\nWe can solve the reward as\n\n\\[\nr(x,a) = \\Gamma(x,a) + \\eta \\log \\frac{\\tilde{\\pi}_r(a|x)}{\\pi_0(a|x)} + \\eta \\log Z(x)\n\\]  \\(24\\)\n\nWe can plug Equation (24) into Equation (22) to get\n\n\\[\n\\hat{\\pi} = \\arg\\max_{\\tilde{\\pi}_r} \\sum_{(x,a_w,a_l) \\in D_{off}} \\log \\sigma \\Big( \\eta \\log \\frac{\\pi_r(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_r(a_l|x)}{\\pi_0(a_l|x)} + \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big)\n\\]  \\(25\\)\n\nwhere the uncertainty serves as an adaptive margin.\n\nClearly, if \\(r\\) is the solution of Equation (22), then the \\(\\pi_r\\) is the solution of Equation (25). In contrast, if \\(\\pi\\) is optimal for the DPO target in Equation (25), then, the induced implicit reward \\(\\eta \\log \\frac{\\pi(y|x)}{\\pi_0(y|x)} - \\Gamma(x,a)\\) is optimal for Equation (22).", "B": "Certainly! Below is a step-by-step derivation of the loss function \\( L_{D_{off}}(\\theta, \\pi_0) \\) from Proposition 1, explaining how it arises from the given preference dataset and uncertainty estimator.\n\n---\n\n### **Derivation of the Loss Function**\n\n#### **1. Preference Modeling via the Bradley-Terry Model**\nThe Bradley-Terry model is commonly used to model pairwise preferences. Given two actions \\( a_w \\) (preferred) and \\( a_l \\) (less preferred) for a prompt \\( x \\), the probability that \\( a_w \\) is preferred over \\( a_l \\) is modeled as:\n\n\\[\nP(a_w \\succ a_l | x) = \\sigma \\Big( \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_{\\theta}(a_l|x)} \\Big),\n\\]\n\nwhere \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\) is the logistic function, and \\( \\pi_{\\theta}(a|x) \\) is the learned policy.\n\n#### **2. Incorporating the Reference Policy \\( \\pi_0 \\)**\nTo prevent the learned policy \\( \\pi_{\\theta} \\) from deviating too far from the reference policy \\( \\pi_0 \\), we introduce a KL-divergence regularization term. The regularized objective maximizes:\n\n\\[\n\\log \\pi_{\\theta}(a_w|x) - \\log \\pi_{\\theta}(a_l|x) - \\frac{1}{\\eta} \\Big( \\text{KL}(\\pi_{\\theta}(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\Big).\n\\]\n\nFor small deviations, the KL-divergence can be approximated as:\n\n\\[\n\\text{KL}(\\pi_{\\theta}(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\approx \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} + \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)}.\n\\]\n\nThus, the preference log-likelihood becomes:\n\n\\[\n\\log \\sigma \\Big( \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_{\\theta}(a_l|x)} - \\frac{1}{\\eta} \\Big( \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} + \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} \\Big) \\Big).\n\\]\n\n#### **3. Simplifying the Expression**\nRearranging terms, we obtain:\n\n\\[\n\\log \\sigma \\Big( \\eta \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} \\Big).\n\\]\n\nThis is similar to the standard KL-regularized preference loss, but with an additional scaling factor \\( \\eta \\).\n\n#### **4. Introducing the Uncertainty-Based Margin**\nThe uncertainty estimator \\( \\Gamma(x,a) \\) quantifies the uncertainty of taking action \\( a \\) under context \\( x \\). To incorporate this, we introduce an **adaptive margin** based on the uncertainty difference between \\( a_w \\) and \\( a_l \\):\n\n\\[\n\\Gamma(x,a_w) - \\Gamma(x,a_l).\n\\]\n\nIf \\( \\Gamma(x,a_w) > \\Gamma(x,a_l) \\), the margin increases, making it harder for \\( \\pi_{\\theta} \\) to prefer \\( a_w \\), since it is more uncertain. Conversely, if \\( \\Gamma(x,a_w) < \\Gamma(x,a_l) \\), the margin decreases, making it easier to prefer \\( a_w \\).\n\nAdding this margin to the log-likelihood, we get:\n\n\\[\n\\log \\sigma \\Big( \\eta \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} + \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big).\n\\]\n\n#### **5. Final Loss Function**\nSumming over all preference tuples \\( (x, a_w, a_l) \\in D_{off} \\), we obtain the final loss function:\n\n\\[\nL_{D_{off}}(\\theta, \\pi_0) = \\sum_{(x, a_w, a_l) \\in D_{off}} \\log \\sigma \\Big( \\eta \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} + \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big).\n\\]\n\n---\n\n### **Interpretation**\n- The term \\( \\eta \\log \\frac{\\pi_{\\theta}(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_{\\theta}(a_l|x)}{\\pi_0(a_l|x)} \\) ensures that the learned policy \\( \\pi_{\\theta} \\) stays close to \\( \\pi_0 \\) while still fitting the preference data.\n- The term \\( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\) acts as an **adaptive margin**, penalizing uncertain actions and encouraging the model to prefer actions with lower uncertainty.\n\nThis loss function effectively balances **preference learning**, **regularization**, and **uncertainty-aware adaptation**, making it suitable for robust policy optimization in offline settings.\n\n---\n\nWould you like any further clarification on any step?", "C": "To derive the loss function \\( L_{D_{off}}(\\theta, \\pi_0) \\) in Proposition 1, we start by constructing a probabilistic model for the preference data and incorporate the uncertainty-based adaptive margin. The key steps are as follows:\n\n---\n\n### **1. Preference Probability via Bradley-Terry Model**\nAssume the preference probability of \\( a_w \\succ a_l \\) given \\( x \\) follows the Bradley-Terry model, augmented with an uncertainty-dependent margin:\n\\[\nP(a_w \\succ a_l | x) = \\sigma\\left( \\underbrace{\\eta \\log \\frac{\\pi_\\theta(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_\\theta(a_l|x)}{\\pi_0(a_l|x)}}_{\\text{Policy improvement term}} + \\underbrace{\\Gamma(x, a_w) - \\Gamma(x, a_l)}_{\\text{Uncertainty margin}} \\right),\n\\]\nwhere:\n- **Policy improvement term**: Measures the relative improvement of \\( \\pi_\\theta \\) over \\( \\pi_0 \\) for \\( a_w \\) vs. \\( a_l \\), scaled by \\( \\eta \\).\n- **Uncertainty margin**: Quantifies the difference in uncertainty between \\( a_w \\) and \\( a_l \\), acting as an adaptive margin.\n\n---\n\n### **2. Likelihood Function**\nGiven the dataset \\( D_{off} \\), the likelihood of observing all preference pairs is:\n\\[\n\\mathcal{L}(\\theta) = \\prod_{(x, a_w, a_l) \\in D_{off}} P(a_w \\succ a_l | x).\n\\]\nTaking the negative logarithm (to convert maximization into minimization), we get the **negative log-likelihood**:\n\\[\nL_{D_{off}}(\\theta, \\pi_0) = -\\sum_{(x, a_w, a_l) \\in D_{off}} \\log \\sigma\\left( \\eta \\log \\frac{\\pi_\\theta(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_\\theta(a_l|x)}{\\pi_0(a_l|x)} + \\Gamma(x, a_w) - \\Gamma(x, a_l) \\right).\n\\]\nHowever, in Proposition 1, the loss is expressed as the **sum of log-sigmoid terms** (without the negative sign). This implies the objective is to **maximize the original likelihood** \\( \\mathcal{L}(\\theta) \\), equivalent to minimizing the negative log-likelihood.\n\n---\n\n### **3. Interpretation of the Loss Function**\nThe loss function encourages:\n1. **Policy Improvement**: \\( \\pi_\\theta(a_w|x) \\) increases relative to \\( \\pi_0(a_w|x) \\), while \\( \\pi_\\theta(a_l|x) \\) decreases relative to \\( \\pi_0(a_l|x) \\).\n2. **Uncertainty Margin**: The term \\( \\Gamma(x, a_w) - \\Gamma(x, a_l) \\) adjusts the effective margin dynamically. If \\( \\Gamma(x, a_w) < \\Gamma(x, a_l) \\), the margin becomes negative, requiring stronger policy improvement to compensate.\n\n---\n\n### **4. Connection to Algorithm 1 (Option II)**\nOption II of Algorithm 1 implicitly performs **weighted contrastive learning**:\n- The policy improvement term ensures \\( \\pi_\\theta \\) deviates from \\( \\pi_0 \\) in directions preferred by \\( D_{off} \\).\n- The uncertainty margin regularizes the policy by penalizing actions with high uncertainty, enhancing robustness.\n\n---\n\n### **5. Derivation Summary**\nBy modeling the preference probability with the uncertainty-adjusted margin and applying maximum likelihood estimation, we derive:\n\\[\nL_{D_{off}}(\\theta, \\pi_0) = \\sum_{(x, a_w, a_l) \\in D_{off}} \\log \\sigma\\left( \\eta \\log \\frac{\\pi_\\theta(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_\\theta(a_l|x)}{\\pi_0(a_l|x)} + \\Gamma(x, a_w) - \\Gamma(x, a_l) \\right).\n\\]\nMinimizing this loss maximizes the likelihood of the preference data under the modeled probabilities, incorporating both policy improvement and uncertainty-based regularization.", "D": "For notation simplicity, we denote the uncertainty bonus as $\\Gamma(x, a)$. We first recall that in Algorithm 1, we optimize the following KL-regularized target:\n\n\\[\n\\hat{\\pi} = \\arg\\max_{\\pi} \\mathbb{E}_{x \\sim d_0, a \\sim \\pi(\\cdot|x)}\\Big[ r_{MLE}(x,a) - \\Gamma(x,a) - \\eta \\log \\frac{\\pi(a|x)}{\\pi_0(a|x)} \\Big]\n\\]  $21$\n\nwhere $r_{MLE}$ is the MLE of the BT model on the offline preference dataset $D$ obtained via\n\n\\[\nr_{MLE} = \\arg\\max_r \\sum_{(x,a_w,a_l) \\in D_{off}} \\log \\sigma \\Big( r(x,a_w) - r(x,a_l) \\Big)\n\\]  $22$\n\nAccording to Lemma 11, for any fixed $r$, we have the following closed-form policy for Equation (21):\n\n\\[\n\\tilde{\\pi}_r(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\big( r(x,a) - \\Gamma(x,a) \\big) \\Big)\n\\]  $23$\n\nWe can solve the reward as\n\n\\[\nr(x,a) = \\Gamma(x,a) + \\eta \\log \\frac{\\tilde{\\pi}_r(a|x)}{\\pi_0(a|x)} + \\eta \\log Z(x)\n\\]  $24$\n\nWe can plug Equation (24) into Equation (22) to get\n\n\\[\n\\hat{\\pi} = \\arg\\max_{\\tilde{\\pi}_r} \\sum_{(x,a_w,a_l) \\in D_{off}} \\log \\sigma \\Big( \\eta \\log \\frac{\\pi_r(a_w|x)}{\\pi_0(a_w|x)} - \\eta \\log \\frac{\\pi_r(a_l|x)}{\\pi_0(a_l|x)} - \\big( \\Gamma(x,a_w) - \\Gamma(x,a_l) \\big) \\Big)\n\\]  $25$\n\nwhere the uncertainty serves as an adaptive margin.\n\nClearly, if $r$ is the solution of Equation (22), then the $\\pi_r$ is the solution of Equation (25). In contrast, if $\\pi$ is optimal for the DPO target in Equation (25), then, the induced implicit reward $\\eta \\log \\frac{\\pi(y|x)}{\\pi_0(y|x)} - \\Gamma(x,a)$ is optimal for Equation (22).", "ground-truth": "A"}
{"paper": "arxiv:2405.19107", "question": "Consider the following setting.\n\n  Let $x \\in \\mathcal{X}$ be a prompt sampled from a distribution $\\rho$, and let $y \\in \\mathcal{Y}$ be a completion sampled from an (unknown) behaviour policy $\\mu(\\cdot|x)$. A scalar reward $r(x,y) \\in \\mathbb{R}$ is observed for each pair $(x,y)$.\n\n  Let $\\pi(\\cdot|x)$ be a conditional policy to be learned, and let $\\pi_{\\mathrm{ref}}(\\cdot|x)$ be a fixed reference policy. Fix a temperature parameter $\\tau>0$.\n\n  For any pair $(\\pi,V)$ with $V:\\mathcal{X}\\to\\mathbb{R}$ define the loss\n\\[\nL(\\pi,V)=\\tfrac{1}{2}\\,\\mathbb{E}_{x\\sim\\rho,\\;y\\sim\\mu(\\cdot|x)}\\Bigl[\\,r(x,y)-V(x)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr]^2.\n\\]\n\n  For a fixed policy $\\pi$ define the minimising value function\n\\[\nV^{\\pi}(x)=\\operatorname*{argmin}_{V}L(\\pi,V)=\\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Bigl[r(x,y)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr].\n\\]\n\n  The optimal KL-regularised policy is\n\\[\n\\pi^{\\ast}(y\\mid x)=\\frac{\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\bigl(\\tfrac{1}{\\tau}\\,r(x,y)\\bigr)}{\\sum_{y'}\\pi_{\\mathrm{ref}}(y'\\mid x)\\exp\\bigl(\\tfrac{1}{\\tau}\\,r(x,y')\\bigr)}.\n\\]\n\nAssume that for every $x$ the support of $\\mu(\\cdot|x)$ coincides with that of $\\pi_{\\mathrm{ref}}(\\cdot|x)$.\n\n**Proposition 1.**\nLet $V:\\mathcal{X}\\to\\mathbb{R}$ be any candidate function (not necessarily equal to $V^{\\pi}$). Define\n\\[\\pi_{V}=\\operatorname*{argmin}_{\\pi}L(\\pi,V).\\]\nThen\n\n1. For all $x\\in\\mathcal{X}$ and $y\\in\\mathcal{Y}$,\n\\[\n\\pi_{V}(y\\mid x)\\;\\propto\\;\\pi_{\\mathrm{ref}}(y\\mid x)\\,\\exp\\Bigl\\{\\frac{1}{\\tau}\\Bigl(r(x,y)-\\frac{\\pi_{V}(y\\mid x)}{\\mu(y\\mid x)}\\bigl(V^{\\pi_{V}}(x)-V(x)\\bigr)\\Bigr)\\Bigr\\}.\n\\]\n\n2. Moreover, for every $x\\in\\mathcal{X}$ and $y\\in\\mathcal{Y}$,\n\\[\n\\left|\\log\\frac{\\pi_{V}(y\\mid x)}{\\pi^{\\ast}(y\\mid x)}\\right|\\le\\frac{2}{\\tau}\\,\\max_{y'}\\Bigl|\\bigl(V^{\\pi_{V}}(x)-V(x)\\bigr)\\bigl(1-\\tfrac{\\pi_{V}(y'\\mid x)}{\\mu(y'\\mid x)}\\bigr)\\Bigr|.\n\\]\n\n**Task** - Provide a complete and rigorous derivation or proof of Proposition 1 . Please include all intermediate steps and justifications.", "A": "We proceed to prove this here. Consider the Lagrangian\n\n\\[G(\\pi)=L(\\pi,V)+\\sum_{x} \\rho(x)\\,\\lambda_{x}\\Big(\\sum_{y} \\pi(y|x)-1\\Big)\\]\n\nwhere \\(\\lambda_{x}\\) are the Lagrange multipliers corresponding to the constraint \\(\\sum_{y}\\pi(y|x)=1\\). Notice there is no need to impose such constraint outside of the support of \\(\\rho\\). The derivative of G(\\pi) with respect to a variable \\(\\pi(y|x)\\) is\n\n\\[\n\\partial_{\\pi(y|x)}G(\\pi)=\\mathbb{E}_{x'\\sim\\rho,\\,y'\\sim\\mu(\\cdot|x')}\\Big[(r(x',y')-\\tau\\log\\tfrac{\\pi(y'|x')}{\\pi_{\\mathrm{ref}}(y'|x')}-V(x'))\\,\\partial_{\\pi(y|x)}\\big(-\\tau\\log\\tfrac{\\pi(y'|x')}{\\pi_{\\mathrm{ref}}(y'|x')}\\big)\\Big]\n+\\partial_{\\pi(y|x)}\\Big(\\sum_{x'}\\rho(x')\\lambda_{x'}\\sum_{y'}\\pi(y'|x')\\Big)\n\\]\n\n\\[= -\\rho(x)\\Big[\\frac{\\mu(y|x)}{\\tau\\,\\pi(y|x)}\\bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\bigr)-\\lambda_{x}\\Big].\\]\n\nSetting the optimality conditions \\(\\partial_{\\pi(y|x)}G(\\pi)=0\\) for all x and y, we deduce that for any x in the support of \\(\\rho\\), we have, for any y,\n\n\\[\\lambda_{x}=\\tau\\,\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\sum_{y}\\pi(y|x)\\,\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\sum_{y}\\mu(y|x)\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\]\n\nThus\n\n\\[\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)=V^{\\pi}(x)-V(x),\\]\n\nor, equivalently,\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}=r(x,y)-V(x)-\\frac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\tag{11}\\]\n\nFrom this we deduce that the optimal policy satisfies\n\n\\[\\pi(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\,\\exp\\Bigl\\{\\tfrac{1}{\\tau}\\bigl[r(x,y)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr)-V(x)\\bigr]\\Bigr\\}\\tag{12}\\]\n\n\\[\\propto\\;\\pi_{\\mathrm{ref}}(y|x)\\,\\exp\\Bigl\\{\\tfrac{1}{\\tau}\\bigl[r(x,y)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr)\\bigr]\\Bigr\\},\\tag{13}\\]\n\nwhich proves the first claim (9).\n\nNow from the property of the optimal policy \\(\\pi^{\\ast}\\), and from Equation (11), we deduce that\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=V^{\\ast}(x)-V(x)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\tag{14}\\]\n\nTaking the expectation with respect to \\(\\mu(\\cdot|x)\\) on both sides, we get\n\n\\[\\tau\\sum_{y}\\mu(y|x)\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=V^{\\ast}(x)-V^{\\pi}(x).\\]\n\nPlugging \\(V^{\\ast}(x)\\) back into Equation (14) we have for any x,y,\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=\\tau\\sum_{y'}\\mu(y'|x)\\log\\tfrac{\\pi(y'|x)}{\\pi^{\\ast}(y'|x)}+(V^{\\pi}(x)-V(x))\\Bigl(1-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\Bigr).\\]\n\nLet us write \\(\\varepsilon(x)=\\tfrac{1}{\\tau}\\max_{y}\\bigl|(V^{\\pi}(x)-V(x))\\bigl(1-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigr)\\bigr|\\) and \\(\\rho(y|x)=\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}\\). We have that\n\n\\[1=\\sum_{y}\\pi(y|x)=\\sum_{y}\\pi^{\\ast}(y|x)\\,e^{\\rho(y|x)}\\le\\sum_{y}\\pi^{\\ast}(y|x)\\,e^{\\sum_{y'}\\mu(y'|x)\\rho(y'|x)+\\varepsilon(x)}=e^{\\sum_{y'}\\mu(y'|x)\\rho(y'|x)+\\varepsilon(x)},\\]\n\nthus \\(\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\ge-\\varepsilon(x)\\). Similarly we can upper-bound \\(\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\) by \\(\\varepsilon(x)\\). We deduce that for all y\n\n\\[|\\rho(y|x)|\\le\\Bigl|\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\Bigr|+\\varepsilon(x)\\le2\\varepsilon(x),\\]\n\nwhich concludes the proof of the second claim.", "B": "**Proof of Proposition 1**\n\n**Part 1: Deriving the Proportionality of $\\pi_V$**\n\nGiven the loss:\n\\[\nL(\\pi, V) = \\frac{1}{2} \\mathbb{E}_{x \\sim \\rho, y \\sim \\mu(\\cdot|x)} \\left[ r(x,y) - V(x) - \\tau \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right]^2,\n\\]\nwe minimize it over $\\pi$ for fixed $V$. Introduce the Lagrangian for each $x$ with multiplier $\\lambda(x)$:\n\\[\n\\mathcal{L}_x = \\frac{1}{2} \\mathbb{E}_{y \\sim \\mu} \\left[ \\left( r - V - \\tau \\log \\frac{\\pi}{\\pi_{\\text{ref}}} \\right)^2 \\right] + \\lambda(x) \\left( 1 - \\sum_y \\pi(y|x) \\right).\n\\]\nTaking the functional derivative w.r.t. $\\pi(y|x)$:\n\\[\n\\frac{\\delta \\mathcal{L}_x}{\\delta \\pi(y|x)} = -\\mu(y|x) \\cdot \\tau \\cdot \\frac{r - V - \\tau \\log \\frac{\\pi}{\\pi_{\\text{ref}}}}{\\pi(y|x)} - \\lambda(x) = 0.\n\\]\nRearranging yields:\n\\[\n\\tau \\left( r - V - \\tau \\log \\frac{\\pi_V}{\\pi_{\\text{ref}}} \\right) = -\\lambda(x) \\frac{\\pi_V(y|x)}{\\mu(y|x)}.\n\\]\nLet $\\Delta(x) = V^{\\pi_V}(x) - V(x)$. From the definition of $V^{\\pi}$:\n\\[\nV^{\\pi_V}(x) = \\mathbb{E}_{y \\sim \\mu} \\left[ r - \\tau \\log \\frac{\\pi_V}{\\pi_{\\text{ref}}} \\right].\n\\]\nSubstituting $\\tau \\log \\frac{\\pi_V}{\\pi_{\\text{ref}}} = r - V - \\Delta \\frac{\\pi_V}{\\mu}$ from the rearranged equation, we get:\n\\[\nV^{\\pi_V}(x) = V(x) + \\Delta(x).\n\\]\nThus, $\\lambda(x) = -\\tau \\Delta(x)$, and substituting back:\n\\[\n\\tau \\log \\frac{\\pi_V}{\\pi_{\\text{ref}}} = r - V - \\Delta \\frac{\\pi_V}{\\mu}.\n\\]\nExponentiating both sides gives:\n\\[\n\\pi_V(y|x) \\propto \\pi_{\\text{ref}}(y|x) \\exp \\left( \\frac{1}{\\tau} \\left( r(x,y) - V(x) - \\Delta(x) \\frac{\\pi_V(y|x)}{\\mu(y|x)} \\right) \\right),\n\\]\nwhich simplifies to the stated proportionality.\n\n**Part 2: Bounding $\\left| \\log \\frac{\\pi_V}{\\pi^*} \\right|$**\n\nDefine $\\Delta(x) = V^{\\pi_V}(x) - V(x)$. The optimal policy is:\n\\[\n\\pi^*(y|x) = \\frac{\\pi_{\\text{ref}}(y|x) \\exp \\left( \\frac{r(x,y)}{\\tau} \\right)}{Z^*(x)}, \\quad Z^*(x) = \\sum_{y'} \\pi_{\\text{ref}}(y'|x) \\exp \\left( \\frac{r(x,y')}{\\tau} \\right).\n\\]\nFrom Part 1:\n\\[\n\\pi_V(y|x) \\propto \\pi_{\\text{ref}}(y|x) \\exp \\left( \\frac{1}{\\tau} \\left( r(x,y) - V(x) - \\Delta(x) \\frac{\\pi_V(y|x)}{\\mu(y|x)} \\right) \\right).\n\\]\nTaking the ratio:\n\\[\n\\frac{\\pi_V}{\\pi^*} \\propto \\exp \\left( -\\frac{V + \\Delta \\frac{\\pi_V}{\\mu}}{\\tau} \\right) \\cdot Z^*(x).\n\\]\nLogarithmizing:\n\\[\n\\log \\frac{\\pi_V}{\\pi^*} = -\\frac{\\Delta}{\\tau} \\frac{\\pi_V}{\\mu} - \\log \\mathbb{E}_{y \\sim \\pi^*} \\left[ \\exp \\left( -\\frac{\\Delta}{\\tau} \\frac{\\pi_V}{\\mu} \\right) \\right].\n\\]\nLet $c(y) = \\Delta \\left( 1 - \\frac{\\pi_V(y|x)}{\\mu(y|x)} \\right)$. Using inequalities for log-expectations:\n\\[\n\\left| \\log \\mathbb{E} \\left[ e^{-c(y')/\\tau} \\right] \\right| \\leq \\max_{y'} \\left| \\frac{c(y')}{\\tau} \\right|.\n\\]\nBounding the difference:\n\\[\n\\left| \\log \\frac{\\pi_V}{\\pi^*} \\right| \\leq \\frac{1}{\\tau} \\left( |c(y)| + \\max_{y'} |c(y')| \\right) \\leq \\frac{2}{\\tau} \\max_{y'} |c(y')|.\n\\]\nSubstituting $c(y') = \\Delta \\left( 1 - \\frac{\\pi_V(y'|x)}{\\mu(y'|x)} \\right)$ yields the final bound:\n\\[\n\\left| \\log \\frac{\\pi_V}{\\pi^*} \\right| \\leq \\frac{2}{\\tau} \\max_{y'} \\left| \\Delta \\left( 1 - \\frac{\\pi_V(y'|x)}{\\mu(y'|x)} \\right) \\right|.\n\\]\nThis completes the proof.", "C": "We will provide a complete and rigorous derivation of Proposition 1. The proof is divided into two parts corresponding to the two claims.\n\n---\n\n### **Part 1: Derivation of the Proportionality Expression for \\(\\pi_V(y|x)\\)**\n\nGiven the loss function:\n\\[\nL(\\pi,V) = \\frac{1}{2}\\,\\mathbb{E}_{x\\sim\\rho,\\;y\\sim\\mu(\\cdot|x)}\\Bigl[\\,r(x,y)-V(x)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr]^2,\n\\]\nwe seek to minimize \\(L(\\pi,V)\\) with respect to \\(\\pi\\) for a fixed \\(V\\). \n\n#### Step 1: Rewrite the Loss as an Optimization Problem\nWe can express the minimization problem as:\n\\[\n\\pi_V = \\operatorname*{argmin}_{\\pi} \\mathbb{E}_{x\\sim\\rho,\\;y\\sim\\mu(\\cdot|x)}\\Bigl[\\,r(x,y)-V(x)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr]^2.\n\\]\n\n#### Step 2: Take the Functional Derivative with Respect to \\(\\pi(y|x)\\)\nTo find the minimizer, we compute the functional derivative of \\(L(\\pi,V)\\) with respect to \\(\\pi(y|x)\\) and set it to zero. For a fixed \\(x\\), the derivative is:\n\\[\n\\frac{\\delta L(\\pi,V)}{\\delta \\pi(y|x)} = \\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Bigl[-\\tau \\cdot \\frac{\\mu(y|x)}{\\pi(y|x)} \\cdot \\Bigl(r(x,y)-V(x)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr)\\Bigr].\n\\]\nSetting this to zero for all \\(y\\) gives:\n\\[\n\\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Bigl[\\frac{\\mu(y|x)}{\\pi(y|x)} \\Bigl(r(x,y)-V(x)-\\tau\\log\\tfrac{\\pi(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\Bigr)\\Bigr] = 0.\n\\]\n\n#### Step 3: Solve for \\(\\pi(y|x)\\)\nThis implies that for all \\(y\\) in the support of \\(\\mu(\\cdot|x)\\):\n\\[\nr(x,y) - V(x) - \\tau \\log \\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)} = \\lambda(x),\n\\]\nwhere \\(\\lambda(x)\\) is a Lagrange multiplier ensuring \\(\\sum_y \\pi(y|x) = 1\\). Solving for \\(\\pi(y|x)\\):\n\\[\n\\log \\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)} = \\frac{1}{\\tau} \\bigl(r(x,y) - V(x) - \\lambda(x)\\bigr),\n\\]\n\\[\n\\pi(y|x) = \\pi_{\\mathrm{ref}}(y|x) \\exp\\Bigl(\\frac{1}{\\tau} \\bigl(r(x,y) - V(x) - \\lambda(x)\\bigr)\\Bigr).\n\\]\n\n#### Step 4: Relate \\(\\lambda(x)\\) to \\(V^{\\pi_V}(x)\\)\nThe minimizer \\(\\pi_V\\) must satisfy the definition of \\(V^{\\pi_V}(x)\\):\n\\[\nV^{\\pi_V}(x) = \\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Bigl[r(x,y) - \\tau \\log \\frac{\\pi_V(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\Bigr].\n\\]\nSubstituting \\(\\pi_V(y|x)\\) from above:\n\\[\nV^{\\pi_V}(x) = \\mathbb{E}_{y\\sim\\mu(\\cdot|x)}\\Bigl[r(x,y) - \\tau \\cdot \\frac{1}{\\tau} \\bigl(r(x,y) - V(x) - \\lambda(x)\\bigr)\\Bigr] = V(x) + \\lambda(x).\n\\]\nThus, \\(\\lambda(x) = V^{\\pi_V}(x) - V(x)\\).\n\n#### Step 5: Substitute Back to Obtain \\(\\pi_V(y|x)\\)\nNow, substitute \\(\\lambda(x)\\) back into the expression for \\(\\pi_V(y|x)\\):\n\\[\n\\pi_V(y|x) = \\pi_{\\mathrm{ref}}(y|x) \\exp\\Bigl(\\frac{1}{\\tau} \\bigl(r(x,y) - V(x) - (V^{\\pi_V}(x) - V(x))\\bigr)\\Bigr),\n\\]\n\\[\n\\pi_V(y|x) = \\pi_{\\mathrm{ref}}(y|x) \\exp\\Bigl(\\frac{1}{\\tau} \\bigl(r(x,y) - V^{\\pi_V}(x)\\bigr)\\Bigr).\n\\]\n\n#### Step 6: Correct for the Expectation Under \\(\\mu(\\cdot|x)\\)\nHowever, this does not account for the expectation under \\(\\mu(\\cdot|x)\\). To correct this, we note that the true minimizer must satisfy:\n\\[\n\\pi_V(y|x) \\propto \\pi_{\\mathrm{ref}}(y|x) \\exp\\Bigl(\\frac{1}{\\tau} \\bigl(r(x,y) - \\frac{\\pi_V(y|x)}{\\mu(y|x)} (V^{\\pi_V}(x) - V(x))\\bigr)\\Bigr).\n\\]\nThis is derived by ensuring consistency with the expectation under \\(\\mu(\\cdot|x)\\). \n\nThus, we obtain the proportionality expression:\n\\[\n\\pi_V(y|x) \\;\\propto\\; \\pi_{\\mathrm{ref}}(y|x) \\exp\\Bigl\\{\\frac{1}{\\tau} \\Bigl(r(x,y) - \\frac{\\pi_V(y|x)}{\\mu(y|x)} \\bigl(V^{\\pi_V}(x) - V(x)\\bigr)\\Bigr)\\Bigr\\}.\n\\]\n\n---\n\n### **Part 2: Bounding the Log-Ratio \\(\\left|\\log \\frac{\\pi_V(y|x)}{\\pi^*(y|x)}\\right|\\)**\n\n#### Step 1: Express \\(\\pi^*(y|x)\\) and \\(\\pi_V(y|x)\\)\nThe optimal policy is:\n\\[\n\\pi^*(y|x) = \\frac{\\pi_{\\mathrm{ref}}(y|x) \\exp\\bigl(\\frac{1}{\\tau} r(x,y)\\bigr)}{\\sum_{y'} \\pi_{\\mathrm{ref}}(y'|x) \\exp\\bigl(\\frac{1}{\\tau} r(x,y')\\bigr)}.\n\\]\nFrom Part 1, we have:\n\\[\n\\pi_V(y|x) \\propto \\pi_{\\mathrm{ref}}(y|x) \\exp\\Bigl(\\frac{1}{\\tau} \\bigl(r(x,y) - \\frac{\\pi_V(y|x)}{\\mu(y|x)} (V^{\\pi_V}(x) - V(x))\\bigr)\\Bigr).\n\\]\n\n#### Step 2: Compute the Log-Ratio\nTaking the logarithm of the ratio:\n\\[\n\\log \\frac{\\pi_V(y|x)}{\\pi^*(y|x)} = \\log \\frac{\\pi_V(y|x)}{Z_V} - \\log \\frac{\\pi^*(y|x)}{Z^*},\n\\]\nwhere \\(Z_V\\) and \\(Z^*\\) are normalization constants. This simplifies to:\n\\[\n\\log \\frac{\\pi_V(y|x)}{\\pi^*(y|x)} = -\\frac{1}{\\tau} \\cdot \\frac{\\pi_V(y|x)}{\\mu(y|x)} (V^{\\pi_V}(x) - V(x)) + \\text{normalization terms}.\n\\]\n\n#### Step 3: Bound the Deviation\nThe normalization terms cancel out when considering the maximum deviation over \\(y\\). Thus:\n\\[\n\\left|\\log \\frac{\\pi_V(y|x)}{\\pi^*(y|x)}\\right| \\leq \\frac{1}{\\tau} \\max_{y'} \\left|\\frac{\\pi_V(y'|x)}{\\mu(y'|x)} (V^{\\pi_V}(x) - V(x))\\right| + \\frac{1}{\\tau} \\left|V^{\\pi_V}(x) - V(x)\\right|.\n\\]\nSimplifying further:\n\\[\n\\left|\\log \\frac{\\pi_V(y|x)}{\\pi^*(y|x)}\\right| \\leq \\frac{2}{\\tau} \\max_{y'} \\left|(V^{\\pi_V}(x) - V(x)) \\left(1 - \\frac{\\pi_V(y'|x)}{\\mu(y'|x)}\\right)\\right|.\n\\]\n\nThis completes the proof of Proposition 1. \n\n---\n\n### **Summary**\n1. The proportionality expression for \\(\\pi_V(y|x)\\) is derived by minimizing \\(L(\\pi,V)\\) with respect to \\(\\pi\\) and accounting for the expectation under \\(\\mu(\\cdot|x)\\).\n2. The bound on the log-ratio follows from comparing \\(\\pi_V(y|x)\\) to \\(\\pi^*(y|x)\\) and bounding the deviation introduced by the mismatch between \\(\\pi_V\\) and \\(\\mu\\). \n\nBoth claims are rigorously justified by the above steps.", "D": "We proceed to prove this here. Consider the Lagrangian\n\n\\[G(\\pi)=L(\\pi,V)+\\sum_{x} \\rho(x)\\,\\lambda_{x}\\Big(\\sum_{y} \\pi(y|x)-1\\Big)\\]\n\nwhere \\(\\lambda_{x}\\) are the Lagrange multipliers corresponding to the constraint \\(\\sum_{y}\\pi(y|x)=1\\). Notice there is no need to impose such constraint outside of the support of \\(\\rho\\). The derivative of G(\\pi) with respect to a variable \\(\\pi(y|x)\\) is\n\n\\[\n\\partial_{\\pi(y|x)}G(\\pi)=\\mathbb{E}_{x'\\sim\\rho,\\,y'\\sim\\mu(\\cdot|x')}\\Big[(r(x',y')-\\tau\\log\\tfrac{\\pi(y'|x')}{\\pi_{\\mathrm{ref}}(y'|x')}-V(x'))\\,\\partial_{\\pi(y|x)}\\big(-\\tau\\log\\tfrac{\\pi(y'|x')}{\\pi_{\\mathrm{ref}}(y'|x')}\\big)\\Big]\n+\\partial_{\\pi(y|x)}\\Big(\\sum_{x'}\\rho(x')\\lambda_{x'}\\sum_{y'}\\pi(y'|x')\\Big)\n\\]\n\n\\[= -\\rho(x)\\Big[\\frac{\\mu(y|x)}{\\tau\\,\\pi(y|x)}\\bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\bigr)-\\lambda_{x}\\Big].\\]\n\nSetting the optimality conditions \\(\\partial_{\\pi(y|x)}G(\\pi)=0\\) for all x and y, we deduce that for any x in the support of \\(\\rho\\), we have, for any y,\n\n\\[\\lambda_{x}=\\tau\\,\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\sum_{y}\\pi(y|x)\\,\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\sum_{y}\\mu(y|x)\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)\n=\\tau\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\]\n\nThus\n\n\\[\\frac{\\mu(y|x)}{\\pi(y|x)}\\Bigl(r(x,y)-\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}-V(x)\\Bigr)=V^{\\pi}(x)-V(x),\\]\n\nor, equivalently,\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}=r(x,y)-V(x)-\\frac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\tag{11}\\]\n\nFrom this we deduce that the optimal policy satisfies\n\n\\[\\pi(y|x)=\\pi_{\\mathrm{ref}(y|x)}\\,\\exp\\Bigl\\{\\tfrac{1}{\\tau}\\bigl[r(x,y)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr)-V(x)\\bigr]\\Bigr\\}\\tag{12}\\]\n\n\\[\\propto\\;\\pi_{\\mathrm{ref}}(y|x)\\,\\exp\\Bigl\\{\\tfrac{1}{\\tau}\\bigl[r(x,y)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr)\\bigr]\\Bigr\\},\\tag{13}\\]\n\nwhich proves the first claim (9).\n\nNow from the property of the optimal policy \\(\\pi^{\\ast}\\), and from Equation (11), we deduce that\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=V^{\\ast}(x)-V(x)-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigl(V^{\\pi}(x)-V(x)\\bigr).\\tag{14}\\]\n\nTaking the expectation with respect to \\(\\mu(\\cdot|x)\\) on both sides, we get\n\n\\[\\tau\\sum_{y}\\mu(y|x)\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=V^{\\ast}(x)-V^{\\pi}(x).\\]\n\nPlugging \\(V^{\\ast}(x)\\) back into Equation (14) we have for any x,y,\n\n\\[\\tau\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}=\\tau\\sum_{y'}\\mu(y'|x)\\log\\tfrac{\\pi(y'|x)}{\\pi^{\\ast}(y'|x)}+(V^{\\pi}(x)-V(x))\\Bigl(1-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\Bigr).\\]\n\nLet us write \\(\\varepsilon(x)=\\tfrac{1}{\\tau}\\max_{y}\\bigl|(V^{\\pi}(x)-V(x))\\bigl(1-\\tfrac{\\pi(y|x)}{\\mu(y|x)}\\bigr)\\bigr|\\) and \\(\\rho(y|x)=\\log\\tfrac{\\pi(y|x)}{\\pi^{\\ast}(y|x)}\\). We have that\n\n\\[1=\\sum_{y}\\pi(y|x)=\\sum_{y}\\pi^{\\ast}(y|x)\\,e^{\\rho(y|x)}\\le\\sum_{y}\\pi^{\\ast}(y|x)\\,e^{\\sum_{y'}\\mu(y'|x)\\rho(y'|x)+\\varepsilon(x)}=e^{\\sum_{y'}\\mu(y'|x)\\rho(y'|x)+\\varepsilon(x)},\\]\n\nthus \\(\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\ge-\\varepsilon(x)\\). Similarly we can upper-bound \\(\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\) by \\(\\varepsilon(x)\\). We deduce that for all y\n\n\\[|\\rho(y|x)|\\le\\Bigl|\\sum_{y'}\\mu(y'|x)\\rho(y'|x)\\Bigr|+\\varepsilon(x)\\le2\\varepsilon(x),\\]\n\nwhich concludes the proof of the second claim.", "ground-truth": "A"}
{"paper": "arxiv:2405.19107", "question": "Consider the following definitions: Let \\(\\rho(x)\\) be a distribution over contexts \\(x\\), and let \\(\\mu(y\\mid x)\\) be a behavior policy generating responses \\(y\\). Let \\(r(x,y)\\) be a known reward function and \\(\\pi_{ref}(y\\mid x)\\) a reference policy. Fix a regularization scalar \\(\\tau>0\\). Define the loss\n\n\\[\nL(\\pi,V) = \\tfrac12 \\,\\mathbb{E}_{x\\sim\\rho,\\,y\\sim\\mu(\\cdot\\mid x)}\\Bigl[ r(x,y) - V(x) - \\tau\\log\\frac{\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)} \\Bigr]^2.\n\\]\n\nLet\n\n\\[\n\\pi^*(y\\mid x) = \\frac{\\pi_{ref}(y\\mid x)\\exp\\bigl(\\tfrac1{\\tau}r(x,y)\\bigr)}{\\exp\\bigl(\\tfrac1{\\tau}V^*(x)\\bigr)},\n\\qquad\nV^*(x) = \\tau\\log\\mathbb{E}_{y'\\sim\\pi_{ref}(\\cdot\\mid x)}\\bigl[\\exp\\bigl(\\tfrac1{\\tau}r(x,y')\\bigr)\\bigr].\n\\]\n\nTheorem 1 states that \\((\\pi^*,V^*)\\) is a global optimum of \\(L(\\pi,V)\\), and assuming that for every \\(x\\) the supports of \\(\\mu(\\cdot\\mid x)\\) and \\(\\pi_{ref}(\\cdot\\mid x)\\) coincide, this optimum is unique. Given only these definitions and without referring to external material, how can one derive a complete proof of Theorem 1?", "A": "Alright, let's tackle this problem step by step. I'm going to try to understand what's given and then figure out how to prove that \\((\\pi^*, V^*)\\) is indeed a global optimum of the loss function \\( L(\\pi, V) \\), and that it's unique under the given conditions.\n\n### Understanding the Definitions\n\nFirst, let's recall the definitions:\n\n1. **Loss Function**:\n   \\[\n   L(\\pi, V) = \\frac{1}{2} \\mathbb{E}_{x \\sim \\rho, y \\sim \\mu(\\cdot \\mid x)} \\left[ r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right]^2.\n   \\]\n   This is a squared error loss where we're comparing some combination of the reward \\( r(x, y) \\), the value function \\( V(x) \\), and a regularization term involving the policies \\( \\pi \\) and \\( \\pi_{ref} \\).\n\n2. **Optimal Policy and Value Function**:\n   \\[\n   \\pi^*(y \\mid x) = \\frac{\\pi_{ref}(y \\mid x) \\exp\\left( \\frac{1}{\\tau} r(x, y) \\right)}{\\exp\\left( \\frac{1}{\\tau} V^*(x) \\right)},\n   \\]\n   \\[\n   V^*(x) = \\tau \\log \\mathbb{E}_{y' \\sim \\pi_{ref}(\\cdot \\mid x)} \\left[ \\exp\\left( \\frac{1}{\\tau} r(x, y') \\right) \\right].\n   \\]\n   These define the optimal policy \\( \\pi^* \\) and the optimal value function \\( V^* \\).\n\n### Goal\n\nWe need to show that \\( (\\pi^*, V^*) \\) minimizes \\( L(\\pi, V) \\) globally and that this minimum is unique when the supports of \\( \\mu(\\cdot \\mid x) \\) and \\( \\pi_{ref}(\\cdot \\mid x) \\) coincide for every \\( x \\).\n\n### Approach to the Proof\n\nTo prove that \\( (\\pi^*, V^*) \\) is a global optimum, we can:\n\n1. Compute the gradient of \\( L \\) with respect to \\( \\pi \\) and \\( V \\) and set them to zero to find critical points.\n2. Show that \\( (\\pi^*, V^*) \\) satisfies these critical point equations.\n3. Verify that this critical point is indeed a minimum (e.g., by checking the second derivative or convexity).\n4. Under the support condition, show that no other \\( (\\pi, V) \\) can satisfy the critical point equations, ensuring uniqueness.\n\nAlternatively, since the loss is quadratic in the argument inside the expectation, we can try to express it in a way that makes the optimality of \\( (\\pi^*, V^*) \\) evident.\n\nLet me try the first approach: setting the gradients to zero.\n\n#### Step 1: Compute the Gradient with Respect to \\( V(x) \\)\n\nThe loss can be written as:\n\\[\nL(\\pi, V) = \\frac{1}{2} \\mathbb{E}_{x \\sim \\rho} \\left[ \\mathbb{E}_{y \\sim \\mu(\\cdot \\mid x)} \\left[ \\left( r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right)^2 \\right] \\right].\n\\]\n\nTo find the gradient with respect to \\( V(x) \\), we can consider \\( V \\) as a function of \\( x \\), and for each \\( x \\), take the derivative inside the expectation over \\( y \\).\n\nFor a fixed \\( x \\), the part inside the expectation over \\( y \\) is:\n\\[\n\\left( r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right)^2.\n\\]\n\nTaking the derivative with respect to \\( V(x) \\):\n\\[\n\\frac{\\partial}{\\partial V(x)} \\left( \\cdot \\right)^2 = 2 \\left( r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right) \\cdot (-1).\n\\]\n\nSo, setting the gradient equal to zero (for optimal \\( V \\)):\n\\[\n\\mathbb{E}_{y \\sim \\mu(\\cdot \\mid x)} \\left[ r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right] = 0.\n\\]\n\nThis gives:\n\\[\nV(x) = \\mathbb{E}_{y \\sim \\mu(\\cdot \\mid x)} \\left[ r(x, y) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right].\n\\]\n\n#### Step 2: Compute the Gradient with Respect to \\( \\pi(y \\mid x) \\)\n\nThis is a bit more involved because \\( \\pi(y \\mid x) \\) is a distribution over \\( y \\) for each \\( x \\). We need to ensure that the derivative respects the constraint that \\( \\pi(\\cdot \\mid x) \\) sums to 1.\n\nHowever, since the loss involves \\( \\log \\pi(y \\mid x) \\), we can proceed by treating \\( \\pi(y \\mid x) \\) as a variable and using the method of Lagrange multipliers to handle the normalization constraint.\n\nAlternatively, since the optimal \\( \\pi^* \\) is given in a form that resembles the solution to a maximum entropy problem, we can guess that at the optimum, the term inside the square is zero for all \\( x, y \\) where \\( \\mu(y \\mid x) > 0 \\).\n\nLet's assume that at the optimum, for all \\( x, y \\) where \\( \\mu(y \\mid x) > 0 \\):\n\\[\nr(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} = 0.\n\\]\n\nThis implies:\n\\[\n\\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} = \\frac{r(x, y) - V(x)}{\\tau},\n\\]\n\\[\n\\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} = \\exp\\left( \\frac{r(x, y) - V(x)}{\\tau} \\right),\n\\]\n\\[\n\\pi(y \\mid x) = \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y) - V(x)}{\\tau} \\right).\n\\]\n\nBut we also have the normalization condition:\n\\[\n\\sum_y \\pi(y \\mid x) = 1 \\implies \\sum_y \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y)}{\\tau} \\right) \\exp\\left( -\\frac{V(x)}{\\tau} \\right) = 1.\n\\]\n\nSolving for \\( \\exp\\left( -\\frac{V(x)}{\\tau} \\right) \\):\n\\[\n\\exp\\left( -\\frac{V(x)}{\\tau} \\right) = \\frac{1}{\\sum_y \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y)}{\\tau} \\right)},\n\\]\n\\[\n\\exp\\left( \\frac{V(x)}{\\tau} \\right) = \\sum_y \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y)}{\\tau} \\right),\n\\]\n\\[\nV(x) = \\tau \\log \\sum_y \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y)}{\\tau} \\right).\n\\]\n\nThis matches the definition of \\( V^*(x) \\). Then, substituting back:\n\\[\n\\pi(y \\mid x) = \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y) - V^*(x)}{\\tau} \\right) = \\pi^*(y \\mid x).\n\\]\n\n#### Step 3: Verifying Optimality\n\nGiven that we've set the term inside the square to zero for all \\( x, y \\) where \\( \\mu(y \\mid x) > 0 \\), the loss \\( L(\\pi^*, V^*) = 0 \\), which is the minimal possible value since the loss is non-negative.\n\n#### Step 4: Uniqueness Under Support Conditions\n\nThe support condition ensures that for all \\( y \\) where \\( \\pi_{ref}(y \\mid x) > 0 \\), \\( \\mu(y \\mid x) > 0 \\), and vice versa. This means that the condition \\( r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} = 0 \\) must hold for all \\( y \\) where \\( \\pi_{ref}(y \\mid x) > 0 \\), which fully determines \\( \\pi(y \\mid x) \\) and \\( V(x) \\) as above. Hence, the solution is unique.\n\n### Final Proof\n\n**Proof of Theorem 1:**\n\n1. **Critical Point Equations**:\n   To find the critical points of \\( L(\\pi, V) \\), we set the derivatives with respect to \\( V(x) \\) and \\( \\pi(y \\mid x) \\) to zero.\n\n   - For \\( V(x) \\):\n     \\[\n     \\frac{\\partial L}{\\partial V(x)} = -\\mathbb{E}_{y \\sim \\mu(\\cdot \\mid x)} \\left[ r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right] = 0.\n     \\]\n     This implies:\n     \\[\n     V(x) = \\mathbb{E}_{y \\sim \\mu(\\cdot \\mid x)} \\left[ r(x, y) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\right].\n     \\]\n\n   - For \\( \\pi(y \\mid x) \\):\n     The loss is minimized when the term inside the square is zero for all \\( x, y \\) where \\( \\mu(y \\mid x) > 0 \\):\n     \\[\n     r(x, y) - V(x) - \\tau \\log \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} = 0.\n     \\]\n     Solving for \\( \\pi(y \\mid x) \\):\n     \\[\n     \\pi(y \\mid x) = \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y) - V(x)}{\\tau} \\right).\n     \\]\n     Normalization requires:\n     \\[\n     \\sum_y \\pi(y \\mid x) = 1 \\implies \\exp\\left( \\frac{V(x)}{\\tau} \\right) = \\sum_y \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y)}{\\tau} \\right),\n     \\]\n     hence:\n     \\[\n     V(x) = \\tau \\log \\sum_y \\pi_{ref}(y \\mid x) \\exp\\left( \\frac{r(x, y)}{\\tau} \\right) = V^*(x).\n     \\]\n     Substituting back gives \\( \\pi(y \\mid x) = \\pi^*(y \\mid x) \\).\n\n2. **Optimality**:\n   At \\( (\\pi^*, V^*) \\), the term inside the expectation in \\( L(\\pi, V) \\) is zero for all \\( x, y \\) where \\( \\mu(y \\mid x) > 0 \\), hence \\( L(\\pi^*, V^*) = 0 \\), which is the global minimum since \\( L(\\pi, V) \\geq 0 \\).\n\n3. **Uniqueness**:\n   Under the support condition, the critical point equations must hold for all \\( y \\) where \\( \\pi_{ref}(y \\mid x) > 0 \\), uniquely determining \\( \\pi(y \\mid x) \\) and \\( V(x) \\). Thus, \\( (\\pi^*, V^*) \\) is the unique global optimum.\n\nThis completes the proof. \\(\\square\\)", "B": "From the definition of \\(\\pi^{*}\\) and \\(V^{*}\\), we have that \\(L(\\pi^{*},V^{*})=0\\). Now notice that \\(L(\\pi,V)\\) is non-negative since it is a sum of quadratic terms \\(t(x,y)^2\\), where \\(t(x,y)=r(x,y)-V(x)-\\tau\\log\\frac{\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}\\). Thus \\((\\pi^{*},V^{*})\\) is a global optimum of \\(L(\\pi,V)\\).\n\nNow let us prove that it is unique. Assume there is another global optimum \\((\\tilde\\pi,\\tilde V)\\) such that \\(L(\\tilde\\pi,\\tilde V)=0\\). This means that for all \\(x\\in\\mathrm{supp}(\\rho)\\) and \\(y\\in\\mathrm{supp}\\,\\mu(\\cdot\\mid x)\\), its \\(t(x,y)\\)-term is zero. Since the supports of \\(\\mu\\) and \\(\\pi_{ref}\\) coincide, we have for all \\(x\\in\\mathrm{supp}(\\rho)\\), \\(y\\in\\mathrm{supp}\\,\\pi_{ref}(\\cdot\\mid x)\\),\n\n\\[\nr(x,y)-\\tilde V(x)-\\tau\\log\\frac{\\tilde\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}=0,\n\\]\n\nfrom which we deduce\n\n\\[\n\\tilde\\pi(y\\mid x)=\\pi_{ref}(y\\mid x)e^{\\tfrac{1}{\\tau}r(x,y)}e^{-\\tfrac{1}{\\tau}\\tilde V(x)}.\n\\]\n\nBut since \\(\\tilde\\pi(\\cdot\\mid x)\\) is a probability distribution, we must have\n\n\\[\n\\tilde V(x)=\\tau\\log\\sum_{y}\\pi_{ref}(y\\mid x)e^{\\tfrac{1}{\\tau}r(x,y)},\n\\]\n\nhence \\(\\tilde V(x)=V^{*}(x)\\) and \\(\\tilde\\pi(y\\mid x)=\\pi^{*}(y\\mid x)\\) for all \\(x,y\\).", "C": "From the definition of \\(\\pi^{*}\\) and \\(V^{*}\\), we have that \\(L(\\pi^{*},V^{*})=0\\). Now notice that \\(L(\\pi,V)\\) is non-negative since it is a sum of quadratic terms \\(t(x,y)^2\\), where \\(t(x,y)=r(x,y)-V(x)-\\tau\\log\\frac{\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}\\). Thus \\((\\pi^{*},V^{*})\\) is a global optimum of \\(L(\\pi,V)\\).\n\nNow let us prove that it is unique. Assume there is another global optimum \\((\\tilde\\pi,\\tilde V)\\) such that \\(L(\\tilde\\pi,\\tilde V)=0\\). This means that for all \\(x\\in\\mathrm{supp}(\\rho)\\) and \\(y\\in\\mathrm{supp}\\,\\mu(\\cdot\\mid x)\\), its \\(t(x,y)\\)-term is zero. Since the supports of \\(\\mu\\) and \\(\\pi_{ref}\\) coincide, we have for all \\(x\\in\\mathrm{supp}(\\rho)\\), \\(y\\in\\mathrm{supp}\\,\\pi_{ref}(\\cdot\\mid x)\\),\n\n\\[\nr(x,y)-\\tilde V(x)-\\tau\\log\\frac{\\tilde\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}=0,\n\\]\n\nfrom which we deduce\n\n\\[\n\\tilde\\pi(y\\mid x)=\\pi_{ref}(y\\mid x)e^{\\tfrac{1}{\\tau}r(x,y)}e^{-\\tfrac{1}{\\tau}\\tilde V(x)}.\n\\]\n\nBut since \\(\\tilde\\pi(\\cdot\\mid x)\\) is a probability distribution, we must have\n\n\\[\n\\tilde V(x)=\\tau\\log\\mathbb{E}_{y'\\sim\\pi_{ref}(\\cdot\\mid x)}\\bigl[e^{\\tfrac{1}{\\tau}r(x,y')}\\bigr],\n\\]\n\nhence \\(\\tilde V(x)=V^{*}(x)\\) and \\(\\tilde\\pi(y\\mid x)=\\pi^{*}(y\\mid x)\\) for all \\(x,y\\).", "D": "To prove Theorem 1, we demonstrate that \\((\\pi^*, V^*)\\) globally minimizes \\(L(\\pi, V)\\) and is unique under support equivalence.\n\n**Step 1: Verify \\((\\pi^*, V^*)\\) attains zero loss**  \nSubstitute \\(\\pi^*\\) and \\(V^*\\) into the loss:  \n\\[\nr(x,y) - V^*(x) - \\tau \\log \\tfrac{\\pi^*(y\\mid x)}{\\pi_{ref}(y\\mid x)} = r(x,y) - V^*(x) - \\left(r(x,y) - V^*(x)\\right) = 0.\n\\]\nThus, \\(L(\\pi^*, V^*) = \\tfrac{1}{2} \\mathbb{E}[\\cdot]^2 = 0\\), the global minimum since squared terms are non-negative.\n\n**Step 2: Show necessity of \\((\\pi^*, V^*)\\) for optimality**  \nAssume another pair \\((\\pi', V')\\) minimizes \\(L(\\pi, V)\\). For \\(L(\\pi', V') = 0\\), the integrand must vanish \\(\\mu\\)-almost surely:  \n\\[\nr(x,y) = V'(x) + \\tau \\log \\tfrac{\\pi'(y\\mid x)}{\\pi_{ref}(y\\mid x)}.\n\\]\nRearranging gives:  \n\\[\n\\pi'(y\\mid x) = \\pi_{ref}(y\\mid x) \\exp\\left(\\tfrac{r(x,y) - V'(x)}{\\tau}\\right).\n\\]\nNormalization requires:  \n\\[\n1 = \\mathbb{E}_{y'\\sim \\pi_{ref}} \\left[\\exp\\left(\\tfrac{r(x,y') - V'(x)}{\\tau}\\right)\\right] \\implies V'(x) = \\tau \\log \\mathbb{E}_{y'\\sim \\pi_{ref}} \\left[\\exp\\left(\\tfrac{r(x,y')}{\\tau}\\right)\\right] = V^*(x).\n\\]\nThus, \\(V' = V^*\\) and \\(\\pi' = \\pi^*\\) follow uniquely.\n\n**Step 3: Prove uniqueness under support equivalence**  \nIf \\(\\text{supp}(\\mu(\\cdot\\mid x)) = \\text{supp}(\\pi_{ref}(\\cdot\\mid x))\\), the equality \\(r(x,y) = V(x) + \\tau \\log \\tfrac{\\pi(y\\mid x)}{\\pi_{ref}(y\\mid x)}\\) must hold for **all** \\(y \\in \\text{supp}(\\pi_{ref})\\). Any deviation from \\(\\pi^*\\) or \\(V^*\\) would violate normalization or the vanishing integrand condition, ensuring uniqueness. If supports differ, \\(\\pi\\) could differ outside \\(\\text{supp}(\\mu)\\), breaking uniqueness.\n\nThus, \\((\\pi^*, V^*)\\) is the unique global optimum when supports coincide.", "ground-truth": "B"}
{"paper": "arxiv:2402.05749", "question": "Theorem 1 (Equivalence of optimal solutions) states:\n\nLet \\pi^*_\\theta be the global minimizer of the offline preference optimization loss\n\\[\nL_{off}(\\theta)=\\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\bigl[f\\bigl(\\beta\\bigl(\\log\\frac{\\pi_\\theta(y_w)}{\\pi_{ref}(y_w)}-\\log\\frac{\\pi_\\theta(y_l)}{\\pi_{ref}(y_l)}\\bigr)\\bigr)\\bigr],\n\\]\nand consider the regularized policy optimization objective\n\\[\nJ(\\pi_\\theta)=\\mathbb{E}_{y\\sim\\pi_\\theta}[r_\\phi(y)]-\\beta\\,D_{KL}(\\pi_\\theta\\|\\pi_{ref}),\n\\]\nwith\n\\[\nD_{KL}(\\pi\\|\\pi')=\\mathbb{E}_{y\\sim\\pi}\\bigl[\\log\\tfrac{\\pi(y)}{\\pi'(y)}\\bigr],\n\\]\nand the reward modeling loss\n\\[\nL_{rm}(\\phi)=\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[f(r_\\phi(y_w)-r_\\phi(y_l))].\n\\]\nHere, \\mu is a distribution over ordered pairs (y_w,y_l), f: \\mathbb{R}\\to\\mathbb{R} is a convex function with f'(0)<0, \\beta>0 is a scalar regularization coefficient, r_\\phi(y) is a pointwise reward function, \\pi_\\theta(y) is the parameterized policy distribution, and \\pi_{ref}(y) is a fixed reference policy. Theorem 1 claims that the policy \\pi^*_\\theta minimizing L_{off}(\\theta) coincides with the policy maximizing J(\\pi_\\theta) when using a reward model r_\\phi that minimizes L_{rm}(\\phi).\n\nCould you derive and prove this equivalence under the above definitions and assumptions?", "A": "The proof is straightforward. When minimizing the reward modeling loss $L_{rm}(\\phi)$ in Reward modeling loss, we may reparameterize the learned reward as\n$$\nr_\\phi(y)=\\beta\\,\\log\\tfrac{\\pi_\\theta(y)}{\\pi_{ref}(y)}+z,\n$$\nwhere $z$ is an additive constant (depending on $\\theta$) that cancels under the difference and does not affect the resulting policy. Substituting into Reward modeling loss shows that its global minimizer $r_\\phi^*$ induces\n$$\n\\pi_\\theta(y)\\propto \\pi_{ref}(y)\\,\\exp\\bigl(\\beta\\,r_\\phi^*(y)\\bigr),\n$$\nwhich is exactly the form of the optimal regularized policy maximizing\n$$\nJ(\\pi_\\theta)=\\mathbb{E}_{y\\sim\\pi_\\theta}[r_\\phi^*(y)]-\\beta\\,D_{KL}(\\pi_\\theta\\|\\pi_{ref}).\n$$\nHence the policy minimizing $L_{off}(\\theta)$ in Offline preference optimization loss coincides with the one maximizing $J(\\pi_\\theta)$ under reward $r_\\phi^*$, establishing the claimed equivalence.", "B": "To prove Theorem 1, we demonstrate the equivalence between the optimal policy \\(\\pi_\\theta^*\\) minimizing \\(L_{\\text{off}}(\\theta)\\) and the policy maximizing \\(J(\\pi_\\theta)\\) under the optimal reward model \\(r_\\phi^*\\) minimizing \\(L_{\\text{rm}}(\\phi)\\). \n\n### Step 1: Optimal Policy for \\(J(\\pi_\\theta)\\)\nThe regularized objective is:\n\\[\nJ(\\pi_\\theta) = \\mathbb{E}_{y \\sim \\pi_\\theta}[r_\\phi(y)] - \\beta D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}}).\n\\]\nUsing variational calculus, the optimal policy satisfies:\n\\[\n\\frac{\\delta J}{\\delta \\pi_\\theta(y)} = r_\\phi(y) - \\beta \\left(\\log \\frac{\\pi_\\theta(y)}{\\pi_{\\text{ref}}(y)} + 1\\right) - \\lambda = 0,\n\\]\nwhere \\(\\lambda\\) enforces \\(\\int \\pi_\\theta(y) dy = 1\\). Solving yields:\n\\[\n\\pi_\\theta^*(y) \\propto \\pi_{\\text{ref}}(y) \\exp\\left(\\frac{r_\\phi(y)}{\\beta}\\right).\n\\]\nThus, the optimal policy is:\n\\[\n\\pi_\\theta^*(y) = \\frac{\\pi_{\\text{ref}}(y) \\exp\\left(\\frac{r_\\phi(y)}{\\beta}\\right)}{Z}, \\quad Z = \\mathbb{E}_{y \\sim \\pi_{\\text{ref}}}\\left[\\exp\\left(\\frac{r_\\phi(y)}{\\beta}\\right)\\right].\n\\]\n\n### Step 2: Optimal Reward Model for \\(L_{\\text{rm}}(\\phi)\\)\nThe reward loss is:\n\\[\nL_{\\text{rm}}(\\phi) = \\mathbb{E}_{(y_w, y_l) \\sim \\mu}\\left[f\\left(r_\\phi(y_w) - r_\\phi(y_l)\\right)\\right].\n\\]\nGiven \\(f\\) is convex with \\(f'(0) < 0\\), the minimizer \\(r_\\phi^*\\) satisfies the first-order condition:\n\\[\n\\mathbb{E}_{(y_w, y_l) \\sim \\mu}\\left[f'\\left(r_\\phi^*(y_w) - r_\\phi^*(y_l)\\right)\\left(\\nabla r_\\phi(y_w) - \\nabla r_\\phi(y_l)\\right)\\right] = 0.\n\\]\nComparing with the optimal policy \\(\\pi_\\theta^*\\), we identify:\n\\[\nr_\\phi^*(y) = \\beta \\log \\frac{\\pi_\\theta^*(y)}{\\pi_{\\text{ref}}(y)} + \\text{const}.\n\\]\nConstants vanish in \\(r_\\phi(y_w) - r_\\phi(y_l)\\), so we set:\n\\[\nr_\\phi^*(y) = \\beta \\log \\frac{\\pi_\\theta^*(y)}{\\pi_{\\text{ref}}(y)}.\n\\]\n\n### Step 3: Equating \\(L_{\\text{off}}(\\theta)\\) and \\(L_{\\text{rm}}(\\phi)\\)\nSubstitute \\(r_\\phi^*(y)\\) into \\(L_{\\text{off}}(\\theta)\\):\n\\[\nL_{\\text{off}}(\\theta) = \\mathbb{E}_{(y_w, y_l) \\sim \\mu}\\left[f\\left(\\beta\\left(\\log \\frac{\\pi_\\theta(y_w)}{\\pi_{\\text{ref}}(y_w)} - \\log \\frac{\\pi_\\theta(y_l)}{\\pi_{\\text{ref}}(y_l)}\\right)\\right)\\right].\n\\]\nWith \\(r_\\phi^*(y) = \\beta \\log \\frac{\\pi_\\theta(y)}{\\pi_{\\text{ref}}(y)}\\), this becomes:\n\\[\nL_{\\text{off}}(\\theta) = \\mathbb{E}_{(y_w, y_l) \\sim \\mu}\\left[f\\left(r_\\phi^*(y_w) - r_\\phi^*(y_l)\\right)\\right] = L_{\\text{rm}}(\\phi).\n\\]\nThus, minimizing \\(L_{\\text{off}}(\\theta)\\) over \\(\\pi_\\theta\\) is equivalent to minimizing \\(L_{\\text{rm}}(\\phi)\\) over \\(r_\\phi\\).\n\n### Conclusion\nThe optimal policy \\(\\pi_\\theta^*\\) minimizing \\(L_{\\text{off}}(\\theta)\\) coincides with the policy maximizing \\(J(\\pi_\\theta)\\) under the optimal reward \\(r_\\phi^*\\) minimizing \\(L_{\\text{rm}}(\\phi)\\). This establishes the equivalence.", "C": "Certainly! Let's derive and prove the equivalence between the optimal policy \\(\\pi^*_\\theta\\) minimizing the offline preference optimization loss \\(L_{off}(\\theta)\\) and the policy maximizing the regularized objective \\(J(\\pi_\\theta)\\) when the reward model \\(r_\\phi\\) minimizes \\(L_{rm}(\\phi)\\).\n\n---\n\n### **Step 1: Optimal Reward Model \\(r^*_\\phi\\)**\n\nGiven the reward modeling loss:\n\\[\nL_{rm}(\\phi) = \\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f\\left(r_\\phi(y_w) - r_\\phi(y_l)\\right)\\right],\n\\]\nwhere \\(f\\) is convex and \\(f'(0) < 0\\), the optimal reward \\(r^*_\\phi\\) satisfies the first-order condition for minimizing \\(L_{rm}(\\phi)\\). For any pair \\((y_w, y_l)\\), the derivative w.r.t. \\(r_\\phi(y_w)\\) and \\(r_\\phi(y_l)\\) must vanish in expectation. This implies:\n\\[\n\\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f'\\left(r^*_\\phi(y_w) - r^*_\\phi(y_l)\\right)\\right] = 0.\n\\]\nAssuming \\(\\mu\\) is sufficiently diverse, the pointwise condition holds:\n\\[\nr^*_\\phi(y_w) - r^*_\\phi(y_l) = \\log\\frac{\\pi^*(y_w)}{\\pi_{ref}(y_w)} - \\log\\frac{\\pi^*(y_l)}{\\pi_{ref}(y_l)},\n\\]\nwhere \\(\\pi^*\\) is the optimal policy. Thus, the optimal reward can be expressed as:\n\\[\nr^*_\\phi(y) = \\beta \\log\\frac{\\pi^*(y)}{\\pi_{ref}(y)} + c,\n\\]\nwhere \\(c\\) is a constant (irrelevant for policy optimization, as it cancels out in comparisons).\n\n---\n\n### **Step 2: Optimal Policy \\(\\pi^*_\\theta\\) for \\(J(\\pi_\\theta)\\)**\n\nThe regularized objective is:\n\\[\nJ(\\pi_\\theta) = \\mathbb{E}_{y\\sim\\pi_\\theta}[r^*_\\phi(y)] - \\beta D_{KL}(\\pi_\\theta \\| \\pi_{ref}).\n\\]\nSubstitute \\(r^*_\\phi(y)\\) from Step 1:\n\\[\nJ(\\pi_\\theta) = \\mathbb{E}_{y\\sim\\pi_\\theta}\\left[\\beta \\log\\frac{\\pi^*(y)}{\\pi_{ref}(y)} + c\\right] - \\beta \\mathbb{E}_{y\\sim\\pi_\\theta}\\left[\\log\\frac{\\pi_\\theta(y)}{\\pi_{ref}(y)}\\right].\n\\]\nThe constant \\(c\\) does not affect the optimization, so we ignore it. Simplify:\n\\[\nJ(\\pi_\\theta) = \\beta \\mathbb{E}_{y\\sim\\pi_\\theta}\\left[\\log\\frac{\\pi^*(y)}{\\pi_\\theta(y)}\\right].\n\\]\nThis is equivalent to:\n\\[\nJ(\\pi_\\theta) = -\\beta D_{KL}(\\pi_\\theta \\| \\pi^*),\n\\]\nwhich is maximized when \\(\\pi_\\theta = \\pi^*\\) (since KL divergence is non-negative and minimized at 0).\n\n---\n\n### **Step 3: Optimal Policy \\(\\pi^*_\\theta\\) for \\(L_{off}(\\theta)\\)**\n\nThe offline preference optimization loss is:\n\\[\nL_{off}(\\theta) = \\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f\\left(\\beta\\left(\\log\\frac{\\pi_\\theta(y_w)}{\\pi_{ref}(y_w)} - \\log\\frac{\\pi_\\theta(y_l)}{\\pi_{ref}(y_l)}\\right)\\right)\\right].\n\\]\nUsing the optimal reward \\(r^*_\\phi(y) = \\beta \\log\\frac{\\pi^*(y)}{\\pi_{ref}(y)} + c\\), the loss becomes:\n\\[\nL_{off}(\\theta) = \\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f\\left(r^*_\\phi(y_w) - r^*_\\phi(y_l)\\right)\\right].\n\\]\nFrom Step 1, \\(r^*_\\phi\\) minimizes \\(L_{rm}(\\phi)\\), and thus \\(\\pi^*\\) minimizes \\(L_{off}(\\theta)\\) when:\n\\[\n\\log\\frac{\\pi_\\theta(y_w)}{\\pi_{ref}(y_w)} - \\log\\frac{\\pi_\\theta(y_l)}{\\pi_{ref}(y_l)} = \\log\\frac{\\pi^*(y_w)}{\\pi_{ref}(y_w)} - \\log\\frac{\\pi^*(y_l)}{\\pi_{ref}(y_l)}.\n\\]\nThis holds iff \\(\\pi_\\theta = \\pi^*\\).\n\n---\n\n### **Conclusion**\n\nThe policy \\(\\pi^*_\\theta\\) minimizing \\(L_{off}(\\theta)\\) coincides with the policy maximizing \\(J(\\pi_\\theta)\\) when the reward model \\(r_\\phi\\) is optimized to minimize \\(L_{rm}(\\phi)\\). This is because:\n1. The optimal reward \\(r^*_\\phi\\) aligns with the log-ratio of \\(\\pi^*\\) and \\(\\pi_{ref}\\).\n2. The regularized objective \\(J(\\pi_\\theta)\\) is maximized when \\(\\pi_\\theta = \\pi^*\\).\n3. The offline loss \\(L_{off}(\\theta)\\) is minimized when \\(\\pi_\\theta = \\pi^*\\).\n\nThus, the two optimization problems are equivalent under the given assumptions. \\(\\square\\)", "D": "The proof is straightforward. When minimizing the reward modeling loss $L_{rm}(\\phi)$ in Reward modeling loss, we may reparameterize the learned reward as\n$$\nr_\\phi(y)=\\beta\\,\\log\\tfrac{\\pi_\\theta(y)}{\\pi_{ref}(y)}+z,\n$$\nwhere $z$ is an additive constant (depending on $\\theta$) that cancels under the difference and does not affect the resulting policy. Substituting into Reward modeling loss shows that its global minimizer $r_\\phi^*$ induces\n$$\n\\pi_\\theta(y)\\propto \\pi_{ref}(y)\\,\\exp\\bigl(\\beta^{-1}r_\\phi^*(y)\\bigr),\n$$\nwhich is exactly the form of the optimal regularized policy maximizing\n$$\nJ(\\pi_\\theta)=\\mathbb{E}_{y\\sim\\pi_\\theta}[r_\\phi^*(y)]-\\beta\\,D_{KL}(\\pi_\\theta\\|\\pi_{ref}).\n$$\nHence the policy minimizing $L_{off}(\\theta)$ in Offline preference optimization loss coincides with the one maximizing $J(\\pi_\\theta)$ under reward $r_\\phi^*$, establishing the claimed equivalence.", "ground-truth": "D"}
{"paper": "arxiv:2402.05749", "question": "Consider a discrete set of outcomes $Y$ and a parametric distribution (policy) $\\pi_{\\theta}(y)$ with parameters $\\theta\\in\\mathbb{R}^d$, and a fixed reference distribution $\\pi_{\\mathrm{ref}}(y)$ on $Y$. Define the Kullback-Leibler divergence\n\n$$\n\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\n$$\n\nIntroduce the log-density ratio for each $y\\in Y$:\n\n$$\n\\rho_{\\theta}(y) := \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}.\n$$\n\nShow how to derive the following gradient identity:\n\n$$\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n$$\n\nYou may assume that you can interchange gradient and expectation and that\n$\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)]=0$ under the shared support of the two distributions.", "A": "By definition we have\\n\\n$$\\n\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\\n$$\\n\\nIts gradient contains two terms:\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr]\n+\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\bigr].\\n$$\\n\\nThe second term vanishes because it is the expectation of the score function under its own distribution. Meanwhile, rewriting the \\mu-weighted squared loss with \\mu=\\pi_{\\theta} gives\\n\\n$$\\n\\frac12\\,\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[\\rho_{\\theta}^2]\n=\\frac12\\,\\mathbb{E}_{(y_1,y_2)\\sim\\mu}\\Bigl[\\Bigl(\\log\\frac{\\pi_{\\theta}(y_1)}{\\pi_{\\mathrm{ref}}(y_1)}-\\log\\frac{\\pi_{\\theta}(y_2)}{\\pi_{\\mathrm{ref}}(y_2)}\\Bigr)^2\\Bigr],\\n$$\\n\\nand taking its gradient under \\mu=\\pi_{\\theta} yields\\n\\n$$\\n\\mathbb{E}_{(y_1,y_2)\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}^2)\\bigr]\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr],\\n$$\\n\\nwhere the cross term and exchange of expectations use independence and identical distribution of y_1,y_2. Equating the two expressions proves\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}(y)^2)\\bigr].\\n$$", "B": "To derive the gradient identity, we start with the definition of the Kullback-Leibler (KL) divergence and proceed step-by-step.\n\n**Given**:\n\\[\n\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)], \\quad \\text{where } \\rho_{\\theta}(y) = \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}.\n\\]\n\n**Goal**: Show that\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n\\]\n\n**Derivation**:\n\n1. **Express the gradient of KL divergence**:\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)].\n   \\]\n\n2. **Interchange gradient and expectation** (as permitted by assumption):\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\rho_{\\theta}(y)] + \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)].\n   \\]\n   *Explanation*: The gradient operator acts on both \\(\\rho_{\\theta}(y)\\) and the distribution \\(\\pi_{\\theta}(y)\\), leading to two terms via the product rule:\n   \\[\n   \\nabla_{\\theta}(\\pi_{\\theta}(y)\\rho_{\\theta}(y)) = \\pi_{\\theta}(y)\\nabla_{\\theta}\\rho_{\\theta}(y) + \\rho_{\\theta}(y)\\nabla_{\\theta}\\pi_{\\theta}(y).\n   \\]\n   The second term uses the identity \\(\\nabla_{\\theta}\\pi_{\\theta}(y) = \\pi_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\).\n\n3. **Simplify the second term**:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)].\n   \\]\n\n4. **Analyze the first term**:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)] = 0.\n   \\]\n   *Explanation*: Since \\(\\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\\), its gradient is \\(\\nabla_{\\theta}\\rho_{\\theta}(y) = \\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\). The expectation of the score function \\(\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\) over \\(y \\sim \\pi_{\\theta}\\) is zero (given assumption).\n\n5. **Combine results**:\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)].\n   \\]\n\n6. **Relate to \\(\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\)**:\n   Compute the gradient inside the expectation:\n   \\[\n   \\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr) = \\rho_{\\theta}(y)\\nabla_{\\theta}\\rho_{\\theta}(y) = \\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y).\n   \\]\n   Substituting this into the expectation gives:\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n   \\]\n\n**Conclusion**: The gradient of the KL divergence is equivalent to the expectation of \\(\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\) under \\(\\pi_{\\theta}\\), as required.", "C": "By definition we have\\n\\n$$\\n\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\\n$$\\n\\nIts gradient contains two terms:\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr]\n+\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\bigr].\\n$$\\n\\nThe second term vanishes because it is the expectation of the score function under its own distribution. Meanwhile, rewriting the \\mu-weighted squared loss with \\mu=\\pi_{\\theta} gives\\n\\n$$\\n\\frac12\\,\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[\\rho_{\\theta}^2]\n=\\frac12\\,\\mathbb{E}_{(y_1,y_2)\\sim\\mu}\\Bigl[\\Bigl(\\log\\frac{\\pi_{\\theta}(y_1)}{\\pi_{\\mathrm{ref}}(y_1)}+\\log\\frac{\\pi_{\\theta}(y_2)}{\\pi_{\\mathrm{ref}}(y_2)}\\Bigr)^2\\Bigr],\\n$$\\n\\nand taking its gradient under \\mu=\\pi_{\\theta} yields\\n\\n$$\\n\\mathbb{E}_{(y_1,y_2)\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}^2)\\bigr]\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr],\\n$$\\n\\nwhere the cross term and exchange of expectations use independence and identical distribution of y_1,y_2. Equating the two expressions proves\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}(y)^2)\\bigr].\\n$$", "D": "Alright, let's tackle this problem step by step. I'm going to derive the gradient of the KL divergence with respect to the parameters \\(\\theta\\) and show how it relates to the given expectation involving \\(\\rho_{\\theta}(y)\\).\n\n### 1. Understanding the Definitions\n\nFirst, let's recall the definitions:\n\n- **KL Divergence**:\n  \\[\n  \\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n  \\]\n  \n- **Log-density ratio**:\n  \\[\n  \\rho_{\\theta}(y) = \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)} = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n  \\]\n\n### 2. Computing the Gradient of KL Divergence\n\nWe want to find:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n\\]\n\nAssuming we can interchange the gradient and expectation (which is given), we have:\n\\[\n\\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n\\]\n\n### 3. Expanding \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\)\n\nNow, let's compute \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\):\n\\[\n\\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n\\]\n\\[\n\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y) - \\nabla_{\\theta} \\log\\pi_{\\mathrm{ref}}(y)\n\\]\n\nBut \\(\\pi_{\\mathrm{ref}}(y)\\) does not depend on \\(\\theta\\), so \\(\\nabla_{\\theta} \\log\\pi_{\\mathrm{ref}}(y) = 0\\). Therefore:\n\\[\n\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\n### 4. Relating to \\(\\rho_{\\theta}(y)\\)\n\nNow, let's look at \\(\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\):\n\\[\n\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\n### 5. Expectation of \\(\\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\)\n\nWe are given that:\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\n\\]\n\nThis is a key property because it tells us that the expectation of the score function is zero.\n\n### 6. Combining the Pieces\n\nNow, let's express \\(\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\\) in terms of \\(\\rho_{\\theta}(y)\\):\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\n\\]\n\nBut this seems to give us zero, which doesn't match the desired identity. Wait, this suggests that perhaps there's more to it. Maybe we need to consider that \\(\\rho_{\\theta}(y)\\) itself is involved in the expectation.\n\nLet me think differently. The desired identity is:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\right]\n\\]\n\nWe know:\n\\[\n\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\nSo:\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nNow, recall that:\n\\[\n\\nabla_{\\theta} \\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nBut this is zero, which doesn't match. Hmm, perhaps the trick is to express \\(\\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\) in terms of \\(\\rho_{\\theta}(y)\\).\n\nWait, let's think about the relationship between \\(\\rho_{\\theta}(y)\\) and \\(\\log\\pi_{\\theta}(y)\\):\n\\[\n\\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n\\]\n\\[\n\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\nBut \\(\\mathbb{E}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), so how does \\(\\rho_{\\theta}(y)\\) come into play?\n\nPerhaps the key is to consider that:\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[(\\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\\[\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\log\\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] - \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\log\\pi_{\\mathrm{ref}}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nThe first term is:\n\\[\n\\mathbb{E}[\\log\\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = \\mathbb{E}[\\nabla_{\\theta} (\\pi_{\\theta}(y) \\log\\pi_{\\theta}(y)) - \\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] \n\\]\nWait, this seems convoluted. Maybe another approach.\n\n### 7. Alternative Approach: Express KL in Terms of \\(\\rho_{\\theta}(y)\\)\n\nLet's write the KL divergence explicitly:\n\\[\n\\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)] = \\sum_{y} \\pi_{\\theta}(y) \\rho_{\\theta}(y)\n\\]\n\nTaking the gradient:\n\\[\n\\nabla_{\\theta} \\mathrm{KL} = \\sum_{y} \\nabla_{\\theta} (\\pi_{\\theta}(y) \\rho_{\\theta}(y)) = \\sum_{y} \\left( \\rho_{\\theta}(y) \\nabla_{\\theta} \\pi_{\\theta}(y) + \\pi_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) \\right)\n\\]\n\nUsing \\(\\nabla_{\\theta} \\pi_{\\theta}(y) = \\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\):\n\\[\n= \\sum_{y} \\left( \\rho_{\\theta}(y) \\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y) + \\pi_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) \\right)\n\\]\n\\[\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] + \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n\\]\n\nBut \\(\\mathbb{E}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), so:\n\\[\n\\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nAnd since \\(\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\), we have:\n\\[\n\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\n\\]\n\nTherefore:\n\\[\n\\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right]\n\\]\n\n### 8. Final Derivation\n\nPutting it all together:\n\n1. Start with the KL divergence:\n   \\[\n   \\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n   \\]\n\n2. Take the gradient:\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n   \\]\n   (by interchanging gradient and expectation)\n\n3. Compute \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\):\n   \\[\n   \\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n   \\]\n   \\[\n   \\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n   (since \\(\\pi_{\\mathrm{ref}}(y)\\) does not depend on \\(\\theta\\))\n\n4. Now, consider \\(\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\):\n   \\[\n   \\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n\n5. Take the expectation:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n   \\]\n\n6. But from step 2, we have:\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n   \\]\n   However, \\(\\mathbb{E}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), which seems to suggest \\(\\nabla_{\\theta} \\mathrm{KL} = 0\\), but this contradicts unless we consider that the gradient of KL is actually captured by the term involving \\(\\rho_{\\theta}(y)\\).\n\n   The correct interpretation is that the gradient of KL is equal to the expectation of \\(\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\), because the direct expectation of \\(\\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\) is zero, but when weighted by \\(\\rho_{\\theta}(y)\\), it gives the gradient.\n\n### 9. Conclusion\n\nThus, we've shown that:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\right]\n\\]\n\n### Final Answer\n\nWe derive the gradient identity as follows:\n\n1. Start with the KL divergence:\n   \\[\n   \\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n   \\]\n\n2. Take the gradient with respect to \\(\\theta\\):\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n   \\]\n   (by interchanging gradient and expectation)\n\n3. Compute \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\):\n   \\[\n   \\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n   (since \\(\\pi_{\\mathrm{ref}}(y)\\) is independent of \\(\\theta\\))\n\n4. Observe that:\n   \\[\n   \\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n\n5. Therefore:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n   \\]\n\n6. Since \\(\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), the gradient of KL is captured by the weighted expectation:\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right]\n   \\]\n\nThus, we have derived the desired identity:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n\\]", "ground-truth": "A"}
{"paper": "arxiv:2310.12036", "question": "Let $Y$ be a finite set of actions.  \n\nLet $\\mu \\in \\Delta_Y$ be a fixed behaviour policy that we can sample from, and let $\\pi_{\\text{ref}}\\in \\Delta_Y$ be a fixed reference policy.  For any candidate policy $\\pi\\in\\Delta_Y$ define the log-ratio function\n\\[\nh_{\\pi}(y,y')\\;\\coloneqq\\;\\log\\frac{\\pi(y)\\,\\pi_{\\text{ref}}(y')}{\\pi(y')\\,\\pi_{\\text{ref}}(y)},\\qquad y,y'\\in Y.\n\\]\n\nAssume that, for every unordered pair $\\{y,y'\\}\\subset Y$, a human rater expresses a stochastic preference between $y$ and $y'$ that is captured by the probability\n\\[\np^{*}(y\\succ y' )\\;\\in\\;[0,1],\\qquad\\text{with }p^{*}(y\\succ y')+p^{*}(y'\\succ y)=1.\n\\]\nFor each ordered pair $(y,y')$ sampled i.i.d. from $\\mu\\otimes\\mu$, let the Bernoulli indicator\n\\[\nI(y,y')\\;\\sim\\;\\operatorname{Bernoulli}\\bigl(p^{*}(y\\succ y')\\bigr)\n\\]\nbe $1$ when the rater prefers $y$ to $y'$ and $0$ otherwise.\n\nFor any action $y\\in Y$ define its average preference against $\\mu$ as\n\\[\np^{*}(y\\succ \\mu)\\;\\coloneqq\\;\\mathbb{E}_{y'\\sim\\mu}\\bigl[p^{*}(y\\succ y')\\bigr].\n\\]\nFix a positive regularisation coefficient $\\tau>0$.  Consider the two squared-error objectives\n\\[\n\\begin{aligned}\n\\text{(13)}\\quad & L_1(\\pi)\\;=\\;\\mathbb{E}_{y,y'\\sim\\mu}\\Bigl[\\,h_{\\pi}(y,y')\\; -\\; \\tau^{-1}\\bigl(p^{*}(y\\succ \\mu)-p^{*}(y'\\succ \\mu)\\bigr)\\Bigr]^{2},\\\\[6pt]\n\\text{(16)}\\quad & L_2(\\pi)\\;=\\;\\mathbb{E}_{y,y'\\sim\\mu}\\Bigl[\\,h_{\\pi}(y,y')\\; -\\; \\tau^{-1} I(y,y')\\Bigr]^{2}.\n\\end{aligned}\n\\]\n\n**Proposition 3.**  *The two loss functions satisfy $L_1(\\pi)=L_2(\\pi)+C$ for every policy $\\pi\\in\\Delta_Y$, where the constant $C$ depends only on $\\mu$, $p^{*}$ and $\\tau$ but not on $\\pi$.*\n\nCould you provide a complete and rigorous derivation of Proposition 3, showing step by step why the additive-constant equality holds under the definitions given above?", "A": "This equivalence is not completely trivial, since in general the conditional expectation\n\n$\\mathbb{E}\\bigl[\\,h_{\\pi}(Y,Y') - \\tau^{-1} I(Y,Y')\\mid Y=y,\\,Y'=y'\\bigr]$\n\nis not equal to the corresponding quantity appearing in Equation (13), namely\n\n$h_{\\pi}(y,y') - \\tau^{-1}\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr).$\n\nWe instead need to exploit some symmetry between the distributions of $y$ and $y'$, and use the fact that $h_{\\pi}(y,y')$ decomposes as an additive function of $y$ and $y'$. To show this equality of losses, it is enough to focus on the \"cross-terms\" obtained when expanding the quadratics in Equations (13) and (16); that is, to show\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\,I(y,y')\\bigr]\n=\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr)\\bigr].\n$$\n\nNow, starting with the right-hand side, and using the shorthand\n\n- $\\pi_y=\\log\\bigl(\\pi(y)\\bigr)$\n- $\\pi^R_y=\\log\\bigl(\\pi_{\\mathrm{ref}}(y)\\bigr)$\n- $p_y=p^*(y\\succ\\mu)$\n\n(and similarly for $y'$), we have\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr)\\bigr]\n=\\mathbb{E}_{y,y'\\sim\\mu}[(\\pi_y-\\pi_{y'}+\\pi^R_{y'}-\\pi^R_y)(p_y-p_{y'})]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[\\pi_y p_y - \\pi_y p_{y'} - \\pi_{y'} p_y + \\pi_{y'} p_{y'} + \\pi^R_{y'} p_y - \\pi^R_{y'} p_{y'} - \\pi^R_y p_y + \\pi^R_y p_{y'}]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[(2p_y-1)\\pi_y - (2p_y-1)\\pi^R_y],\n$$\n\nwhere we have used the i.i.d.-ness of $y,y'$ and $\\mathbb{E}_{y\\sim\\mu}[p_y]=\\tfrac12$.\n\nTurning to the left-hand side, we have\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}[h_{\\pi}(y,y')\\,I(y,y')]\n=\\mathbb{E}_{y,y'\\sim\\mu}[(\\pi_y-\\pi_{y'}+\\pi^R_{y'}-\\pi^R_y)\\,I(y,y')]\n$$\n\n$$\n=\\mathbb{E}_{y\\sim\\mu}[(\\pi_y-\\pi^R_y)\\,\\mathbb{E}_{y'\\sim\\mu}[I(y,y')\\mid y]]\n+\\mathbb{E}_{y'\\sim\\mu}[(-\\pi_{y'}+\\pi^R_{y'})\\,\\mathbb{E}_{y\\sim\\mu}[I(y,y')\\mid y']]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[(2p_y-1)\\pi_y - (2p_{y'}-1)\\pi^R_y],\n$$\n\nwhere we used $\\mathbb{E}_{y'\\sim\\mu}[I(y,y')]=p_y$ and $\\mathbb{E}_{y\\sim\\mu}[I(y,y')]=1-p_{y'}$. This establishes the equality of the two losses, as required.", "B": "Certainly! Let's rigorously derive Proposition 3 by showing that \\( L_1(\\pi) = L_2(\\pi) + C \\), where \\( C \\) is a constant independent of \\( \\pi \\).\n\n---\n\n### **Step 1: Expand \\( L_1(\\pi) \\) and \\( L_2(\\pi) \\)**\nFirst, expand the squared terms in both loss functions.\n\n#### **Expansion of \\( L_1(\\pi) \\):**\n\\[\nL_1(\\pi) = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') - \\tau^{-1} \\left( p^{*}(y \\succ \\mu) - p^{*}(y' \\succ \\mu) \\right) \\right]^2.\n\\]\nLet \\( \\Delta p^{*}(y,y') = p^{*}(y \\succ \\mu) - p^{*}(y' \\succ \\mu) \\). Then:\n\\[\nL_1(\\pi) = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y')^2 - 2 \\tau^{-1} h_{\\pi}(y,y') \\Delta p^{*}(y,y') + \\tau^{-2} \\Delta p^{*}(y,y')^2 \\right].\n\\]\n\n#### **Expansion of \\( L_2(\\pi) \\):**\n\\[\nL_2(\\pi) = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') - \\tau^{-1} I(y,y') \\right]^2.\n\\]\nExpanding:\n\\[\nL_2(\\pi) = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y')^2 - 2 \\tau^{-1} h_{\\pi}(y,y') I(y,y') + \\tau^{-2} I(y,y')^2 \\right].\n\\]\n\nSince \\( I(y,y') \\) is Bernoulli, \\( I(y,y')^2 = I(y,y') \\), so:\n\\[\nL_2(\\pi) = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y')^2 - 2 \\tau^{-1} h_{\\pi}(y,y') I(y,y') + \\tau^{-2} I(y,y') \\right].\n\\]\n\n---\n\n### **Step 2: Compare the Cross Terms**\nThe key difference between \\( L_1(\\pi) \\) and \\( L_2(\\pi) \\) lies in the cross terms involving \\( h_{\\pi}(y,y') \\).\n\n#### **Cross Term in \\( L_1(\\pi) \\):**\n\\[\n-2 \\tau^{-1} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') \\Delta p^{*}(y,y') \\right].\n\\]\n\n#### **Cross Term in \\( L_2(\\pi) \\):**\n\\[\n-2 \\tau^{-1} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') I(y,y') \\right].\n\\]\n\nBut since \\( I(y,y') \\sim \\text{Bernoulli}(p^{*}(y \\succ y')) \\), we have:\n\\[\n\\mathbb{E}[I(y,y')] = p^{*}(y \\succ y').\n\\]\nThus:\n\\[\n\\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') I(y,y') \\right] = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') p^{*}(y \\succ y') \\right].\n\\]\n\nNow, recall that:\n\\[\n\\Delta p^{*}(y,y') = p^{*}(y \\succ \\mu) - p^{*}(y' \\succ \\mu) = \\mathbb{E}_{y'' \\sim \\mu} \\left[ p^{*}(y \\succ y'') - p^{*}(y' \\succ y'') \\right].\n\\]\n\nBut since \\( p^{*}(y \\succ y') \\) is antisymmetric (i.e., \\( p^{*}(y \\succ y') = 1 - p^{*}(y' \\succ y) \\)), we can write:\n\\[\n\\Delta p^{*}(y,y') = \\mathbb{E}_{y'' \\sim \\mu} \\left[ 2 p^{*}(y \\succ y'') - 1 \\right] - \\mathbb{E}_{y'' \\sim \\mu} \\left[ 2 p^{*}(y' \\succ y'') - 1 \\right] = 2 \\left( p^{*}(y \\succ \\mu) - p^{*}(y' \\succ \\mu) \\right).\n\\]\n\nHowever, a simpler observation is that:\n\\[\n\\mathbb{E}_{y' \\sim \\mu} \\left[ p^{*}(y \\succ y') \\right] = p^{*}(y \\succ \\mu),\n\\]\nso:\n\\[\n\\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') p^{*}(y \\succ y') \\right] = \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') \\left( p^{*}(y \\succ \\mu) - \\frac{1}{2} \\Delta p^{*}(y,y') \\right) \\right].\n\\]\n\nBut since \\( h_{\\pi}(y,y') \\) is antisymmetric (i.e., \\( h_{\\pi}(y,y') = -h_{\\pi}(y',y) \\)), we have:\n\\[\n\\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') p^{*}(y \\succ y') \\right] = \\frac{1}{2} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') \\Delta p^{*}(y,y') \\right].\n\\]\n\nThus, the cross terms in \\( L_1(\\pi) \\) and \\( L_2(\\pi) \\) are identical:\n\\[\n-2 \\tau^{-1} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ h_{\\pi}(y,y') \\Delta p^{*}(y,y') \\right].\n\\]\n\n---\n\n### **Step 3: Compare the Remaining Terms**\nThe remaining terms in \\( L_1(\\pi) \\) and \\( L_2(\\pi) \\) are:\n\n#### **From \\( L_1(\\pi) \\):**\n\\[\n\\mathbb{E}_{y,y' \\sim \\mu} \\left[ \\tau^{-2} \\Delta p^{*}(y,y')^2 \\right].\n\\]\n\n#### **From \\( L_2(\\pi) \\):**\n\\[\n\\mathbb{E}_{y,y' \\sim \\mu} \\left[ \\tau^{-2} I(y,y') \\right].\n\\]\n\nThe difference between \\( L_1(\\pi) \\) and \\( L_2(\\pi) \\) is:\n\\[\nL_1(\\pi) - L_2(\\pi) = \\tau^{-2} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ \\Delta p^{*}(y,y')^2 - I(y,y') \\right].\n\\]\n\nBut \\( \\mathbb{E}[I(y,y')] = p^{*}(y \\succ y') \\), and:\n\\[\n\\Delta p^{*}(y,y')^2 = \\left( p^{*}(y \\succ \\mu) - p^{*}(y' \\succ \\mu) \\right)^2,\n\\]\nwhich does not depend on \\( \\pi \\). Thus:\n\\[\nC = \\tau^{-2} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ \\Delta p^{*}(y,y')^2 - p^{*}(y \\succ y') \\right],\n\\]\nis a constant independent of \\( \\pi \\).\n\n---\n\n### **Final Conclusion**\nWe have shown:\n\\[\nL_1(\\pi) = L_2(\\pi) + C,\n\\]\nwhere:\n\\[\nC = \\tau^{-2} \\mathbb{E}_{y,y' \\sim \\mu} \\left[ \\left( p^{*}(y \\succ \\mu) - p^{*}(y' \\succ \\mu) \\right)^2 - p^{*}(y \\succ y') \\right],\n\\]\ndepends only on \\( \\mu \\), \\( p^{*} \\), and \\( \\tau \\), but not on \\( \\pi \\). This completes the proof of **Proposition 3**. \\(\\square\\)", "C": "This equivalence is not completely trivial, since in general the conditional expectation\n\n$\\mathbb{E}\\bigl[\\,h_{\\pi}(Y,Y') - \\tau^{-1} I(Y,Y')\\mid Y=y,\\,Y'=y'\\bigr]$\n\nis not equal to the corresponding quantity appearing in Equation (13), namely\n\n$h_{\\pi}(y,y') - \\tau^{-1}\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr).$\n\nWe instead need to exploit some symmetry between the distributions of $y$ and $y'$, and use the fact that $h_{\\pi}(y,y')$ decomposes as an additive function of $y$ and $y'$. To show this equality of losses, it is enough to focus on the \"cross-terms\" obtained when expanding the quadratics in Equations (13) and (16); that is, to show\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\,I(y,y')\\bigr]\n=\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr)\\bigr].\n$$\n\nNow, starting with the right-hand side, and using the shorthand\n\n- $\\pi_y=\\log\\bigl(\\pi(y)\\bigr)$\n- $\\pi^R_y=\\log\\bigl(\\pi_{\\mathrm{ref}}(y)\\bigr)$\n- $p_y=p^*(y\\succ\\mu)$\n\n(and similarly for $y'$), we have\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}\\bigl[h_{\\pi}(y,y')\\bigl(p^*(y\\succ\\mu)-p^*(y'\\succ\\mu)\\bigr)\\bigr]\n=\\mathbb{E}_{y,y'\\sim\\mu}[(\\pi_y-\\pi_{y'}+\\pi^R_{y'}-\\pi^R_y)(p_y-p_{y'})]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[\\pi_y p_y - \\pi_y p_{y'} - \\pi_{y'} p_y + \\pi_{y'} p_{y'} + \\pi^R_{y'} p_y - \\pi^R_{y'} p_{y'} - \\pi^R_y p_y + \\pi^R_y p_{y'}]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[(2p_y-1)\\pi_y - (2p_y-1)\\pi^R_y],\n$$\n\nwhere we have used the i.i.d.-ness of $y,y'$ and $\\mathbb{E}_{y\\sim\\mu}[p_y]=\\tfrac12$.\n\nTurning to the left-hand side, we have\n\n$$\n\\mathbb{E}_{y,y'\\sim\\mu}[h_{\\pi}(y,y')\\,I(y,y')]\n=\\mathbb{E}_{y,y'\\sim\\mu}[(\\pi_y-\\pi_{y'}+\\pi^R_{y'}-\\pi^R_y)\\,I(y,y')]\n$$\n\n$$\n=\\mathbb{E}_{y\\sim\\mu}[(\\pi_y-\\pi^R_y)\\,\\mathbb{E}_{y'\\sim\\mu}[I(y,y')\\mid y]]\n+\\mathbb{E}_{y'\\sim\\mu}[(-\\pi_{y'}+\\pi^R_{y'})\\,\\mathbb{E}_{y\\sim\\mu}[I(y,y')\\mid y']]\n$$\n\n$$\n=\\mathbb{E}_{y,y'\\sim\\mu}[(2p_y-1)\\pi_y - (2p_y-1)\\pi^R_y],\n$$\n\nwhere we used $\\mathbb{E}_{y'\\sim\\mu}[I(y,y')]=p_y$ and $\\mathbb{E}_{y\\sim\\mu}[I(y,y')]=1-p_{y'}$. This establishes the equality of the two losses, as required.", "D": "To prove Proposition 3, we expand both loss functions \\( L_1(\\pi) \\) and \\( L_2(\\pi) \\), then show their difference is a constant \\( C \\) independent of \\( \\pi \\).\n\n### Expansion of \\( L_2(\\pi) \\):\n\\[\nL_2(\\pi) = \\mathbb{E}_{y,y'}\\left[\\left(h_\\pi(y,y') - \\tau^{-1}I(y,y')\\right)^2\\right]\n\\]\nExpanding the square:\n\\[\nL_2 = \\mathbb{E}\\left[h_\\pi^2\\right] - 2\\tau^{-1}\\mathbb{E}\\left[h_\\pi I\\right] + \\tau^{-2}\\mathbb{E}\\left[I^2\\right]\n\\]\nSince \\( I(y,y') \\sim \\text{Bernoulli}(p^*(y \\succ y')) \\), we have \\( \\mathbb{E}[I^2] = \\mathbb{E}[I] = \\mathbb{E}_{y,y'}[p^*(y \\succ y')] \\). Thus:\n\\[\nL_2 = \\mathbb{E}\\left[h_\\pi^2\\right] - 2\\tau^{-1}\\mathbb{E}\\left[h_\\pi p^*(y \\succ y')\\right] + \\tau^{-2}\\mathbb{E}\\left[p^*(y \\succ y')\\right]\n\\]\n\n### Expansion of \\( L_1(\\pi) \\):\n\\[\nL_1(\\pi) = \\mathbb{E}_{y,y'}\\left[\\left(h_\\pi(y,y') - \\tau^{-1}\\big(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu)\\big)\\right)^2\\right]\n\\]\nExpanding the square:\n\\[\nL_1 = \\mathbb{E}\\left[h_\\pi^2\\right] - 2\\tau^{-1}\\mathbb{E}\\left[h_\\pi\\big(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu)\\big)\\right] + \\tau^{-2}\\mathbb{E}\\left[\\big(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu)\\big)^2\\right]\n\\]\n\n### Difference \\( L_1 - L_2 \\):\nSubtracting \\( L_2 \\) from \\( L_1 \\):\n\\[\nL_1 - L_2 = \\tau^{-2}\\left[\\mathbb{E}\\big[(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu))^2\\big] - \\mathbb{E}\\big[p^*(y \\succ y')\\big]\\right] - 2\\tau^{-1}\\left[\\mathbb{E}\\big[h_\\pi(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu))\\big] - \\mathbb{E}\\big[h_\\pi p^*(y \\succ y')\\big]\\right]\n\\]\n\n#### Simplifying the Linear Terms:\nUsing \\( h_\\pi(y,y') = \\log\\frac{\\pi(y)}{\\pi_{\\text{ref}}(y)} - \\log\\frac{\\pi(y')}{\\pi_{\\text{ref}}(y')} \\triangleq f(y) - f(y') \\), we analyze:\n\\[\n\\mathbb{E}\\big[h_\\pi(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu))\\big] = \\mathbb{E}_{y,y'}\\left[(f(y) - f(y'))\\big(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu)\\big)\\right]\n\\]\n\\[\n\\mathbb{E}\\big[h_\\pi p^*(y \\succ y')\\big] = \\mathbb{E}_{y,y'}\\left[(f(y) - f(y'))p^*(y \\succ y')\\right]\n\\]\nThe difference of these terms involves:\n\\[\n\\mathbb{E}\\left[(f(y) - f(y'))\\left(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu) - p^*(y \\succ y')\\right)\\right]\n\\]\nBy symmetry and properties of \\( p^* \\), this expectation **vanishes** (detailed cancellation via relabeling \\( y \\leftrightarrow y' \\)), leaving:\n\\[\n-2\\tau^{-1} \\times 0 = 0\n\\]\n\n#### Constant Term:\nThe remaining difference is:\n\\[\nC = \\tau^{-2}\\left[\\mathbb{E}\\big[(p^*(y \\succ \\mu) - p^*(y' \\succ \\mu))^2\\big] - \\mathbb{E}\\big[p^*(y \\succ y')\\big]\\right]\n\\]\nThis \\( C \\) depends only on \\( \\mu \\), \\( p^* \\), and \\( \\tau \\), not on \\( \\pi \\).\n\n### Conclusion:\n\\[\nL_1(\\pi) = L_2(\\pi) + C\n\\]\nwhere \\( C \\) is constant w.r.t. \\( \\pi \\), proving Proposition 3.", "ground-truth": "C"}
{"paper": "arxiv:2403.03419", "question": "Could you please provide a complete derivation and proof of the following lemma, making use only of the accompanying definitions and assumptions? \n\nDefinitions and assumptions\n  \\(\\pi_r(y)\\): the probability distribution over responses y produced by the original (pre-trained) language model.\n  \\(\\pi_{\\theta}(y)\\): a trainable policy over responses.\n  \\(r^{*}(y)\\): the ground-truth scalar reward assigned to an individual response.\n  \\(\\beta>0\\): a fixed temperature (scaling) constant; \\(Z>0\\) is a finite normalising factor.\n  For any distribution \\(\\pi\\), let \\(\\varphi(\\pi)\\) be its distributional preference value.\n  Let \\(\\mu\\) denote a fixed distribution of dispreferred responses.\n  A Generation-with-Distributional-Control (GDC) problem is solved under the moment constraint \\(\\varphi(\\pi) > \\varphi(\\mu)\\).\n\nLemma 1\nIf the policy \\(\\pi_{\\theta}(y)\\) obtained from the GDC optimisation satisfies the constraint above, then\n\\[\n r^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\qquad\n \\varphi^{*}(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\Big[\\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)}\\Big].\n\\]\n\nPlease present a rigorous, step-by-step derivation and proof of Lemma 1.", "A": "The Generation-with-Distributional-Control (GDC) can be formalized as\n\\[\\theta^{*}=\\arg\\min_{\\theta} \\mathrm{KL}\\big[p(x)\\,\\|\\,\\pi_{\\theta}(x)\\big]-\\big[\\varphi(p)-\\varphi(\\mu)\\big].\\tag{5}\\]\nThe KL term is minimized when \\(p(x)=\\pi_{\\theta}(x)\\), that is, \\(\\pi_r(x)e^{r^{*}(x)/\\beta}/Z=\\pi_{\\theta}(x)\\). Hence we obtain \\(r^{*}(x)=\\beta\\,\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}+\\beta\\log Z\\). We then need only maximize \\(\\varphi(p)-\\varphi(\\mu)\\). \\(\\varphi\\) is defined as the expectation of reward over the given distribution, yielding\n\\[\\begin{aligned}\n\\arg\\max_{\\pi} \\varphi(p)-\\varphi(\\mu)\n&=\\mathbb{E}_{p}[r(x)]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{p(x)}{\\pi_{\\theta}(x)}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{\\pi_r(x)}{\\pi_{\\theta}(x)}\\,\\frac{\\pi_r(x)e^{r^{*}(x)/\\beta}}{Z}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\beta\\,\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big]+\\beta\\,\\mathbb{E}_{\\mu}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big].\\tag{6}\n\\end{aligned}\\]\nThus the GDC problem can be solved by maximizing Eq. (6). Consequently, we may directly define the distributional reward (a constant for a fixed \\(\\pi\\)) as \\(\\varphi^{*}(\\pi)=\\mathbb{E}_{\\pi}\\big[\\beta\\,\\log\\tfrac{\\pi^{*}(x)}{\\pi_r(x)}\\big]\\). Therefore, learning a distributionally controlled LLM is equivalent to maximizing the gap \\(\\varphi(\\pi)-\\varphi(\\mu)\\) when the constraint reflects a human preference relation.", "B": "The Generation-with-Distributional-Control (GDC) can be formalized as\n\\[\\theta^{*}=\\arg\\min_{\\theta} \\mathrm{KL}\\big[p(x)\\,\\|\\,\\pi_{\\theta}(x)\\big]-\\big[\\varphi(p)-\\varphi(\\mu)\\big].\\tag{5}\\]\nThe KL term is minimized when \\(p(x)=\\pi_{\\theta}(x)\\), that is, \\(\\pi_r(x)e^{r^{*}(x)/\\beta}/Z=\\pi_{\\theta}(x)\\). Hence we obtain \\(r^{*}(x)=\\beta\\,\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}+\\beta\\log Z\\). We then need only maximize \\(\\varphi(p)-\\varphi(\\mu)\\). \\(\\varphi\\) is defined as the expectation of reward over the given distribution, yielding\n\\[\\begin{aligned}\n\\arg\\max_{\\pi} \\varphi(p)-\\varphi(\\mu)\n&=\\mathbb{E}_{p}[r(x)]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{p(x)}{\\pi_{\\theta}(x)}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{\\pi_r(x)}{\\pi_{\\theta}(x)}\\,\\frac{\\pi_r(x)e^{r^{*}(x)/\\beta}}{Z}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\beta\\,\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big]-\\beta\\,\\mathbb{E}_{\\mu}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big].\\tag{6}\n\\end{aligned}\\]\nThus the GDC problem can be solved by maximizing Eq. (6). Consequently, we may directly define the distributional reward (a constant for a fixed \\(\\pi\\)) as \\(\\varphi^{*}(\\pi)=\\mathbb{E}_{\\pi}\\big[\\beta\\,\\log\\tfrac{\\pi^{*}(x)}{\\pi_r(x)}\\big]\\). Therefore, learning a distributionally controlled LLM is equivalent to maximizing the gap \\(\\varphi(\\pi)-\\varphi(\\mu)\\) when the constraint reflects a human preference relation.", "C": "Certainly! Below is a rigorous, step-by-step derivation and proof of **Lemma 1**, based on the given definitions and assumptions.\n\n---\n\n### **Step 1: Formulating the GDC Optimization Problem**\n\nThe **Generation-with-Distributional-Control (GDC)** problem involves optimizing the policy \\(\\pi_{\\theta}(y)\\) to maximize the expected reward while satisfying the moment constraint \\(\\varphi(\\pi) > \\varphi(\\mu)\\).  \n\nThe constrained optimization problem can be written as:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}}[r^{*}(y)] \\quad \\text{subject to} \\quad \\varphi(\\pi_{\\theta}) > \\varphi(\\mu).\n\\]\n\nUsing the **method of Lagrange multipliers**, we introduce a dual variable \\(\\beta > 0\\) (the temperature constant) and write the Lagrangian:\n\\[\n\\mathcal{L}(\\pi_{\\theta}, \\beta) = \\mathbb{E}_{y \\sim \\pi_{\\theta}}[r^{*}(y)] - \\beta \\left( \\varphi(\\pi_{\\theta}) - \\varphi(\\mu) \\right).\n\\]\n\nSince \\(\\varphi(\\mu)\\) is fixed, we can ignore it in the optimization (as it does not depend on \\(\\pi_{\\theta}\\)), simplifying the problem to:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}}[r^{*}(y)] - \\beta \\varphi(\\pi_{\\theta}).\n\\]\n\n---\n\n### **Step 2: Solving for the Optimal Policy \\(\\pi^{*}(y)\\)**\n\nTo find the optimal policy \\(\\pi^{*}(y)\\), we take the **variational derivative** of \\(\\mathcal{L}\\) with respect to \\(\\pi_{\\theta}\\) and set it to zero:\n\\[\n\\frac{\\delta \\mathcal{L}}{\\delta \\pi_{\\theta}} = r^{*}(y) - \\beta \\frac{\\delta \\varphi(\\pi_{\\theta})}{\\delta \\pi_{\\theta}} = 0.\n\\]\n\nFrom the definition of \\(\\varphi(\\pi)\\), we assume it is a **linear expectation functional**:\n\\[\n\\varphi(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi(y)}{\\pi_r(y)} \\right].\n\\]\nThus, its variational derivative is:\n\\[\n\\frac{\\delta \\varphi(\\pi)}{\\delta \\pi} = \\log \\frac{\\pi(y)}{\\pi_r(y)} + 1.\n\\]\n\nSubstituting back into the optimality condition:\n\\[\nr^{*}(y) = \\beta \\left( \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + 1 \\right).\n\\]\n\nTo ensure \\(\\pi^{*}(y)\\) is normalized, we introduce a **partition function** \\(Z\\):\n\\[\n\\pi^{*}(y) = \\frac{1}{Z} \\pi_r(y) \\exp \\left( \\frac{r^{*}(y)}{\\beta} \\right), \\quad \\text{where} \\quad Z = \\sum_y \\pi_r(y) \\exp \\left( \\frac{r^{*}(y)}{\\beta} \\right).\n\\]\n\nTaking the logarithm and rearranging:\n\\[\n\\log \\pi^{*}(y) = \\log \\pi_r(y) + \\frac{r^{*}(y)}{\\beta} - \\log Z.\n\\]\nSolving for \\(r^{*}(y)\\):\n\\[\nr^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z.\n\\]\nThis proves the **first part of Lemma 1**.\n\n---\n\n### **Step 3: Deriving \\(\\varphi^{*}(\\pi)\\)**\n\nGiven the optimal policy \\(\\pi^{*}(y)\\), the **distributional preference value** \\(\\varphi^{*}(\\pi)\\) is:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\n\nSubstituting the expression for \\(\\pi^{*}(y)\\) from Step 2:\n\\[\n\\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} = \\frac{r^{*}(y)}{\\beta} - \\log Z.\n\\]\nThus:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\frac{r^{*}(y)}{\\beta} - \\log Z \\right].\n\\]\nSince \\(\\log Z\\) is a constant, it factors out of the expectation:\n\\[\n\\varphi^{*}(\\pi) = \\frac{1}{\\beta} \\mathbb{E}_{y \\sim \\pi} [r^{*}(y)] - \\log Z.\n\\]\n\nHowever, from the **first part of Lemma 1**, we have:\n\\[\n\\mathbb{E}_{y \\sim \\pi} [r^{*}(y)] = \\beta \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right] + \\beta \\log Z.\n\\]\nSubstituting back:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right] + \\log Z - \\log Z = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\n\nBut this is circular. Instead, let us directly compute \\(\\varphi^{*}(\\pi)\\) using the optimal policy:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\nFrom Step 2, we have:\n\\[\n\\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} = \\frac{r^{*}(y)}{\\beta} - \\log Z.\n\\]\nThus:\n\\[\n\\varphi^{*}(\\pi) = \\frac{1}{\\beta} \\mathbb{E}_{y \\sim \\pi} [r^{*}(y)] - \\log Z.\n\\]\n\nBut since \\(\\mathbb{E}_{y \\sim \\pi} [r^{*}(y)]\\) is not directly given, we instead use the **definition of \\(\\varphi\\)** and the **optimal policy** to conclude:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right],\n\\]\nwhere the \\(\\beta\\) scaling is introduced to match the **first part of Lemma 1**.\n\nThis proves the **second part of Lemma 1**.\n\n---\n\n### **Final Result**\n\nCombining the results from Steps 2 and 3, we obtain **Lemma 1**:\n\\[\nr^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\qquad\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\n\nThis completes the proof. \\(\\square\\)", "D": "Here is the step-by-step derivation and proof of Lemma 1:\n\n**Step 1: Formulate the GDC Optimization Problem**\n\nWe aim to find the optimal policy $\\pi^*(y)$ that maximizes the expected reward while maintaining proximity to the original model $\\pi_r(y)$, subject to the constraint $\\varphi(\\pi) > \\varphi(\\mu)$. This is equivalent to solving:\n\n$$\n\\max_{\\pi} \\mathbb{E}_{y\\sim\\pi}[r^*(y)] - \\beta \\cdot \\text{KL}(\\pi \\parallel \\pi_r)\n$$\n\nwhere $\\text{KL}(\\pi \\parallel \\pi_r) = \\mathbb{E}_{y\\sim\\pi}\\left[\\log\\frac{\\pi(y)}{\\pi_r(y)}\\right]$ ensures policy proximity, and $\\beta > 0$ is the temperature parameter.\n\n---\n\n**Step 2: Solve the Unconstrained Optimization**\n\nIntroduce the Lagrangian for normalization constraint $\\sum_y \\pi(y) = 1$:\n\n$$\n\\mathcal{L} = \\mathbb{E}_{y\\sim\\pi}[r^*(y)] - \\beta \\cdot \\mathbb{E}_{y\\sim\\pi}\\left[\\log\\frac{\\pi(y)}{\\pi_r(y)}\\right] + \\lambda\\left(1 - \\sum_y \\pi(y)\\right)\n$$\n\nTake functional derivative w.r.t. $\\pi(y)$ and set to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi(y)} = r^*(y) - \\beta\\left(\\log\\frac{\\pi(y)}{\\pi_r(y)} + 1\\right) - \\lambda = 0\n$$\n\nSolve for $\\pi(y)$:\n$$\n\\log\\frac{\\pi^*(y)}{\\pi_r(y)} = \\frac{r^*(y)}{\\beta} - \\frac{\\lambda + \\beta}{\\beta}\n$$\nExponentiate both sides:\n$$\n\\pi^*(y) = \\pi_r(y) \\exp\\left(\\frac{r^*(y)}{\\beta}\\right) \\cdot \\exp\\left(-\\frac{\\lambda + \\beta}{\\beta}\\right)\n$$\n\n---\n\n**Step 3: Identify Normalization Constant**\n\nLet $Z = \\exp\\left(\\frac{\\lambda + \\beta}{\\beta}\\right)$, which ensures $\\sum_y \\pi^*(y) = 1$:\n$$\n\\pi^*(y) = \\frac{\\pi_r(y) \\exp\\left(r^*(y)/\\beta\\right)}{Z}\n$$\n\nSolve for $r^*(y)$ by rearranging:\n$$\nr^*(y) = \\beta \\log\\frac{\\pi^*(y)}{\\pi_r(y)} + \\beta \\log Z\n$$\n\n---\n\n**Step 4: Derive Distributional Preference Value**\n\nBy definition, $\\varphi^*(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[\\beta \\log\\frac{\\pi^*(y)}{\\pi_r(y)}\\right]$. Substitute $r^*(y)$ from above:\n$$\n\\varphi^*(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[r^*(y) - \\beta \\log Z\\right] = \\mathbb{E}_{y\\sim\\pi}\\left[r^*(y)\\right] - \\beta \\log Z\n$$\n\nHowever, for the **optimal policy** $\\pi^*$, the normalization term $\\beta \\log Z$ equals the maximum achievable reward:\n$$\n\\beta \\log Z = \\mathbb{E}_{y\\sim\\pi^*}\\left[r^*(y)\\right] - \\beta \\cdot \\text{KL}(\\pi^* \\parallel \\pi_r)\n$$\n\nThus, under the GDC constraint $\\varphi(\\pi) > \\varphi(\\mu)$, the preference value reduces to:\n$$\n\\varphi^*(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[\\beta \\log\\frac{\\pi^*(y)}{\\pi_r(y)}\\right]\n$$\n\n---\n\n**Conclusion**\n\nThe optimal reward function and preference value under GDC constraints are:\n$$\n\\boxed{\nr^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\quad\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[\\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)}\\right]\n}\n$$\n\nThis completes the proof of Lemma 1.", "ground-truth": "B"}
{"paper": "arxiv:2403.03419", "question": "Could you please provide a complete, step-by-step derivation and proof of **Theorem 1** stated below?  All necessary symbols, functions, and assumptions are defined in the statement, so you may rely solely on the information given here.\n\n---  \n**Setting and notation**  \n   Let a prompt-response pair (x,y) be drawn from a dataset D.  \n   Let \\(\\pi_{\\theta}(y\\mid x)\\) denote the current large-language-model (LLM) policy parametrised by \\(\\theta\\).  \n   Two fixed reference policies are used: a \"less harmful\" one \\(\\pi_{r^{+}}(y\\mid x)\\) and a \"more harmful\" one \\(\\pi_{r^{-}}(y\\mid x)\\).  Their mixture is written \\(\\pi_{r}(y\\mid x)\\).  \n   A distribution \\(\\mu(y_l\\mid x)\\) is the empirical distribution of *dispreferred* (negative) responses \\(y_l\\).  \n   Let \\(K\\in\\mathbb{N}_{+}\\) be the number of self-sampled responses \\(\\{y_i\\}_{i=1}^{K}\\) drawn from \\(\\pi_{r}(\\,\\cdot\\mid x)\\).  \n   Hyper-parameters \\(\\alpha,\\beta>0\\).  \n   The sigmoid function is \\(\\sigma(z)=1/(1+e^{-z})\\).  \n   The Jeffrey divergence between two distributions p and q is \\(D_{J}[p\\Vert q]=\\tfrac12\\bigl(D_{\\mathrm{KL}}[p\\Vert q]+D_{\\mathrm{KL}}[q\\Vert p]\\bigr).  \n\n---  \n**Loss function (Eq. (3))**  \n\\[\nL_{\\mathrm{D2O}}\\;=\\;-\\;\\mathbb{E}_{(x,y_l)\\sim \\mathcal{D}}\\Bigg[\\,\\log\\, \\sigma\\Bigg(\\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)}\\;\\; -\\;\\; \\alpha\\,\\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)}\\Bigg)\\Bigg]\\,,\\quad y_i\\sim\\pi_{r}(\\,\\cdot\\mid x).\n\\]\n\n---  \n**Distributional Bradley-Terry preference model**  \nFor two response distributions \\(\\pi_{\\theta}(\\,\\cdot\\mid x)\\) and \\(\\mu(\\,\\cdot\\mid x)\\), define the preference probability  \n\\[\np\\bigl(\\pi_{\\theta}(y\\mid x)\\succ \\mu(y\\mid x)\\bigr)\\;=\\;\\frac{\\exp\\bigl(r_{\\!*}(\\pi_{\\theta})\\bigr)}{\\exp\\bigl(r_{\\!*}(\\pi_{\\theta})\\bigr)+\\exp\\bigl(r_{\\!*}(\\mu)\\bigr)}\\,,\n\\]  \nwhere \\(r_{\\!*}(\\,\\cdot\\,)\\) is a distribution-level reward function to be learned.\n\n---  \n**Theorem 1**  \n*Optimising the loss \\(L_{\\mathrm{D2O}}\\) in Eq. (3) approximately learns the distributional Bradley-Terry preference model*  \n\\[ p\\bigl(\\pi_{\\theta}(y\\mid x)\\succ\\mu(y_l\\mid x)\\bigr)\\;, \\]  \n*which, when \\(\\alpha=\\beta\\), upper-bounds the instance-level preference model used in Direct Preference Optimisation (DPO), namely*  \n\\[ \\mathbb{E}_{y\\sim\\pi_{\\theta}(\\,\\cdot\\mid x)}\\,\\mathbb{E}_{y_l\\sim\\mu(\\,\\cdot\\mid x)}\\bigl[\\,p(y\\succ y_l)\\bigr]. \\]  \n*Moreover, when the two reference policies coincide \\(\\bigl(\\pi_{r^{-}}=\\pi_{r^{+}}=\\pi_{r}\\bigr),\\;L_{\\mathrm{D2O}}\\) implicitly applies a Jeffrey-divergence regularisation*  \n\\[ D_{J}\\bigl[\\pi_{\\theta}(y\\mid x)\\;\\Vert\\;\\pi_{r}(y\\mid x)\\bigr]\\,. \\]\n\n---  \n**Task**  \nPlease deliver a rigorous derivation and proof of Theorem 1, clearly explaining each mathematical step and any intermediate results that are required to reach the conclusion.", "A": "Given a prompt $x$, we first define the reward of a response generated from $x$ as $r(x,y)$, and the reward of a distribution (policy) $\\pi(y\\mid x)$ as the expectation of reward over $\\pi(y\\mid x)$, $$r(\\pi(\\cdot\\mid x))=E_{\\pi(y\\mid x)}[r(x,y)].$$ Consider the general objective for preference optimization (Azar et al., 2023):\n\n$$\n\\arg\\max_{\\pi}\\;E_{x\\sim p(x)}\\Bigl\\{E_{y\\sim\\pi(y\\mid x),\\,y'\\sim\\mu(y\\mid x)}[\\Psi(p^*(y\\succ y'\\mid x))]-\\beta\\;\\mathrm{KL}[\\pi(y\\mid x)\\|\\pi_r(y\\mid x)]\\Bigr\\},\n$$\n\nwhere $\\Psi\\colon[0,1]\\to\\mathbb{R}$ is a non-decreasing function. For brevity, we omit $x$ in what follows.\n\nDifferent from the above, we consider two reference policies: $\\pi_r^{+}(y)$ (more helpful) and $\\pi_r^{-}(y)$ (more harmful). Then:\n\n$$\nE_{y\\sim\\pi,\\,y'\\sim\\pi_r^{-}}[\\Psi(p^*(y\\succ y'))]-\\beta\\;\\mathrm{KL}[\\pi(y)\\|\\pi_r^{-}(y)] = \\int\\pi(y)\\Bigl\\{E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]-\\beta\\log\\tfrac{\\pi(y)}{\\pi_r^{-}(y)}\\Bigr\\}\\,dy.\n$$\n\nBy algebraic steps, this equals\n\n$$\n-\\beta\\;\\mathrm{KL}\\Bigl[\\pi(y)\\Bigm\\|\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}\\Bigr]+\\log Z.\\tag{8}\n$$\n\nHence maximizing Eq.(8) is equivalent to minimizing\n\n$$\n\\mathrm{KL}\\Bigl[\\pi(y)\\Bigm\\|\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}\\Bigr],\\tag{9}\n$$\n\nand the optimal policy is\n\n$$\n\\pi^*(y)=\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}.\\tag{10}\n$$\n\nIf $\\Psi(p^*(y\\succ y'))$ is the Bradley--Terry model with $\\Psi(q)=\\log\\tfrac{q}{1-q}$, the ground-truth reward is\n\n$$\nr^*(y)=E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]=\\beta\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}+\\beta\\log Z,\n$$\n\nso\n\n$$\nr^*(\\pi)=E_{\\pi(y)}[r^*(y)]=\\beta\\,E_{\\pi(y)}\\Bigl[\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}\\Bigr]+C_1,\n$$\n\nwhere $C_1=E_{\\pi_r^{-}(y')}[r^*(y')]+\\beta\\log Z$ is constant. Similarly, using reverse KL regularization $\\mathrm{KL}[\\pi_r^{+}(y)\\|\\pi(y)]$ yields\n\n$$\nr^*(\\mu)=\\alpha\\,E_{\\mu(y')}\\Bigl[\\log\\tfrac{\\pi^*(y')}{\\pi_r^{+}(y')}\\Bigr]+C_2,\n$$\n\nleading to the Bradley--Terry model on distributions:\n\n$$\np^*(\\pi\\succ\\mu)=\\frac{\\exp(r^*(\\pi))}{\\exp(r^*(\\pi))+\\exp(r^*(\\mu))} = \\sigma\\bigl(\\beta\\,E_{\\pi(y)}[\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}]-\\alpha\\,E_{\\mu(y')}[\\log\\tfrac{\\pi^*(y')}{\\pi_r^{+}(y')}] + C_1 - C_2\\bigr).\\tag{11}\n$$\n\nFinally, aligning $\\pi_\\theta$ via\n\n$$\n\\theta^*=\\arg\\max_\\theta E_D[\\log p(\\pi\\succ\\mu)] = \\arg\\max_\\theta E_D\\bigl[\\log\\sigma(\\beta\\,E_{\\pi(y)}[\\log\\tfrac{\\pi_\\theta(y)}{\\pi_r^{-}(y)}]-\\alpha\\,E_{\\mu(y')}[\\log\\tfrac{\\pi_\\theta(y')}{\\pi_r^{+}(y')}] )\\bigr],\\tag{12}\n$$\n\nand approximating expectations recovers Eq.(7) when $M=1$. This concludes the proof.", "B": "Given a prompt $x$, we first define the reward of a response generated from $x$ as $r(x,y)$, and the reward of a distribution (policy) $\\pi(y\\mid x)$ as the expectation of reward over $\\pi(y\\mid x)$, $$r(\\pi(\\cdot\\mid x))=E_{\\pi(y\\mid x)}[r(x,y)].$$ Consider the general objective for preference optimization (Azar et al., 2023):\n\n$$\n\\arg\\max_{\\pi}\\;E_{x\\sim p(x)}\\Bigl\\{E_{y\\sim\\pi(y\\mid x),\\,y'\\sim\\mu(y\\mid x)}[\\Psi(p^*(y\\succ y'\\mid x))]-\\beta\\;\\mathrm{KL}[\\pi(y\\mid x)\\|\\pi_r(y\\mid x)]\\Bigr\\},\n$$\n\nwhere $\\Psi\\colon[0,1]\\to\\mathbb{R}$ is a non-decreasing function. For brevity, we omit $x$ in what follows.\n\nDifferent from the above, we consider two reference policies: $\\pi_r^{+}(y)$ (more helpful) and $\\pi_r^{-}(y)$ (more harmful). Then:\n\n$$\nE_{y\\sim\\pi,\\,y'\\sim\\pi_r^{-}}[\\Psi(p^*(y\\succ y'))]-\\beta\\;\\mathrm{KL}[\\pi(y)\\|\\pi_r^{-}(y)] = \\int\\pi(y)\\Bigl\\{E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]-\\beta\\log\\tfrac{\\pi(y)}{\\pi_r^{-}(y)}\\Bigr\\}\\,dy.\n$$\n\nBy algebraic steps, this equals\n\n$$\n-\\beta\\;\\mathrm{KL}\\Bigl[\\pi(y)\\Bigm\\|\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}\\Bigr]+\\log Z.\\tag{8}\n$$\n\nHence maximizing Eq.(8) is equivalent to minimizing\n\n$$\n\\mathrm{KL}\\Bigl[\\pi(y)\\Bigm\\|\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}\\Bigr],\\tag{9}\n$$\n\nand the optimal policy is\n\n$$\n\\pi^*(y)=\\exp\\bigl(\\tfrac{1}{\\beta}E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]\\bigr)\\;\\tfrac{\\pi_r^{-}(y)}{Z}.\\tag{10}\n$$\n\nIf $\\Psi(p^*(y\\succ y'))$ is the Bradley--Terry model with $\\Psi(q)=\\log\\tfrac{q}{1-q}$, the ground-truth reward is\n\n$$\nr^*(y)=E_{\\pi_r^{-}(y')}[\\Psi(p^*(y\\succ y'))]=\\beta\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}+\\beta\\log Z,\n$$\n\nso\n\n$$\nr^*(\\pi)=E_{\\pi(y)}[r^*(y)]=\\beta\\,E_{\\pi(y)}\\Bigl[\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}\\Bigr]+C_1,\n$$\n\nwhere $C_1=E_{\\pi_r^{-}(y')}[r^*(y')]+\\beta\\log Z$ is constant. Similarly, using reverse KL regularization $\\mathrm{KL}[\\pi_r^{+}(y)\\|\\pi(y)]$ yields\n\n$$\nr^*(\\mu)=\\alpha\\,E_{\\mu(y')}\\Bigl[\\log\\tfrac{\\pi^*(y')}{\\pi_r^{+}(y')}\\Bigr]+C_2,\n$$\n\nleading to the Bradley--Terry model on distributions:\n\n$$\np^*(\\pi\\succ\\mu)=\\frac{\\exp(r^*(\\pi))}{\\exp(r^*(\\pi))+\\exp(r^*(\\mu))} = \\sigma\\bigl(\\beta\\,E_{\\pi(y)}[\\log\\tfrac{\\pi^*(y)}{\\pi_r^{-}(y)}]-\\alpha\\,E_{\\mu(y')}[\\log\\tfrac{\\pi^*(y')}{\\pi_r^{+}(y')}] + C_1 - C_2\\bigr).\\tag{11}\n$$\n\nFinally, aligning $\\pi_\\theta$ via\n\n$$\n\\theta^*=\\arg\\max_\\theta E_D[\\log p(\\pi\\succ\\mu)] = \\arg\\max_\\theta E_D\\bigl[\\log\\sigma(\\beta\\,E_{\\pi(y)}[\\log\\tfrac{\\pi_\\theta(y)}{\\pi_r^{-}(y)}]-\\alpha\\,E_{\\mu(y')}[\\log\\tfrac{\\pi_\\theta(y')}{\\pi_r^{+}(y')}] )\\bigr],\\tag{12}\n$$\n\nand approximating expectations recovers Eq.(7) when $M=1$. This concludes the proof.", "C": "Certainly! Below is a rigorous, step-by-step derivation and proof of **Theorem 1**, explaining each mathematical step and intermediate result.\n\n---\n\n### **Step 1: Rewriting the Loss Function \\( L_{\\mathrm{D2O}} \\)**\nThe loss function is given by:\n\\[\nL_{\\mathrm{D2O}} = -\\mathbb{E}_{(x,y_l)\\sim \\mathcal{D}}\\left[ \\log \\sigma\\left( \\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)} - \\alpha \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)} \\right) \\right], \\quad y_i \\sim \\pi_{r}(\\cdot \\mid x).\n\\]\nUsing the sigmoid property \\(\\sigma(-z) = 1 - \\sigma(z)\\), we rewrite the loss as:\n\\[\nL_{\\mathrm{D2O}} = \\mathbb{E}_{(x,y_l)\\sim \\mathcal{D}}\\left[ \\log \\left(1 + \\exp\\left( -\\left( \\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)} - \\alpha \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)} \\right) \\right) \\right) \\right].\n\\]\n\n### **Step 2: Connecting to the Bradley-Terry Model**\nThe **Distributional Bradley-Terry (DBT) preference model** defines:\n\\[\np(\\pi_{\\theta} \\succ \\mu) = \\frac{\\exp(r_*(\\pi_{\\theta}))}{\\exp(r_*(\\pi_{\\theta})) + \\exp(r_*(\\mu))}.\n\\]\nWe hypothesize that the argument of the sigmoid in \\( L_{\\mathrm{D2O}} \\) approximates the log-odds of \\(\\pi_{\\theta}\\) being preferred over \\(\\mu\\):\n\\[\n\\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)} - \\alpha \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)} \\approx r_*(\\pi_{\\theta}) - r_*(\\mu).\n\\]\nThis suggests:\n\\[\nr_*(\\pi_{\\theta}) \\approx \\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)}, \\quad r_*(\\mu) \\approx \\alpha \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)}.\n\\]\n\n### **Step 3: Special Case \\(\\alpha = \\beta\\)**\nWhen \\(\\alpha = \\beta\\), the loss simplifies to:\n\\[\nL_{\\mathrm{D2O}} = \\mathbb{E}_{(x,y_l)\\sim \\mathcal{D}}\\left[ \\log \\left(1 + \\exp\\left( -\\beta \\left( \\frac{1}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)} - \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)} \\right) \\right) \\right) \\right].\n\\]\nThis resembles the **DPO instance-level preference model**:\n\\[\n\\mathbb{E}_{y \\sim \\pi_{\\theta}(\\cdot \\mid x)} \\mathbb{E}_{y_l \\sim \\mu(\\cdot \\mid x)} \\left[ p(y \\succ y_l) \\right],\n\\]\nbut operates at the **distributional level**, thus upper-bounding it.\n\n### **Step 4: Jeffrey Divergence Regularization**\nWhen \\(\\pi_{r^{-}} = \\pi_{r^{+}} = \\pi_r\\), the loss becomes:\n\\[\nL_{\\mathrm{D2O}} = -\\mathbb{E}_{(x,y_l)\\sim \\mathcal{D}}\\left[ \\log \\sigma\\left( \\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r}(y_i\\mid x)} - \\alpha \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r}(y_l\\mid x)} \\right) \\right].\n\\]\nTaking a second-order Taylor expansion of \\(\\log \\pi_{\\theta}\\) around \\(\\pi_r\\), we find:\n\\[\n\\frac{1}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r}(y_i\\mid x)} \\approx \\mathbb{E}_{y \\sim \\pi_r} \\left[ \\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_{r}(y\\mid x)} \\right] + \\text{higher-order terms}.\n\\]\nThe leading term is the **KL divergence** \\( D_{\\mathrm{KL}}[\\pi_r \\Vert \\pi_{\\theta}] \\), and symmetrically, the second term contributes \\( D_{\\mathrm{KL}}[\\pi_{\\theta} \\Vert \\pi_r] \\). Thus:\n\\[\nL_{\\mathrm{D2O}} \\approx \\text{const} + \\frac{\\beta - \\alpha}{2} \\left( D_{\\mathrm{KL}}[\\pi_r \\Vert \\pi_{\\theta}] + D_{\\mathrm{KL}}[\\pi_{\\theta} \\Vert \\pi_r] \\right) = \\text{const} + (\\beta - \\alpha) D_J[\\pi_{\\theta} \\Vert \\pi_r],\n\\]\nwhere \\( D_J \\) is the **Jeffrey divergence**.\n\n### **Conclusion (Proof of Theorem 1)**\n1. **DBT Preference Model Approximation**:  \n   Optimizing \\( L_{\\mathrm{D2O}} \\) learns a distributional reward \\( r_*(\\pi_{\\theta}) \\) that approximates the DBT preference model:\n   \\[\n   p(\\pi_{\\theta} \\succ \\mu) = \\sigma\\left( r_*(\\pi_{\\theta}) - r_*(\\mu) \\right).\n   \\]\n2. **Upper-Bound on DPO**:  \n   When \\(\\alpha = \\beta\\), the loss implicitly bounds the instance-level DPO preference model:\n   \\[\n   \\mathbb{E}_{y \\sim \\pi_{\\theta}} \\mathbb{E}_{y_l \\sim \\mu} \\left[ p(y \\succ y_l) \\right] \\leq p(\\pi_{\\theta} \\succ \\mu).\n   \\]\n3. **Jeffrey Divergence Regularization**:  \n   When \\(\\pi_{r^{-}} = \\pi_{r^{+}} = \\pi_r\\), the loss enforces:\n   \\[\n   L_{\\mathrm{D2O}} \\approx \\text{const} + (\\beta - \\alpha) D_J[\\pi_{\\theta} \\Vert \\pi_r].\n   \\]\n\nThus, **Theorem 1** is rigorously proven. \\(\\square\\)  \n\n---\n\n### **Key Takeaways**\n- The loss \\( L_{\\mathrm{D2O}} \\) implicitly learns a **distributional Bradley-Terry model**.\n- When \\(\\alpha = \\beta\\), it **upper-bounds** the instance-level DPO objective.\n- If reference policies coincide, it **regularizes** \\(\\pi_{\\theta}\\) toward \\(\\pi_r\\) via the **Jeffrey divergence**.\n\nThis completes the proof. Let me know if any steps need further clarification!", "D": "To prove Theorem 1, we proceed in three parts: (1) connecting \\(L_{\\mathrm{D2O}}\\) to the distributional Bradley-Terry model, (2) establishing the upper bound on the instance-level preference model when \\(\\alpha = \\beta\\), and (3) deriving the implicit Jeffrey divergence regularization when \\(\\pi_{r^{-}} = \\pi_{r^{+}} = \\pi_r\\).\n\n---\n\n### **1. Relating \\(L_{\\mathrm{D2O}}\\) to the Distributional Bradley-Terry Model**\n\n**Key Steps:**  \nThe loss \\(L_{\\mathrm{D2O}}\\) involves the sigmoid of the difference between two terms:  \n\\[\n\\underbrace{\\frac{\\beta}{K}\\sum_{i=1}^{K}\\log\\frac{\\pi_{\\theta}(y_i\\mid x)}{\\pi_{r^{-}}(y_i\\mid x)}}_{\\text{Term 1}} - \\underbrace{\\alpha\\,\\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)}}_{\\text{Term 2}}.\n\\]\n\n**Step 1.1: Interpret Term 1 as a Distributional Reward**  \nWhen \\(K \\to \\infty\\), Term 1 converges to:  \n\\[\n\\beta \\cdot \\mathbb{E}_{y \\sim \\pi_r(\\cdot|x)}\\left[\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_{r^{-}}(y\\mid x)}\\right].\n\\]\nThis represents a scaled *distributional reward* for \\(\\pi_\\theta\\) relative to \\(\\pi_{r^{-}}\\):  \n\\[\nr_*(\\pi_\\theta) \\triangleq \\mathbb{E}_{y \\sim \\pi_r}\\left[\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_{r^{-}}(y\\mid x)}\\right].\n\\]\n\n**Step 1.2: Interpret Term 2 as a Reward for \\(\\mu\\)**  \nTerm 2 is:  \n\\[\n\\alpha \\cdot \\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)},\n\\]\nwhere \\(y_l \\sim \\mu\\). Taking expectations over \\(\\mu\\), this becomes:  \n\\[\n\\alpha \\cdot \\mathbb{E}_{y_l \\sim \\mu}\\left[\\log\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_{r^{+}}(y_l\\mid x)}\\right] \\triangleq \\alpha \\cdot r_*(\\mu).\n\\]\n\n**Step 1.3: Link to Bradley-Terry Preference Probability**  \nThe sigmoid argument becomes:  \n\\[\n\\beta \\cdot r_*(\\pi_\\theta) - \\alpha \\cdot r_*(\\mu).\n\\]\nSetting \\(\\alpha = \\beta\\), this simplifies to:  \n\\[\n\\beta \\left(r_*(\\pi_\\theta) - r_*(\\mu)\\right).\n\\]\nThe Bradley-Terry preference probability is then:  \n\\[\np(\\pi_\\theta \\succ \\mu) = \\sigma\\left(\\beta \\left(r_*(\\pi_\\theta) - r_*(\\mu)\\right)\\right).\n\\]\nThus, minimizing \\(L_{\\mathrm{D2O}}\\) corresponds to maximizing \\(p(\\pi_\\theta \\succ \\mu)\\).\n\n---\n\n### **2. Upper Bound on Instance-Level Preference Model**\n\n**Key Steps:**  \nThe instance-level preference model in DPO is:  \n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\mathbb{E}_{y_l\\sim\\mu}\\left[p(y \\succ y_l)\\right], \\quad \\text{where } p(y \\succ y_l) = \\frac{e^{r(y)}}{e^{r(y)} + e^{r(y_l)}}.\n\\]\n\n**Step 2.1: Relate Instance-Level and Distributional Rewards**  \nAssume the instance-level reward \\(r(y)\\) is defined as:  \n\\[\nr(y) = \\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_{r}(y\\mid x)}.\n\\]\nThis aligns with standard RLHF/DPO setups where rewards are derived from policy likelihood ratios.\n\n**Step 2.2: Apply Jensen’s Inequality**  \nThe distributional Bradley-Terry model satisfies:  \n\\[\np(\\pi_\\theta \\succ \\mu) = \\sigma\\left(\\beta \\cdot \\mathbb{E}_{y \\sim \\pi_r}\\left[r(y)\\right] - \\beta \\cdot \\mathbb{E}_{y_l \\sim \\mu}\\left[r(y_l)\\right]\\right).\n\\]\nSince \\(\\sigma\\) is convex for inputs \\(< 0\\) and concave for inputs \\(> 0\\), Jensen’s inequality gives:  \n\\[\n\\sigma\\left(\\mathbb{E}[r(y) - r(y_l)]\\right) \\geq \\mathbb{E}\\left[\\sigma(r(y) - r(y_l))\\right].\n\\]\nThus:  \n\\[\np(\\pi_\\theta \\succ \\mu) \\geq \\mathbb{E}_{y,y_l}\\left[p(y \\succ y_l)\\right].\n\\]\n\n---\n\n### **3. Implicit Jeffrey Divergence Regularization**\n\n**Key Steps:**  \nWhen \\(\\pi_{r^{-}} = \\pi_{r^{+}} = \\pi_r\\), the loss simplifies. Let \\(\\pi_r(y|x)\\) be the common reference policy.\n\n**Step 3.1: Rewrite the Loss Argument**  \nThe argument of \\(\\sigma\\) becomes:  \n\\[\n\\beta \\left(\\frac{1}{K}\\sum_{i=1}^K \\log\\frac{\\pi_\\theta(y_i|x)}{\\pi_r(y_i|x)} - \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_r(y_l|x)}\\right).\n\\]\n\n**Step 3.2: Approximate with KL Divergences**  \nFor large \\(K\\), the first term converges to:  \n\\[\n\\beta \\cdot \\mathbb{E}_{y \\sim \\pi_r}\\left[\\log\\frac{\\pi_\\theta(y|x)}{\\pi_r(y|x)}\\right] = -\\beta \\cdot D_{\\mathrm{KL}}(\\pi_r \\Vert \\pi_\\theta).\n\\]\nThe second term becomes:  \n\\[\n-\\beta \\cdot \\mathbb{E}_{y_l \\sim \\mu}\\left[\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_r(y_l|x)}\\right] = \\beta \\cdot \\left(D_{\\mathrm{KL}}(\\mu \\Vert \\pi_\\theta) - D_{\\mathrm{KL}}(\\mu \\Vert \\pi_r)\\right).\n\\]\n\n**Step 3.3: Combine Terms and Identify Jeffrey Divergence**  \nThe total loss argument is:  \n\\[\n-\\beta \\cdot D_{\\mathrm{KL}}(\\pi_r \\Vert \\pi_\\theta) + \\beta \\cdot \\left(D_{\\mathrm{KL}}(\\mu \\Vert \\pi_\\theta) - D_{\\mathrm{KL}}(\\mu \\Vert \\pi_r)\\right).\n\\]\nWhen \\(\\mu = \\pi_r\\) (e.g., if dispreferred responses are drawn from \\(\\pi_r\\)), the terms involving \\(\\mu\\) cancel, leaving:  \n\\[\n-\\beta \\cdot D_{\\mathrm{KL}}(\\pi_r \\Vert \\pi_\\theta) + \\beta \\cdot D_{\\mathrm{KL}}(\\pi_\\theta \\Vert \\pi_r).\n\\]\nThis simplifies to:  \n\\[\n\\beta \\cdot \\left(D_{\\mathrm{KL}}(\\pi_\\theta \\Vert \\pi_r) - D_{\\mathrm{KL}}(\\pi_r \\Vert \\pi_\\theta)\\right) = 2\\beta \\cdot D_J[\\pi_\\theta \\Vert \\pi_r],\n\\]\nwhere \\(D_J[p \\Vert q] = \\frac{1}{2}\\left(D_{\\mathrm{KL}}(p \\Vert q) + D_{\\mathrm{KL}}(q \\Vert p)\\right)\\). Thus, the loss implicitly regularizes the Jeffrey divergence between \\(\\pi_\\theta\\) and \\(\\pi_r\\).\n\n---\n\n### **Conclusion**  \n1. **Preference Model Connection**: \\(L_{\\mathrm{D2O}}\\) aligns with the distributional Bradley-Terry model by comparing \\(\\pi_\\theta\\) and \\(\\mu\\) via their implicit rewards.  \n2. **Upper Bound**: When \\(\\alpha = \\beta\\), the distributional model upper-bounds the instance-level expectation via Jensen’s inequality.  \n3. **Jeffrey Divergence**: When \\(\\pi_{r^{-}} = \\pi_{r^{+}}\\), the loss enforces symmetry in KL divergences, equivalent to Jeffrey divergence regularization.  \n\nThis completes the proof of Theorem 1.", "ground-truth": "A"}
{"paper": "arxiv:2308.02585", "question": "How is the gradient of the upper-level objective $G(\\nu, \\theta^*(\\nu))$ derived with respect to the design parameter $\\nu$?\n\nAssume a bilevel optimization problem:\n\\begin{align*}\n   \\text{(upper)} \\hspace{5mm} &\\max_{\\nu} \\quad {G}(\\nu, \\theta^*(\\nu)) \\\\\n   \\text{(lower)} \\hspace{5mm} &\\text{s.t.} \\quad \\theta^*(\\nu) := \\argmax_{\\theta} \\mathbb{E}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid a_h \\sim \\pi_{\\theta}(a_h|s_h), s_0=s \\right]\n\\end{align*}\nwhere $\\nu \\in \\mathbb{R}^n$ are design parameters, $\\theta \\in \\mathbb{R}^d$ are policy parameters, $\\theta^*(\\nu)$ is the optimal lower-level policy parameter for a given $\\nu$.\nThe lower-level objective (value function) is $V_s(\\nu, \\theta) = \\mathbb{E}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid a_h \\sim \\pi_{\\theta}(\\cdot|s_h), s_0=s \\right]$.\nThe upper-level objective $G$ is defined as:\n\\[\n    G(\\nu, \\theta^*(\\nu)) = \\Upsilon(\\pi_{\\theta^*(\\nu)}) + Z(\\nu) \\tag{Eq. 3}\n\\]\nwhere $Z(\\nu)$ is a regularizer, and $\\Upsilon$ is defined as:\n\\[\n    \\Upsilon(\\pi_{\\theta^*(\\nu)}) = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau)] \\tag{Eq. 4}\n\\]\nrepresenting the utility $U_{\\nu}(\\tau)$ evaluated over trajectories $\\tau = \\{s_h, a_h\\}_{h=0}^{H_u-1}$ generated by the optimal policy $\\pi_{\\theta^*(\\nu)}$. The trajectory distribution is $\\rho(\\tau;\\theta) = \\rho(s_0)\\prod_{h=0}^{H_u-1} P(s_{h+1}\\mid s_h,a_h)\\pi_{\\theta}(a_h|s_h)$. Let $f_h(\\theta) = \\log \\pi_{\\theta}(a_h|s_h)$.", "A": "To derive the gradient of the upper-level objective \\( G(\\nu, \\theta^*(\\nu)) \\) with respect to \\(\\nu\\), we use the total derivative:\n\n\\[\n\\nabla_\\nu G = \\underbrace{\\nabla_\\nu \\Upsilon + \\nabla_\\nu Z}_{\\text{Direct terms}} + \\underbrace{\\left( \\nabla_\\theta \\Upsilon \\right) \\big|_{\\theta = \\theta^*(\\nu)} \\cdot \\nabla_\\nu \\theta^*(\\nu)}_{\\text{Implicit term via lower-level optimization}}.\n\\]\n\n### 1. **Direct Terms**:\n   - \\(\\nabla_\\nu Z(\\nu)\\) is straightforward from the regularizer.\n   - \\(\\nabla_\\nu \\Upsilon\\) accounts for explicit \\(\\nu\\)-dependence in \\(U_\\nu(\\tau)\\):\n     \\[\n     \\nabla_\\nu \\Upsilon = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\left[ \\nabla_\\nu U_\\nu(\\tau) \\right].\n     \\]\n\n### 2. **Implicit Term via \\(\\theta^*(\\nu)\\)**:\n   - **Policy gradient for \\(\\nabla_\\theta \\Upsilon\\)**:\n     \\[\n     \\nabla_\\theta \\Upsilon = \\mathbb{E}_{\\rho(\\tau;\\theta)} \\left[ U_\\nu(\\tau) \\sum_{h=0}^{H_u-1} \\nabla_\\theta f_h(\\theta) \\right],\n     \\]\n     where \\(f_h(\\theta) = \\log \\pi_\\theta(a_h|s_h)\\). This follows the log-likelihood ratio trick.\n\n   - **Derivative of \\(\\theta^*(\\nu)\\) via implicit function theorem**:\n     The lower-level optimality condition \\(\\nabla_\\theta V(\\nu, \\theta^*(\\nu)) = 0\\) implies:\n     \\[\n     \\nabla_\\nu \\theta^*(\\nu) = -\\left[ \\nabla_{\\theta\\theta}^2 V(\\nu, \\theta) \\right]^{-1} \\nabla_{\\theta\\nu}^2 V(\\nu, \\theta) \\big|_{\\theta = \\theta^*(\\nu)}.\n     \\]\n     Here, \\(\\nabla_{\\theta\\theta}^2 V\\) and \\(\\nabla_{\\theta\\nu}^2 V\\) are the Hessians of the lower-level value function.\n\n### Final Gradient Expression:\nCombining all terms:\n\\[\n\\boxed{\n\\nabla_\\nu G = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\left[ \\nabla_\\nu U_\\nu(\\tau) \\right] + \\nabla_\\nu Z(\\nu) - \\left( \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\left[ U_\\nu(\\tau) \\sum_{h=0}^{H_u-1} \\nabla_\\theta f_h(\\theta^*) \\right] \\right) \\left[ \\nabla_{\\theta\\theta}^2 V \\right]^{-1} \\nabla_{\\theta\\nu}^2 V\n}\n\\]\n\n### Key Assumptions:\n1. **Invertibility**: \\(\\nabla_{\\theta\\theta}^2 V\\) is invertible at \\(\\theta = \\theta^*(\\nu)\\).\n2. **Differentiability**: \\(U_\\nu(\\tau)\\), \\(Z(\\nu)\\), and \\(\\pi_\\theta(a|s)\\) are differentiable in \\(\\nu\\) and \\(\\theta\\).\n\nThis derivation leverages the implicit function theorem to handle bilevel optimization and the policy gradient theorem to differentiate through the trajectory distribution.", "B": "We start by substituting the definitions of $G$ (Eq. 3) and $\\Upsilon$ (Eq. 4) into the gradient calculation:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\nabla_{\\nu} \\left( \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau)] + Z(\\nu) \\right)\n\\]\n\\[\n    = \\nabla_{\\nu} \\left( \\sum_{\\tau} U_{\\nu}(\\tau) \\cdot \\rho(\\tau;\\theta^*(\\nu)) \\right) + \\nabla_{\\nu} Z(\\nu)\n\\]\nApplying the product rule and the log-derivative trick $\\nabla_{\\nu} \\rho = \\rho \\nabla_{\\nu} \\log \\rho$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\sum_{\\tau} \\left( \\nabla_{\\nu} U_{\\nu}(\\tau) \\cdot \\rho(\\tau;\\theta^*(\\nu)) + U_{\\nu}(\\tau) \\cdot \\nabla_{\\nu} \\rho(\\tau;\\theta^*(\\nu)) \\right) + \\nabla_{\\nu} Z(\\nu)\n\\]\n\\[\n    = \\sum_{\\tau} \\nabla_{\\nu} U_{\\nu}(\\tau) \\rho(\\tau;\\theta^*(\\nu)) + \\sum_{\\tau} U_{\\nu}(\\tau) \\rho(\\tau;\\theta^*(\\nu)) \\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu)) + \\nabla_{\\nu} Z(\\nu)\n\\]\n\\[\n    = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau) \\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu))] + \\nabla_{\\nu} Z(\\nu)\n\\]\nUsing $\\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu)) = \\sum_{h=0}^{H_u-1} \\nabla_{\\nu} \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\Bigg[U_{\\nu}(\\tau)\\cdot \\sum_{h =0}^{H_u-1} \\nabla_{\\nu} \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)\\Bigg] + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\nabla_{\\nu} Z(\\nu)\n\\]\nLet $f_h(\\theta^*(\\nu)) = \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)$. We need $\\nabla_{\\nu} f_h(\\theta^*(\\nu))$. By the chain rule:\n\\[\n   \\nabla_{\\nu} f_h(\\theta^*(\\nu))=  (\\nabla_{\\nu}\\theta^*(\\nu))^T \\nabla_{\\theta} f_h(\\theta^*(\\nu))\n\\]\nFrom the lower-level optimality $\\nabla_{\\theta} V_s{( \\nu, \\theta^*(\\nu))} = 0$. Differentiating w.r.t. $\\nu$ using the chain rule gives the implicit function theorem result:\n\\[\n    \\nabla^2_{\\nu,\\theta} V_s{(\\nu,\\theta^*(\\nu))} +  (\\nabla_{\\nu} {\\theta^*(\\nu)})^T \\nabla^2_{\\theta} V_s{(\\nu,\\theta^*(\\nu))}  = 0\n\\]\nAssuming $\\nabla^2_{\\theta} V_s$ is invertible, we solve for $(\\nabla_{\\nu} \\theta^*(\\nu))^T$:\n\\[\n    (\\nabla_{\\nu} \\theta^*(\\nu))^T = - \\nabla^2_{\\nu,\\theta} V_s(\\nu,\\theta^*(\\nu)) (\\nabla^2_{\\theta} V_s(\\nu,\\theta^*(\\nu)))^{-1}\n\\]\nSubstituting this back into the expression for $\\nabla_{\\nu} f_h(\\theta^*(\\nu))$:\n\\[\n   \\nabla_{\\nu} f_h(\\theta^*(\\nu)) = -   \\nabla^2_{\\nu,\\theta} V_s(\\nu,\\theta^*(\\nu))(\\nabla^2_{\\theta} V_s(\\nu,\\theta^*(\\nu)))^{-1}\\nabla_{\\theta} f_h(\\theta^*(\\nu))\n\\]\nFinally, substitute this back into the expression for $\\nabla_{\\nu}G$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu))\n     =  \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\Bigg[U_{\\nu}(\\tau)\\cdot \\sum_{h =0}^{H_u-1} [-   \\nabla^2_{\\nu,\\theta} V_s(\\nabla^2_{\\theta} V_s)^{-1}\\nabla_{\\theta} f_h(\\theta^*(\\nu))]\\]\n\\]\n\\[\n     + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\nabla_{\\nu} Z(\\nu)\n\\]\nwhere Hessians/Jacobians are evaluated at $(\\nu, \\theta^*(\\nu))$.", "C": "We start by substituting the definitions of $G$ (Eq. 3) and $\\Upsilon$ (Eq. 4) into the gradient calculation:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\nabla_{\\nu} \\left( \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau)] + Z(\\nu) \\right)\n\\]\n\\[\n    = \\nabla_{\\nu} \\left( \\sum_{\\tau} U_{\\nu}(\\tau) \\cdot \\rho(\\tau;\\theta^*(\\nu)) \\right) + \\nabla_{\\nu} Z(\\nu)\n\\]\nApplying the product rule and the log-derivative trick $\\nabla_{\\nu} \\rho = \\rho \\nabla_{\\nu} \\log \\rho$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\sum_{\\tau} \\left( \\nabla_{\\nu} U_{\\nu}(\\tau) \\cdot \\rho(\\tau;\\theta^*(\\nu)) + U_{\\nu}(\\tau) \\cdot \\nabla_{\\nu} \\rho(\\tau;\\theta^*(\\nu)) \\right) + \\nabla_{\\nu} Z(\\nu)\n\\]\n\\[\n    = \\sum_{\\tau} \\nabla_{\\nu} U_{\\nu}(\\tau) \\rho(\\tau;\\theta^*(\\nu)) + \\sum_{\\tau} U_{\\nu}(\\tau) \\rho(\\tau;\\theta^*(\\nu)) \\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu)) + \\nabla_{\\nu} Z(\\nu)\n\\]\n\\[\n    = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [U_{\\nu}(\\tau) \\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu))] + \\nabla_{\\nu} Z(\\nu)\n\\]\nUsing $\\nabla_{\\nu} \\log \\rho(\\tau;\\theta^*(\\nu)) = \\sum_{h=0}^{H_u-1} \\nabla_{\\nu} \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu)) = \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\Bigg[U_{\\nu}(\\tau)\\cdot \\sum_{h =0}^{H_u-1} \\nabla_{\\nu} \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)\\Bigg] + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\nabla_{\\nu} Z(\\nu)\n\\]\nLet $f_h(\\theta^*(\\nu)) = \\log \\pi_{\\theta^*(\\nu)}(a_h|s_h)$. We need $\\nabla_{\\nu} f_h(\\theta^*(\\nu))$. By the chain rule:\n\\[\n   \\nabla_{\\nu} f_h(\\theta^*(\\nu))=  (\\nabla_{\\nu}\\theta^*(\\nu))^T \\nabla_{\\theta} f_h(\\theta^*(\\nu))\n\\]\nFrom the lower-level optimality $\\nabla_{\\theta} V_s{( \\nu, \\theta^*(\\nu))} = 0$. Differentiating w.r.t. $\\nu$ using the chain rule gives the implicit function theorem result:\n\\[\n    \\nabla^2_{\\nu,\\theta} V_s{(\\nu,\\theta^*(\\nu))} +  (\\nabla_{\\nu} {\\theta^*(\\nu)})^T \\nabla^2_{\\theta} V_s{(\\nu,\\theta^*(\\nu))}  = 0\n\\]\nAssuming $\\nabla^2_{\\theta} V_s$ is invertible, we solve for $(\\nabla_{\\nu} \\theta^*(\\nu))^T$:\n\\[\n    (\\nabla_{\\nu} \\theta^*(\\nu))^T = - \\nabla^2_{\\nu,\\theta} V_s(\\nu,\\theta^*(\\nu)) (\\nabla^2_{\\theta} V_s(\\nu,\\theta^*(\\nu)))^{-1}\n\\]\nSubstituting this back into the expression for $\\nabla_{\\nu} f_h(\\theta^*(\\nu))$:\n\\[\n   \\nabla_{\\nu} f_h(\\theta^*(\\nu)) = -   \\nabla^2_{\\nu,\\theta} V_s(\\nu,\\theta^*(\\nu))(\\nabla^2_{\\theta} V_s(\\nu,\\theta^*(\\nu)))^{-1}\\nabla_{\\theta} f_h(\\theta^*(\\nu))\n\\]\nFinally, substitute this back into the expression for $\\nabla_{\\nu}G$:\n\\[\n    \\nabla_{\\nu}G(\\nu, \\theta^*(\\nu))\n     =  \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} \\Bigg[U_{\\nu}(\\tau)\\cdot \\sum_{h =0}^{H_u-1} [   \\nabla^2_{\\nu,\\theta} V_s(\\nabla^2_{\\theta} V_s)^{-1}\\nabla_{\\theta} f_h(\\theta^*(\\nu))]\\]\n\\]\n\\[\n     + \\mathbb{E}_{\\rho(\\tau;\\theta^*(\\nu))} [\\nabla_{\\nu} U_{\\nu}(\\tau)] + \\nabla_{\\nu} Z(\\nu)\n\\]\nwhere Hessians/Jacobians are evaluated at $(\\nu, \\theta^*(\\nu))$.", "D": "To derive the gradient of the upper-level objective \\( G(\\nu, \\theta^*(\\nu)) \\) with respect to the design parameter \\(\\nu\\), we need to account for both the direct dependence of \\( G \\) on \\(\\nu\\) and the indirect dependence through \\(\\theta^*(\\nu)\\). Here's the step-by-step derivation:\n\n### 1. Total Gradient of \\( G(\\nu, \\theta^*(\\nu)) \\)\nThe total gradient is given by the chain rule:\n\\[\n\\nabla_\\nu G(\\nu, \\theta^*(\\nu)) = \\nabla_\\nu G(\\nu, \\theta) \\Big|_{\\theta = \\theta^*(\\nu)} + \\nabla_\\theta G(\\nu, \\theta) \\Big|_{\\theta = \\theta^*(\\nu)} \\cdot \\nabla_\\nu \\theta^*(\\nu).\n\\]\nFrom Eq. 3, \\( G(\\nu, \\theta) = \\Upsilon(\\pi_\\theta) + Z(\\nu) \\), so:\n\\[\n\\nabla_\\nu G(\\nu, \\theta) = \\nabla_\\nu \\Upsilon(\\pi_\\theta) + \\nabla_\\nu Z(\\nu).\n\\]\nThus:\n\\[\n\\nabla_\\nu G(\\nu, \\theta^*(\\nu)) = \\nabla_\\nu \\Upsilon(\\pi_{\\theta^*(\\nu)}) + \\nabla_\\nu Z(\\nu) + \\nabla_\\theta \\Upsilon(\\pi_\\theta) \\Big|_{\\theta = \\theta^*(\\nu)} \\cdot \\nabla_\\nu \\theta^*(\\nu).\n\\]\n\n### 2. Gradient of \\(\\Upsilon(\\pi_{\\theta^*(\\nu)})\\)\nFrom Eq. 4, \\(\\Upsilon(\\pi_\\theta) = \\mathbb{E}_{\\rho(\\tau;\\theta)} [U_\\nu(\\tau)]\\), where \\(\\tau = \\{s_h, a_h\\}_{h=0}^{H_u-1}\\). The gradient \\(\\nabla_\\theta \\Upsilon(\\pi_\\theta)\\) is:\n\\[\n\\nabla_\\theta \\Upsilon(\\pi_\\theta) = \\mathbb{E}_{\\rho(\\tau;\\theta)} \\left[ U_\\nu(\\tau) \\nabla_\\theta \\log \\rho(\\tau;\\theta) \\right],\n\\]\nwhere \\(\\nabla_\\theta \\log \\rho(\\tau;\\theta) = \\sum_{h=0}^{H_u-1} \\nabla_\\theta \\log \\pi_\\theta(a_h|s_h) = \\sum_{h=0}^{H_u-1} \\nabla_\\theta f_h(\\theta)\\).\n\n### 3. Gradient of \\(\\theta^*(\\nu)\\) with Respect to \\(\\nu\\)\nThe lower-level optimality condition is \\(\\nabla_\\theta V_s(\\nu, \\theta^*(\\nu)) = 0\\). Differentiating this with respect to \\(\\nu\\) gives:\n\\[\n\\nabla_\\nu \\nabla_\\theta V_s(\\nu, \\theta^*(\\nu)) + \\nabla_\\theta^2 V_s(\\nu, \\theta^*(\\nu)) \\cdot \\nabla_\\nu \\theta^*(\\nu) = 0.\n\\]\nSolving for \\(\\nabla_\\nu \\theta^*(\\nu)\\):\n\\[\n\\nabla_\\nu \\theta^*(\\nu) = - \\left[ \\nabla_\\theta^2 V_s(\\nu, \\theta^*(\\nu)) \\right]^{-1} \\nabla_\\nu \\nabla_\\theta V_s(\\nu, \\theta^*(\\nu)).\n\\]\n\n### 4. Final Gradient Expression\nCombining the results, the gradient of the upper-level objective is:\n\\[\n\\nabla_\\nu G(\\nu, \\theta^*(\\nu)) = \\nabla_\\nu \\Upsilon(\\pi_{\\theta^*(\\nu)}) + \\nabla_\\nu Z(\\nu) - \\nabla_\\theta \\Upsilon(\\pi_\\theta) \\Big|_{\\theta = \\theta^*(\\nu)} \\cdot \\left[ \\nabla_\\theta^2 V_s(\\nu, \\theta^*(\\nu)) \\right]^{-1} \\nabla_\\nu \\nabla_\\theta V_s(\\nu, \\theta^*(\\nu)).\n\\]\n\n### Key Components:\n1. **Direct Gradient Terms**:\n   - \\(\\nabla_\\nu \\Upsilon(\\pi_{\\theta^*(\\nu)})\\): Gradient of \\(\\Upsilon\\) w.r.t. \\(\\nu\\) through the trajectory distribution.\n   - \\(\\nabla_\\nu Z(\\nu)\\): Gradient of the regularizer \\(Z(\\nu)\\).\n\n2. **Implicit Gradient Term**:\n   - \\(\\nabla_\\theta \\Upsilon(\\pi_\\theta) \\Big|_{\\theta = \\theta^*(\\nu)}\\): Policy gradient of \\(\\Upsilon\\).\n   - \\(\\left[ \\nabla_\\theta^2 V_s(\\nu, \\theta^*(\\nu)) \\right]^{-1}\\): Inverse of the Hessian of the lower-level value function.\n   - \\(\\nabla_\\nu \\nabla_\\theta V_s(\\nu, \\theta^*(\\nu))\\): Cross-derivative of the lower-level value function.\n\nThis derivation leverages the implicit function theorem to handle the dependence of \\(\\theta^*(\\nu)\\) on \\(\\nu\\), which is common in bilevel optimization. The final gradient involves both explicit and implicit terms, reflecting the nested structure of the problem.", "ground-truth": "B"}
{"paper": "arxiv:2308.02585", "question": "How can the f-divergence $D_f$ between trajectory distributions $\\rho(\\tau ; \\theta^*(\\nu))$ and $\\rho(\\tau ; \\theta^K(\\nu))$, induced by the optimal policy $\\pi_{\\theta^*(\\nu)}$ and an approximate policy $\\pi_{\\theta^K(\\nu)}$ respectively, be bounded?\n\nDefine:\n\\begin{itemize}\n    \\item Trajectory $\\tau=\\{s_h,a_h\\}_{h=1}^{H}$.\n    \\item Trajectory distribution $\\rho(\\tau ; \\theta) = \\rho(s_0) \\prod_{h = 1}^{H} \\pi_{\\theta}(a_h|s_h) P(s_{h+1} |s_h, a_h)$.\n    \\item $\\theta^*(\\nu)$ is the optimal policy parameter, $\\theta^K(\\nu)$ is an approximate parameter. Let $\\theta^* = \\theta^*(\\nu)$ and $\\theta^K = \\theta^K(\\nu)$.\n\\end{itemize}\nAssume:\n\\begin{itemize}\n    \\item The policy $\\pi_{\\theta}$ is Lipschitz w.r.t. $\\theta$: $\\|\\pi_{\\theta_1}(\\cdot|s) - \\pi_{\\theta_2}(\\cdot|s)\\|_1 \\leq L_\\pi \\|\\theta_1 - \\theta_2\\|$ for some constant $L_\\pi$.\n    \\item The Hessian of the score function is Lipschitz: $\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta_1}(\\cdot|s) - \\nabla_{\\theta}^2 \\log \\pi_{\\theta_2}(\\cdot|s)\\| \\leq L_{2} \\|\\theta_1 - \\theta_2\\|$ for some constant $L_2$.\n    \\item $D_f$ is an f-divergence satisfying the triangle inequality, and $f$ is its generating function.\n\\end{itemize}\nShow that $D_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq \\frac{HL_{2}}{2} \\|\\theta^* - \\theta^K\\|$.", "A": "To bound the $f$-divergence $D_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K))$ between trajectory distributions induced by the optimal and approximate policies, we proceed as follows:\n\n**Step 1: Decompose the trajectory divergence**  \nThe trajectory distribution $\\rho(\\tau ; \\theta)$ factorizes over time steps:  \n$$\n\\rho(\\tau ; \\theta) = \\rho(s_0) \\prod_{h=1}^H \\pi_\\theta(a_h|s_h) P(s_{h+1}|s_h, a_h).\n$$  \nSince the transition dynamics $P(s_{h+1}|s_h, a_h)$ are shared between $\\theta^*$ and $\\theta^K$, the divergence arises solely from policy differences. Using the triangle inequality property of $D_f$, we decompose:  \n$$\nD_f(\\rho(\\theta^*), \\rho(\\theta^K)) \\leq \\sum_{h=1}^H D_f\\left(\\pi_{\\theta^*}(\\cdot|s_h), \\pi_{\\theta^K}(\\cdot|s_h)\\right).\n$$\n\n**Step 2: Analyze per-step policy divergence**  \nFor each $h$, consider the policy divergence at state $s_h$. Using a second-order Taylor expansion of $\\log \\pi_\\theta(a|s)$ around $\\theta^*$:  \n$$\n\\log \\pi_{\\theta^K}(a|s) \\approx \\log \\pi_{\\theta^*}(a|s) + \\frac{1}{2}(\\theta^K - \\theta^*)^\\top \\nabla_\\theta^2 \\log \\pi_{\\tilde{\\theta}}(a|s) (\\theta^K - \\theta^*),\n$$  \nwhere $\\tilde{\\theta}$ lies between $\\theta^*$ and $\\theta^K$. The linear term vanishes at $\\theta^*$ due to the optimality condition $\\nabla_\\theta \\log \\pi_{\\theta^*}(a|s) = 0$ (implied by $\\theta^*$ maximizing expected return).  \n\n**Step 3: Apply Hessian Lipschitz condition**  \nBy assumption, $\\|\\nabla_\\theta^2 \\log \\pi_{\\theta_1}(\\cdot|s) - \\nabla_\\theta^2 \\log \\pi_{\\theta_2}(\\cdot|s)\\| \\leq L_2 \\|\\theta_1 - \\theta_2\\|$. For $\\tilde{\\theta}$ near $\\theta^*$:  \n$$\n\\|\\nabla_\\theta^2 \\log \\pi_{\\tilde{\\theta}}(\\cdot|s)\\| \\leq \\|\\nabla_\\theta^2 \\log \\pi_{\\theta^*}(\\cdot|s)\\| + L_2 \\|\\theta^* - \\theta^K\\|.\n$$  \nThe $f$-divergence $D_f(\\pi_{\\theta^*}, \\pi_{\\theta^K})$ is approximated by its quadratic behavior for small $\\|\\theta^* - \\theta^K\\|$, dominated by the Hessian term:  \n$$\nD_f\\left(\\pi_{\\theta^*}(\\cdot|s), \\pi_{\\theta^K}(\\cdot|s)\\right) \\leq \\frac{L_2}{2} \\|\\theta^* - \\theta^K\\|.\n$$\n\n**Step 4: Sum over trajectory horizon**  \nSumming the per-step divergence over $H$ steps:  \n$$\nD_f(\\rho(\\theta^*), \\rho(\\theta^K)) \\leq \\sum_{h=1}^H \\frac{L_2}{2} \\|\\theta^* - \\theta^K\\| = \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|.\n$$\n\n**Conclusion:**  \nThe $f$-divergence between trajectory distributions is bounded by:  \n$$\n\\boxed{D_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|}\n$$  \nThis result leverages the Hessian's Lipschitz continuity and the triangle inequality of $D_f$ to accumulate policy approximation errors over the horizon.", "B": "To derive the bound on the \\( f \\)-divergence \\( D_f \\) between the trajectory distributions \\( \\rho(\\tau ; \\theta^*) \\) and \\( \\rho(\\tau ; \\theta^K) \\), we proceed with the following steps:\n\n### 1. Express the \\( f \\)-Divergence Between Trajectory Distributions\nThe \\( f \\)-divergence between two trajectory distributions is given by:\n\\[\nD_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) = \\mathbb{E}_{\\tau \\sim \\rho(\\tau ; \\theta^*)} \\left[ f\\left( \\frac{\\rho(\\tau ; \\theta^K)}{\\rho(\\tau ; \\theta^*)} \\right) \\right].\n\\]\nGiven the trajectory distribution:\n\\[\n\\rho(\\tau ; \\theta) = \\rho(s_0) \\prod_{h=1}^H \\pi_\\theta(a_h | s_h) P(s_{h+1} | s_h, a_h),\n\\]\nthe ratio of the two distributions is:\n\\[\n\\frac{\\rho(\\tau ; \\theta^K)}{\\rho(\\tau ; \\theta^*)} = \\prod_{h=1}^H \\frac{\\pi_{\\theta^K}(a_h | s_h)}{\\pi_{\\theta^*}(a_h | s_h)}.\n\\]\n\n### 2. Bound the Ratio Using Lipschitz Continuity of the Policy\nUsing the Lipschitz assumption on the policy:\n\\[\n\\|\\pi_{\\theta_1}(\\cdot|s) - \\pi_{\\theta_2}(\\cdot|s)\\|_1 \\leq L_\\pi \\|\\theta_1 - \\theta_2\\|,\n\\]\nwe can bound the log-ratio of the policies. For each \\( h \\), the difference in log-probabilities is:\n\\[\n\\left| \\log \\pi_{\\theta^K}(a_h | s_h) - \\log \\pi_{\\theta^*}(a_h | s_h) \\right| \\leq \\left| \\nabla_\\theta \\log \\pi_{\\theta}(a_h | s_h) \\right| \\cdot \\|\\theta^* - \\theta^K\\| + \\frac{L_2}{2} \\|\\theta^* - \\theta^K\\|^2,\n\\]\nwhere the second term comes from the Lipschitz continuity of the Hessian of the log-policy (i.e., the score function).\n\n### 3. Sum Over the Trajectory\nSumming over the \\( H \\) steps in the trajectory, the total log-ratio is bounded by:\n\\[\n\\left| \\log \\frac{\\rho(\\tau ; \\theta^K)}{\\rho(\\tau ; \\theta^*)} \\right| \\leq H \\left( \\max_{h} \\left| \\nabla_\\theta \\log \\pi_{\\theta}(a_h | s_h) \\right| \\cdot \\|\\theta^* - \\theta^K\\| + \\frac{L_2}{2} \\|\\theta^* - \\theta^K\\|^2 \\right).\n\\]\nAssuming the gradient term is bounded (or its expectation vanishes), the dominant term is the second-order term:\n\\[\n\\left| \\log \\frac{\\rho(\\tau ; \\theta^K)}{\\rho(\\tau ; \\theta^*)} \\right| \\approx \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|^2.\n\\]\n\n### 4. Relate to \\( f \\)-Divergence\nFor \\( f \\)-divergences satisfying the triangle inequality (e.g., KL divergence, total variation), the divergence can be bounded by the log-ratio. Specifically, using the property of \\( f \\)-divergences and the Lipschitzness of \\( f \\), we have:\n\\[\nD_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq C \\cdot \\mathbb{E}_{\\tau \\sim \\rho(\\tau ; \\theta^*)} \\left[ \\left| \\log \\frac{\\rho(\\tau ; \\theta^K)}{\\rho(\\tau ; \\theta^*)} \\right| \\right],\n\\]\nwhere \\( C \\) is a constant depending on \\( f \\). Substituting the bound on the log-ratio:\n\\[\nD_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq C \\cdot \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|^2.\n\\]\nFor simplicity, if we assume \\( C = 1 \\) (e.g., for KL divergence), we obtain the final bound:\n\\[\nD_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|^2.\n\\]\n\nHowever, the problem statement asks for a linear bound in \\( \\|\\theta^* - \\theta^K\\| \\). To achieve this, we note that under additional assumptions (e.g., small \\( \\|\\theta^* - \\theta^K\\| \\)), the squared term can be linearized, yielding:\n\\[\nD_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|.\n\\]\n\n### Final Answer:\n\\[\n\\boxed{D_f(\\rho(\\tau ; \\theta^*), \\rho(\\tau ; \\theta^K)) \\leq \\frac{H L_2}{2} \\|\\theta^* - \\theta^K\\|}\n\\]", "C": "The probability distribution of the trajectory $\\tau=\\{s_h,a_h\\}_{h=1}^H$ is given by\n\\begin{align}\n\t\\rho(\\tau ; \\theta^*(\\nu)) = \\rho(s_0) \\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h).\n\\end{align}\n%\nSimilarly, we can derive an equivalent expression for the probability of trajectory induced by the policy $\\pi_{\\theta^K(\\nu)}$ by replacing $\\theta^*(\\nu)$ with $\\theta^K(\\nu)$. Here, ${P}(s_{h+1} |s_h, a_h)$ is the transition probability which remains the same for both and $\\rho(s_0)$ is the initial state distribution.\n%\nNext, the f-divergence between the two distributions  $D_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu)))$ can be written as \n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) \\leq \\underbrace{D_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\beta))}_{I} + \\underbrace{D_f(\\rho(\\tau ; \\beta), \\rho(\\tau ; \\theta^K(\\nu)))}_{II},\n\\end{align}\n%\nwhich holds by triangle inequality of the class of f-divergences we considered as discussed above. $\\rho(\\tau ; \\beta)$ represents the trajectory probability induced by another hybrid policy $\\pi_{\\beta}(\\cdot|s)$ which executes the action based on the policy $\\pi_{\\theta^K(\\nu)}(\\cdot|s)$ for the first time-step and then follows the policy $\\pi_{\\theta^*(\\nu)}(\\cdot|s)$ for subsequent timesteps. Now, we focus on term I in (2), we get\n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; &\\theta^*(\\nu)), \\rho(\\tau ; \\beta)) \n\t\\nonumber\n\t\\\\\n\t&= \\sum_{\\tau}\\rho(\\tau ; \\beta)  f\\Bigg(\\frac{\\rho(\\tau ; \\theta^*(\\nu))}{\\rho(\\tau ; \\beta)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\beta))  f\\Bigg(\\frac{\\rho(s_0) \\pi_{\\theta^*(\\nu)}(a_0|s_0)P(s_1|s_0, a_0)\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\rho(s_0)\\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0) \\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\beta))f\\Bigg(\\frac{\\pi_{\\theta^*(\\nu)}(a_0|s_0)}{\\pi_{\\theta^K(\\nu)}(a_0|s_0)}\\Bigg),\n\\end{align}\n%\nwhere first we expand upon the definition of the trajectory distribution induced by both policies and get the final expression of the equation (3). By expanding the term $\\rho(\\tau ; \\beta)$ in (3), we obtain\n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\beta)) &= \\sum_{s_0}\\rho(s_0) \\sum_{a_0}\\pi_{\\theta^K(\\nu)}(a_0|s_0)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a_0|s_0)}{\\pi_{\\theta^K(\\nu)}(a_0|s_0)}\\Big)\\sum_{s_1} P(s_1|s_0, a_0) \\cdots \\nonumber\\\\\n\t& = \\sum_{s} \\rho(s) \\sum_{a}\\pi_{\\theta^K(\\nu)}(a|s)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a|s)}{\\pi_{\\theta^K(\\nu)}(a|s)}\\Big) \\nonumber\\\\\n\t& = \\mathbb{E}_{\\rho(s)}\\sum_{a}\\pi_{\\theta^K(\\nu)}(a|s)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a|s)}{\\pi_{\\theta^K(\\nu)}(a|s)}\\Big)\\nonumber\\\\\n\t& = \\mathbb{E}_{\\rho(s)} [D_f(\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s))],\n\\end{align}\n%\nwhere, in the first equation we expand upon the sum over all trajectories with the occupancy distribution over states and actions, and replacing with f-divergence, we get the final expression. \n\n\nNext, we expand similarly for the term II in (2) and expand as\n%\n\\begin{align} \n\tD_f&(\\rho(\\tau  ; \\beta), \\rho(\\tau ; \\theta^K(\\nu))) \n\t\\nonumber\n\t\\\\\n\t &= \\sum_{\\tau}\\rho(\\tau ; \\theta^K(\\nu)))  f\\Bigg(\\frac{\\rho(\\tau ; \\beta)}{\\rho(\\tau ; \\theta^K(\\nu))}\\Bigg)\\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\theta^K(\\nu))  f\\Bigg(\\frac{\\rho(s_0) \\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0)\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\rho_0(s_0)\\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0) \\prod_{h = 1}^{H} \\pi_{\\theta^K(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_0|s_0) \\sum_{s_1} P(s_1|s_0, a_0) \\cdots f\\Bigg(\\frac{\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\prod_{h = 1}^{H} \\pi_{\\theta^K(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg)\\\\ \\nonumber\n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) \\sum_{\\tau_1} \\rho(\\tau_1;\\theta^K(\\nu), \\frac{\\rho(\\tau_1;\\theta^*(\\nu)}{\\rho(\\tau_1;\\theta^K(\\nu))} \\nonumber \\\\ \n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) D_f(\\rho(\\tau_1;\\theta^*(\\nu), \\rho(\\tau_1;\\theta^K(\\nu)),\n\\end{align}\n%\nwhere we expand the trajectory distribution induced by the policies and subsequently express as the ratio of the probability of trajectories wrt $\\tau_1$, we get the final expression. Now, we expand upon the f-divergence of the trajectory $\\tau_1$ distribution as \n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\beta), \\rho(\\tau ; \\theta^K(\\nu))) =&  \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) D_f(\\rho(\\tau_1;\\theta^*(\\nu), \\rho(\\tau_1;\\theta^K(\\nu)) \\\\ \\nonumber\n\t%\n\t \\leq &  \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) \\Big(D_f(\\rho(\\tau_1;\\theta^*(\\nu)), \\rho(\\tau_1;\\beta)) \\\\ \\nonumber\n\t&+ D_f(\\rho(\\tau_1;\\beta), \\rho(\\tau_1;\\theta^K(\\nu)))\\Big),\n\\end{align}\n%\nwhere using the triangle inequality and get back the similar form with which we had started in equation (2) similar to term I and term II. Here, similarly continuing this expansion, we finally get \n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) & \\leq \\sum_{h = 0}^{H-1} \\mathbb{E}_{s \\sim \\rho_{\\theta^K(\\nu)}^P(s)} D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\\n\t& \\leq H \\mathbb{E}_{s \\sim \\rho_{\\theta^K(\\nu)}(s)} D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\  \n\t& \\leq H \\sum_{s} \\rho_{\\theta^K(\\nu)}(s) D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\\n\t& \\leq  H D_f (\\pi_{\\theta^*(\\nu)}(a'|s'), \\pi_{\\theta^K(\\nu)}(a'|s')),\n\\end{align}\n%\n where, we upper bound the first equation by the total number of timesteps or horizon length $H$ of the trajectory and subsequently upper-bound the divergence by the state $(s,a)$ pair for which the $D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s))$ is maximum and is given as $(s',a')$. Next, in (7), by considering the total variation as the f-divergence (it falls into the class of f-divergence under consideration) and expanding using definition with countable measures to obtain \n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) & \\leq  H D_{TV} (\\pi_{\\theta^*(\\nu)}(a'|s'), \\pi_{\\theta^K(\\nu)}(a'|s')) \\nonumber  \\\\\n\t& \\leq \\frac{H}{2} \\|\\pi_{\\theta^*(\\nu)}(a'|s')- \\pi_{\\theta^K(\\nu)}(a'|s')\\|_1 \\nonumber\\\\\n\t& \\leq \\frac{HL_{\\pi}}{2} \\|\\theta^*(\\nu) - \\theta^K(\\nu)\\|, \n\\end{align}", "D": "The probability distribution of the trajectory $\\tau=\\{s_h,a_h\\}_{h=1}^H$ is given by\n\\begin{align}\n\t\\rho(\\tau ; \\theta^*(\\nu)) = \\rho(s_0) \\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h).\n\\end{align}\n%\nSimilarly, we can derive an equivalent expression for the probability of trajectory induced by the policy $\\pi_{\\theta^K(\\nu)}$ by replacing $\\theta^*(\\nu)$ with $\\theta^K(\\nu)$. Here, ${P}(s_{h+1} |s_h, a_h)$ is the transition probability which remains the same for both and $\\rho(s_0)$ is the initial state distribution.\n%\nNext, the f-divergence between the two distributions  $D_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu)))$ can be written as \n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) \\leq \\underbrace{D_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\beta))}_{I} + \\underbrace{D_f(\\rho(\\tau ; \\beta), \\rho(\\tau ; \\theta^K(\\nu)))}_{II},\n\\end{align}\n%\nwhich holds by triangle inequality of the class of f-divergences we considered as discussed above. $\\rho(\\tau ; \\beta)$ represents the trajectory probability induced by another hybrid policy $\\pi_{\\beta}(\\cdot|s)$ which executes the action based on the policy $\\pi_{\\theta^K(\\nu)}(\\cdot|s)$ for the first time-step and then follows the policy $\\pi_{\\theta^*(\\nu)}(\\cdot|s)$ for subsequent timesteps. Now, we focus on term I in (2), we get\n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; &\\theta^*(\\nu)), \\rho(\\tau ; \\beta)) \n\t\\nonumber\n\t\\\\\n\t&= \\sum_{\\tau}\\rho(\\tau ; \\beta)  f\\Bigg(\\frac{\\rho(\\tau ; \\theta^*(\\nu))}{\\rho(\\tau ; \\beta)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\beta))  f\\Bigg(\\frac{\\rho(s_0) \\pi_{\\theta^*(\\nu)}(a_0|s_0)P(s_1|s_0, a_0)\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\rho(s_0)\\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0) \\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\beta))f\\Bigg(\\frac{\\pi_{\\theta^*(\\nu)}(a_0|s_0)}{\\pi_{\\theta^K(\\nu)}(a_0|s_0)}\\Bigg),\n\\end{align}\n%\nwhere first we expand upon the definition of the trajectory distribution induced by both policies and get the final expression of the equation (3). By expanding the term $\\rho(\\tau ; \\beta)$ in (3), we obtain\n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\beta)) &= \\sum_{s_0}\\rho(s_0) \\sum_{a_0}\\pi_{\\theta^K(\\nu)}(a_0|s_0)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a_0|s_0)}{\\pi_{\\theta^K(\\nu)}(a_0|s_0)}\\Big)\\sum_{s_1} P(s_1|s_0, a_0) \\cdots \\nonumber\\\\\n\t& = \\sum_{s} \\rho(s) \\sum_{a}\\pi_{\\theta^K(\\nu)}(a|s)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a|s)}{\\pi_{\\theta^K(\\nu)}(a|s)}\\Big) \\nonumber\\\\\n\t& = \\mathbb{E}_{\\rho(s)}\\sum_{a}\\pi_{\\theta^K(\\nu)}(a|s)f\\Big(\\frac{\\pi_{\\theta^*(\\nu)}(a|s)}{\\pi_{\\theta^K(\\nu)}(a|s)}\\Big)\\nonumber\\\\\n\t& = \\mathbb{E}_{\\rho(s)} [D_f(\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s))],\n\\end{align}\n%\nwhere, in the first equation we expand upon the sum over all trajectories with the occupancy distribution over states and actions, and replacing with f-divergence, we get the final expression. \n\n\nNext, we expand similarly for the term II in (2) and expand as\n%\n\\begin{align} \n\tD_f&(\\rho(\\tau  ; \\beta), \\rho(\\tau ; \\theta^K(\\nu))) \n\t\\nonumber\n\t\\\\\n\t &= \\sum_{\\tau}\\rho(\\tau ; \\theta^K(\\nu)))  f\\Bigg(\\frac{\\rho(\\tau ; \\beta)}{\\rho(\\tau ; \\theta^K(\\nu))}\\Bigg)\\\\ \\nonumber\n\t& = \\sum_{\\tau}\\rho(\\tau ; \\theta^K(\\nu))  f\\Bigg(\\frac{\\rho(s_0) \\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0)\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\rho_0(s_0)\\pi_{\\theta^K(\\nu)}(a_0|s_0)P(s_1|s_0, a_0) \\prod_{h = 1}^{H} \\pi_{\\theta^K(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg) \\\\ \\nonumber\n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_0|s_0) \\sum_{s_1} P(s_1|s_0, a_0) \\cdots f\\Bigg(\\frac{\\prod_{h = 1}^{H} \\pi_{\\theta^*(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}{\\prod_{h = 1}^{H} \\pi_{\\theta^K(\\nu)}(a_h|s_h) P(s_{h+1} |s_h, a_h)}\\Bigg)\\\\ \\nonumber\n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) \\sum_{\\tau_1} \\rho(\\tau_1;\\theta^K(\\nu), \\frac{\\rho(\\tau_1;\\theta^*(\\nu)}{\\rho(\\tau_1;\\theta^K(\\nu))} \\nonumber \\\\ \n\t& = \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) D_f(\\rho(\\tau_1;\\theta^*(\\nu), \\rho(\\tau_1;\\theta^K(\\nu)),\n\\end{align}\n%\nwhere we expand the trajectory distribution induced by the policies and subsequently express as the ratio of the probability of trajectories wrt $\\tau_1$, we get the final expression. Now, we expand upon the f-divergence of the trajectory $\\tau_1$ distribution as \n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\beta), \\rho(\\tau ; \\theta^K(\\nu))) =&  \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) D_f(\\rho(\\tau_1;\\theta^*(\\nu), \\rho(\\tau_1;\\theta^K(\\nu)) \\\\ \\nonumber\n\t%\n\t \\leq &  \\sum_{s_0}\\rho(s_0) \\sum_{a_1}\\pi_{\\theta^K(\\nu)}(a_1|s_0) \\Big(D_f(\\rho(\\tau_1;\\theta^*(\\nu)), \\rho(\\tau_1;\\beta)) \\\\ \\nonumber\n\t&+ D_f(\\rho(\\tau_1;\\beta), \\rho(\\tau_1;\\theta^K(\\nu)))\\Big),\n\\end{align}\n%\nwhere using the triangle inequality and get back the similar form with which we had started in equation (2) similar to term I and term II. Here, similarly continuing this expansion, we finally get \n%\n\\begin{align}\n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) & \\leq \\sum_{h = 0}^{H-1} \\mathbb{E}_{s \\sim \\rho_{\\theta^K(\\nu)}^P(s)} D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\\n\t& \\leq H \\mathbb{E}_{s \\sim \\rho_{\\theta^K(\\nu)}(s)} D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\  \n\t& \\leq H \\sum_{s} \\rho_{\\theta^K(\\nu)}(s) D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s)) \\nonumber\\\\\n\t& \\leq  H D_f (\\pi_{\\theta^*(\\nu)}(a'|s'), \\pi_{\\theta^K(\\nu)}(a'|s')),\n\\end{align}\n%\n where, we upper bound the first equation by the total number of timesteps or horizon length $H$ of the trajectory and subsequently upper-bound the divergence by the state $(s,a)$ pair for which the $D_f (\\pi_{\\theta^*(\\nu)}(a|s), \\pi_{\\theta^K(\\nu)}(a|s))$ is maximum and is given as $(s',a')$. Next, in (7), by considering the total variation as the f-divergence (it falls into the class of f-divergence under consideration) and expanding using definition with countable measures to obtain \n%\n\\begin{align} \n\tD_f(\\rho(\\tau ; \\theta^*(\\nu)), \\rho(\\tau ; \\theta^K(\\nu))) & \\leq  H D_{TV} (\\pi_{\\theta^*(\\nu)}(a'|s'), \\pi_{\\theta^K(\\nu)}(a'|s')) \\nonumber  \\\\\n\t& \\leq \\frac{H}{2} \\|\\pi_{\\theta^*(\\nu)}(a'|s')- \\pi_{\\theta^K(\\nu)}(a'|s')\\|_1 \\nonumber\\\\\n\t& \\leq \\frac{HL_{2}}{2} \\|\\theta^*(\\nu) - \\theta^K(\\nu)\\|, \n\\end{align}\n", "ground-truth": "D"}
{"paper": "arxiv:2308.02585", "question": "How can the norm of the second-order mixed Hessian term of the lower-level value function, $\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|$, be bounded?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. Here $H_\\ell$ is the lower-level horizon length and $\\gamma \\in (0,1)$ is the discount factor.\n    \\item Approximate policy parameter at iteration $t$: $\\theta^K(\\nu_t)$.\n    \\item The mixed Hessian term $\\nabla^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))$ (representing second derivatives w.r.t. $\\nu$ and $\\theta$) is defined via expectation over trajectories $\\tau$ generated by the policy $\\pi_{\\theta^K(\\nu_t)}$ with distribution $\\rho(\\tau;\\theta^K(\\nu_t))$:\n    \\[\n        {\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t)) = \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg]\n    \\]\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Reward Gradient Bound:} The reward function $r_\\nu(s,a)$ has a bounded gradient w.r.t. $\\nu$: $\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r$ for all $s, a$.\n    \\item \\textbf{Policy Score Bound:} The score function (log-policy gradient) is bounded: $\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B$ for all $s, a$.\n\\end{itemize}\n\n**Goal:** Using these definitions and assumptions, derive the upper bound:\n\\[\n    \\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq H_\\ell^2 L_r B\n\\]", "A": "To derive the upper bound for the norm of the second-order mixed Hessian term \\(\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|\\), we will proceed step-by-step using the given definitions and assumptions.\n\n### Step 1: Express the Mixed Hessian Term\nThe mixed Hessian term is given by:\n\\[\n{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t)) = \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg]\n\\]\n\n### Step 2: Take the Norm Inside the Expectation\nUsing the linearity of expectation and the property that the norm of an expectation is less than or equal to the expectation of the norm (Jensen's inequality), we have:\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\Bigg\\|\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\Bigg]\n\\]\n\n### Step 3: Apply the Triangle Inequality\nThe triangle inequality allows us to bound the norm of the sum by the sum of the norms:\n\\[\n\\Bigg\\|\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\leq \\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\Bigg\\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\|\n\\]\n\n### Step 4: Bound the Inner Terms\nUsing the submultiplicative property of norms, we have:\n\\[\n\\Bigg\\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\leq \\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\| \\cdot \\Bigg\\|\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\Bigg\\|\n\\]\n\nFrom the **Reward Gradient Bound** assumption, \\(\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r\\). From the **Policy Score Bound** assumption, \\(\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B\\). Therefore:\n\\[\n\\Bigg\\|\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\Bigg\\| \\leq \\sum_{j=0}^{h} \\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\| \\leq (h+1)B\n\\]\n\nThus:\n\\[\n\\Bigg\\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\leq L_r \\cdot (h+1)B\n\\]\n\n### Step 5: Sum Over the Horizon\nSubstituting back into the sum over \\(h\\):\n\\[\n\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} L_r (h+1)B \\leq L_r B \\sum_{h =0}^{H_{\\ell}-1} (h+1)\n\\]\nsince \\(\\gamma \\in (0,1)\\) and \\(\\gamma^h \\leq 1\\).\n\nThe sum \\(\\sum_{h=0}^{H_{\\ell}-1} (h+1)\\) is a simple arithmetic series:\n\\[\n\\sum_{h=0}^{H_{\\ell}-1} (h+1) = \\frac{H_{\\ell}(H_{\\ell}+1)}{2} \\leq H_{\\ell}^2\n\\]\nfor \\(H_{\\ell} \\geq 1\\).\n\n### Step 6: Final Bound\nCombining all the steps, we obtain:\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq L_r B \\cdot H_{\\ell}^2\n\\]\n\n### Conclusion\nThus, under the given assumptions, the norm of the second-order mixed Hessian term is bounded by:\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq H_\\ell^2 L_r B\n\\]\n\nThis completes the derivation. The key steps involved applying the triangle inequality, submultiplicative property of norms, and the given bounds on the reward gradient and policy score function. The horizon length \\(H_\\ell\\) appears quadratically due to the nested summation over the trajectory length.", "B": "We start with the definition:\n\\begin{align}\n    \\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t)) &=  \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right)^T \\Bigg] \\nonumber \\\\\n    & \\leq \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\nonumber \\\\\n    & = \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \n\\end{align}\nwhere $\\tau' = [(s_0', a_0') \\cdot (s_h', a_h') \\cdots]$ represents the trajectory for which the term inside the expectation is maximum and thereby upper-bounding with that leads to the expression above. Next we upper-bound the norm of the $\\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\|$ as\n\\begin{align}\n    \\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\| &\\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T\\Bigg\\| \\nonumber \\\\\n    & \\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\Bigg\\|\\cdot\\Bigg\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\Bigg\\| \\nonumber\\\\\n    & \\leq H_\\ell^2 L_r B \n\\end{align}\nwhere we apply the Cauchy-Schwarz inequality to get the inequality in the second line. Subsequently, we apply triangle inequality with the reward gradient bound assumption ($L_r$) and the bounded score function assumption ($B$) to get the final bound for the mixed hessian term.", "C": "To bound the norm of the second-order mixed Hessian term \\(\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|\\), we proceed as follows:\n\n1. **Apply Norm Properties:**  \n   Using the triangle inequality and Jensen’s inequality for the expectation over trajectories:  \n   \\[\n   \\|{\\nabla}^2_{\\nu, \\theta}V_s\\| \\leq \\mathbb{E}_{\\rho}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h \\|\\nabla_{\\nu} r_{\\nu_t}\\| \\left\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right\\|\\right]\n   \\]\n\n2. **Substitute Bounds from Assumptions:**  \n   - By the **Reward Gradient Bound**: \\(\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r\\) for all \\(s, a\\).  \n   - By the **Policy Score Bound**: \\(\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B\\) for all \\(s, a\\).  \n   Thus, the inner sum is bounded as:  \n   \\[\n   \\left\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}\\right\\| \\leq \\sum_{j=0}^{h} B = (h+1)B\n   \\]\n\n3. **Bound the Sum Over Horizon Steps:**  \n   Substitute these into the inequality:  \n   \\[\n   \\|{\\nabla}^2_{\\nu, \\theta}V_s\\| \\leq \\mathbb{E}_{\\rho}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h L_r (h+1)B \\right] = L_r B \\sum_{h=0}^{H_{\\ell}-1} \\gamma^h (h+1)\n   \\]  \n   Since \\(\\gamma \\in (0,1)\\), the sum \\(\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h (h+1)\\) is maximized when \\(\\gamma = 1\\), yielding:  \n   \\[\n   \\sum_{h=0}^{H_{\\ell}-1} (h+1) = \\frac{H_{\\ell}(H_{\\ell}+1)}{2} \\leq H_{\\ell}^2 \\quad \\text{(for \\(H_{\\ell} \\geq 1\\))}\n   \\]\n\n4. **Final Upper Bound:**  \n   Combining these results:  \n   \\[\n   \\|{\\nabla}^2_{\\nu, \\theta}V_s\\| \\leq H_{\\ell}^2 L_r B\n   \\]\n\n**Conclusion:** The norm of the mixed Hessian is bounded by \\(\\boxed{H_{\\ell}^2 L_r B}\\).", "D": "We start with the definition:\n\\begin{align}\n    \\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t)) &=  \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right)^T \\Bigg] \\nonumber \\\\\n    & \\leq \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\nonumber \\\\\n    & = \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \n\\end{align}\nwhere $\\tau' = [(s_0', a_0') \\cdot (s_h', a_h') \\cdots]$ represents the trajectory for which the term inside the expectation is maximum and thereby upper-bounding with that leads to the expression above. Next we upper-bound the norm of the $\\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\|$ as\n\\begin{align}\n    \\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\| &\\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T\\Bigg\\| \\nonumber \\\\\n    & \\leq \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\Bigg\\| \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\Bigg\\|\\cdot\\Bigg\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\Bigg\\| \\nonumber\\\\\n    & \\leq H_\\ell^2 L_r B \n\\end{align}\nwhere we apply the triangle inequality to get the inequality in the second line. Subsequently, we apply the reward gradient bound assumption ($L_r$) and the bounded score function assumption ($B$) to get the final bound for the mixed hessian term.", "ground-truth": "B"}
{"paper": "arxiv:2308.02585", "question": "How is the Lipschitz continuity of the Hessian of the lower-level value function, $\\nabla^2_{\\theta} V_s(\\nu, \\theta)$, with respect to the policy parameter $\\theta$ established?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. ($H_\\ell$: horizon, $\\gamma$: discount factor).\n    \\item Hessian of the value function:\n    \\[\n        {\\nabla}_{\\theta}^2V_s(\\nu, \\theta) =  \\mathbb{E}_{\\rho(\\tau;\\theta)} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} r_{\\nu}(s_h,a_h)\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta}(a_j|s_j)\\right) \\Bigg]\n    \\]\n    \\item Let $\\theta^* = \\theta^*(\\nu)$ be the optimal parameter and $\\theta^K = \\theta^K(\\nu)$ be an approximate parameter.\n    \\item Define functions over trajectory $\\tau$ (Note: Using $\\gamma^{h-1}$ and sum up to $H_\\ell$ as in the proof text):\n    \\[\n        f_1(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1} r_{\\nu_t}(s_h,a_h)\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j|s_j)\\right)\n    \\]\n    \\[\n        f_2(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1} r_{\\nu_t}(s_h,a_h)\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j|s_j)\\right)\n    \\]\n    \\item Let $\\rho(\\tau; \\theta)$ be the trajectory distribution induced by policy $\\pi_\\theta$.\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Bounded Reward:} $\\|r_{\\nu_t}(s,a)\\| \\leq R$.\n    \\item \\textbf{Lipschitz Hessian of Score Function:} $\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta_1}(a|s) - \\nabla_{\\theta}^2 \\log \\pi_{\\theta_2}(a|s)\\| \\leq L_{2} \\|\\theta_1 - \\theta_2\\|$.\n    \\item \\textbf{Bounded Norm of $f_1$:} The norm of $f_1$ is bounded: $\\|f_1(\\cdot)\\| \\leq \\chi_1$, where the proof text specifies $\\chi_1 = H_\\ell^2 R L_1$ (assuming $L_1$ bounds $\\|\\nabla^2_\\theta \\log \\pi_\\theta\\|$).\n    \\item \\textbf{Total Variation Bound:} The Total Variation distance between trajectory distributions is bounded: $d_{TV}(\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\leq \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\|$.\n\\end{itemize}\n\n**Goal:** Show that $\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \\leq L' \\|\\theta^* - \\theta^K\\|$ with $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2$, following the derivation steps.\n", "A": "We start by considering the term \n\t\\begin{align}\n\t\t{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K) = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau),\n\t\\end{align}\n\twhere we define \n\t\\begin{align}\n\t\tf_1(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j|s_j)\\right)\n\t\t\\\\\n\t\tf_2(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j|s_j)\\right).\n\t\\end{align}\n\tSubsequently, we write the norm of the equation above into 2 parts as\n\t\\begin{align}\n\t\t&\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \n\t\t\\nonumber\\\\\n\t\t & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)  + \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)}_{T_1}\\| + \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)}_{T_2}\\|,\n\t\\end{align}\nFirst we use triangle inequality and then add and subtract $\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)$ to get the expression where, $T_1 = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)$ and $T_2 = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)$. {This is an interesting point of departure from existing Bilevel analysis where the stochasticity is mainly iid noise. Hence to deal with this, we separate the terms 1. divergence b/w two probability distributions and 2. expected diff b/w two functions under the same distribution.} We next, upper-bound the individual terms to get the final Lispchitz constant.\n\t\n\tFirst, we focus on the first term of inequality i.e $T_1$ given as\n\t\\begin{align} \n\t\t\\|T_1\\| & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\sup_{f_1} |\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)| \\nonumber\\\\\n\t\t& \\leq \\chi_1 d_{TV} (\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\nonumber\\\\\n\t\t& \\leq \\chi_1 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\n\twhere we first upper-bound the expression to a standard Integral Probability Metric form by taking the supremum and then dividing and multiplying with the norm of the function given by $\\chi_1 =\\|f_1(\\cdot)\\| = H_\\ell^2 R L_1$. With that, we get the expression as the of Total variation distance. Then, we upper-bounded the total variation using the assumed bound.\n        \n\tNow, we proceed to the second term of the equation $T_2$ and derive an upper bound as \n\t\\begin{align} \n\t\tT_2 & = \\sum_{\\tau} \\rho(\\tau;\\theta^K(\\nu)) (f_1(\\tau) - f_2(\\tau)) \\nonumber \\\\\n\t\t& \\leq f_1(\\tau') - f_2(\\tau') \\nonumber\\\\\n\t\t& = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\n\t\\end{align}\n\twhere we consider the trajectory $\\tau'$ in the sum with the maximum value and upper bound by that and expand the definition of $f_1, f_2$. \n\t\\begin{align} \n\t\t\\|T_2\\| & \\leq \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\\| \\nonumber \\\\\n\t\t& \\leq \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\\| \\nonumber\\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|r_{\\nu_t}(s_h', a_h')\\| \\left(\\sum_{j=0}^{h}(\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j')\\|)\\right) \\nonumber\\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|r_{\\nu_t}(s_h', a_h')\\| H_{\\ell} L_{2} \\|\\theta^* - \\theta^K\\| \\nonumber\\\\\n\t\t& = L_{2} R H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \n\t\\end{align}\n\twhere we use Cauchy-Schwartz and triangle inequality repetitively to get to the third inequality. Next, we use the assumption on the Hessian Lipschitzness of the score function ($L_2$) and the bounded reward norm $\\|r_{\\nu_t}(s,a)\\| = R$ to get the next inequality. Finally, we use the upper bound on the geometric series to obtain the final expression. \n\tAdding the bounds for $\\|T_1\\|$ and $\\|T_2\\|$, we get\n\t\\begin{align}\n\t\t\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| & \\leq \\|T_1\\| + \\|T_2\\| \\\\ \n        & \\leq \\chi_1 \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\| + L_{2} R H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \\\\ \n        &= \\left( L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2 \\right) \\|\\theta^* - \\theta^K\\| = L' \\|\\theta^* - \\theta^K\\|\n\t\\end{align}\n\twhere $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2$ (using $\\chi_1 = H_\\ell^2 R L_1$).\n", "B": "To establish the Lipschitz continuity of the Hessian of the lower-level value function, $\\nabla^2_{\\theta} V_s(\\nu, \\theta)$, with respect to the policy parameter $\\theta$, we proceed with the following steps:\n\n### Step 1: Express the Difference in Hessians\nThe difference between the Hessians evaluated at $\\theta^*$ and $\\theta^K$ is:\n\\[\n\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| = \\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_1(\\tau)] - \\mathbb{E}_{\\rho(\\tau;\\theta^K)}[f_2(\\tau)] \\right\\|\n\\]\n\n### Step 2: Decompose the Difference\nAdd and subtract $\\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_2(\\tau)]$:\n\\[\n\\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_1(\\tau)] - \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_2(\\tau)] + \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_2(\\tau)] - \\mathbb{E}_{\\rho(\\tau;\\theta^K)}[f_2(\\tau)] \\right\\|\n\\]\nBy the triangle inequality:\n\\[\n\\leq \\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_1(\\tau) - f_2(\\tau)] \\right\\| + \\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_2(\\tau)] - \\mathbb{E}_{\\rho(\\tau;\\theta^K)}[f_2(\\tau)] \\right\\|\n\\]\n\n### Step 3: Bound the First Term\nThe first term is bounded using the Lipschitz assumption on the Hessian of the score function:\n\\[\n\\|f_1(\\tau) - f_2(\\tau)\\| = \\left\\| \\sum_{h=0}^{H_\\ell} \\gamma^{h-1} r_{\\nu_t}(s_h,a_h) \\left( \\sum_{j=0}^h \\left( \\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j|s_j) - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j|s_j) \\right) \\right) \\right\\|\n\\]\nUsing the Lipschitz condition $\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a|s) - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a|s)\\| \\leq L_2 \\|\\theta^* - \\theta^K\\|$ and the bounded reward $\\|r_{\\nu_t}(s,a)\\| \\leq R$:\n\\[\n\\leq R \\sum_{h=0}^{H_\\ell} \\gamma^{h-1} \\sum_{j=0}^h L_2 \\|\\theta^* - \\theta^K\\| = R L_2 \\|\\theta^* - \\theta^K\\| \\sum_{h=0}^{H_\\ell} (h+1) \\gamma^{h-1}\n\\]\nFor $\\gamma \\leq 1$, the sum $\\sum_{h=0}^{H_\\ell} (h+1) \\gamma^{h-1}$ is bounded by $H_\\ell^2$. Thus:\n\\[\n\\|f_1(\\tau) - f_2(\\tau)\\| \\leq R L_2 H_\\ell^2 \\|\\theta^* - \\theta^K\\|\n\\]\nTaking the expectation:\n\\[\n\\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_1(\\tau) - f_2(\\tau)] \\right\\| \\leq R L_2 H_\\ell^2 \\|\\theta^* - \\theta^K\\|\n\\]\n\n### Step 4: Bound the Second Term\nThe second term involves the difference in expectations of $f_2(\\tau)$ under two distributions. Using the Total Variation (TV) distance bound:\n\\[\n\\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_2(\\tau)] - \\mathbb{E}_{\\rho(\\tau;\\theta^K)}[f_2(\\tau)] \\right\\| \\leq 2 \\|f_2\\|_{\\infty} \\cdot d_{TV}(\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K))\n\\]\nFrom the boundedness of $f_1$ and the Lipschitz condition, $\\|f_2(\\tau)\\| \\leq \\chi_1 + R L_2 H_\\ell^2 \\|\\theta^* - \\theta^K\\|$. However, for simplicity, we use $\\|f_2\\|_{\\infty} \\leq \\chi_1 = H_\\ell^2 R L_1$ (as $\\|\\nabla^2_\\theta \\log \\pi_\\theta\\| \\leq L_1$). The TV distance is bounded by:\n\\[\nd_{TV}(\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\leq \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\|\n\\]\nThus:\n\\[\n\\left\\| \\mathbb{E}_{\\rho(\\tau;\\theta^*)}[f_2(\\tau)] - \\mathbb{E}_{\\rho(\\tau;\\theta^K)}[f_2(\\tau)] \\right\\| \\leq 2 \\cdot H_\\ell^2 R L_1 \\cdot \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\| = L_1 L_2 R H_\\ell^3 \\|\\theta^* - \\theta^K\\|\n\\]\n\n### Step 5: Combine the Bounds\nCombining the bounds from Step 3 and Step 4:\n\\[\n\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \\leq \\left( L_1 L_2 R H_\\ell^3 + L_2 R H_\\ell^2 \\right) \\|\\theta^* - \\theta^K\\|\n\\]\nThus, the Lipschitz constant is:\n\\[\nL' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_\\ell^2\n\\]\n\n### Final Result\nThe Hessian of the lower-level value function is Lipschitz continuous with respect to $\\theta$:\n\\[\n\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \\leq L' \\|\\theta^* - \\theta^K\\|\n\\]\nwhere $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2$.", "C": "We start by considering the term \n\t\\begin{align}\n\t\t{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K) = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau),\n\t\\end{align}\n\twhere we define \n\t\\begin{align}\n\t\tf_1(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j|s_j)\\right)\n\t\t\\\\\n\t\tf_2(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j|s_j)\\right).\n\t\\end{align}\n\tSubsequently, we write the norm of the equation above into 2 parts as\n\t\\begin{align}\n\t\t&\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| \n\t\t\\nonumber\\\\\n\t\t & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)  + \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)}_{T_1}\\| + \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)}_{T_2}\\|,\n\t\\end{align}\nFirst we use triangle inequality and then add and subtract $\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)$ to get the expression where, $T_1 = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)$ and $T_2 = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_2(\\tau)$. {This is an interesting point of departure from existing Bilevel analysis where the stochasticity is mainly iid noise. Hence to deal with this, we separate the terms 1. divergence b/w two probability distributions and 2. expected diff b/w two functions under the same distribution.} We next, upper-bound the individual terms to get the final Lispchitz constant.\n\t\n\tFirst, we focus on the first term of inequality i.e $T_1$ given as\n\t\\begin{align} \n\t\t\\|T_1\\| & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\sup_{f_1} |\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_1(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_1(\\tau)| \\nonumber\\\\\n\t\t& \\leq \\chi_1 d_{TV} (\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\nonumber\\\\\n\t\t& \\leq \\chi_1 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\n\twhere we first upper-bound the expression to a standard Integral Probability Metric form by taking the supremum and then dividing and multiplying with the norm of the function given by $\\chi_1 =\\|f_1(\\cdot)\\| = H_\\ell^2 R L_1$. With that, we get the expression as the of Total variation distance. Then, we upper-bounded the total variation using the assumed bound.\n        \n\tNow, we proceed to the second term of the equation $T_2$ and derive an upper bound as \n\t\\begin{align} \n\t\tT_2 & = \\sum_{\\tau} \\rho(\\tau;\\theta^K(\\nu)) (f_1(\\tau) - f_2(\\tau)) \\nonumber \\\\\n\t\t& \\leq f_1(\\tau') - f_2(\\tau') \\nonumber\\\\\n\t\t& = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\n\t\\end{align}\n\twhere we consider the trajectory $\\tau'$ in the sum with the maximum value and upper bound by that and expand the definition of $f_1, f_2$. \n\t\\begin{align} \n\t\t\\|T_2\\| & \\leq \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\\| \\nonumber \\\\\n\t\t& \\leq \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot r_{\\nu_t}(s_h', a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)\\| \\nonumber\\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|r_{\\nu_t}(s_h', a_h')\\| \\left(\\sum_{j=0}^{h}(\\|\\nabla_{\\theta}^2 \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta}^2 \\log \\pi_{\\theta^K}(a_j'|s_j')\\|)\\right) \\nonumber\\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|r_{\\nu_t}(s_h', a_h')\\| H_{\\ell} L_{2} \\|\\theta^* - \\theta^K\\| \\nonumber\\\\\n\t\t& = L_{2} R H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \n\t\\end{align}\n\twhere we use Cauchy-Schwartz and triangle inequality repetitively to get to the third inequality. Next, we use the assumption on the Hessian Lipschitzness of the score function ($L_2$) and the bounded reward norm $\\|r_{\\nu_t}(s,a)\\| = R$ to get the next inequality. Finally, we use the upper bound on the geometric series to obtain the final expression. \n\tAdding the bounds for $\\|T_1\\|$ and $\\|T_2\\|$, we get\n\t\\begin{align}\n\t\t\\|{\\nabla}_{\\theta}^2V_s(\\nu, \\theta^*) - {\\nabla}_{\\theta}^2V_s(\\nu, \\theta^K)\\| & \\leq \\|T_1\\| + \\|T_2\\| \\\\ \n        & \\leq \\chi_1 \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\| + L_{2} R H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \\\\ \n        &= \\left( L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}^2 \\right) \\|\\theta^* - \\theta^K\\| = L' \\|\\theta^* - \\theta^K\\|\n\t\\end{align}\n\twhere $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_{\\ell}$ (using $\\chi_1 = H_\\ell^2 R L_1$).", "D": "To establish the Lipschitz continuity of $\\nabla_{\\theta}^2 V_s(\\nu, \\theta)$, we bound the difference $\\|\\nabla_{\\theta}^2 V_s(\\nu, \\theta^*) - \\nabla_{\\theta}^2 V_s(\\nu, \\theta^K)\\|$ by decomposing it into two components and applying the given assumptions:\n\n**Step 1: Decompose the Difference**  \n\\[\n\\|\\mathbb{E}_{\\rho(\\theta^*)}[f_1] - \\mathbb{E}_{\\rho(\\theta^K)}[f_2]\\| \\leq \\underbrace{\\|\\mathbb{E}_{\\rho(\\theta^*)}[f_1 - f_2]\\|}_{\\text{Term 1}} + \\underbrace{\\|\\mathbb{E}_{\\rho(\\theta^*)}[f_2] - \\mathbb{E}_{\\rho(\\theta^K)}[f_2]\\|}_{\\text{Term 2}}.\n\\]\n\n**Step 2: Bound Term 1 (Function Difference)**  \nUsing the Lipschitz Hessian assumption ($\\|\\nabla_\\theta^2 \\log \\pi_{\\theta_1} - \\nabla_\\theta^2 \\log \\pi_{\\theta_2}\\| \\leq L_2 \\|\\theta_1 - \\theta_2\\|$):  \nFor each trajectory $\\tau$,  \n\\[\n\\|f_1(\\tau) - f_2(\\tau)\\| \\leq \\sum_{h=0}^{H_\\ell} \\gamma^{h-1} R \\sum_{j=0}^h L_2 \\|\\theta^* - \\theta^K\\| = R L_2 \\|\\theta^* - \\theta^K\\| \\sum_{h=0}^{H_\\ell} \\gamma^{h-1} (h+1).\n\\]  \nSumming over $h$ and using $\\sum_{h=0}^{H_\\ell} (h+1) \\gamma^{h-1} \\leq \\frac{H_\\ell(H_\\ell + 1)}{2}$ (for $\\gamma = 1$):  \n\\[\n\\text{Term 1} \\leq R L_2 \\cdot \\frac{H_\\ell^2}{2} \\|\\theta^* - \\theta^K\\| = \\frac{L_2 R H_\\ell^2}{2} \\|\\theta^* - \\theta^K\\|.\n\\]\n\n**Step 3: Bound Term 2 (Distribution Shift)**  \nUsing the Total Variation (TV) bound $d_{TV}(\\rho(\\theta^*), \\rho(\\theta^K)) \\leq \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\|$ and $\\|f_2(\\tau)\\| \\leq \\chi_1 = H_\\ell^2 R L_1$:  \n\\[\n\\text{Term 2} \\leq 2 \\chi_1 d_{TV} \\leq 2 \\cdot H_\\ell^2 R L_1 \\cdot \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\| = L_1 L_2 R H_\\ell^3 \\|\\theta^* - \\theta^K\\|.\n\\]\n\n**Step 4: Combine Both Terms**  \nSumming Term 1 and Term 2:  \n\\[\n\\|\\nabla_{\\theta}^2 V_s(\\nu, \\theta^*) - \\nabla_{\\theta}^2 V_s(\\nu, \\theta^K)\\| \\leq \\underbrace{\\left(L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_\\ell^2\\right)}_{L'} \\|\\theta^* - \\theta^K\\|.\n\\]\n\nThus, the Lipschitz constant is $L' = L_1 L_2 R \\frac{H_\\ell^3}{2} + L_2 R H_\\ell^2$.", "ground-truth": "A"}
{"paper": "arxiv:2308.02585", "question": "How is the Lipschitz continuity of the second-order mixed Jacobian term ${\\nabla}^2_{\\nu, \\theta}V_s(\\nu, \\theta)$ with respect to the policy parameter $\\theta$ established?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. ($H_\\ell$: horizon, $\\gamma$: discount factor).\n    \\item Mixed Jacobian term:\n    \\[\n        {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta) = \\mathbb{E}_{\\rho(\\tau;\\theta)} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_j|s_j)]^T\\bigg) \\Bigg]\n    \\]\n    \\item Let $\\theta^* = \\theta^*(\\nu)$ be the optimal parameter and $\\theta^K = \\theta^K(\\nu)$ be an approximate parameter.\n    \\item Define functions over trajectory $\\tau$ (Note: Using $\\gamma^{h-1}$ and sum up to $H_\\ell$ as in the proof text, also $H_\\ell$ in inner sum limit for $f_3, f_4$):\n    \\[\n        f_3(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j|s_j)\\right)^T \n    \\]\n    \\[\n        f_4(\\tau) = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j|s_j)\\right)^T\n    \\]\n    \\item Let $\\rho(\\tau; \\theta)$ be the trajectory distribution induced by policy $\\pi_\\theta$.\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Lipschitz Reward Gradient:} $\\|\\nabla_{\\nu} r_{\\nu}(s,a)\\| \\leq L_r$.\n    \\item \\textbf{Lipschitz Score Function:} $\\|\\nabla_{\\theta} \\log \\pi_{\\theta_1}(a|s) - \\nabla_{\\theta} \\log \\pi_{\\theta_2}(a|s)\\| \\leq L_{1} \\|\\theta_1 - \\theta_2\\|$.\n    \\item \\textbf{Bounded Score Function:} $\\|\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s)\\| \\leq B$.\n    \\item \\textbf{Bounded Norm of $f_3$:} The norm of $f_3$ is bounded: $\\|f_3(\\cdot)\\| \\leq \\chi_3$, where the proof text specifies $\\chi_3 = H_\\ell^2 B L_r$.\n    \\item \\textbf{Total Variation Bound:} $d_{TV}(\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\leq \\frac{H_{\\ell}L_{2}}{2} \\|\\theta^* - \\theta^K\\|$ (where $L_2$ arises from the Lipschitz Hessian of score).\n\\end{itemize}\n\n**Goal:** Show that $\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| \\leq L'' \\|\\theta^* - \\theta^K\\|$ with $L'' = L_2 L_r B \\frac{H_\\ell^3}{2} + L_{1} L_r H_{\\ell}^2$, following the derivation steps.\n", "A": "To establish the Lipschitz continuity of the second-order mixed Jacobian term ${\\nabla}^2_{\\nu, \\theta}V_s(\\nu, \\theta)$ with respect to the policy parameter $\\theta$, we need to show that:\n\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| \\leq L'' \\|\\theta^* - \\theta^K\\|\n\\]\n\nwhere $L'' = L_2 L_r B \\frac{H_\\ell^3}{2} + L_{1} L_r H_{\\ell}^2$. \n\n### Key Steps:\n\n1. **Decompose the Difference**:\n   The difference between the Jacobians at $\\theta^*$ and $\\theta^K$ can be split into two parts using the triangle inequality:\n   \\[\n   \\| \\mathbb{E}_{\\rho(\\theta^*)}[f_3] - \\mathbb{E}_{\\rho(\\theta^K)}[f_4] \\| \\leq \\| \\mathbb{E}_{\\rho(\\theta^*)}[f_3] - \\mathbb{E}_{\\rho(\\theta^K)}[f_3] \\| + \\| \\mathbb{E}_{\\rho(\\theta^K)}[f_3 - f_4] \\|\n   \\]\n\n2. **Bound the Distribution Shift Term**:\n   - The Total Variation (TV) distance between the trajectory distributions $\\rho(\\theta^*)$ and $\\rho(\\theta^K)$ is bounded by:\n     \\[\n     d_{TV}(\\rho(\\theta^*), \\rho(\\theta^K)) \\leq \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\|\n     \\]\n   - Using the bounded norm of $f_3$ ($\\|f_3\\| \\leq H_\\ell^2 B L_r$), the first term is bounded by:\n     \\[\n     \\| \\mathbb{E}_{\\rho(\\theta^*)}[f_3] - \\mathbb{E}_{\\rho(\\theta^K)}[f_3] \\| \\leq 2 \\chi_3 d_{TV} = 2 \\cdot H_\\ell^2 B L_r \\cdot \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\| = H_\\ell^3 L_2 L_r B \\|\\theta^* - \\theta^K\\|\n     \\]\n   - Incorporating the factor from the TV bound, this contributes $\\frac{H_\\ell^3}{2} L_2 L_r B$ to $L''$.\n\n3. **Bound the Function Difference Term**:\n   - The difference $f_3 - f_4$ involves the Lipschitz score function assumption:\n     \\[\n     \\|\\nabla_\\theta \\log \\pi_{\\theta^*}(a|s) - \\nabla_\\theta \\log \\pi_{\\theta^K}(a|s)\\| \\leq L_1 \\|\\theta^* - \\theta^K\\|\n     \\]\n   - Summing over $H_\\ell$ steps and bounding by $L_r$ for each term, the second term is:\n     \\[\n     \\| \\mathbb{E}_{\\rho(\\theta^K)}[f_3 - f_4] \\| \\leq L_1 L_r H_\\ell^2 \\|\\theta^* - \\theta^K\\|\n     \\]\n   - This contributes $L_1 L_r H_\\ell^2$ to $L''$.\n\n### Final Answer:\nCombining both terms, the Lipschitz constant $L''$ is:\n\\[\n\\boxed{L'' = \\frac{H_\\ell^3}{2} L_2 L_r B + H_\\ell^2 L_1 L_r}\n\\]", "B": "We start by considering the term \n\t\\begin{align}\n\t\t{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K) \\leq \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)\n\t\\end{align}\n\twhere we define\n\t\\begin{align}\n\t\tf_3(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j|s_j)\\right)^T\n\t\t\\\\\n\t\tf_4(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j|s_j)\\right)^T .\n\t\\end{align}\n\tSubsequently, we write the norm of the difference term into 2 parts as\n\t\\begin{align}\n\t\t&\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| \\\\ \n        & \\leq \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)}_{T_3}\\| \n\t\t\\nonumber\\\\\n\t\t& + \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)}_{T_4}\\| \n\t\t\\nonumber\n\t\\end{align}\n\twhere, $T_3 = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)$ and $T_4 = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)$. We next, upper-bound the individual terms to get the final Lispchitz constant.\n\t\n\tFirst, we focus on the first term of inequality i.e $T_3$ given as\n\t\\begin{align} \n\t\t\\|T_3\\| & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\sup_{f_3} |\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)| \\nonumber\\\\\n\t\t& \\leq \\chi_3 d_{TV} (\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\nonumber\\\\\n\t\t& \\leq  \\chi_3 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\nwhere we convert the inequality first to a standard Integral Probability Metric form by taking the supremum. Then we relate it to the Total variation distance multiplied by the norm of the function $f_3$ given by $\\|f_{3}(\\cdot)\\| = \\chi_3 = H_\\ell^2 B L_r$. Then, we upper-bounded the total variation using the assumed bound.\n\t\n\tNow, we proceed to the second term of the equation $T_4$ and derive an upper bound as \n\t\\begin{align} \n\t\tT_4 & = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} [f_3(\\tau) - f_4(\\tau)] \\\\ \n        & = \\sum_{\\tau} \\rho(\\tau;\\theta^K(\\nu)) (f_3(\\tau) - f_4(\\tau)) \\nonumber\n\t\\end{align}\n\tThe proof text then bounds this term (potentially using a max argument implicitly) before taking the norm:\n\t\\begin{align} \n\t\t T_4 & \\leq f_3(\\tau') - f_4(\\tau') \\nonumber\\\\\n\t\t& = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}(\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)^T  \n\t\\end{align}\n\twhere we consider the trajectory $\\tau'$ with the maximum value. Next, we upper-bound the norm $\\|T_4\\|$ as \n\t\\begin{align} \n\t\t\\|T_4\\| & \\leq \\Bigg \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)^T\\Bigg\\| \\nonumber \n\t\t\\\\   \n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|\\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\| \\left(\\sum_{j=0}^{h}\\|\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\|\\right) \\nonumber \\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot L_r \\cdot H_{\\ell} L_{1} \\|\\theta^* - \\theta^K\\| \\nonumber \\\\\n\t\t& \\leq L_{1} L_r H_{\\ell} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\n\twhere we use Cauchy-Schwartz and triangle inequality repetitively to get to the third inequality. Next, we use the assumption on the Lipschitzness of the gradient of the score function ($L_1$) and the bounded reward gradient ($L_r$). Finally, we upper-bound sum of this geometric series to obtain the final expression. \n\tAdding the bounds for $\\|T_3\\|$ and $\\|T_4\\|$, we get \n\t\\begin{align}\n\t\t\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| & \\leq \\|T_3\\| + \\|T_4\\| \\\\ \n        & \\leq \\chi_3 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\| + L_{1} L_r H_{\\ell} \\|\\theta^* - \\theta^K\\| \\\\ \n        & = \\left( (H_\\ell^2 B L_r) \\frac{H_{\\ell}L_{2}}{2} + L_{1} L_r H_{\\ell} \\right) \\|\\theta^* - \\theta^K\\| = L'' \\|\\theta^* - \\theta^K\\|\n\t\\end{align}\n\twhere, $L'' =  L_2 L_r B \\frac{H_\\ell^3}{2} + L_{1} L_r H_{\\ell}$ (using $\\chi_3 = H_\\ell^2 B L_r$).", "C": "To establish the Lipschitz continuity of the second-order mixed Jacobian term $\\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta)$ with respect to the policy parameter $\\theta$, we analyze the difference between the Jacobian evaluated at the optimal parameter $\\theta^*$ and an approximate parameter $\\theta^K$. \n\nThe key steps involve bounding the difference in terms of the trajectory distributions and the Lipschitz properties of the reward gradient and score function. Here is the detailed derivation:\n\n### Step 1: Express the Difference in Terms of $f_3$ and $f_4$\nThe mixed Jacobian can be written as expectations over trajectories:\n\\[\n\\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^*) = \\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_3(\\tau)], \\quad \\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^K) = \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)].\n\\]\nThus, the difference is:\n\\[\n\\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^*) - \\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^K) = \\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_3(\\tau)] - \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)].\n\\]\n\n### Step 2: Decompose the Difference\nWe decompose the difference into two terms:\n\\[\n\\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_3(\\tau)] - \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)] = \\underbrace{\\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_3(\\tau) - f_4(\\tau)]}_{\\text{Term 1}} + \\underbrace{\\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_4(\\tau)] - \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)]}_{\\text{Term 2}}.\n\\]\n\n#### Term 1: Bounding $f_3(\\tau) - f_4(\\tau)$\nUsing the definitions of $f_3$ and $f_4$, the difference is:\n\\[\nf_3(\\tau) - f_4(\\tau) = \\sum_{h=0}^{H_\\ell} \\gamma^{h-1} \\nabla_\\nu r_\\nu(s_h, a_h) \\left( \\sum_{j=0}^{H_\\ell} \\left( \\nabla_\\theta \\log \\pi_{\\theta^*}(a_j|s_j) - \\nabla_\\theta \\log \\pi_{\\theta^K}(a_j|s_j) \\right) \\right)^T.\n\\]\nBy the **Lipschitz score function assumption**, we have:\n\\[\n\\|\\nabla_\\theta \\log \\pi_{\\theta^*}(a_j|s_j) - \\nabla_\\theta \\log \\pi_{\\theta^K}(a_j|s_j)\\| \\leq L_1 \\|\\theta^* - \\theta^K\\|.\n\\]\nThus, the norm of the difference is bounded as:\n\\[\n\\|f_3(\\tau) - f_4(\\tau)\\| \\leq \\sum_{h=0}^{H_\\ell} \\gamma^{h-1} L_r \\cdot \\sum_{j=0}^{H_\\ell} L_1 \\|\\theta^* - \\theta^K\\| = L_1 L_r H_\\ell^2 \\|\\theta^* - \\theta^K\\|,\n\\]\nwhere we used $\\sum_{h=0}^{H_\\ell} \\gamma^{h-1} \\leq H_\\ell$ (assuming $\\gamma \\leq 1$) and $\\sum_{j=0}^{H_\\ell} 1 = H_\\ell + 1 \\approx H_\\ell$ for large $H_\\ell$.\n\n#### Term 2: Bounding the Distribution Shift\nThe second term involves the difference in expectations due to the change in trajectory distribution:\n\\[\n\\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_4(\\tau)] - \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)].\n\\]\nUsing the **total variation bound**, we have:\n\\[\n\\left\\| \\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_4(\\tau)] - \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)] \\right\\| \\leq 2 \\cdot d_{TV}(\\rho(\\tau; \\theta^*), \\rho(\\tau; \\theta^K)) \\cdot \\sup_\\tau \\|f_4(\\tau)\\|.\n\\]\nFrom the **bounded norm of $f_3$** and the fact that $f_4$ is similar to $f_3$ (but evaluated at $\\theta^K$), we can bound $\\|f_4(\\tau)\\|$ similarly:\n\\[\n\\|f_4(\\tau)\\| \\leq H_\\ell^2 B L_r.\n\\]\nThe total variation distance is bounded as:\n\\[\nd_{TV}(\\rho(\\tau; \\theta^*), \\rho(\\tau; \\theta^K)) \\leq \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\|.\n\\]\nThus, the second term is bounded by:\n\\[\n\\left\\| \\mathbb{E}_{\\rho(\\tau; \\theta^*)}[f_4(\\tau)] - \\mathbb{E}_{\\rho(\\tau; \\theta^K)}[f_4(\\tau)] \\right\\| \\leq 2 \\cdot \\frac{H_\\ell L_2}{2} \\|\\theta^* - \\theta^K\\| \\cdot H_\\ell^2 B L_r = L_2 L_r B H_\\ell^3 \\|\\theta^* - \\theta^K\\|.\n\\]\n\n### Step 3: Combine the Bounds\nCombining the bounds for Term 1 and Term 2, we obtain:\n\\[\n\\left\\| \\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^*) - \\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^K) \\right\\| \\leq \\left( L_1 L_r H_\\ell^2 + L_2 L_r B \\frac{H_\\ell^3}{2} \\right) \\|\\theta^* - \\theta^K\\|.\n\\]\nThus, the Lipschitz constant is:\n\\[\nL'' = L_2 L_r B \\frac{H_\\ell^3}{2} + L_1 L_r H_\\ell^2.\n\\]\n\n### Final Result\nThe second-order mixed Jacobian term is Lipschitz continuous with respect to $\\theta$:\n\\[\n\\left\\| \\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^*) - \\nabla^2_{\\nu, \\theta} V_s(\\nu, \\theta^K) \\right\\| \\leq L'' \\|\\theta^* - \\theta^K\\|,\n\\]\nwhere\n\\[\nL'' = L_2 L_r B \\frac{H_\\ell^3}{2} + L_1 L_r H_\\ell^2.\n\\]", "D": "We start by considering the term \n\t\\begin{align}\n\t\t{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K) \\leq \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)\n\t\\end{align}\n\twhere we define\n\t\\begin{align}\n\t\tf_3(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j|s_j)\\right)^T\n\t\t\\\\\n\t\tf_4(\\tau) &= \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}\\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j|s_j)\\right)^T .\n\t\\end{align}\n\tSubsequently, we write the norm of the difference term into 2 parts as\n\t\\begin{align}\n\t\t&\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| \\\\ \n        & \\leq \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)}_{T_3}\\| \n\t\t\\nonumber\\\\\n\t\t& + \\|\\underbrace{\\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)}_{T_4}\\| \n\t\t\\nonumber\n\t\\end{align}\n\twhere, $T_3 = \\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)$ and $T_4 = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_4(\\tau)$. We next, upper-bound the individual terms to get the final Lispchitz constant.\n\t\n\tFirst, we focus on the first term of inequality i.e $T_3$ given as\n\t\\begin{align} \n\t\t\\|T_3\\| & = \\|\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)\\| \\nonumber\\\\\n\t\t& \\leq \\sup_{f_3} |\\mathbb{E}_{\\rho(\\tau;\\theta^*)} f_3(\\tau) -  \\mathbb{E}_{\\rho(\\tau;\\theta^K)} f_3(\\tau)| \\nonumber\\\\\n\t\t& \\leq \\chi_3 d_{TV} (\\rho(\\tau;\\theta^*), \\rho(\\tau;\\theta^K)) \\nonumber\\\\\n\t\t& \\leq  \\chi_3 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\nwhere we convert the inequality first to a standard Integral Probability Metric form by taking the supremum. Then we relate it to the Total variation distance multiplied by the norm of the function $f_3$ given by $\\|f_{3}(\\cdot)\\| = \\chi_3 = H_\\ell^2 B L_r$. Then, we upper-bounded the total variation using the assumed bound.\n\t\n\tNow, we proceed to the second term of the equation $T_4$ and derive an upper bound as \n\t\\begin{align} \n\t\tT_4 & = \\mathbb{E}_{\\rho(\\tau;\\theta^K)} [f_3(\\tau) - f_4(\\tau)] \\\\ \n        & = \\sum_{\\tau} \\rho(\\tau;\\theta^K(\\nu)) (f_3(\\tau) - f_4(\\tau)) \\nonumber\n\t\\end{align}\n\tThe proof text then bounds this term (potentially using a max argument implicitly) before taking the norm:\n\t\\begin{align} \n\t\t T_4 & \\leq f_3(\\tau') - f_4(\\tau') \\nonumber\\\\\n\t\t& = \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{H_{\\ell}}(\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)^T  \n\t\\end{align}\n\twhere we consider the trajectory $\\tau'$ with the maximum value. Next, we upper-bound the norm $\\|T_4\\|$ as \n\t\\begin{align} \n\t\t\\|T_4\\| & \\leq \\Bigg \\|\\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}(\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\right)^T\\Bigg\\| \\nonumber \n\t\t\\\\   \n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot \\|\\nabla_{\\nu} r_{\\nu}(s_h',a_h')\\| \\left(\\sum_{j=0}^{h}\\|\\nabla_{\\theta} \\log \\pi_{\\theta^*}(a_j'|s_j') - \\nabla_{\\theta} \\log \\pi_{\\theta^K}(a_j'|s_j'))\\|\\right) \\nonumber \\\\\n\t\t& \\leq \\sum_{h =0}^{H_{\\ell}}\\gamma^{h-1}\\cdot L_r \\cdot H_{\\ell} L_{1} \\|\\theta^* - \\theta^K\\| \\nonumber \\\\\n\t\t& \\leq L_{1} L_r H_{\\ell}^2 \\|\\theta^* - \\theta^K\\|,\n\t\\end{align}\n\twhere we use Cauchy-Schwartz and triangle inequality repetitively to get to the third inequality. Next, we use the assumption on the Lipschitzness of the gradient of the score function ($L_1$) and the bounded reward gradient ($L_r$). Finally, we upper-bound sum of this geometric series to obtain the final expression. \n\tAdding the bounds for $\\|T_3\\|$ and $\\|T_4\\|$, we get \n\t\\begin{align}\n\t\t\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^*) - {\\nabla}^2_{\\nu, \\theta}V_s(\\nu,\\theta^K)\\| & \\leq \\|T_3\\| + \\|T_4\\| \\\\ \n        & \\leq \\chi_3 \\frac{H_{\\ell}}{2}L_{2} \\|\\theta^* - \\theta^K\\| + L_{1} L_r H_{\\ell}^2 \\|\\theta^* - \\theta^K\\| \\\\ \n        & = \\left( (H_\\ell^2 B L_r) \\frac{H_{\\ell}L_{2}}{2} + L_{1} L_r H_{\\ell}^2 \\right) \\|\\theta^* - \\theta^K\\| = L'' \\|\\theta^* - \\theta^K\\|\n\t\\end{align}\n\twhere, $L'' =  L_2 L_r B \\frac{H_\\ell^3}{2} + L_{1} L_r H_{\\ell}^2$ (using $\\chi_3 = H_\\ell^2 B L_r$).\n", "ground-truth": "D"}
{"paper": "arxiv:2302.03655", "question": "Given a 3D unit vector $r$, let $R$ be a rotation matrix with $R\\cdot r=(0, 1, 0)$ . Show why \n$$\nY(R\\cdot r)\\propto \\delta_{m}^{(l)}=\\begin{cases}1& \\text{if }l= m,\\\\0 &\\text{else,}\\end{cases}\n$$\nwhere $Y$ here denotes the spherical harmonics.", "A": "We know that, given the input vector $(x,y,z)$, each term of the spherical harmonics is a polynomial in $x,y,z$. Note that for $R\\cdot r$, $x=z=0$, so the value is non-zero only when the result cannot be used to factor $x,z$, that is, when $l=-m$.", "B": "We know that, given the input vector $(x,y,z)$, each term of the spherical harmonics is a polynomial in $x,y,z$. Note that for $R\\cdot r$, $x=z=0$, so the value is non-zero only when the result cannot be used to factor $x,z$, that is, when $l=m$.", "C": "To demonstrate why $Y(R\\cdot \\mathbf{r}) \\propto \\delta_{m}^{(l)}$, we analyze the spherical harmonics under rotation and symmetry considerations. \n\n1. **Rotation of Coordinates**:  \n   Let $\\mathbf{r}' = R \\cdot \\mathbf{r} = (0, 1, 0)$, aligning with the $y$-axis. In spherical coordinates, this corresponds to $\\theta' = \\frac{\\pi}{2}$ and $\\phi' = \\frac{\\pi}{2}$. The spherical harmonics in this direction are given by:  \n   $$\n   Y_l^m\\left(\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right) = \\sqrt{\\frac{2l+1}{4\\pi} \\frac{(l-m)!}{(l+m)!}} \\, P_l^m(0) \\, e^{i m \\frac{\\pi}{2}},\n   $$\n   where $P_l^m(x)$ are the associated Legendre polynomials.\n\n2. **Associated Legendre Polynomial at $x=0$**:  \n   The value $P_l^m(0)$ is non-zero **only** when $l + m$ is **even**. This arises from the parity properties of $P_l^m(x)$:  \n   - For $l + m$ odd, $P_l^m(0) = 0$.  \n   - For $l + m$ even, $P_l^m(0) \\neq 0$.  \n\n3. **Phase Factor**:  \n   The term $e^{i m \\frac{\\pi}{2}}$ contributes a complex phase but does not affect the vanishing/non-vanishing of $Y_l^m$.\n\n4. **Symmetry of the $y$-Axis**:  \n   When $\\mathbf{r}'$ aligns with the $y$-axis, the system exhibits symmetry under rotations about the $y$-axis. This restricts the non-zero contributions to spherical harmonics with **azimuthal symmetry** ($m = l$). For $m \\neq l$, the mismatch in angular momentum projection forces $Y_l^m\\left(\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right) = 0$.  \n\n5. **Conclusion**:  \n   Combining the above, $Y_l^m(R \\cdot \\mathbf{r})$ is non-zero **only** when $l = m$, leading to:\n   $$\n   Y(R \\cdot \\mathbf{r}) \\propto \\delta_{m}^{(l)} = \\begin{cases} \n   1 & \\text{if } l = m, \\\\\n   0 & \\text{otherwise.}\n   \\end{cases}\n   $$\n\nThus, the spherical harmonics vanish except when $l = m$ due to rotational symmetry and the parity constraints of $P_l^m(0)$.", "D": "To understand why \\( Y(R \\cdot r) \\propto \\delta_{m0}^{(l)} \\), let's carefully analyze the given conditions and properties of spherical harmonics.\n\n### Given:\n- A 3D unit vector \\( \\mathbf{r} \\).\n- A rotation matrix \\( R \\) such that \\( R \\cdot \\mathbf{r} = (0, 1, 0) \\). This means \\( R \\) rotates \\( \\mathbf{r} \\) to align with the \\( y \\)-axis.\n- \\( Y \\) denotes the spherical harmonics.\n\n### Spherical Harmonics:\nSpherical harmonics \\( Y_{l}^{m}(\\theta, \\phi) \\) are functions defined on the unit sphere, where:\n- \\( l \\) is the degree (non-negative integer).\n- \\( m \\) is the order (integer with \\( -l \\leq m \\leq l \\)).\n- \\( \\theta \\) is the polar angle (from the \\( z \\)-axis).\n- \\( \\phi \\) is the azimuthal angle (from the \\( x \\)-axis in the \\( xy \\)-plane).\n\n### Key Observations:\n1. **Rotation of \\( \\mathbf{r} \\) to the \\( y \\)-axis**:\n   - After rotation, \\( R \\cdot \\mathbf{r} = (0, 1, 0) \\). In spherical coordinates, this corresponds to \\( \\theta = \\pi/2 \\) (since it lies in the \\( xy \\)-plane) and \\( \\phi = \\pi/2 \\) (since it points along the \\( y \\)-axis).\n   - Thus, \\( Y(R \\cdot \\mathbf{r}) = Y_{l}^{m}(\\pi/2, \\pi/2) \\).\n\n2. **Special Case for \\( \\theta = \\pi/2 \\)**:\n   - Spherical harmonics \\( Y_{l}^{m}(\\theta, \\phi) \\) have specific properties when \\( \\theta = \\pi/2 \\). In particular, for \\( \\theta = \\pi/2 \\), the associated Legendre polynomials \\( P_{l}^{m}(\\cos \\theta) \\) simplify.\n   - The associated Legendre polynomial \\( P_{l}^{m}(0) \\) (since \\( \\cos(\\pi/2) = 0 \\)) is non-zero only when \\( l + m \\) is even, but more importantly, it vanishes for \\( m \\neq 0 \\) unless \\( l \\) and \\( m \\) satisfy specific parity conditions.\n\n3. **Dominance of \\( m = 0 \\)**:\n   - For \\( \\theta = \\pi/2 \\), the spherical harmonics \\( Y_{l}^{m}(\\pi/2, \\phi) \\) are proportional to \\( \\delta_{m0} \\) because the \\( \\phi \\)-dependence (via \\( e^{im\\phi} \\)) must be consistent with the symmetry of the problem.\n   - Specifically, when \\( \\mathbf{r} \\) is aligned with the \\( y \\)-axis, the azimuthal symmetry implies that the only non-zero contributions come from \\( m = 0 \\), as other \\( m \\) values would introduce \\( \\phi \\)-dependence that breaks the symmetry.\n\n4. **Normalization**:\n   - The spherical harmonics are normalized such that \\( Y_{l}^{0}(\\pi/2, \\pi/2) \\) is non-zero and depends only on \\( l \\). For \\( m \\neq 0 \\), \\( Y_{l}^{m}(\\pi/2, \\pi/2) = 0 \\).\n\n### Conclusion:\nThus, \\( Y(R \\cdot \\mathbf{r}) = Y_{l}^{m}(\\pi/2, \\pi/2) \\) is non-zero only when \\( m = 0 \\), and its value depends only on \\( l \\). This is precisely the behavior of the Kronecker delta \\( \\delta_{m0}^{(l)} \\), which is 1 if \\( m = 0 \\) and 0 otherwise. Therefore:\n\n\\[\nY(R \\cdot \\mathbf{r}) \\propto \\delta_{m0}^{(l)} = \\begin{cases} \n1 & \\text{if } m = 0, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\n\n### Final Answer:\nThe spherical harmonics evaluated at \\( R \\cdot \\mathbf{r} = (0, 1, 0) \\) satisfy:\n\n\\[\nY(R \\cdot \\mathbf{r}) \\propto \\delta_{m0}^{(l)} = \\begin{cases} \n1 & \\text{if } m = 0, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\n\nThis is because rotating \\( \\mathbf{r} \\) to the \\( y \\)-axis imposes azimuthal symmetry, leaving only the \\( m = 0 \\) component non-zero.", "ground-truth": "B"}
{"paper": "arxiv:2302.03655", "question": "Show that the \"Point-wise spherical non-linearity\" is equivariant, with the fomula as follows: \n$$\nx\\mapsto\\int Y^{(l)}(r)\\cdot \\texttt{MLP}(x^\\top r)\\,\\mathrm{d}r.\n$$", "A": "To show that the point-wise spherical non-linearity defined by\n$$\nx \\mapsto \\int Y^{(l)}(r) \\cdot \\texttt{MLP}(x^\\top r) \\, \\mathrm{d}r\n$$\nis equivariant under rotations, we need to demonstrate that for any rotation matrix \\( R \\in \\text{SO}(3) \\), the following holds:\n$$\nf(Rx) = D^{(l)}(R) \\, f(x),\n$$\nwhere \\( D^{(l)}(R) \\) is the Wigner D-matrix for the \\( l \\)-th irreducible representation of \\(\\text{SO}(3)\\).\n\n**Proof:**\n\n1. **Start with the rotated input:**\n   For a rotated input \\( Rx \\), the mapping becomes:\n   $$\n   f(Rx) = \\int Y^{(l)}(r) \\cdot \\texttt{MLP}\\left( (Rx)^\\top r \\right) \\, \\mathrm{d}r.\n   $$\n\n2. **Exploit rotational invariance of \\( x^\\top r \\):**\n   The inner product \\( x^\\top r \\) is rotationally invariant:\n   $$\n   (Rx)^\\top r = x^\\top R^\\top r = x^\\top (R^{-1}r).\n   $$\n   Let \\( s = R^{-1}r \\), so \\( r = Rs \\). Since \\( R \\) preserves the integration measure (\\( \\mathrm{d}r = \\mathrm{d}s \\)) over the unit sphere \\( S^2 \\), we substitute \\( r = Rs \\):\n   $$\n   f(Rx) = \\int Y^{(l)}(Rs) \\cdot \\texttt{MLP}(x^\\top s) \\, \\mathrm{d}s.\n   $$\n\n3. **Apply the rotation property of spherical harmonics:**\n   Under rotation, the spherical harmonics transform as:\n   $$\n   Y^{(l)}(Rs) = D^{(l)}(R) \\cdot Y^{(l)}(s),\n   $$\n   where \\( D^{(l)}(R) \\) acts linearly on the \\( (2l+1) \\)-dimensional vector \\( Y^{(l)}(s) \\).\n\n4. **Substitute into the integral:**\n   $$\n   f(Rx) = \\int \\left[ D^{(l)}(R) \\cdot Y^{(l)}(s) \\right] \\cdot \\texttt{MLP}(x^\\top s) \\, \\mathrm{d}s.\n   $$\n   Since \\( D^{(l)}(R) \\) is linear and independent of \\( s \\), it can be factored out:\n   $$\n   f(Rx) = D^{(l)}(R) \\int Y^{(l)}(s) \\cdot \\texttt{MLP}(x^\\top s) \\, \\mathrm{d}s = D^{(l)}(R) \\, f(x).\n   $$\n\n5. **Conclusion:**\n   The mapping satisfies \\( f(Rx) = D^{(l)}(R)f(x) \\), which is the definition of equivariance under \\(\\text{SO}(3)\\) for the \\( l \\)-th representation. Thus, the point-wise spherical non-linearity is rotation-equivariant.\n\n**Key Points:**\n- Rotational invariance of \\( x^\\top r \\) ensures the \\(\\texttt{MLP}\\) output is a scalar invariant.\n- The spherical harmonics \\( Y^{(l)}(r) \\) transform linearly under rotations via \\( D^{(l)}(R) \\).\n- The equivariance follows from the linearity of the integral and the representation property of \\( Y^{(l)}(r) \\).", "B": "We know that, the inner product $x^\\top r$ is an invariant scalar, thus will not distroy the equivariance. And the integral traverses only a hemisphere, so rotation of the action does not affect the equivariance.", "C": "To show that the point-wise spherical non-linearity is equivariant, we need to demonstrate that the transformation commutes with the action of the rotation group \\( SO(3) \\). \n\n### Given:\nThe point-wise spherical non-linearity is defined as:\n\\[\nx \\mapsto \\int Y^{(l)}(r) \\cdot \\texttt{MLP}(x^\\top r) \\, \\mathrm{d}r,\n\\]\nwhere:\n- \\( x \\) is an input vector (typically a feature vector in \\( \\mathbb{R}^3 \\) or higher-dimensional space),\n- \\( r \\) is a unit vector on the sphere \\( S^2 \\),\n- \\( Y^{(l)}(r) \\) is the spherical harmonic of degree \\( l \\),\n- \\( \\texttt{MLP} \\) is a multi-layer perceptron (a point-wise non-linear function),\n- The integral is taken over the unit sphere \\( S^2 \\).\n\n### Goal:\nShow that the transformation is equivariant under rotations, i.e., for any rotation \\( R \\in SO(3) \\), the following holds:\n\\[\n\\int Y^{(l)}(r) \\cdot \\texttt{MLP}((R x)^\\top r) \\, \\mathrm{d}r = R \\left( \\int Y^{(l)}(r) \\cdot \\texttt{MLP}(x^\\top r) \\, \\mathrm{d}r \\right).\n\\]\n\n### Proof:\n1. **Rotate the input \\( x \\):**\n   The rotated input is \\( R x \\), where \\( R \\) is a rotation matrix. The transformation applied to \\( R x \\) is:\n   \\[\n   \\int Y^{(l)}(r) \\cdot \\texttt{MLP}((R x)^\\top r) \\, \\mathrm{d}r.\n   \\]\n\n2. **Change of variables in the integral:**\n   Let \\( r' = R^\\top r \\). Since \\( R \\) is a rotation, \\( \\mathrm{d}r' = \\mathrm{d}r \\) (the measure is invariant under rotations). The inner product transforms as:\n   \\[\n   (R x)^\\top r = x^\\top R^\\top r = x^\\top r'.\n   \\]\n   The spherical harmonic \\( Y^{(l)}(r) \\) transforms under rotation as:\n   \\[\n   Y^{(l)}(r) = Y^{(l)}(R r') = \\sum_{m=-l}^l D^{(l)}_{m'm}(R) Y^{(l)}_{m'}(r'),\n   \\]\n   where \\( D^{(l)}_{m'm}(R) \\) are the Wigner D-matrices representing the rotation \\( R \\) in the \\( l \\)-th irreducible representation of \\( SO(3) \\).\n\n   However, for equivariance, we can directly substitute \\( r = R r' \\) and note that \\( Y^{(l)}(r) \\) transforms as:\n   \\[\n   Y^{(l)}(R r') = R \\cdot Y^{(l)}(r'),\n   \\]\n   where \\( R \\cdot Y^{(l)} \\) denotes the rotated spherical harmonic. \n\n3. **Substitute into the integral:**\n   The integral becomes:\n   \\[\n   \\int Y^{(l)}(R r') \\cdot \\texttt{MLP}(x^\\top r') \\, \\mathrm{d}r' = \\int (R \\cdot Y^{(l)}(r')) \\cdot \\texttt{MLP}(x^\\top r') \\, \\mathrm{d}r'.\n   \\]\n   Since \\( R \\) is linear and does not depend on \\( r' \\), it can be factored out of the integral:\n   \\[\n   R \\cdot \\left( \\int Y^{(l)}(r') \\cdot \\texttt{MLP}(x^\\top r') \\, \\mathrm{d}r' \\right).\n   \\]\n\n4. **Conclusion:**\n   This shows that:\n   \\[\n   \\int Y^{(l)}(r) \\cdot \\texttt{MLP}((R x)^\\top r) \\, \\mathrm{d}r = R \\cdot \\left( \\int Y^{(l)}(r) \\cdot \\texttt{MLP}(x^\\top r) \\, \\mathrm{d}r \\right),\n   \\]\n   which is the definition of equivariance. Thus, the point-wise spherical non-linearity is equivariant under \\( SO(3) \\) rotations.\n\n### Key Points:\n- The spherical harmonics \\( Y^{(l)}(r) \\) transform under rotations in a well-defined way (via Wigner D-matrices).\n- The inner product \\( x^\\top r \\) is rotationally invariant in the sense that \\( (R x)^\\top r = x^\\top (R^\\top r) \\).\n- The integral over the sphere is invariant under rotations, allowing the change of variables \\( r' = R^\\top r \\).\n- The MLP is point-wise and does not affect the equivariance property.\n\nThus, the transformation is equivariant under \\( SO(3) \\).", "D": "We know that, the inner product $x^\\top r$ is an invariant scalar, thus will not distroy the equivariance. And the integral traverses the entire sphere, so rotation of the action does not affect the equivariance.", "ground-truth": "D"}
{"paper": "arxiv:2302.03655", "question": "Let $\\rho:G\\to \\mathrm{GL}(V)$ be a representation. Given graph $\\mathcal{G}$, $\\mathfrak{H}$ is the symmetirc group of $\\mathcal{G}$ ($\\mathfrak h\\cdot \\mathcal G=\\mathcal G,\\forall \\mathfrak h\\in\\mathfrak H$), show why $f(\\mathcal{G})$ will always a zero function if $I-\\rho^{(l)}(\\mathfrak H)$ is  non-singular.", "A": "First, we have \n$$\nf(\\mathcal{G})=f(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h\\in H}}f(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h\\in H}}\\rho^{(l)}(\\mathfrak{h})f(\\mathcal{G})=\\rho^{(l)}(\\mathfrak{H})f(\\mathcal{G}).\n$$\nThen it is equivalent to \n$$\n(I+\\rho^{(l)}(\\mathfrak H))f(\\mathcal G)=0\n$$\nAccording to knowledge of linear algebra, $f(\\mathcal{G})$ will always a zero function if $I-\\rho^{(l)}(\\mathfrak H)$ is  non-singular.", "B": "To address the problem, let's carefully analyze the given conditions and derive why \\( f(\\mathcal{G}) \\) must be the zero function when \\( I - \\rho^{(l)}(\\mathfrak{H}) \\) is non-singular.\n\n### Given:\n1. **Representation**: \\(\\rho: G \\to \\mathrm{GL}(V)\\) is a group representation.\n2. **Graph Symmetry**: \\(\\mathfrak{H}\\) is the symmetric group of the graph \\(\\mathcal{G}\\), meaning \\(\\mathfrak{h} \\cdot \\mathcal{G} = \\mathcal{G}\\) for all \\(\\mathfrak{h} \\in \\mathfrak{H}\\).\n3. **Function \\( f \\)**: \\( f(\\mathcal{G}) \\) is a function (presumably invariant under the action of \\(\\mathfrak{H}\\)).\n4. **Non-singularity Condition**: \\( I - \\rho^{(l)}(\\mathfrak{H}) \\) is non-singular, where \\(\\rho^{(l)}\\) is the representation of \\(\\mathfrak{H}\\) acting on the space where \\( f \\) resides.\n\n### Objective:\nShow that \\( f(\\mathcal{G}) \\) is the zero function under the given conditions.\n\n### Proof:\n\n1. **Invariance of \\( f \\)**:\n   Since \\(\\mathfrak{H}\\) is the symmetry group of \\(\\mathcal{G}\\), any function \\( f \\) that is invariant under \\(\\mathfrak{H}\\) must satisfy:\n   \\[\n   f(\\mathfrak{h} \\cdot \\mathcal{G}) = f(\\mathcal{G}), \\quad \\forall \\mathfrak{h} \\in \\mathfrak{H}.\n   \\]\n   If \\( f \\) is linear or can be expressed in terms of the representation \\(\\rho^{(l)}\\), this implies:\n   \\[\n   \\rho^{(l)}(\\mathfrak{h}) f(\\mathcal{G}) = f(\\mathcal{G}), \\quad \\forall \\mathfrak{h} \\in \\mathfrak{H}.\n   \\]\n   Rearranging, we get:\n   \\[\n   (I - \\rho^{(l)}(\\mathfrak{h})) f(\\mathcal{G}) = 0, \\quad \\forall \\mathfrak{h} \\in \\mathfrak{H}.\n   \\]\n   This must hold for all \\(\\mathfrak{h} \\in \\mathfrak{H}\\).\n\n2. **Matrix Form**:\n   The condition can be written collectively for all \\(\\mathfrak{h} \\in \\mathfrak{H}\\) as:\n   \\[\n   (I - \\rho^{(l)}(\\mathfrak{H})) f(\\mathcal{G}) = 0.\n   \\]\n   Here, \\( I - \\rho^{(l)}(\\mathfrak{H}) \\) is a matrix (or linear operator) acting on \\( f(\\mathcal{G}) \\).\n\n3. **Non-singularity Implies Injectivity**:\n   The non-singularity of \\( I - \\rho^{(l)}(\\mathfrak{H}) \\) means that the only solution to:\n   \\[\n   (I - \\rho^{(l)}(\\mathfrak{H})) x = 0\n   \\]\n   is \\( x = 0 \\). Therefore, the equation:\n   \\[\n   (I - \\rho^{(l)}(\\mathfrak{H})) f(\\mathcal{G}) = 0\n   \\]\n   has the unique solution \\( f(\\mathcal{G}) = 0 \\).\n\n### Conclusion:\nSince \\( I - \\rho^{(l)}(\\mathfrak{H}) \\) is non-singular, the only possible solution for \\( f(\\mathcal{G}) \\) is the zero function. Thus, \\( f(\\mathcal{G}) \\) must be identically zero under the given conditions.\n\n\\[\n\\boxed{f(\\mathcal{G}) = 0}\n\\]", "C": "First, we have \n$$\nf(\\mathcal{G})=f(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h\\in H}}f(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h\\in H}}\\rho^{(l)}(\\mathfrak{h})f(\\mathcal{G})=\\rho^{(l)}(\\mathfrak{H})f(\\mathcal{G}).\n$$\nThen it is equivalent to \n$$\n(I-\\rho^{(l)}(\\mathfrak H))f(\\mathcal G)=0\n$$\nAccording to knowledge of linear algebra, $f(\\mathcal{G})$ will always a zero function if $I-\\rho^{(l)}(\\mathfrak H)$ is  non-singular.", "D": "To address the problem, we analyze the interaction between the group action and the function $f$ under the given representation. Let's proceed step-by-step.\n\n**Key Concepts & Setup:**\n- Let $\\rho: G \\to \\mathrm{GL}(V)$ be a representation, and $\\mathfrak{H} \\leq G$ be the symmetry group of a graph $\\mathcal{G}$, satisfying $\\mathfrak{h} \\cdot \\mathcal{G} = \\mathcal{G}$ for all $\\mathfrak{h} \\in \\mathfrak{H}$.\n- Suppose $f: \\mathcal{G} \\to V$ is a function associated with $\\mathcal{G}$. If $f$ respects the symmetry of $\\mathcal{G}$ under $\\mathfrak{H}$, it must satisfy the **equivariance condition**:\n  \\[\n  f(\\mathfrak{h} \\cdot \\mathcal{G}) = \\rho^{(l)}(\\mathfrak{h}) f(\\mathcal{G}), \\quad \\forall \\mathfrak{h} \\in \\mathfrak{H}.\n  \\]\n  Since $\\mathfrak{H}$ preserves $\\mathcal{G}$ (i.e., $\\mathfrak{h} \\cdot \\mathcal{G} = \\mathcal{G}$), this simplifies to:\n  \\[\n  f(\\mathcal{G}) = \\rho^{(l)}(\\mathfrak{h}) f(\\mathcal{G}), \\quad \\forall \\mathfrak{h} \\in \\mathfrak{H}.\n  \\]\n  Rearranging gives:\n  \\[\n  (I - \\rho^{(l)}(\\mathfrak{h})) f(\\mathcal{G}) = 0, \\quad \\forall \\mathfrak{h} \\in \\mathfrak{H}.\n  \\]\n\n**Critical Condition:**\n- The matrix $I - \\rho^{(l)}(\\mathfrak{h})$ is **non-singular** (invertible) for all $\\mathfrak{h} \\in \\mathfrak{H}$. This implies that the equation $(I - \\rho^{(l)}(\\mathfrak{h}))v = 0$ has **only the trivial solution** $v = 0$ in $V$. Equivalently, $\\rho^{(l)}(\\mathfrak{h})$ has **no non-trivial fixed points** in $V$.\n\n**Conclusion:**\n- For $f(\\mathcal{G})$ to satisfy $(I - \\rho^{(l)}(\\mathfrak{h}))f(\\mathcal{G}) = 0$ for all $\\mathfrak{h} \\in \\mathfrak{H}$, the only solution is:\n  \\[\n  f(\\mathcal{G}) = 0.\n  \\]\n  Thus, $f(\\mathcal{G})$ must vanish identically under the non-singularity condition.\n\n**Summary:**  \nThe non-singularity of $I - \\rho^{(l)}(\\mathfrak{H})$ enforces that no non-zero vector in $V$ is fixed by $\\rho^{(l)}(\\mathfrak{H})$. Hence, $f(\\mathcal{G}) \\equiv 0$ is the only function compatible with the symmetry constraints of $\\mathfrak{H}$ on $\\mathcal{G}$.", "ground-truth": "C"}
{"paper": "arxiv:2410.11443", "question": "Let $\\rho:G\\to \\mathrm{GL}(V)$ be a representation, $\\mathfrak H$ is a subgroup. Show why the representation of $\\rho(\\mathfrak H)=0$ is equivalent to $\\text{tr}(\\rho(\\mathfrak H))=0$.", "A": "The sufficiency is obvious, and we will now prove the necessity. Note that the group average representation is an idempotent matrix, so all eigenvalues can only be 0 or 1. Note that the trace is 0, that is, the sum of the eigenvalues is 0. Therefore, the matrix must be an all-zero matrix.", "B": "The problem asks to show that the condition $\\rho(\\mathfrak{H}) = 0$ (i.e., $\\rho(h)$ is the zero matrix for all $h \\in \\mathfrak{H}$) is equivalent to $\\text{tr}(\\rho(\\mathfrak{H})) = 0$ (i.e., $\\text{tr}(\\rho(h)) = 0$ for all $h \\in \\mathfrak{H}$). Here’s the proof:\n\n---\n\n**Proof:**\n\n1. **Forward Direction ($\\rho(\\mathfrak{H}) = 0 \\implies \\text{tr}(\\rho(\\mathfrak{H})) = 0$):**  \n   If $\\rho(h) = 0$ (the zero matrix) for all $h \\in \\mathfrak{H}$, then $\\text{tr}(\\rho(h)) = \\text{tr}(0) = 0$. This holds trivially.\n\n2. **Reverse Direction ($\\text{tr}(\\rho(\\mathfrak{H})) = 0 \\implies \\rho(\\mathfrak{H}) = 0$):**  \n   Assume $\\text{tr}(\\rho(h)) = 0$ for all $h \\in \\mathfrak{H}$. Since $\\mathfrak{H}$ is a subgroup, it contains the identity element $e \\in \\mathfrak{H}$. For the identity:  \n   $$\\text{tr}(\\rho(e)) = \\text{tr}(I_V) = \\dim(V),$$  \n   where $I_V$ is the identity matrix on $V$. Given $\\text{tr}(\\rho(e)) = 0$, we conclude $\\dim(V) = 0$. Thus, $V$ is the zero vector space. In this case, all linear transformations on $V$ are zero matrices, so $\\rho(h) = 0$ for all $h \\in \\mathfrak{H}$.\n\n---\n\n**Conclusion:**  \n$\\rho(\\mathfrak{H}) = 0$ and $\\text{tr}(\\rho(\\mathfrak{H})) = 0$ are equivalent **if and only if** the representation space $V$ is zero-dimensional. This follows because $\\text{tr}(\\rho(e)) = \\dim(V)$ forces $V$ to be trivial when the trace condition holds for the identity element.", "C": "We aim to show that the representation \\(\\rho(\\mathfrak{H}) = 0\\) is equivalent to \\(\\text{tr}(\\rho(\\mathfrak{H})) = 0\\). Here, \\(\\rho: G \\to \\mathrm{GL}(V)\\) is a representation, and \\(\\mathfrak{H}\\) is a subgroup of \\(G\\).\n\n### Proof:\n\n1. **Forward Implication (\\(\\rho(\\mathfrak{H}) = 0 \\implies \\text{tr}(\\rho(\\mathfrak{H})) = 0\\)):**\n   - If \\(\\rho(\\mathfrak{H}) = 0\\), then for every \\(h \\in \\mathfrak{H}\\), \\(\\rho(h)\\) is the zero matrix in \\(\\mathrm{GL}(V)\\).\n   - The trace of the zero matrix is \\(0\\). Thus, \\(\\text{tr}(\\rho(h)) = 0\\) for all \\(h \\in \\mathfrak{H}\\).\n   - Therefore, \\(\\text{tr}(\\rho(\\mathfrak{H})) = 0\\).\n\n2. **Reverse Implication (\\(\\text{tr}(\\rho(\\mathfrak{H})) = 0 \\implies \\rho(\\mathfrak{H}) = 0\\)):**\n   - Suppose \\(\\text{tr}(\\rho(h)) = 0\\) for all \\(h \\in \\mathfrak{H}\\).\n   - The trace of a matrix is the sum of its eigenvalues. If \\(\\text{tr}(\\rho(h)) = 0\\) for all \\(h \\in \\mathfrak{H}\\), it implies that the sum of the eigenvalues of \\(\\rho(h)\\) is zero for each \\(h\\).\n   - However, this alone does not imply that \\(\\rho(h)\\) is the zero matrix, as there exist non-zero matrices with trace zero (e.g., \\(\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\)).\n   - The key observation is that \\(\\rho\\) is a representation, and thus \\(\\rho(h)\\) is a linear transformation. For the trace condition to hold for all \\(h \\in \\mathfrak{H}\\), the representation must be trivial (i.e., \\(\\rho(h) = 0\\) for all \\(h \\in \\mathfrak{H}\\)).\n   - This is because if \\(\\rho\\) were non-trivial, there would exist some \\(h \\in \\mathfrak{H}\\) such that \\(\\rho(h)\\) has non-zero eigenvalues, contradicting \\(\\text{tr}(\\rho(h)) = 0\\) unless all eigenvalues are zero (which implies \\(\\rho(h) = 0\\)).\n\n### Conclusion:\nThe representation \\(\\rho(\\mathfrak{H}) = 0\\) is indeed equivalent to \\(\\text{tr}(\\rho(\\mathfrak{H})) = 0\\). The forward implication is straightforward, while the reverse implication relies on the properties of representations and the trace of linear transformations.\n\n\\[\n\\boxed{\\rho(\\mathfrak{H}) = 0 \\iff \\text{tr}(\\rho(\\mathfrak{H})) = 0}\n\\]", "D": "The sufficiency is obvious, and we will now prove the necessity. Note that the group average representation is an idempotent matrix, so all eigenvalues can only be 0 or 1. Note that the trace is 0, that is, all eigenvalues are 0. Therefore, the matrix must be an all-zero matrix.", "ground-truth": "D"}
{"paper": "arxiv:2410.11443", "question": "Let $x_{is}, x_{jt}$ be difference between node positions and $Y$ be the harmonics. Why there exists a bijection between the set of inner products $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ and angles $\\{\\arccos \\langle x_{is}, x_{jt}\\rangle\\}$?", "A": "First, the output of inner product $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ is Legendre polynomials, which span a space isomorphic to power polynomials. Moreover, through Newton's identities and Vieta's theorem, we can get the set of all angle cosines. Also note that trigonometric functions directly form bijections to angles, so this bijection exists.", "B": "The existence of a bijection between the set of inner products $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ and angles $\\{\\arccos \\langle x_{is}, x_{jt} \\rangle\\}$ arises from the properties of spherical harmonics and the relationship between their inner products and angular geometry. Here's the key derivation:\n\n1. **Spherical Harmonics and Addition Theorem**:  \n   For unit vectors $x, y \\in \\mathbb{S}^2$, the inner product of spherical harmonics $Y^{(l)}(x)$ and $Y^{(l)}(y)$ (treated as vectors over $m = -l, \\dots, l$) satisfies:  \n   \\begin{align*}\n   \\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = \\sum_{m=-l}^l Y_{l,m}(x) Y_{l,m}(y) = \\frac{2l + 1}{4\\pi} P_l(\\cos\\theta),\n   \\end{align*}\n   where $\\theta = \\arccos \\langle x, y \\rangle$ is the angle between $x$ and $y$, and $P_l$ is the Legendre polynomial of degree $l$.\n\n2. **Bijection Condition**:  \n   The bijection exists if and only if the function $P_l(\\cos\\theta)$ is injective over $\\theta \\in [0, \\pi]$. This holds **specifically for $l = 1$**:  \n   - For $l = 1$, $P_1(t) = t$, so $\\frac{2(1) + 1}{4\\pi} P_1(\\cos\\theta) = \\frac{3}{4\\pi} \\cos\\theta$. Since $\\cos\\theta$ is strictly decreasing and injective on $[0, \\pi]$, there is a one-to-one correspondence between $\\langle Y^{(1)}(x), Y^{(1)}(y)\\rangle$ and $\\theta$.  \n   - For $l \\geq 2$, $P_l(\\cos\\theta)$ is not injective. For example, $P_2(\\cos\\theta) = \\frac{3\\cos^2\\theta - 1}{2}$ is symmetric about $\\theta = \\pi/2$, leading to multiple $\\theta$ values mapping to the same inner product.\n\n3. **Conclusion**:  \n   The bijection exists **only for $l = 1$** because $P_1(\\cos\\theta)$ linearly and uniquely encodes the angle $\\theta$. For higher $l$, $P_l(\\cos\\theta)$ loses injectivity due to polynomial oscillations, breaking the bijection. Thus, the bijection is fundamentally tied to the linearity of the dipole term ($l = 1$) in spherical harmonics.", "C": "First, the output of inner product $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ is Legendre polynomials, which span a space isomorphic to power polynomials. Moreover, through Newton's identities and Vieta's theorem, we can get the set of all angle cosines. Also note that trigonometric functions directly form bijections to angles, so this bijection exists. However, this bijection is only valid for angles in the interval $[0, \\pi/2]$.", "D": "To establish the bijection between the set of inner products \\(\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle\\) and the angles \\(\\{\\arccos \\langle x_{is}, x_{jt} \\rangle\\}\\), we need to carefully analyze the properties of spherical harmonics and the geometric interpretation of the inner products.\n\n### 1. **Spherical Harmonics and Their Properties**\nSpherical harmonics \\(Y^{(l)}\\) are eigenfunctions of the Laplacian on the sphere \\(S^{d-1}\\) and form an orthonormal basis for the space of square-integrable functions on the sphere. A key property is that the inner product of two spherical harmonics of the same degree \\(l\\) depends only on the angle between the points \\(x\\) and \\(y\\) on the sphere. Specifically, for unit vectors \\(x, y \\in S^{d-1}\\), we have:\n\n\\[\n\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = P_l(\\langle x, y \\rangle),\n\\]\n\nwhere \\(P_l\\) is the Legendre polynomial of degree \\(l\\) (in the case of \\(S^2\\)) or more generally, the Gegenbauer polynomial for higher-dimensional spheres. The Legendre/Gegenbauer polynomials are functions that depend solely on the inner product \\(\\langle x, y \\rangle\\).\n\n### 2. **Geometric Interpretation**\nThe inner product \\(\\langle x, y \\rangle\\) between two unit vectors \\(x\\) and \\(y\\) is equal to the cosine of the angle \\(\\theta\\) between them:\n\n\\[\n\\langle x, y \\rangle = \\cos \\theta, \\quad \\theta = \\arccos \\langle x, y \\rangle.\n\\]\n\nThus, the angle \\(\\theta\\) is uniquely determined by \\(\\langle x, y \\rangle\\), and vice versa, since \\(\\cos\\) is bijective on \\([0, \\pi]\\).\n\n### 3. **Bijection Between Inner Products and Angles**\nGiven the above, the inner product \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = P_l(\\langle x, y \\rangle)\\) is a function of \\(\\langle x, y \\rangle\\), which in turn is a function of \\(\\theta = \\arccos \\langle x, y \\rangle\\). \n\nThe Legendre/Gegenbauer polynomial \\(P_l\\) is a polynomial of degree \\(l\\), and for fixed \\(l\\), it is a strictly monotonic function on \\([-1, 1]\\) for certain ranges (e.g., \\(P_l\\) is strictly increasing on \\([0, 1]\\) for even \\(l\\) and strictly decreasing for odd \\(l\\) on some intervals). This monotonicity ensures that \\(P_l\\) is injective (one-to-one) on the relevant domain, meaning that each value of \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle\\) corresponds to exactly one value of \\(\\langle x, y \\rangle\\), and hence to exactly one angle \\(\\theta = \\arccos \\langle x, y \\rangle\\).\n\n### 4. **Conclusion**\nTherefore, there exists a bijection between:\n- The set of inner products \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle\\), and \n- The set of angles \\(\\{\\arccos \\langle x, y \\rangle\\}\\), \n\nbecause:\n1. \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle\\) is a function of \\(\\langle x, y \\rangle\\) via \\(P_l\\).\n2. \\(\\langle x, y \\rangle\\) is a bijective function of \\(\\theta = \\arccos \\langle x, y \\rangle\\).\n3. The composition of these relationships preserves the bijection, as \\(P_l\\) is injective on the relevant domain.\n\nThis bijection is mathematically expressed as:\n\n\\[\n\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = P_l(\\cos \\theta), \\quad \\theta = \\arccos \\langle x, y \\rangle,\n\\]\n\nwhere \\(P_l\\) is invertible on the appropriate interval, ensuring the bijection.", "ground-truth": "A"}
{"paper": "arxiv:2410.11443", "question": "Why replacing the integral in \"Point-wise spherical non-linearity\" with a discrete approximation causes the model to degenerate into qusai-equivariant? The fomula of \"Point-wise spherical non-linearity\" is shown as follows:\n$$\nx\\mapsto\\int Y^{(l)}(r)\\cdot \texttt{MLP}(x^\top r)\\,\\mathrm{d}r.\n$$", "A": "To understand why replacing the integral in the \"Point-wise spherical non-linearity\" with a discrete approximation causes the model to degenerate into quasi-equivariant, let's analyze the mathematical structure and implications of this approximation.\n\n### Original Formulation\nThe \"Point-wise spherical non-linearity\" is defined as:\n$$\nx \\mapsto \\int Y^{(l)}(r) \\cdot \\texttt{MLP}(x^\\top r) \\, \\mathrm{d}r,\n$$\nwhere:\n- \\( Y^{(l)}(r) \\) is the spherical harmonic of degree \\( l \\) evaluated at \\( r \\),\n- \\( \\texttt{MLP} \\) is a multi-layer perceptron,\n- \\( r \\) is a unit vector on the sphere \\( S^2 \\),\n- The integral is taken over the entire sphere \\( S^2 \\).\n\nThis operation is designed to be equivariant under rotations because the spherical harmonics \\( Y^{(l)}(r) \\) transform in a well-defined way under rotations, and the integral averages over all directions \\( r \\), preserving the equivariance.\n\n### Discrete Approximation\nWhen the integral is replaced with a discrete approximation, the operation becomes:\n$$\nx \\mapsto \\sum_{i=1}^N w_i Y^{(l)}(r_i) \\cdot \\texttt{MLP}(x^\\top r_i),\n$$\nwhere \\( \\{r_i\\}_{i=1}^N \\) is a finite set of points on \\( S^2 \\), and \\( w_i \\) are the corresponding quadrature weights.\n\n### Degeneration into Quasi-Equivariance\nThe discrete approximation breaks the exact equivariance for the following reasons:\n\n1. **Finite Sampling**: The integral over \\( S^2 \\) is replaced by a finite sum, which cannot perfectly capture the continuous symmetry of the sphere. The discrete set of points \\( \\{r_i\\} \\) does not transform in the same way as the continuous sphere under arbitrary rotations. \n\n2. **Aliasing Effects**: For a finite \\( N \\), the discrete approximation may introduce aliasing, where high-frequency components of \\( \\texttt{MLP}(x^\\top r) \\) or \\( Y^{(l)}(r) \\) are not fully resolved. This can lead to artifacts that violate strict equivariance.\n\n3. **Quadrature Error**: The weights \\( w_i \\) are chosen to approximate the integral, but the approximation is not exact. The error in the quadrature rule means that the transformation properties of the spherical harmonics are not perfectly preserved.\n\n4. **Quasi-Equivariance**: The model becomes quasi-equivariant because the discrete approximation approximately preserves equivariance for small or \"typical\" rotations, but fails for arbitrary rotations. The equivariance error depends on the choice of \\( \\{r_i\\} \\) and \\( N \\). For example, if \\( \\{r_i\\} \\) is a highly symmetric set (e.g., vertices of a Platonic solid), the model may retain equivariance for a discrete subgroup of rotations but not for the full \\( SO(3) \\) group.\n\n### Mathematical Intuition\nThe spherical harmonics \\( Y^{(l)}(r) \\) form a complete basis for functions on \\( S^2 \\), and the integral ensures that the transformation properties of \\( Y^{(l)}(r) \\) are preserved under rotations. The discrete approximation effectively truncates this basis, leading to a loss of information and symmetry. The model can only approximate the true equivariant transformation, hence the term \"quasi-equivariant.\"\n\n### Conclusion\nReplacing the integral with a discrete approximation introduces errors in the transformation properties of the spherical harmonics, causing the model to lose strict equivariance and degenerate into quasi-equivariance. The degree of quasi-equivariance depends on the quality of the discrete approximation (e.g., number of points \\( N \\) and their distribution on \\( S^2 \\)). For large \\( N \\) and well-chosen quadrature rules, the model may remain approximately equivariant in practice, but it will never be perfectly equivariant unless the integral is computed exactly.", "B": "When the integral in the \"Point-wise spherical non-linearity\" is replaced with a discrete approximation, the model degenerates into quasi-equivariant due to the loss of **exact rotational symmetry preservation**. Here’s the step-by-step explanation:\n\n---\n\n### 1. **Continuous Integral and Equivariance**\nThe original operation is defined as:\n$$\nx \\mapsto \\int_{S^2} Y^{(l)}(r) \\cdot \\texttt{MLP}(x^\\top r) \\, \\mathrm{d}r,\n$$\nwhere the integral over the sphere \\( S^2 \\) inherently preserves **rotational equivariance**:\n- For a rotation \\( g \\in SO(3) \\), the transformation \\( x \\to gx \\) leads to \\( r \\to g^{-1}r \\) (due to integration measure invariance).\n- Spherical harmonics \\( Y^{(l)}(r) \\) transform as \\( Y^{(l)}(g^{-1}r) = D^{(l)}(g) Y^{(l)}(r) \\), where \\( D^{(l)} \\) is the \\( l \\)-th irreducible representation of \\( SO(3) \\).\n- The integral ensures the output transforms as \\( D^{(l)}(g) \\cdot \\text{result} \\), guaranteeing strict equivariance.\n\n---\n\n### 2. **Discrete Approximation Breaks Symmetry**\nReplacing the integral with a finite sum over sampled directions \\( \\{ r_i \\} \\):\n$$\nx \\mapsto \\sum_{i} w_i Y^{(l)}(r_i) \\cdot \\texttt{MLP}(x^\\top r_i),\n$$\nintroduces two critical issues:\n\n#### (a) **Sampling Bias**\n- Discrete sampling \\( \\{ r_i \\} \\) cannot perfectly cover all orientations on \\( S^2 \\), breaking the **rotational symmetry** of the integral.\n- The weights \\( w_i \\) (e.g., quadrature weights) approximate the measure \\( \\mathrm{d}r \\), but finite samples cannot replicate the continuous symmetry of \\( SO(3) \\).\n\n#### (b) **Aliasing in Harmonic Basis**\n- Spherical harmonics \\( Y^{(l)}(r) \\) form a complete basis for \\( SO(3) \\)-equivariant functions only under continuous integration.\n- Discrete sums cause **aliasing**: higher-frequency components (higher \\( l \\)) are not orthogonalized properly, violating the equivariance condition \\( f(gx) = D^{(l)}(g)f(x) \\).\n\n---\n\n### 3. **Degeneration to Quasi-Equivariance**\n- The discrete approximation preserves equivariance **only for rotations that map sampled points \\( \\{ r_i \\} \\) to themselves** (e.g., finite symmetry groups like polyhedral groups).\n- For general \\( g \\in SO(3) \\), the output transformation \\( D^{(l)}(g) \\) is only approximately satisfied, resulting in **quasi-equivariance** (equivariance up to approximation error).\n\n---\n\n### 4. **Mathematical Intuition**\n- Let \\( \\mathcal{F}(x) = \\int Y^{(l)}(r) \\cdot h(x^\\top r) \\mathrm{d}r \\). For \\( h = \\texttt{MLP} \\), \\( \\mathcal{F}(gx) = D^{(l)}(g)\\mathcal{F}(x) \\) holds due to the Haar measure’s invariance.\n- For discrete \\( \\hat{\\mathcal{F}}(x) = \\sum_i w_i Y^{(l)}(r_i) h(x^\\top r_i) \\), we have:\n  $$\n  \\hat{\\mathcal{F}}(gx) = \\sum_i w_i Y^{(l)}(r_i) h(x^\\top g^{-1}r_i).\n  $$\n  Unless \\( \\{ r_i \\} \\) is closed under \\( g^{-1} \\), this cannot be expressed as \\( D^{(l)}(g)\\hat{\\mathcal{F}}(x) \\), causing equivariance error.\n\n---\n\n### Conclusion\nDiscrete approximation replaces the **symmetry-adapted integral** with a finite sum, which fails to fully align with the irreducible representations of \\( SO(3) \\). This leads to quasi-equivariance, where equivariance holds only approximately or for specific subgroups. Exact equivariance requires the continuous integral’s symmetry preservation, which discrete methods cannot achieve.", "C": "For a spherical integral, the integral region after rotation is consistent with the original one. However, for the approximation of discrete grids, the rotated grid may not be approximate to the original grid, so the equivariance will be destroyed. When the grid is fine enough, the equivariant error generated will be reduced due to the linearity of the function, so the corresponding network is called qusai-equivariant.", "D": "For a spherical integral, the integral region after rotation is consistent with the original one. However, for the approximation of discrete grids, the rotated grid may not be approximate to the original grid, so the equivariance will be destroyed. When the grid is fine enough, the equivariant error generated will be reduced due to the continuity of the function, so the corresponding network is called qusai-equivariant.", "ground-truth": "D"}
{"paper": "Convex Optimization", "question": "For a convex optimization problem:\n\n\\[\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x}) \\quad \\text{subject to} \\quad g_i(\\mathbf{x}) \\leq 0, \\quad h_j(\\mathbf{x}) = 0,\n\\]\n\nwhere \\( g_i(\\mathbf{x}) \\) are convex functions and \\( h_j(\\mathbf{x}) \\) are affine functions, Slater's condition states that if there exists a strictly feasible point \\( \\mathbf{x}_0 \\) such that:\n\n\\[\ng_i(\\mathbf{x}_0) < 0, \\quad \\forall i,\n\\]\n\nthen strong duality holds.\nProve that strong duality holds for the saddle point problem \n\\[\n\\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]", "A": "Consider a convex optimization problem with a primal objective function \\( f(x) \\) subject to constraints:\n\\[\ng_i(x) \\leq 0, \\quad h_j(x) = 0.\n\\]\nThe corresponding augmented Lagrangian function is given by:\n\\[\n\\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = f(x) + \\sum_{i} \\lambda_i g_i(x) + \\sum_{j} \\mu_j h_j(x),\n\\]\nwhere \\( \\lambda \\geq 0 \\) are Lagrange multipliers for the inequality constraints, and \\( \\mu \\) are multipliers for the equality constraints.\n\nBy Slater's condition, strong duality holds if there exists a strictly feasible point \\( x_0 \\) such that:\n\\[\ng_i(x_0) \\leq 0, \\quad \\forall i.\n\\]\nThis ensures that the duality gap is zero, leading to:\n\\[\n\\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]\n\nA point \\( (x^*, \\lambda^*, \\mu^*) \\) is a saddle point of \\( \\mathbb{L}_{\\rho} \\) if:\n\\[\n\\mathbb{L}_{\\rho}(x^*, \\lambda, \\mu) \\leq \\mathbb{L}_{\\rho}(x^*, \\lambda^*, \\mu^*) \\leq \\mathbb{L}_{\\rho}(x, \\lambda^*, \\mu^*),\n\\]\nfor all \\( x, \\lambda, \\mu \\). This implies:\n\\[\n\\min_x \\mathbb{L}_{\\rho}(x, \\lambda^*, \\mu^*) = \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x^*, \\lambda, \\mu).\n\\]\n\nSince Slater's condition ensures the existence of a saddle point, we conclude that strong duality holds:\n\\[\n\\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]", "B": "To prove that strong duality holds for the saddle point problem under Slater's condition, we will follow these steps:\n\n1. **Define the Lagrangian:**\n   The Lagrangian for the convex optimization problem is given by:\n   \\[\n   \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = f(\\mathbf{x}) + \\sum_{i} \\lambda_i g_i(\\mathbf{x}) + \\sum_{j} \\mu_j h_j(\\mathbf{x}),\n   \\]\n   where \\(\\boldsymbol{\\lambda} \\geq \\mathbf{0}\\) and \\(\\boldsymbol{\\mu}\\) are the Lagrange multipliers for the inequality and equality constraints, respectively.\n\n2. **Primal and Dual Problems:**\n   - The primal problem is:\n     \\[\n     p^* = \\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}).\n     \\]\n   - The dual problem is:\n     \\[\n     d^* = \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}).\n     \\]\n   By weak duality, we always have \\(d^* \\leq p^*\\). Strong duality holds if \\(d^* = p^*\\).\n\n3. **Slater's Condition Implies Strong Duality:**\n   Under Slater's condition, there exists a strictly feasible point \\(\\mathbf{x}_0\\) such that:\n   \\[\n   g_i(\\mathbf{x}_0) < 0 \\quad \\forall i, \\quad \\text{and} \\quad h_j(\\mathbf{x}_0) = 0 \\quad \\forall j.\n   \\]\n   This ensures that the problem satisfies the conditions for strong duality in convex optimization.\n\n4. **Saddle Point Interpretation:**\n   The saddle point problem is:\n   \\[\n   \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = \\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}).\n   \\]\n   The equality holds because:\n   - The Lagrangian \\(\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})\\) is convex in \\(\\mathbf{x}\\) (since \\(f\\) and \\(g_i\\) are convex, and \\(h_j\\) are affine) and concave (linear) in \\((\\boldsymbol{\\lambda}, \\boldsymbol{\\mu})\\).\n   - Under Slater's condition, the primal and dual problems have the same optimal value, i.e., \\(p^* = d^*\\), which implies the existence of a saddle point \\((\\mathbf{x}^*, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*)\\) such that:\n     \\[\n     \\mathcal{L}(\\mathbf{x}^*, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) \\leq \\mathcal{L}(\\mathbf{x}^*, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*) \\leq \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*) \\quad \\forall \\mathbf{x}, \\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}.\n     \\]\n   - This saddle point property ensures that:\n     \\[\n     \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = \\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}).\n     \\]\n\n5. **Conclusion:**\n   Therefore, under Slater's condition, strong duality holds for the saddle point problem, and the optimal values of the primal and dual problems coincide. The saddle point equality is satisfied, ensuring that:\n   \\[\n   \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = \\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq \\mathbf{0}, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}).\n   \\]\n\nThis completes the proof.", "C": "Consider a convex optimization problem with a primal objective function \\( f(x) \\) subject to constraints:\n\\[\ng_i(x) \\leq 0, \\quad h_j(x) = 0.\n\\]\nThe corresponding augmented Lagrangian function is given by:\n\\[\n\\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = f(x) + \\sum_{i} \\lambda_i g_i(x) + \\sum_{j} \\mu_j h_j(x),\n\\]\nwhere \\( \\lambda \\geq 0 \\) are Lagrange multipliers for the inequality constraints, and \\( \\mu \\) are multipliers for the equality constraints.\n\nBy Slater's condition, strong duality holds if there exists a strictly feasible point \\( x_0 \\) such that:\n\\[\ng_i(x_0) < 0, \\quad \\forall i.\n\\]\nThis ensures that the duality gap is zero, leading to:\n\\[\n\\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]\n\nA point \\( (x^*, \\lambda^*, \\mu^*) \\) is a saddle point of \\( \\mathbb{L}_{\\rho} \\) if:\n\\[\n\\mathbb{L}_{\\rho}(x^*, \\lambda, \\mu) \\leq \\mathbb{L}_{\\rho}(x^*, \\lambda^*, \\mu^*) \\leq \\mathbb{L}_{\\rho}(x, \\lambda^*, \\mu^*),\n\\]\nfor all \\( x, \\lambda, \\mu \\). This implies:\n\\[\n\\min_x \\mathbb{L}_{\\rho}(x, \\lambda^*, \\mu^*) = \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x^*, \\lambda, \\mu).\n\\]\n\nSince Slater's condition ensures the existence of a saddle point, we conclude that strong duality holds:\n\\[\n\\max_{\\lambda, \\mu} \\min_{x} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu) = \\min_{x} \\max_{\\lambda, \\mu} \\mathbb{L}_{\\rho}(x, \\lambda, \\mu).\n\\]", "D": "To prove that strong duality holds for the saddle point problem under Slater's condition, we analyze the primal and dual formulations of the convex optimization problem and leverage Lagrangian duality.\n\n### Primal and Dual Problems\n**Primal Problem (P):**\n\\[\np^* = \\min_{\\mathbf{x}} f(\\mathbf{x}) \\quad \\text{s.t.} \\quad g_i(\\mathbf{x}) \\leq 0, \\quad h_j(\\mathbf{x}) = 0.\n\\]\n**Dual Problem (D):**\n\\[\nd^* = \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}),\n\\]\nwhere \\(\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = f(\\mathbf{x}) + \\sum_i \\lambda_i g_i(\\mathbf{x}) + \\sum_j \\mu_j h_j(\\mathbf{x})\\) is the Lagrangian.\n\n### Key Steps\n1. **Slater’s Condition**:  \n   There exists a strictly feasible \\(\\mathbf{x}_0\\) such that:\n   \\[\n   g_i(\\mathbf{x}_0) < 0 \\quad (\\forall i), \\quad h_j(\\mathbf{x}_0) = 0 \\quad (\\forall j).\n   \\]\n   This ensures:\n   - The primal problem is convex with non-empty relative interior.\n   - **Strong duality**: \\(p^* = d^*\\).\n\n2. **Saddle Point Interpretation**:  \n   A point \\((\\mathbf{x}^*, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*)\\) is a saddle point of \\(\\mathcal{L}\\) if:\n   \\[\n   \\mathcal{L}(\\mathbf{x}^*, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) \\leq \\mathcal{L}(\\mathbf{x}^*, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*) \\leq \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*), \\quad \\forall \\mathbf{x}, \\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}.\n   \\]\n   This implies:\n   \\[\n   \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = \\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}).\n   \\]\n\n3. **Equivalence via Strong Duality**:\n   - **Left-hand side (LHS)**:  \n     \\(\\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = d^*\\) (by definition of the dual problem).\n   - **Right-hand side (RHS)**:  \n     \\(\\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})\\) evaluates to \\(p^*\\).  \n     *Reason*: For any \\(\\mathbf{x}\\):\n     - If \\(\\mathbf{x}\\) is primal-feasible, \\(\\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = f(\\mathbf{x})\\) (since \\(\\lambda_i g_i(\\mathbf{x}) \\leq 0\\) and equality holds when \\(\\lambda_i = 0\\)).\n     - If \\(\\mathbf{x}\\) is infeasible, \\(\\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = +\\infty\\) (violated constraints are penalized by \\(\\lambda_i \\to \\infty\\)).\n     Thus, \\(\\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = p^*\\).\n\n4. **Conclusion**:  \n   By Slater’s condition, \\(p^* = d^*\\). Therefore:\n   \\[\n   \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\min_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = \\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda} \\geq 0, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}),\n   \\]\n   proving strong duality for the saddle point problem.", "ground-truth": "C"}
{"paper": "Convex Optimization", "question": "Let $X$ is a random variable defined on a probability space $(\\Omega, \\mathcal{F}, P)$ such that $\\mathbb{E}[X]$ exists. Suppose $f: \\mathbb{R} \\to \\mathbb{R}$ be a convex function:\n\\begin{equation*}\n    f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y).\n\\end{equation*}\nThen, Jensen's inequality states:\n\\begin{equation*}\n    f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n\\end{equation*}", "A": "Certainly! Below is the proof of **Jensen's Inequality** for a convex function \\( f \\) and a random variable \\( X \\) with finite expectation \\( \\mathbb{E}[X] \\).\n\n---\n\n### **Proof of Jensen's Inequality**\n\n1. **Convexity Definition**:  \n   A function \\( f: \\mathbb{R} \\to \\mathbb{R} \\) is convex if for all \\( x, y \\in \\mathbb{R} \\) and \\( \\lambda \\in [0, 1] \\),\n   \\[\n   f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y).\n   \\]\n   This implies that the graph of \\( f \\) lies below its chords.\n\n2. **Supporting Hyperplane (Subgradient)**:  \n   For a convex function \\( f \\), at any point \\( a \\in \\mathbb{R} \\), there exists a **subgradient** \\( m \\in \\mathbb{R} \\) such that for all \\( x \\in \\mathbb{R} \\),\n   \\[\n   f(x) \\geq f(a) + m (x - a).\n   \\]\n   This is a linear lower bound (supporting hyperplane) to \\( f \\) at \\( a \\).\n\n3. **Apply to \\( \\mathbb{E}[X] \\)**:\n   Let \\( a = \\mathbb{E}[X] \\). Then, there exists \\( m \\) such that:\n   \\[\n   f(X) \\geq f(\\mathbb{E}[X]) + m (X - \\mathbb{E}[X]).\n   \\]\n\n4. **Take Expectations**:  \n   Taking expectations on both sides (and using linearity of expectation):\n   \\[\n   \\mathbb{E}[f(X)] \\geq \\mathbb{E}\\left[ f(\\mathbb{E}[X]) + m (X - \\mathbb{E}[X]) \\right].\n   \\]\n   Since \\( f(\\mathbb{E}[X]) \\) and \\( m \\) are constants,\n   \\[\n   \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + m \\left( \\mathbb{E}[X] - \\mathbb{E}[X] \\right).\n   \\]\n   The second term vanishes, yielding:\n   \\[\n   \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]).\n   \\]\n\n5. **Conclusion**:  \n   Thus, we have proved **Jensen's Inequality**:\n   \\[\n   f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n   \\]\n\n---\n\n### **Remarks**\n- If \\( f \\) is **strictly convex**, equality holds **only if \\( X \\) is deterministic** (i.e., \\( X \\) is a constant almost surely).\n- The proof extends to **conditional expectations** and **higher dimensions** (for vector-valued \\( X \\) and convex \\( f: \\mathbb{R}^n \\to \\mathbb{R} \\)).\n- This inequality is fundamental in probability, statistics, and optimization, often used to derive bounds (e.g., in information theory with \\( f(x) = -\\log x \\)).\n\nWould you like further elaboration on any step?", "B": "Since $f$ is convex, it satisfies the following property for any $a \\in \\mathbb{R}$:\n\\begin{equation*}\n    f(x) \\geq f(a) + g(x-a), \\quad \\forall x \\in \\mathbb{R},\n\\end{equation*}\nwhere $g$ is a subgradient of $f$ at $a$. \n\nIf $f$ is differentiable, we can take $g = f'(a)$, giving:\n\\begin{equation*}\n    f(x) \\geq f(a) + f'(a) (x - a), \\quad \\forall x \\in \\mathbb{R}.\n\\end{equation*}\n\nSetting $a = \\mathbb{E}[X]$ and taking expectations on both sides:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq \\mathbb{E}[f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) (X - \\mathbb{E}[X])].\n\\end{equation*}\n\nUsing linearity of expectation:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) \\mathbb{E}[X - \\mathbb{E}[X]].\n\\end{equation*}\n\nSince $\\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X]$, the last term becomes $f'(\\mathbb{E}[X]) \\mathbb{E}[X]$, yielding:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) \\mathbb{E}[X].\n\\end{equation*}\n\nIf $f$ is not differentiable, we use the definition of subgradients: there exists a subgradient $g$ such that:\n\\begin{equation*}\n    f(x) \\geq f(a) + g(x - a), \\quad \\forall x.\n\\end{equation*}\n\nTaking expectations:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq \\mathbb{E}[f(\\mathbb{E}[X]) + g (X - \\mathbb{E}[X])].\n\\end{equation*}\n\nSince $\\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X]$, we obtain:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + g \\mathbb{E}[X].\n\\end{equation*}", "C": "To prove Jensen's inequality for a convex function \\( f: \\mathbb{R} \\to \\mathbb{R} \\) and a random variable \\( X \\) with \\( \\mathbb{E}[X] \\) existing, we proceed as follows:\n\n**Key Steps of the Proof:**\n\n1. **Convexity and Supporting Hyperplane:**  \n   Since \\( f \\) is convex, for \\( x_0 = \\mathbb{E}[X] \\), there exists a subgradient (slope) \\( a \\in \\mathbb{R} \\) such that:\n   \\[\n   f(x) \\geq f(x_0) + a(x - x_0) \\quad \\forall x \\in \\mathbb{R}.\n   \\]\n   This inequality represents an affine function \\( L(x) = f(x_0) + a(x - x_0) \\) that \"supports\" \\( f \\) at \\( x_0 \\).\n\n2. **Substitute \\( X \\) into the Inequality:**  \n   Apply the above inequality to the random variable \\( X \\):\n   \\[\n   f(X) \\geq f(\\mathbb{E}[X]) + a(X - \\mathbb{E}[X]).\n   \\]\n\n3. **Take Expectation on Both Sides:**  \n   Using the linearity of expectation:\n   \\[\n   \\mathbb{E}[f(X)] \\geq \\mathbb{E}\\left[ f(\\mathbb{E}[X]) + a(X - \\mathbb{E}[X]) \\right].\n   \\]\n   Simplify the right-hand side:\n   \\[\n   \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + a \\cdot \\mathbb{E}[X - \\mathbb{E}[X]] = f(\\mathbb{E}[X]) + a \\cdot 0 = f(\\mathbb{E}[X]).\n   \\]\n\n4. **Conclusion:**  \n   Thus, Jensen's inequality holds:\n   \\[\n   f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n   \\]\n\n**Summary:**  \nThe existence of a subgradient (supporting affine function) at \\( \\mathbb{E}[X] \\) ensures that the inequality propagates through expectation, leveraging the linearity of \\( \\mathbb{E} \\). This demonstrates how convexity guarantees \\( f(\\mathbb{E}[X]) \\) is dominated by \\( \\mathbb{E}[f(X)] \\).", "D": "Since $f$ is convex, it satisfies the following property for any $a \\in \\mathbb{R}$:\n\\begin{equation*}\n    f(x) \\geq f(a) + g(x-a), \\quad \\forall x \\in \\mathbb{R},\n\\end{equation*}\nwhere $g$ is a subgradient of $f$ at $a$. \n\nIf $f$ is differentiable, we can take $g = f'(a)$, giving:\n\\begin{equation*}\n    f(x) \\geq f(a) + f'(a) (x - a), \\quad \\forall x \\in \\mathbb{R}.\n\\end{equation*}\n\nSetting $a = \\mathbb{E}[X]$ and taking expectations on both sides:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq \\mathbb{E}[f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) (X - \\mathbb{E}[X])].\n\\end{equation*}\n\nUsing linearity of expectation:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) + f'(\\mathbb{E}[X]) \\mathbb{E}[X - \\mathbb{E}[X]].\n\\end{equation*}\n\nSince $\\mathbb{E}[X - \\mathbb{E}[X]] = 0$, the last term vanishes, yielding:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]).\n\\end{equation*}\n\nIf $f$ is not differentiable, we use the definition of subgradients: there exists a subgradient $g$ such that:\n\\begin{equation*}\n    f(x) \\geq f(a) + g(x - a), \\quad \\forall x.\n\\end{equation*}\n\nTaking expectations:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq \\mathbb{E}[f(\\mathbb{E}[X]) + g (X - \\mathbb{E}[X])].\n\\end{equation*}\n\nSince $\\mathbb{E}[X - \\mathbb{E}[X]] = 0$, we again obtain:\n\\begin{equation*}\n    \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]).\n\\end{equation*}", "ground-truth": "D"}
{"paper": "Real Analysis: Modern Techniques and Their Applications", "question": "For $a, b \\geq 0$ and conjugate exponents $p, q > 1$ satisfying  \\[\\frac{1}{p} + \\frac{1}{q} = 1.\\] , \nYoung's inequality states:\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]", "A": "Certainly! Here is the proof of **Young's Inequality** for \\( a, b \\geq 0 \\) and conjugate exponents \\( p, q > 1 \\) satisfying \\( \\frac{1}{p} + \\frac{1}{q} = 1 \\):\n\n### **Proof:**\n\nWe aim to show:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\n\n#### **Step 1: Use the Concavity of the Logarithm Function**\nThe natural logarithm \\( \\ln(x) \\) is a **concave** function, meaning that for any \\( x, y > 0 \\) and \\( \\lambda \\in [0,1] \\), the following holds:\n\\[\n\\ln(\\lambda x + (1 - \\lambda)y) \\geq \\lambda \\ln(x) + (1 - \\lambda) \\ln(y).\n\\]\n\n#### **Step 2: Apply the Logarithm to the Terms**\nLet \\( \\lambda = \\frac{1}{p} \\) and \\( 1 - \\lambda = \\frac{1}{q} \\). Then, for \\( x = a^p \\) and \\( y = b^q \\), we have:\n\\[\n\\ln\\left( \\frac{a^p}{p} + \\frac{b^q}{q} \\right) \\geq \\frac{1}{p} \\ln(a^p) + \\frac{1}{q} \\ln(b^q).\n\\]\n\nSimplifying the right-hand side:\n\\[\n\\ln\\left( \\frac{a^p}{p} + \\frac{b^q}{q} \\right) \\geq \\ln(a) + \\ln(b) = \\ln(ab).\n\\]\n\n#### **Step 3: Exponentiate Both Sides**\nSince the exponential function is strictly increasing, we can exponentiate both sides to remove the logarithm:\n\\[\n\\frac{a^p}{p} + \\frac{b^q}{q} \\geq ab.\n\\]\n\n#### **Step 4: Equality Condition**\nEquality holds if and only if \\( a^p = b^q \\), which follows from the strict concavity of the logarithm.\n\n### **Conclusion:**\nThus, we have proven **Young's Inequality**:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\n\n\\boxed{ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}}", "B": "Consider the function $\\Phi(t) = \\frac{t^p}{p}$ for $t \\geq 0$, which is convex. By the supporting line property of convex functions,\n\\[\n    \\Phi(b) \\geq \\Phi(a) + \\Phi'(a)(b - a).\n\\]\n\nSetting $\\Phi'(a) = a^{p-1}$, we obtain\n\\[\n    \\frac{b^p}{p} \\geq \\frac{a^p}{p} + a^{p-1} (b - a).\n\\]\n\nRearranging gives\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^p}{q},\n\\]", "C": "Consider the function $\\Phi(t) = \\frac{t^p}{p}$ for $t \\geq 0$, which is convex. By the supporting line property of convex functions,\n\\[\n    \\Phi(b) \\geq \\Phi(a) + \\Phi'(a)(b - a).\n\\]\n\nSetting $\\Phi'(a) = a^{p-1}$, we obtain\n\\[\n    \\frac{b^p}{p} \\geq \\frac{a^p}{p} + a^{p-1} (b - a).\n\\]\n\nRearranging gives\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]", "D": "To prove Young's inequality for non-negative real numbers \\( a, b \\geq 0 \\) and conjugate exponents \\( p, q > 1 \\) satisfying \\(\\frac{1}{p} + \\frac{1}{q} = 1\\):\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]\nwe present two approaches:\n\n---\n\n**1. Weighted AM-GM Inequality (Convexity Argument):**\n\nThe weighted arithmetic mean-geometric mean (AM-GM) inequality states that for non-negative \\( x, y \\) and weights \\( \\theta, 1-\\theta \\) with \\( \\theta + (1-\\theta) = 1 \\):\n\\[\nx^\\theta y^{1-\\theta} \\leq \\theta x + (1-\\theta)y.\n\\]\nSet \\( \\theta = \\frac{1}{p} \\), \\( 1-\\theta = \\frac{1}{q} \\), \\( x = a^p \\), and \\( y = b^q \\). Substituting these into the inequality:\n\\[\n(a^p)^{\\frac{1}{p}} (b^q)^{\\frac{1}{q}} \\leq \\frac{1}{p}a^p + \\frac{1}{q}b^q.\n\\]\nSimplifying the left-hand side:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\nEquality holds when \\( a^p = b^q \\), i.e., \\( a = b^{q/p} \\).\n\n---\n\n**2. Calculus Approach (Critical Point Analysis):**\n\nDefine \\( f(a) = \\frac{a^p}{p} + \\frac{b^q}{q} - ab \\). To show \\( f(a) \\geq 0 \\) for all \\( a \\geq 0 \\):\n- Compute the first derivative: \\( f'(a) = a^{p-1} - b \\).\n- Set \\( f'(a) = 0 \\): \\( a^{p-1} = b \\) ⟹ \\( a = b^{\\frac{1}{p-1}} \\).\n- Substitute \\( a = b^{\\frac{1}{p-1}} \\) into \\( f(a) \\):\n\\[\nf\\left(b^{\\frac{1}{p-1}}\\right) = \\frac{\\left(b^{\\frac{1}{p-1}}\\right)^p}{p} + \\frac{b^q}{q} - b^{\\frac{1}{p-1}} \\cdot b.\n\\]\nSimplify using \\( q = \\frac{p}{p-1} \\):\n\\[\n\\frac{b^q}{p} + \\frac{b^q}{q} - b^q = b^q\\left(\\frac{1}{p} + \\frac{1}{q} - 1\\right) = 0.\n\\]\n- Verify the second derivative \\( f''(a) = (p-1)a^{p-2} > 0 \\) (since \\( p > 1 \\)), confirming a minimum at \\( a = b^{\\frac{1}{p-1}} \\). Thus, \\( f(a) \\geq 0 \\) for all \\( a \\geq 0 \\).\n\n---\n\n**Conclusion:** Young's inequality holds with equality if and only if \\( a^p = b^q \\). Both methods confirm the validity of the inequality through convexity and optimization principles.", "ground-truth": "C"}
{"paper": "Real Analysis: Modern Techniques and Their Applications", "question": "For any measurable functions $f$ and $g$ on a measure space $(X, \\mu)$, let $p, q > 1$ be conjugate exponents, meaning they satisfy \n \\[\\frac{1}{p} + \\frac{1}{q} = 1.\\] \n Holder's inequality states:\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\left( \\int_X |g(x)|^q d\\mu(x) \\right)^{\\frac{1}{q}}.\n\\]", "A": "**Proof of Hölder's Inequality:**\n\nWe will prove Hölder's inequality for measurable functions \\( f \\) and \\( g \\) on a measure space \\( (X, \\mu) \\), where \\( p, q > 1 \\) are conjugate exponents, i.e., \\( \\frac{1}{p} + \\frac{1}{q} = 1 \\).\n\n### Step 1: Reduction to the Case Where \\( \\|f\\|_p = \\|g\\|_q = 1 \\)\n\nFirst, observe that if either \\( \\|f\\|_p = 0 \\) or \\( \\|g\\|_q = 0 \\), then \\( f \\) or \\( g \\) is zero almost everywhere, and the inequality holds trivially. Thus, assume \\( \\|f\\|_p > 0 \\) and \\( \\|g\\|_q > 0 \\). Define the normalized functions:\n\\[\n\\tilde{f} = \\frac{f}{\\|f\\|_p}, \\quad \\tilde{g} = \\frac{g}{\\|g\\|_q}.\n\\]\nThen \\( \\|\\tilde{f}\\|_p = \\|\\tilde{g}\\|_q = 1 \\). If we can show that\n\\[\n\\int_X |\\tilde{f}(x) \\tilde{g}(x)| d\\mu(x) \\leq 1,\n\\]\nthen multiplying both sides by \\( \\|f\\|_p \\|g\\|_q \\) yields the desired inequality:\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\|f\\|_p \\|g\\|_q.\n\\]\nThus, it suffices to prove the inequality under the assumption \\( \\|f\\|_p = \\|g\\|_q = 1 \\).\n\n### Step 2: Application of Young's Inequality\n\nFor any \\( a, b \\geq 0 \\), Young's inequality states:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\nApply this to \\( a = |f(x)| \\) and \\( b = |g(x)| \\):\n\\[\n|f(x) g(x)| \\leq \\frac{|f(x)|^p}{p} + \\frac{|g(x)|^q}{q}.\n\\]\nIntegrate both sides over \\( X \\):\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} \\int_X |f(x)|^p d\\mu(x) + \\frac{1}{q} \\int_X |g(x)|^q d\\mu(x).\n\\]\nBy our assumption \\( \\|f\\|_p = \\|g\\|_q = 1 \\), this simplifies to:\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\nThis completes the proof under the normalization \\( \\|f\\|_p = \\|g\\|_q = 1 \\).\n\n### Step 3: General Case\n\nFor general \\( f \\) and \\( g \\), as shown in Step 1, we can normalize and then multiply by \\( \\|f\\|_p \\|g\\|_q \\) to obtain:\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\left( \\int_X |g(x)|^q d\\mu(x) \\right)^{\\frac{1}{q}}.\n\\]\nThis is Hölder's inequality. \\(\\square\\)", "B": "Define the function\n\\[\n    h(x) = \\frac{|f(x)|}{\\|f\\|_p} \\quad \\text{and} \\quad k(x) = \\frac{|g(x)|}{\\|g\\|_q}.\n\\]\n\nBy Young's inequality, which states that for nonnegative $a, b$,\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]\n\nwe apply this to $h(x)^p$ and $k(x)^q$:\n\\[\n    |f(x) g(x)| \\leq \\frac{|f(x)|^p}{p \\|f\\|_p^p} + \\frac{|g(x)|^q}{q \\|g\\|_q^q}.\n\\]\n\nIntegrating both sides over $X$ gives\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\nMultiplying both sides by $\\|f\\|_p \\|g\\|_q$ completes the proof.\n", "C": "Define the function\n\\[\n    h(x) = \\frac{|f(x)|}{\\|f\\|_p} \\quad \\text{and} \\quad k(x) = \\frac{|g(x)|}{\\|g\\|_q}.\n\\]\n\nBy Young's inequality, which states that for nonnegative $a, b$,\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]\n\nwe apply this to $h(x)$ and $k(x)$:\n\\[\n    |f(x) g(x)| \\leq \\frac{|f(x)|^p}{p \\|f\\|_p^p} + \\frac{|g(x)|^q}{q \\|g\\|_q^q}.\n\\]\n\nIntegrating both sides over $X$ gives\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\nMultiplying both sides by $\\|f\\|_p \\|g\\|_q$ completes the proof.", "D": "To prove Hölder's inequality, we proceed as follows:\n\n**Step 1: Handle Trivial Cases**  \nIf \\(\\|f\\|_p = 0\\) or \\(\\|g\\|_q = 0\\), then \\(f\\) or \\(g\\) vanishes \\(\\mu\\)-almost everywhere, making the left-hand side zero. The inequality holds trivially. Hence, assume \\(\\|f\\|_p > 0\\) and \\(\\|g\\|_q > 0\\).\n\n**Step 2: Normalize \\(f\\) and \\(g\\)**  \nDefine normalized functions:  \n\\[\nF(x) = \\frac{f(x)}{\\|f\\|_p}, \\quad G(x) = \\frac{g(x)}{\\|g\\|_q},\n\\]  \nso that \\(\\|F\\|_p = 1\\) and \\(\\|G\\|_q = 1\\). The inequality reduces to:  \n\\[\n\\int_X |F(x)G(x)| \\, d\\mu(x) \\leq 1.\n\\]\n\n**Step 3: Apply Young's Inequality**  \nFor \\(a, b \\geq 0\\), Young's inequality states:  \n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]  \nSubstitute \\(a = |F(x)|\\) and \\(b = |G(x)|\\):  \n\\[\n|F(x)G(x)| \\leq \\frac{|F(x)|^p}{p} + \\frac{|G(x)|^q}{q}.\n\\]\n\n**Step 4: Integrate Both Sides**  \nIntegrate over \\(X\\):  \n\\[\n\\int_X |F(x)G(x)| \\, d\\mu(x) \\leq \\frac{1}{p} \\int_X |F(x)|^p \\, d\\mu(x) + \\frac{1}{q} \\int_X |G(x)|^q \\, d\\mu(x).\n\\]  \nSince \\(\\|F\\|_p = 1\\) and \\(\\|G\\|_q = 1\\), we have:  \n\\[\n\\int_X |F(x)|^p \\, d\\mu(x) = 1, \\quad \\int_X |G(x)|^q \\, d\\mu(x) = 1.\n\\]  \nThus:  \n\\[\n\\int_X |F(x)G(x)| \\, d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\n**Step 5: Recover Original Inequality**  \nMultiplying back by \\(\\|f\\|_p \\|g\\|_q\\) gives:  \n\\[\n\\int_X |f(x)g(x)| \\, d\\mu(x) \\leq \\|f\\|_p \\|g\\|_q.\n\\]  \nThis completes the proof of Hölder's inequality. \n\n**Key Points:**  \n- The normalization ensures the application of Young's inequality.  \n- Equality holds in Young’s inequality if and only if \\(a^p = b^q\\), corresponding to \\(|f(x)|^p\\) and \\(|g(x)|^q\\) being proportional \\(\\mu\\)-a.e. in the original inequality.", "ground-truth": "B"}
{"paper": "Convex Optimization", "question": "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, and let $X$ be an integrable random variable, i.e., $\\mathbb{E}[|X|] < \\infty$. The conditional expectation of $X$ given a sub-$\\sigma$-algebra $\\mathcal{G} \\subseteq \\mathcal{F}$, denoted as $\\mathbb{E}[X | \\mathcal{G}]$, is a $\\mathcal{G}$-measurable function satisfying:\n\\[\n    \\int_A \\mathbb{E}[X | \\mathcal{G}] d\\mathbb{P} = \\int_A X d\\mathbb{P}, \\quad \\forall A \\in \\mathcal{G}.\n\\]\n\nThe Law of Total Expectation states that for a random variable $X$ and a random variable $Y$ (or a $\\sigma$-algebra $\\mathcal{F}$), then we have:\n\\[\n    \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]].\n\\]", "A": "Let $X$ be a random variable, and let $Y$ be another random variable (or a $\\sigma$-algebra) on which we are conditioning. The conditional expectation $\\mathbb{E}[X | Y]$ is a random variable that depends on $Y$, and it satisfies:\n\\[\n    \\int_{-\\infty}^{\\infty} \\mathbb{E}[X | Y = y] f_Y(y) \\, dy = \\mathbb{E}[X],\n\\]\nwhere $f_Y(y)$ is the probability density function of $Y$.\n\nBy the definition of conditional expectation, we can rewrite $\\mathbb{E}[X]$ as:\n\\[\n    \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx.\n\\]\n\nWe can now apply the law of total probability, where we first condition on $Y$ and then take the expectation with respect to $Y$. The total expectation is given by:\n\\[\n    \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y = y]],\n\\]", "B": "To prove the **Law of Total Expectation**, we will use the definition of conditional expectation and properties of integrals over measurable sets. Here's the step-by-step derivation:\n\n### Given:\n- A probability space \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\).\n- An integrable random variable \\(X\\) (i.e., \\(\\mathbb{E}[|X|] < \\infty\\)).\n- A sub-\\(\\sigma\\)-algebra \\(\\mathcal{G} \\subseteq \\mathcal{F}\\).\n- The conditional expectation \\(\\mathbb{E}[X | \\mathcal{G}]\\) is a \\(\\mathcal{G}\\)-measurable function satisfying:\n  \\[\n  \\int_A \\mathbb{E}[X | \\mathcal{G}] \\, d\\mathbb{P} = \\int_A X \\, d\\mathbb{P}, \\quad \\forall A \\in \\mathcal{G}.\n  \\]\n\n### To Prove:\n\\[\n\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | \\mathcal{G}]].\n\\]\n\n### Proof:\n1. **Take \\(A = \\Omega\\)**: Since \\(\\mathcal{G}\\) is a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), we have \\(\\Omega \\in \\mathcal{G}\\). Applying the definition of conditional expectation with \\(A = \\Omega\\):\n   \\[\n   \\int_\\Omega \\mathbb{E}[X | \\mathcal{G}] \\, d\\mathbb{P} = \\int_\\Omega X \\, d\\mathbb{P}.\n   \\]\n   \n2. **Express the integrals as expectations**:\n   - The left-hand side is the expectation of \\(\\mathbb{E}[X | \\mathcal{G}]\\):\n     \\[\n     \\int_\\Omega \\mathbb{E}[X | \\mathcal{G}] \\, d\\mathbb{P} = \\mathbb{E}[\\mathbb{E}[X | \\mathcal{G}]].\n     \\]\n   - The right-hand side is the expectation of \\(X\\):\n     \\[\n     \\int_\\Omega X \\, d\\mathbb{P} = \\mathbb{E}[X].\n     \\]\n   \n3. **Combine the results**:\n   \\[\n   \\mathbb{E}[\\mathbb{E}[X | \\mathcal{G}]] = \\mathbb{E}[X].\n   \\]\n   \nThis completes the proof of the Law of Total Expectation.\n\n### Special Case for Random Variable \\(Y\\):\nIf \\(\\mathcal{G} = \\sigma(Y)\\) (the \\(\\sigma\\)-algebra generated by \\(Y\\)), then \\(\\mathbb{E}[X | \\mathcal{G}]\\) is often written as \\(\\mathbb{E}[X | Y]\\). In this case, the Law of Total Expectation becomes:\n\\[\n\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]].\n\\]\n\n### Intuition:\nThe Law of Total Expectation states that the overall expectation of \\(X\\) can be computed by first conditioning on some information (\\(\\mathcal{G}\\) or \\(Y\\)) and then averaging over all possible values of that information. This is a powerful tool in probability theory and statistics for decomposing expectations.", "C": "The Law of Total Expectation, also known as the tower property, follows directly from the definition of conditional expectation. Here is the proof:\n\nLet $\\mathcal{G} = \\sigma(Y)$ be the sub-$\\sigma$-algebra generated by the random variable $Y$. By definition, the conditional expectation $\\mathbb{E}[X | \\mathcal{G}]$ satisfies:\n\\[\n\\int_A \\mathbb{E}[X | \\mathcal{G}] \\, d\\mathbb{P} = \\int_A X \\, d\\mathbb{P}, \\quad \\forall A \\in \\mathcal{G}.\n\\]\n\nTo prove $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | \\mathcal{G}]]$, consider $A = \\Omega \\in \\mathcal{G}$ (since any $\\sigma$-algebra contains the whole space). Substituting $A = \\Omega$ into the definition:\n\\[\n\\int_\\Omega \\mathbb{E}[X | \\mathcal{G}] \\, d\\mathbb{P} = \\int_\\Omega X \\, d\\mathbb{P}.\n\\]\n\nBy the definition of expectation, this simplifies to:\n\\[\n\\mathbb{E}\\left[\\mathbb{E}[X | \\mathcal{G}]\\right] = \\mathbb{E}[X].\n\\]\n\nThus, the Law of Total Expectation $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]]$ is proved. The key step uses the fact that $\\Omega$ belongs to every $\\sigma$-algebra, allowing direct application of the conditional expectation's defining property.", "D": "Let $X$ be a random variable, and let $Y$ be another random variable (or a $\\sigma$-algebra) on which we are conditioning. The conditional expectation $\\mathbb{E}[X | Y]$ is a random variable that depends on $Y$, and it satisfies:\n\\[\n    \\int_{-\\infty}^{\\infty} \\mathbb{E}[X | Y = y] f_Y(y) \\, dy = \\mathbb{E}[X],\n\\]\nwhere $f_Y(y)$ is the probability density function of $Y$.\n\nBy the definition of conditional expectation, we can rewrite $\\mathbb{E}[X]$ as:\n\\[\n    \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx.\n\\]\n\nWe can now apply the law of total probability, where we first condition on $Y$ and then take the expectation with respect to $Y$. The total expectation is given by:\n\\[\n    \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]],\n\\]", "ground-truth": "D"}
{"paper": "Real Analysis: Modern Techniques and Their Applications", "question": "For measurable functions $f$ and $g$ on a measure space $(X, \\mu)$ and for $p \\geq 1$, we have Young's inequality:\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\nthen Minkowski's inequality states holds:\n\\[\n    \\left( \\int_X |f(x) + g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]", "A": "Applying Young's inequality to $|f(x)|$ and $|g(x)|$, we obtain:\n\\[\n    |f(x) + g(x)|^p \\leq (|f(x)| + |g(x)|)^p \\leq 2^{p-1} (|f(x)|^p + |g(x)|^p).\n\\]\nIntegrating both sides gives:\n\\[\n    \\int_X |f(x) + g(x)|^p d\\mu(x) \\leq 2^{p-1} \\left( \\int_X |f(x)|^p d\\mu(x) + \\int_X |g(x)|^p d\\mu(x) \\right).\n\\]\nTaking the $p$-th root completes the proof:\n\\[\n    \\left( \\int_X |f(x) + g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\leq 2^{1 - \\frac{1}{p}} \\left( \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\right).\n\\]", "B": "To prove Minkowski's inequality using Hölder's inequality (which can be derived from Young's inequality), we proceed as follows:\n\n**Minkowski's Inequality**: For \\( p \\geq 1 \\) and measurable functions \\( f, g \\) on a measure space \\( (X, \\mu) \\),\n\\[\n\\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]\n\n**Proof**:\n\n1. **Case \\( p = 1 \\)**:\n   By the triangle inequality for real numbers:\n   \\[\n   \\int_X |f + g| \\, d\\mu \\leq \\int_X (|f| + |g|) \\, d\\mu = \\int_X |f| \\, d\\mu + \\int_X |g| \\, d\\mu.\n   \\]\n   Raising both sides to the power \\( \\frac{1}{1} \\) preserves the inequality.\n\n2. **Case \\( p > 1 \\)**:\n   Let \\( q = \\frac{p}{p - 1} \\) (the Hölder conjugate of \\( p \\)), satisfying \\( \\frac{1}{p} + \\frac{1}{q} = 1 \\).\n\n   Start with:\n   \\[\n   \\int_X |f + g|^p \\, d\\mu = \\int_X |f + g| \\cdot |f + g|^{p - 1} \\, d\\mu.\n   \\]\n   Apply the triangle inequality \\( |f + g| \\leq |f| + |g| \\):\n   \\[\n   \\int_X |f + g|^p \\, d\\mu \\leq \\int_X |f| \\cdot |f + g|^{p - 1} \\, d\\mu + \\int_X |g| \\cdot |f + g|^{p - 1} \\, d\\mu.\n   \\]\n\n3. **Apply Hölder's Inequality**:\n   For each term, use Hölder's inequality \\( \\int |uv| \\, d\\mu \\leq \\|u\\|_p \\|v\\|_q \\):\n   - For \\( \\int |f| \\cdot |f + g|^{p - 1} \\, d\\mu \\):\n     \\[\n     \\int |f| \\cdot |f + g|^{p - 1} \\, d\\mu \\leq \\|f\\|_p \\cdot \\left( \\int |f + g|^{(p - 1)q} \\, d\\mu \\right)^{\\frac{1}{q}}.\n     \\]\n     Since \\( (p - 1)q = p \\), this simplifies to:\n     \\[\n     \\|f\\|_p \\cdot \\left( \\int |f + g|^p \\, d\\mu \\right)^{\\frac{1}{q}}.\n     \\]\n   - Similarly, for \\( \\int |g| \\cdot |f + g|^{p - 1} \\, d\\mu \\):\n     \\[\n     \\int |g| \\cdot |f + g|^{p - 1} \\, d\\mu \\leq \\|g\\|_p \\cdot \\left( \\int |f + g|^p \\, d\\mu \\right)^{\\frac{1}{q}}.\n     \\]\n\n4. **Combine Results**:\n   Substitute back into the earlier inequality:\n   \\[\n   \\int |f + g|^p \\, d\\mu \\leq \\left( \\|f\\|_p + \\|g\\|_p \\right) \\cdot \\left( \\int |f + g|^p \\, d\\mu \\right)^{\\frac{1}{q}}.\n   \\]\n   Divide both sides by \\( \\left( \\int |f + g|^p \\, d\\mu \\right)^{\\frac{1}{q}} \\) (assuming \\( \\int |f + g|^p \\, d\\mu \\neq 0 \\)):\n   \\[\n   \\left( \\int |f + g|^p \\, d\\mu \\right)^{1 - \\frac{1}{q}} \\leq \\|f\\|_p + \\|g\\|_p.\n   \\]\n   Since \\( 1 - \\frac{1}{q} = \\frac{1}{p} \\), we obtain:\n   \\[\n   \\left( \\int |f + g|^p \\, d\\mu \\right)^{\\frac{1}{p}} \\leq \\|f\\|_p + \\|g\\|_p.\n   \\]\n   This completes the proof of Minkowski's inequality. \\(\\square\\)", "C": "Applying Young's inequality to $|f(x)|$ and $|g(x)|$, we obtain:\n\\[\n    |f(x) + g(x)|^p \\leq (|f(x)| + |g(x)|)^p \\leq 2^{p-1} (|f(x)|^p + |g(x)|^p).\n\\]\nIntegrating both sides gives:\n\\[\n    \\int_X |f(x) + g(x)|^p d\\mu(x) \\leq 2^{p-1} \\left( \\int_X |f(x)|^p d\\mu(x) + \\int_X |g(x)|^p d\\mu(x) \\right).\n\\]\nTaking the $p$-th root completes the proof:\n\\[\n    \\left( \\int_X |f(x) + g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]", "D": "To prove Minkowski's inequality using Young's inequality, we proceed as follows:\n\n### Step 1: Apply Young's Inequality\nYoung's inequality states that for \\( a, b \\geq 0 \\) and \\( p, q > 1 \\) with \\( \\frac{1}{p} + \\frac{1}{q} = 1 \\), we have:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\n\n### Step 2: Express \\( |f + g|^p \\) in Terms of \\( |f| \\) and \\( |g| \\)\nConsider the integral \\( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\). We can write:\n\\[\n|f(x) + g(x)|^p \\leq |f(x) + g(x)|^{p-1} \\cdot |f(x)| + |f(x) + g(x)|^{p-1} \\cdot |g(x)|.\n\\]\nIntegrating both sides gives:\n\\[\n\\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\leq \\int_X |f(x) + g(x)|^{p-1} |f(x)| \\, d\\mu(x) + \\int_X |f(x) + g(x)|^{p-1} |g(x)| \\, d\\mu(x).\n\\]\n\n### Step 3: Apply Hölder's Inequality\nLet \\( q = \\frac{p}{p-1} \\) so that \\( \\frac{1}{p} + \\frac{1}{q} = 1 \\). By Hölder's inequality (which follows from Young's inequality), we have:\n\\[\n\\int_X |f(x) + g(x)|^{p-1} |f(x)| \\, d\\mu(x) \\leq \\left( \\int_X |f(x) + g(x)|^{(p-1)q} \\, d\\mu(x) \\right)^{\\frac{1}{q}} \\left( \\int_X |f(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]\nSimplifying \\( (p-1)q = p \\), this becomes:\n\\[\n\\int_X |f(x) + g(x)|^{p-1} |f(x)| \\, d\\mu(x) \\leq \\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{q}} \\left( \\int_X |f(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]\nSimilarly:\n\\[\n\\int_X |f(x) + g(x)|^{p-1} |g(x)| \\, d\\mu(x) \\leq \\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{q}} \\left( \\int_X |g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]\n\n### Step 4: Combine the Results\nSubstituting back, we get:\n\\[\n\\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\leq \\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{q}} \\left[ \\left( \\int_X |f(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} \\right].\n\\]\nAssuming \\( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) > 0 \\), we can divide both sides by \\( \\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{q}} \\) to obtain:\n\\[\n\\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]\nThis completes the proof of Minkowski's inequality. \n\n### Final Answer:\n\\[\n\\left( \\int_X |f(x) + g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} \\leq \\left( \\int_X |f(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}} + \\left( \\int_X |g(x)|^p \\, d\\mu(x) \\right)^{\\frac{1}{p}}.\n\\]", "ground-truth": "C"}
{"paper": "Numerical Optimization", "question": "Let the function \\( f(x) \\) be twice continuously differentiable in a neighborhood of the minimum point \\( x^* \\). Suppose there exist \\(\\varepsilon > 0\\) and \\( M > m > 0 \\) such that when \\(\\|x - x^*\\| < \\varepsilon\\),\n\\[\nm \\|y\\|^2 \\leq y^T G(x) y \\leq M \\|y\\|^2, \\quad \\forall y \\in \\mathbb{R}^n \n\\]\nholds. \n\nAdditionally, given the expansion:\n\\[\nf(x + \\alpha d) = f(x) + \\alpha g^T d + \\alpha^2 \\int_{0}^{1} (1 - t) \\left[ d^T G(x + t\\alpha d) \\right] dt,\n\\]\n\nwe aim to prove:\n\\begin{align*}\n   \\|g(x)\\| \\geq m \\|x - x^*\\|. \n\\end{align*}", "A": "To prove \\(\\|g(x)\\| \\geq m \\|x - x^*\\|\\), we proceed as follows:\n\nSince \\(x^*\\) is a minimum point, \\(g(x^*) = 0\\). For \\(x\\) near \\(x^*\\), consider the integral representation of the gradient:\n\\[\ng(x) = \\int_{0}^{1} G(x^* + t(x - x^*)) (x - x^*) \\, dt = H (x - x^*),\n\\]\nwhere \\(H = \\int_{0}^{1} G(x^* + t(x - x^*)) \\, dt\\). \n\nUnder the given Hessian condition, for any \\(y \\in \\mathbb{R}^n\\) and \\(t \\in [0,1]\\):\n\\[\ny^T G(x^* + t(x - x^*)) y \\geq m \\|y\\|^2.\n\\]\nIntegrating over \\(t\\), we have:\n\\[\ny^T H y = \\int_{0}^{1} y^T G(x^* + t(x - x^*)) y \\, dt \\geq m \\|y\\|^2.\n\\]\nThus, \\(H \\succeq m I\\) (positive semi-definite with eigenvalues \\(\\geq m\\)).\n\nTaking \\(y = x - x^*\\), the quadratic form gives:\n\\[\n(x - x^*)^T H (x - x^*) \\geq m \\|x - x^*\\|^2.\n\\]\nBut \\((x - x^*)^T H (x - x^*) = (x - x^*)^T g(x)\\). By the Cauchy-Schwarz inequality:\n\\[\n(x - x^*)^T g(x) \\leq \\|x - x^*\\| \\|g(x)\\|.\n\\]\nCombining these:\n\\[\nm \\|x - x^*\\|^2 \\leq \\|x - x^*\\| \\|g(x)\\| \\implies \\|g(x)\\| \\geq m \\|x - x^*\\|.\n\\]\nHence, the inequality is proven.", "B": "By the expansion,\n\\begin{align*}\nf(x) - f(x^*) &= g(x^*)^T (x - x^*) +\\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\mathrm{d}t \\\\ \n&= \\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\, \\mathrm{d}t,\n\\end{align*}\n\nPerforming a Taylor expansion on \\( g(x) \\):\n\\[\ng(x) = g(x) - g(x^*) = \\int_0^1 G(tx + (1-t)x^*) (x - x^*) dt.\n\\]\n\nThus, we have:\n\\begin{align*}\n\\|g(x)\\|\\|x - x^*\\| &\\geqslant (x - x^*)^\\top g(x) \\\\\n&= \\int_0^1 (x - x^*)^\\top G(tx + (1 - t)x^*)(x - x^*) \\, \\mathrm{d}t \\\\\n&\\geqslant m \\|x - x^*\\|^2\n\\end{align*}", "C": "To prove the inequality \\(\\|g(x)\\| \\geq m \\|x - x^*\\|\\) under the given conditions, we proceed with the following steps:\n\n### 1. Taylor Expansion Around the Minimum Point \\(x^*\\)\nSince \\(x^*\\) is a minimum point, the gradient \\(g(x^*) = 0\\). Using the second-order Taylor expansion of \\(f(x)\\) around \\(x^*\\), we have:\n\\[\nf(x) = f(x^*) + \\frac{1}{2} (x - x^*)^T G(\\xi) (x - x^*),\n\\]\nwhere \\(\\xi\\) lies on the line segment between \\(x\\) and \\(x^*\\). Taking the gradient of both sides with respect to \\(x\\) yields:\n\\[\ng(x) = G(\\xi)(x - x^*).\n\\]\n\n### 2. Bounding the Hessian \\(G(\\xi)\\)\nGiven the condition on the Hessian \\(G(x)\\) in the neighborhood \\(\\|x - x^*\\| < \\varepsilon\\):\n\\[\nm \\|y\\|^2 \\leq y^T G(x) y \\leq M \\|y\\|^2, \\quad \\forall y \\in \\mathbb{R}^n,\n\\]\nit follows that the eigenvalues of \\(G(\\xi)\\) are bounded below by \\(m\\) and above by \\(M\\). Therefore, the operator norm of \\(G(\\xi)\\) satisfies:\n\\[\n\\|G(\\xi)\\| \\geq m.\n\\]\n\n### 3. Estimating the Norm of the Gradient\nFrom the expression \\(g(x) = G(\\xi)(x - x^*)\\), we take norms on both sides:\n\\[\n\\|g(x)\\| = \\|G(\\xi)(x - x^*)\\|.\n\\]\nUsing the property of the operator norm:\n\\[\n\\|G(\\xi)(x - x^*)\\| \\geq \\|G(\\xi)\\| \\cdot \\|x - x^*\\| \\geq m \\|x - x^*\\|.\n\\]\n\n### 4. Conclusion\nThus, we have established the desired inequality:\n\\[\n\\|g(x)\\| \\geq m \\|x - x^*\\|.\n\\]\n\nThis completes the proof. \\(\\square\\)", "D": "By the expansion,\n\\begin{align*}\nf(x) - f(x^*) &= g(x^*)^T (x - x^*) +\\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\mathrm{d}t \\\\ \n&= \\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\, \\mathrm{d}t,\n\\end{align*}\n\nPerforming a Taylor expansion on \\( g(x) \\):\n\\[\ng(x) = g(x) - g(x^*) = \\int_0^1 G(tx + (1-t)x^*) (x - x^*) dt.\n\\]\n\nThus, we have:\n\\begin{align*}\n\\|g(x)\\|\\|x - x^*\\| &\\geqslant (x - x^*)^\\top g(x) \\\\\n&= \\int_0^1 (x - x^*)^\\top G(tx + (1 - t)x^*)(x - x^*) \\, \\mathrm{d}t \\\\\n&\\geqslant M \\|x - x^*\\|^2\n\\end{align*}", "ground-truth": "B"}
{"paper": "arxiv:2502.16894", "question": "In a parameter-efficient fine-tuning setting, a weight matrix $W\\in\\mathbb{R}^{m\\times n}$ is reparameterized as $W = W_0 + sBA$, where $W_0$ is fixed, $s\\in\\mathbb{R}$ is a scaling factor, and $B\\in\\mathbb{R}^{m\\times r}$, $A\\in\\mathbb{R}^{r\\times n}$ are low-rank matrices. At the $t$-th optimization step, define the equivalent weight as $\\tilde W_t = W_0 + sB_tA_t$, and denote the equivalent gradient as $\\tilde g_t = \\partial L/\\partial \\tilde W_t$. Let the full-tuning gradient at step $t$ be $g_t = \\partial L/\\partial W_t$. Show that\n\n$\\displaystyle \\tilde g_t = s^2\\bigl(B_t B_t^{\\top} g_t + g_t A_t^{\\top} A_t\\bigr).$\n\nHow can this lemma be derived and proved?", "A": "According to the assumption, \\(\\tilde W_t = W_t\\). Let LoRA reparameterize \\(W\\) as \\(W = W_0 + sBA\\) where \\(B\\in\\mathbb R^{m\\times r}\\), \\(A\\in\\mathbb R^{r\\times n}\\), and \\(s\\in\\mathbb R\\). Denote the equivalent weight as\n\n\\[\\tilde W_t = W_{\\mathrm{init}} + s\\,B_t A_t.\\]\n\nThen the gradients of \\(B\\) and \\(A\\) are\n\n\\[G_{B_t} = \\frac{\\partial L}{\\partial \\tilde W_t}\\frac{\\partial \\tilde W_t}{\\partial B} = \\frac{\\partial L}{\\partial W_t}\\frac{\\partial \\tilde W_t}{\\partial B} = s\\,g_t A_t^{\\top},\\tag{30}\\]\n\\[G_{A_t} = \\frac{\\partial L}{\\partial \\tilde W_t}\\frac{\\partial \\tilde W_t}{\\partial A} = \\frac{\\partial L}{\\partial W_t}\\frac{\\partial \\tilde W_t}{\\partial A} = s\\,B_t^{\\top} g_t.\\tag{31}\\]\n\nUnder SGD updates,\n\n\\[dB_t = -\\eta\\,G_{B_t} = -s\\,\\eta\\,g_t A_t^{\\top},\\quad dA_t = -\\eta\\,G_{A_t} = -s\\,\\eta\\,B_t^{\\top} g_t.\\tag{32}\\]\n\nThe change in the equivalent weight is\n\n\\[d\\tilde W = \\frac{\\partial \\tilde W_t}{\\partial A_t}dA_t + \\frac{\\partial \\tilde W_t}{\\partial B_t}dB_t = s\\,B_t\\,dA_t + s\\,dB_t\\,A_t\\]\n\\[= s\\bigl[B_t(-\\eta s B_t^{\\top}g_t) + (-\\eta s g_t A_t^{\\top})A_t\\bigr] = -\\eta s^2\\bigl[B_tB_t^{\\top}g_t - g_tA_t^{\\top}A_t\\bigr].\\tag{36}\\]\n\nTherefore, the equivalent gradient is\n\n\\[\\tilde g_t = s^2\\bigl(B_t B_t^{\\top} g_t - g_t A_t^{\\top} A_t\\bigr).\\tag{37}\\]\n\nThis concludes the proof.", "B": "To derive and prove the given lemma, we will analyze the relationship between the equivalent gradient \\(\\tilde{g}_t\\) and the full-tuning gradient \\(g_t\\) in the context of the reparameterized weight matrix \\(W = W_0 + sBA\\). Here's the step-by-step derivation:\n\n---\n\n### **1. Problem Setup**\nThe weight matrix is reparameterized as:\n\\[\n\\tilde{W}_t = W_0 + sB_tA_t,\n\\]\nwhere:\n- \\(W_0 \\in \\mathbb{R}^{m \\times n}\\) is fixed,\n- \\(s \\in \\mathbb{R}\\) is a scaling factor,\n- \\(B_t \\in \\mathbb{R}^{m \\times r}\\) and \\(A_t \\in \\mathbb{R}^{r \\times n}\\) are low-rank matrices.\n\nThe equivalent gradient is:\n\\[\n\\tilde{g}_t = \\frac{\\partial L}{\\partial \\tilde{W}_t},\n\\]\nand the full-tuning gradient is:\n\\[\ng_t = \\frac{\\partial L}{\\partial W_t}.\n\\]\n\nOur goal is to express \\(\\tilde{g}_t\\) in terms of \\(g_t\\), \\(B_t\\), and \\(A_t\\).\n\n---\n\n### **2. Gradient Chain Rule**\nThe key observation is that the equivalent gradient \\(\\tilde{g}_t\\) arises from the chain rule through the reparameterization \\(W = W_0 + sBA\\). Specifically, the gradient with respect to \\(\\tilde{W}_t\\) can be decomposed into contributions from \\(B_t\\) and \\(A_t\\):\n\\[\n\\tilde{g}_t = \\frac{\\partial L}{\\partial \\tilde{W}_t} = \\frac{\\partial L}{\\partial W_t} \\cdot \\frac{\\partial W_t}{\\partial \\tilde{W}_t}.\n\\]\n\nHowever, since \\(W_t\\) and \\(\\tilde{W}_t\\) are the same in terms of their functional form (both represent the weight matrix at step \\(t\\)), we can instead consider how the gradient propagates through the low-rank factors \\(B_t\\) and \\(A_t\\).\n\n---\n\n### **3. Gradient Propagation Through \\(B_t\\) and \\(A_t\\)**\nThe gradient \\(\\tilde{g}_t\\) can be expressed as the sum of the gradients contributed by \\(B_t\\) and \\(A_t\\):\n\\[\n\\tilde{g}_t = s \\left( \\frac{\\partial L}{\\partial B_t} A_t^\\top + B_t^\\top \\frac{\\partial L}{\\partial A_t} \\right).\n\\]\n\nBut we need to relate \\(\\frac{\\partial L}{\\partial B_t}\\) and \\(\\frac{\\partial L}{\\partial A_t}\\) to \\(g_t\\). Using the chain rule:\n\\[\n\\frac{\\partial L}{\\partial B_t} = \\frac{\\partial L}{\\partial W_t} \\cdot \\frac{\\partial W_t}{\\partial B_t} = g_t \\cdot (sA_t^\\top),\n\\]\n\\[\n\\frac{\\partial L}{\\partial A_t} = \\frac{\\partial L}{\\partial W_t} \\cdot \\frac{\\partial W_t}{\\partial A_t} = (sB_t^\\top) \\cdot g_t.\n\\]\n\nSubstituting these into the expression for \\(\\tilde{g}_t\\):\n\\[\n\\tilde{g}_t = s \\left( (g_t \\cdot sA_t^\\top) A_t^\\top + B_t^\\top (sB_t^\\top \\cdot g_t) \\right).\n\\]\n\nSimplifying:\n\\[\n\\tilde{g}_t = s^2 \\left( g_t A_t^\\top A_t + B_t^\\top B_t^\\top g_t \\right).\n\\]\n\n---\n\n### **4. Final Expression**\nNote that \\(B_t^\\top B_t^\\top g_t\\) is not dimensionally consistent. Instead, the correct term should be \\(B_t B_t^\\top g_t\\) (since \\(B_t^\\top g_t\\) would not align properly). Adjusting for this, we get:\n\\[\n\\tilde{g}_t = s^2 \\left( B_t B_t^\\top g_t + g_t A_t^\\top A_t \\right).\n\\]\n\nThis matches the given lemma.\n\n---\n\n### **5. Proof Summary**\nThe derivation relies on:\n1. Expressing the gradient \\(\\tilde{g}_t\\) in terms of the contributions from \\(B_t\\) and \\(A_t\\),\n2. Using the chain rule to relate \\(\\frac{\\partial L}{\\partial B_t}\\) and \\(\\frac{\\partial L}{\\partial A_t}\\) to \\(g_t\\),\n3. Combining these contributions and simplifying to arrive at the final expression.\n\nThus, the lemma is proved:\n\\[\n\\boxed{\\tilde{g}_t = s^2 \\left( B_t B_t^\\top g_t + g_t A_t^\\top A_t \\right)}.\n\\]", "C": "According to the assumption, \\(\\tilde W_t = W_t\\). Let LoRA reparameterize \\(W\\) as \\(W = W_0 + sBA\\) where \\(B\\in\\mathbb R^{m\\times r}\\), \\(A\\in\\mathbb R^{r\\times n}\\), and \\(s\\in\\mathbb R\\). Denote the equivalent weight as\n\n\\[\\tilde W_t = W_{\\mathrm{init}} + s\\,B_t A_t.\\]\n\nThen the gradients of \\(B\\) and \\(A\\) are\n\n\\[G_{B_t} = \\frac{\\partial L}{\\partial \\tilde W_t}\\frac{\\partial \\tilde W_t}{\\partial B} = \\frac{\\partial L}{\\partial W_t}\\frac{\\partial \\tilde W_t}{\\partial B} = s\\,g_t A_t^{\\top},\\tag{30}\\]\n\\[G_{A_t} = \\frac{\\partial L}{\\partial \\tilde W_t}\\frac{\\partial \\tilde W_t}{\\partial A} = \\frac{\\partial L}{\\partial W_t}\\frac{\\partial \\tilde W_t}{\\partial A} = s\\,B_t^{\\top} g_t.\\tag{31}\\]\n\nUnder SGD updates,\n\n\\[dB_t = -\\eta\\,G_{B_t} = -s\\,\\eta\\,g_t A_t^{\\top},\\quad dA_t = -\\eta\\,G_{A_t} = -s\\,\\eta\\,B_t^{\\top} g_t.\\tag{32}\\]\n\nThe change in the equivalent weight is\n\n\\[d\\tilde W = \\frac{\\partial \\tilde W_t}{\\partial A_t}dA_t + \\frac{\\partial \\tilde W_t}{\\partial B_t}dB_t = s\\,B_t\\,dA_t + s\\,dB_t\\,A_t\\]\n\\[= s\\bigl[B_t(-\\eta s B_t^{\\top}g_t) + (-\\eta s g_t A_t^{\\top})A_t\\bigr] = -\\eta s^2\\bigl[B_tB_t^{\\top}g_t + g_tA_t^{\\top}A_t\\bigr].\\tag{36}\\]\n\nTherefore, the equivalent gradient is\n\n\\[\\tilde g_t = s^2\\bigl(B_t B_t^{\\top} g_t + g_t A_t^{\\top} A_t\\bigr).\\tag{37}\\]\n\nThis concludes the proof.", "D": "To derive the lemma, we analyze the relationship between the equivalent gradient $\\tilde g_t = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde W_t}$ and the full-tuning gradient $g_t = \\frac{\\partial \\mathcal{L}}{\\partial W_t}$ in the reparameterized setting. Here, $\\tilde W_t = W_0 + sB_tA_t$, and $W_0$ is fixed. The key steps involve chain rule differentiation through the low-rank matrices $B_t$ and $A_t$.\n\n**Step 1: Compute gradients w.r.t. $B_t$ and $A_t$**  \nUsing the chain rule, the gradients of $\\mathcal{L}$ with respect to $B_t$ and $A_t$ are:  \n$$\n\\frac{\\partial \\mathcal{L}}{\\partial B_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde W_t} \\cdot \\frac{\\partial \\tilde W_t}{\\partial B_t} = \\tilde g_t \\cdot sA_t^\\top,  \n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial A_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde W_t} \\cdot \\frac{\\partial \\tilde W_t}{\\partial A_t} = sB_t^\\top \\tilde g_t.\n$$\n\n**Step 2: Relate parameter-efficient updates to full-tuning gradients**  \nIn full-tuning, the weight update is $\\Delta W_t = -\\eta g_t$, where $\\eta$ is the learning rate. In parameter-efficient tuning, the equivalent weight update $\\Delta \\tilde W_t$ arises from updates to $B_t$ and $A_t$:  \n$$\n\\Delta \\tilde W_t = s(\\Delta B_t A_t + B_t \\Delta A_t).\n$$  \nSubstituting the gradients $\\Delta B_t = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial B_t}$ and $\\Delta A_t = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial A_t}$, we get:  \n$$\n\\Delta \\tilde W_t = -\\eta s^2 \\left( \\tilde g_t A_t^\\top A_t + B_t B_t^\\top \\tilde g_t \\right).\n$$\n\n**Step 3: Equate the equivalent update to the full-tuning update**  \nSince $\\Delta \\tilde W_t = -\\eta \\tilde g_t$ must match the full-tuning update $\\Delta W_t = -\\eta g_t$, we equate the two expressions:  \n$$\n-\\eta \\tilde g_t = -\\eta s^2 \\left( \\tilde g_t A_t^\\top A_t + B_t B_t^\\top \\tilde g_t \\right).\n$$  \nCanceling $-\\eta$ and solving for $\\tilde g_t$, we obtain:  \n$$\n\\tilde g_t = s^2 \\left( B_t B_t^\\top g_t + g_t A_t^\\top A_t \\right).\n$$\n\n**Conclusion**  \nThe equivalence arises because the parameter-efficient updates implicitly project the full-tuning gradient $g_t$ onto the low-rank structure via $B_t$ and $A_t$. The scaling $s^2$ accounts for the double contribution of $s$ in both $B_t$ and $A_t$.\n\n\\boxed{\\tilde g_t = s^2\\left(B_t B_t^{\\top} g_t + g_t A_t^{\\top} A_t\\right)}", "ground-truth": "C"}
{"paper": "arxiv:2502.16894", "question": "In Theorem 3.1, let the pretrained weight matrix $W_0\\in \\mathbb{R}^{m\\times n}$ be reparameterized using low-rank adapters $B\\in \\mathbb{R}^{m\\times r}$ and $A\\in \\mathbb{R}^{r\\times n}$ with a scalar scale factor $s$. At each iteration $t$, the equivalent weight is defined as\n\n$$\\tilde W_t = W_0 + s\\,B_t\\,A_t,$$\n\nand the equivalent gradient is defined as\n\n$$\\tilde g_t = \\frac{\\partial L}{\\partial \\tilde W_t}.$$ \n\nThe full fine-tuning update rule is\n\n$$W_{t+1} = W_t - \\eta_{FT}\\,g_t,$$\n\nand the LoRA update rule is\n\n$$\\tilde W_{t+1} = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t.$$ \n\nTheorem 3.1 claims that if initially $\\tilde W_0 = W_0$ and for every $t$ the scaled gradients satisfy\n\n$$\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t,$$\n\nthen it follows that $\\tilde W_t = W_t$ for all iterations $t$. How can one derive a formal proof of this theorem?", "A": "To formally prove Theorem 3.1, we use mathematical induction to show that \\(\\tilde W_t = W_t\\) for all iterations \\(t\\), given the initial condition \\(\\tilde W_0 = W_0\\) and the scaled gradient condition \\(\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t\\) at every iteration \\(t\\).\n\n---\n\n**Base Case (\\(t=0\\)):**  \nBy assumption, \\(\\tilde W_0 = W_0\\). Since the full fine-tuning initialization is \\(W_0\\), we have \\(W_0 = W_0\\). Thus, \\(\\tilde W_0 = W_0\\) holds.\n\n---\n\n**Inductive Step:**  \nAssume \\(\\tilde W_t = W_t\\) holds for some \\(t \\geq 0\\). We aim to show \\(\\tilde W_{t+1} = W_{t+1}\\).\n\n1. **Full Fine-Tuning Update Rule:**  \n   By definition,  \n   \\[\n   W_{t+1} = W_t - \\eta_{FT}\\,g_t.\n   \\]\n\n2. **LoRA Update Rule:**  \n   By definition,  \n   \\[\n   \\tilde W_{t+1} = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t.\n   \\]  \n   Substituting the inductive hypothesis \\(\\tilde W_t = W_t\\):  \n   \\[\n   \\tilde W_{t+1} = W_t - \\eta_{LoRA}\\,\\tilde g_t.\n   \\]\n\n3. **Equating Updates via Scaled Gradient Condition:**  \n   The theorem assumes \\(\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t\\) for all \\(t\\). Substituting this into the LoRA update:  \n   \\[\n   \\tilde W_{t+1} = W_t - \\eta_{FT}\\,g_t.\n   \\]\n\n4. **Conclusion:**  \n   Comparing the full fine-tuning and LoRA updates:  \n   \\[\n   W_{t+1} = W_t - \\eta_{FT}\\,g_t \\quad \\text{and} \\quad \\tilde W_{t+1} = W_t - \\eta_{FT}\\,g_t.\n   \\]  \n   Therefore, \\(\\tilde W_{t+1} = W_{t+1}\\).\n\n---\n\nBy induction, \\(\\tilde W_t = W_t\\) holds for all iterations \\(t\\). This completes the proof of Theorem 3.1.", "B": "We verify the claimed alignment by induction on the iteration $t$.  \n\n- The full fine-tuning update is  \n  \\[W_{t+1} = W_t - \\eta_{FT}\\,g_t,\\]  \n  and the LoRA update is  \n  \\[\\tilde W_{t+1} = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t.\\]  \n\n- **Base case** ($t=0$).  By assumption, the models are initialized identically:  \\(\\tilde W_0 = W_0\\).  \n\n- **Inductive step.**  Suppose at iteration $t$ we have  \\(\\tilde W_t = W_t\\)  and  \\(\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t\\).  Then  \n  \\[\n  \\tilde W_{t+1} \n    = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t  \n    = W_t - \\eta_{LoRA}\\,g_t  \n    = W_{t+1}.  \n  \\]  \n\nBy induction, \\(\\tilde W_t = W_t\\) holds for all iterations $t$, proving that LoRA updates exactly match full fine-tuning under the stated conditions.", "C": "We verify the claimed alignment by induction on the iteration $t$.  \n\n- The full fine-tuning update is  \n  \\[W_{t+1} = W_t - \\eta_{FT}\\,g_t,\\]  \n  and the LoRA update is  \n  \\[\\tilde W_{t+1} = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t.\\]  \n\n- **Base case** ($t=0$).  By assumption, the models are initialized identically:  \\(\\tilde W_0 = W_0\\).  \n\n- **Inductive step.**  Suppose at iteration $t$ we have  \\(\\tilde W_t = W_t\\)  and  \\(\\eta_{LoRA}\\,\\tilde g_t = \\eta_{FT}\\,g_t\\).  Then  \n  \\[\n  \\tilde W_{t+1} \n    = \\tilde W_t - \\eta_{LoRA}\\,\\tilde g_t  \n    = W_t - \\eta_{FT}\\,g_t  \n    = W_{t+1}.  \n  \\]  \n\nBy induction, \\(\\tilde W_t = W_t\\) holds for all iterations $t$, proving that LoRA updates exactly match full fine-tuning under the stated conditions.", "D": "To formally prove Theorem 3.1, we will use mathematical induction to show that if the initial conditions and gradient scaling conditions hold, then \\(\\tilde W_t = W_t\\) for all iterations \\(t\\).\n\n### **Base Case (\\(t = 0\\)):**\nBy the initial condition given in the theorem:\n\\[\n\\tilde W_0 = W_0.\n\\]\nThus, the base case holds.\n\n### **Inductive Hypothesis:**\nAssume that for some iteration \\(k \\geq 0\\), the equality holds:\n\\[\n\\tilde W_k = W_k.\n\\]\n\n### **Inductive Step (\\(t = k+1\\)):**\nWe need to show that \\(\\tilde W_{k+1} = W_{k+1}\\).\n\n1. **Full Fine-Tuning Update:**\n   \\[\n   W_{k+1} = W_k - \\eta_{FT} \\, g_k.\n   \\]\n\n2. **LoRA Update:**\n   \\[\n   \\tilde W_{k+1} = \\tilde W_k - \\eta_{LoRA} \\, \\tilde g_k.\n   \\]\n   By the inductive hypothesis (\\(\\tilde W_k = W_k\\)), this becomes:\n   \\[\n   \\tilde W_{k+1} = W_k - \\eta_{LoRA} \\, \\tilde g_k.\n   \\]\n\n3. **Gradient Scaling Condition:**\n   The theorem states that for all \\(t\\), the gradients satisfy:\n   \\[\n   \\eta_{LoRA} \\, \\tilde g_t = \\eta_{FT} \\, g_t.\n   \\]\n   Applying this at iteration \\(k\\):\n   \\[\n   \\eta_{LoRA} \\, \\tilde g_k = \\eta_{FT} \\, g_k.\n   \\]\n\n4. **Substitute into LoRA Update:**\n   \\[\n   \\tilde W_{k+1} = W_k - \\eta_{FT} \\, g_k.\n   \\]\n   But from the full fine-tuning update:\n   \\[\n   W_{k+1} = W_k - \\eta_{FT} \\, g_k.\n   \\]\n   Therefore:\n   \\[\n   \\tilde W_{k+1} = W_{k+1}.\n   \\]\n\n### **Conclusion:**\nBy the principle of mathematical induction, if:\n- \\(\\tilde W_0 = W_0\\) (initial condition),\n- \\(\\eta_{LoRA} \\, \\tilde g_t = \\eta_{FT} \\, g_t\\) for all \\(t\\) (gradient scaling condition),\n\nthen \\(\\tilde W_t = W_t\\) holds for all iterations \\(t\\). \\(\\square\\)", "ground-truth": "C"}
{"paper": "arxiv:2502.16894", "question": "Consider a mixture-of-experts model with \\(E\\) experts. For each expert \\(i\\in\\{1,\\dots,E\\}\\), let:\n  1. \\(W_i^0 \\in \\mathbb{R}^{m\\times n}\\) be the pre-trained full-rank weight for expert \\(i\\).\n  2. \\(\\widetilde W_i^0 \\in \\mathbb{R}^{m\\times n}\\) be the equivalent initialized weight for expert \\(i\\) under the LoRA MoE framework.\n  3. \\(g_{i,t} = \\frac{\\partial L}{\\partial W_i}\\bigl|_{t}\\) be the gradient of expert \\(i\\) at step \\(t\\) in full-rank fine-tuning.\n  4. \\(\\widetilde g_{i,t} = \\frac{\\partial L}{\\partial \\widetilde W_i}\\bigl|_{t}\\) be the corresponding equivalent gradient of expert \\(i\\) in LoRA MoE.\n\nTheorem 3.2 states:\n\n  \"For every expert \\(i\\in\\{1,\\dots,E\\}\\), if at initialization\n  \\[\\widetilde W_i^0 = W_i^0\\]\n  and at each optimization step\n  \\[\\widetilde g_{i,t} = g_{i,t},\\]\n  then the LoRA MoE model behaves identically to an upcycled MoE with full-rank fine-tuning.\"\n\nPlease derive a complete proof of this theorem using only the definitions above, without referring to any other parts of the paper or hinting at the proof strategy.", "A": "**Proof of Theorem 3.2:**\n\nWe aim to show that under the given conditions, the LoRA MoE model behaves identically to an upcycled MoE with full-rank fine-tuning. We proceed by induction on the optimization step \\( t \\).\n\n**Base Case (\\( t = 0 \\)):**\nAt initialization, the condition \\(\\widetilde W_i^0 = W_i^0\\) holds for all experts \\( i \\in \\{1, \\dots, E\\} \\). Thus, the initial weights of the LoRA MoE model and the full-rank MoE model are identical:\n\\[\n\\widetilde W_i^0 = W_i^0 \\quad \\forall i.\n\\]\nThis ensures that the models start from the same point.\n\n**Inductive Step:**\nAssume that at optimization step \\( t \\), the weights of the LoRA MoE model and the full-rank MoE model are identical:\n\\[\n\\widetilde W_i^t = W_i^t \\quad \\forall i.\n\\]\nWe need to show that this equality holds at step \\( t+1 \\).\n\nGiven the gradient condition \\(\\widetilde g_{i,t} = g_{i,t}\\) for all \\( i \\), the weight updates for both models are computed using the same gradients. Let \\( \\eta \\) be the learning rate. The weight updates are:\n\\[\nW_i^{t+1} = W_i^t - \\eta g_{i,t},\n\\]\n\\[\n\\widetilde W_i^{t+1} = \\widetilde W_i^t - \\eta \\widetilde g_{i,t}.\n\\]\nBy the inductive hypothesis and the gradient condition:\n\\[\n\\widetilde W_i^{t+1} = W_i^t - \\eta g_{i,t} = W_i^{t+1}.\n\\]\nThus, \\(\\widetilde W_i^{t+1} = W_i^{t+1}\\) for all \\( i \\).\n\n**Conclusion:**\nBy induction, the weights of the LoRA MoE model and the full-rank MoE model remain identical at every optimization step \\( t \\). Since the models share the same initialization and update rules, their behaviors are identical throughout the optimization process. This completes the proof. \\(\\square\\)", "B": "To prove Theorem 3.2, we demonstrate that under the given conditions, the LoRA MoE model’s parameters evolve identically to the full-rank MoE parameters at every optimization step. \n\n**Proof:**\n\nLet the optimization use gradient descent with learning rate $\\eta$. For any expert $i$ and step $t$, the full-rank weight update is:\n\\[\nW_i^{t+1} = W_i^t - \\eta \\cdot g_{i,t}.\n\\]\nUnder the LoRA MoE framework, the equivalent weight update is:\n\\[\n\\widetilde W_i^{t+1} = \\widetilde W_i^t - \\eta \\cdot \\widetilde g_{i,t}.\n\\]\n\n**Base Case ($t=0$):**  \nBy initialization, $\\widetilde W_i^0 = W_i^0$ (given).  \n\n**Inductive Step:**  \nAssume $\\widetilde W_i^t = W_i^t$ holds at step $t$. By the theorem’s gradient equivalence $\\widetilde g_{i,t} = g_{i,t}$, the next-step weights become:\n\\[\n\\widetilde W_i^{t+1} = \\widetilde W_i^t - \\eta \\cdot \\widetilde g_{i,t} = W_i^t - \\eta \\cdot g_{i,t} = W_i^{t+1}.\n\\]\nThus, $\\widetilde W_i^{t+1} = W_i^{t+1}$.  \n\nBy induction, $\\widetilde W_i^t = W_i^t$ holds for all $t \\geq 0$.  \n\n**Behavioral Equivalence:**  \nSince $\\widetilde W_i^t = W_i^t$ at all steps, the forward computations (dependent on $\\{W_i^t\\}$) and gradient computations (dependent on $\\{\\frac{\\partial L}{\\partial W_i^t}\\}$) are identical between the two models. Hence, the LoRA MoE and full-rank MoE exhibit identical outputs, loss trajectories, and parameter updates throughout training.  \n\nThis completes the proof. $\\blacksquare$", "C": "We aim to show that under the given conditions, the LoRA MoE aligns with Full FT MoE by effectively making the MoE routers behave identically in both models.\n\nBase Case (t = 0): At initialization, by assumption, the equivalent weights of each expert satisfy \\(\\widetilde W_i^0 = W_i^0\\) and Full FT MoE upcycles all expert weights from the same pre-trained weight. Moreover, since both models use the same random seed, the routers are initialized identically, ensuring identical routing decisions in both Full FT MoE and LoRA MoE at t=0.\n\nInductive Step: Assume that at step t, \\(\\widetilde W_{i,t} = W_{i,t}\\) for all experts i, and the routers in both models are identical. During the t-th optimization step, the gradients satisfy \\(\\widetilde g_{i,t} = g_{i,t}\\) and the learning rates are chosen so that the weight updates coincide:\n\n\\[\n\\widetilde W_{i,t+1} = \\widetilde W_{i,t} - \\eta_{\\mathrm{LoRA}}\\,\\widetilde g_{i,t} \\approx W_{i,t} - \\eta_{\\mathrm{FullFT}}\\,g_{i,t} = W_{i,t+1}.\n\\]\n\nSince the routers remain identical and assign the same weights \\(w_i(x)\\) to each expert, the layer outputs satisfy:\n\n\\[\n\\mathrm{MoE}(x)\n= \\sum_{i=1}^E w_i(x)\\,W_{i}(x)\n= \\sum_{i=1}^E w_i(x)\\,\\widetilde W_{i}(x)\n= \\sum_{i=1}^E w_i(x)\\bigl(W + sB_iA_i\\bigr)(x)\n= W(x) + \\sum_{i=1}^E w_i(x)\\,sB_iA_i(x)\n= \\mathrm{MoE}_{\\mathrm{LoRA}}(x).\n\\]\n\nBecause the routers are updated based on these outputs, they remain identical at step t+1. By induction, the routers---and hence routing decisions and weight updates---for all steps are the same in Full FT MoE and LoRA MoE. Therefore, under the stated conditions, LoRA MoE is behaviorally equivalent to full-rank fine-tuning of the MoE model.", "D": "We aim to show that under the given conditions, the LoRA MoE aligns with Full FT MoE by effectively making the MoE routers behave identically in both models.\n\nBase Case (t = 0): At initialization, by assumption, the equivalent weights of each expert satisfy \\(\\widetilde W_i^0 = W_i^0\\) and Full FT MoE upcycles all expert weights from the same pre-trained weight. Moreover, since both models use the same random seed, the routers are initialized identically, ensuring identical routing decisions in both Full FT MoE and LoRA MoE at t=0.\n\nInductive Step: Assume that at step t, \\(\\widetilde W_{i,t} = W_{i,t}\\) for all experts i, and the routers in both models are identical. During the t-th optimization step, the gradients satisfy \\(\\widetilde g_{i,t} = g_{i,t}\\) and the learning rates are chosen so that the weight updates coincide:\n\n\\[\n\\widetilde W_{i,t+1} = \\widetilde W_{i,t} - \\eta_{\\mathrm{LoRA}}\\,\\widetilde g_{i,t} \\approx W_{i,t} - \\eta_{\\mathrm{FullFT}}\\,g_{i,t} = W_{i,t+1}.\n\\]\n\nSince the routers remain identical and assign the same weights \\(w_i(x)\\) to each expert, the layer outputs satisfy:\n\n\\[\n\\mathrm{MoE}(x)\n= \\sum_{i=1}^E w_i(x)\\,W_{i}(x)\n= \\sum_{i=1}^E w_i(x)\\,\\widetilde W_{i}(x)\n= \\sum_{i=1}^E w_i(x)\\bigl(W + sB_iA_i\\bigr)(x)\n= W(x) + \\sum_{i=1}^E w_i(x)\\,sB_iA_i(x)\n= \\mathrm{MoE}_{\\mathrm{LoRA}}(x).\n\\]\n\nBecause the routers are updated based on these outputs, they remain identical at step t+1. By induction, the routers---and hence routing decisions and weight updates---for all steps are the same in Full FT MoE and LoRA MoE. Therefore, under the stated conditions, LoRA MoE is behaviorally equivalent to full-rank fine-tuning of the MoE model, provided the learning rates \\(\\eta_{\\mathrm{LoRA}}\\) and \\(\\eta_{\\mathrm{FullFT}}\\) are equal.", "ground-truth": "C"}
{"paper": "arxiv:2502.16894", "question": "Consider a mixture-of-experts (MoE) model with E experts. For an input x, let z_i(x) for i=1,\\ldots,E be independent and identically distributed random variables. Let \\Omega_k(x) \\subset \\{1,\\ldots,E\\} denote the set of indices of the top-k values among \\{z_1(x),\\ldots,z_E(x)\\}. Define the expert weight w_i(x) = \\exp(z_i(x)) / \\sum_{j\\in\\Omega_k(x)}\\exp(z_j(x)) if i\\in\\Omega_k(x), and w_i(x)=0 otherwise. Assuming k\\le E and the z_i(x) are i.i.d., how can one derive E_x[w_i(x)] = \\frac{1}{E} and Var_x[w_i(x)] = \\frac{E-k}{k E^2} as stated in Lemma 3.3? Provide a self-contained proof under these definitions and assumptions.", "A": "To derive the expected value and variance of the expert weights \\( w_i(x) \\) under the given assumptions, we proceed with the following proof.\n\n### Definitions and Assumptions\n1. **Experts and Random Variables**: We have \\( E \\) experts, and for an input \\( x \\), the random variables \\( z_i(x) \\) for \\( i = 1, \\ldots, E \\) are independent and identically distributed (i.i.d.).\n2. **Top-k Selection**: \\( \\Omega_k(x) \\subset \\{1, \\ldots, E\\} \\) is the set of indices of the top-\\( k \\) values among \\( \\{z_1(x), \\ldots, z_E(x)\\} \\).\n3. **Expert Weights**: The weight for expert \\( i \\) is defined as:\n   \\[\n   w_i(x) = \\begin{cases}\n   \\frac{\\exp(z_i(x))}{\\sum_{j \\in \\Omega_k(x)} \\exp(z_j(x))} & \\text{if } i \\in \\Omega_k(x), \\\\\n   0 & \\text{otherwise}.\n   \\end{cases}\n   \\]\n4. **Symmetry**: Due to the i.i.d. assumption, the \\( z_i(x) \\) are exchangeable, and thus the indices \\( i \\) are symmetric in their roles.\n\n### Expected Value \\( E_x[w_i(x)] \\)\n1. **Symmetry Argument**: Since the \\( z_i(x) \\) are i.i.d., the probability that any particular expert \\( i \\) is in the top-\\( k \\) is \\( \\frac{k}{E} \\). This is because there are \\( k \\) positions in \\( \\Omega_k(x) \\) out of \\( E \\) total experts.\n2. **Conditional Expectation**: Given that \\( i \\in \\Omega_k(x) \\), the weights \\( w_i(x) \\) are normalized over the \\( k \\) selected experts. By symmetry, each selected expert contributes equally to the denominator. Thus, the expected value of \\( w_i(x) \\) conditioned on \\( i \\in \\Omega_k(x) \\) is \\( \\frac{1}{k} \\).\n3. **Total Expectation**: Combining these, the unconditional expectation is:\n   \\[\n   E_x[w_i(x)] = P(i \\in \\Omega_k(x)) \\cdot E[w_i(x) \\mid i \\in \\Omega_k(x)] + P(i \\notin \\Omega_k(x)) \\cdot 0 = \\frac{k}{E} \\cdot \\frac{1}{k} = \\frac{1}{E}.\n   \\]\n\n### Variance \\( Var_x[w_i(x)] \\)\n1. **Second Moment**: To compute the variance, we first compute \\( E_x[w_i(x)^2] \\). Again, by symmetry, this is non-zero only when \\( i \\in \\Omega_k(x) \\).\n2. **Conditional Second Moment**: Given \\( i \\in \\Omega_k(x) \\), the second moment of \\( w_i(x) \\) is the expected value of \\( \\left(\\frac{\\exp(z_i(x))}{\\sum_{j \\in \\Omega_k(x)} \\exp(z_j(x))}\\right)^2 \\). Due to the i.i.d. assumption, this simplifies to \\( \\frac{2}{k(k+1)} \\) (derived from order statistics of exponential distributions, but we omit the detailed derivation here for brevity). However, a simpler symmetry argument suffices:\n   - The denominator \\( \\sum_{j \\in \\Omega_k(x)} \\exp(z_j(x)) \\) is the sum of \\( k \\) i.i.d. terms, and the numerator is one of these terms. The expected value of \\( \\frac{X_i^2}{(\\sum_{j=1}^k X_j)^2} \\) where \\( X_j \\) are i.i.d. is \\( \\frac{2}{k(k+1)} \\) for exponential distributions, but for our purposes, we can use the fact that \\( E[w_i(x)^2 \\mid i \\in \\Omega_k(x)] = \\frac{1}{k^2} \\) (assuming equal contributions, which is approximate but sufficient for the variance calculation).\n   - A more precise calculation would involve the variance of the weights, but under symmetry, the variance can be derived as follows.\n3. **Variance Calculation**: The variance is:\n   \\[\n   Var_x[w_i(x)] = E_x[w_i(x)^2] - (E_x[w_i(x)])^2.\n   \\]\n   Using the second moment \\( E_x[w_i(x)^2] = \\frac{k}{E} \\cdot \\frac{1}{k^2} = \\frac{1}{k E} \\) (from the conditional expectation), and the squared expectation \\( \\left(\\frac{1}{E}\\right)^2 = \\frac{1}{E^2} \\), we get:\n   \\[\n   Var_x[w_i(x)] = \\frac{1}{k E} - \\frac{1}{E^2} = \\frac{E - k}{k E^2}.\n   \\]\n\n### Conclusion\nUnder the given assumptions and definitions, we have derived:\n\\[\nE_x[w_i(x)] = \\frac{1}{E}, \\quad Var_x[w_i(x)] = \\frac{E - k}{k E^2}.\n\\]", "B": "Because the scores \\(z_1(x),...,z_E(x)\\) are i.i.d. random variables, any permutation of the indices leaves their joint distribution unchanged, and the Top-k operation is symmetric under permutation. Moreover, by definition, \\(\\sum_{i=1}^E w_i(x)=1\\). Taking expectation:\n\n\\[\\sum_{i=1}^E \\mathbb{E}_x[w_i(x)] = \\mathbb{E}_x\\bigl[\\sum_{i=1}^E w_i(x)\\bigr] = 1,\\]\n\nso by symmetry,\n\n\\[\\mathbb{E}_x[w_i(x)] = \\frac1E,\\quad\\forall i.\\]\n\nTo compute the variance, observe\n\n\\[\\mathrm{Var}_x(w_i(x)) = \\mathbb{E}_x[w_i(x)^2] - \\bigl(\\mathbb{E}_x[w_i(x)]\\bigr)^2 = \\mathbb{E}_x[w_i(x)^2] - \\frac1{E^2}.\\]\n\nWe use that\n\n\\[1 = \\bigl(\\sum_{i=1}^E w_i(x)\\bigr)^2 = \\sum_{i=1}^E w_i(x)^2 + 2\\sum_{i<j} w_i(x)w_j(x).\\]\n\nTaking expectation and using symmetry gives\n\n\\[1 = E\\,\\mathbb{E}_x[w_i(x)^2] + E(E-1)\\,\\mathbb{E}_x[w_i(x)w_j(x)],\\quad i\\neq j.\\]\n\nNext, since exactly k of the \\(w_i(x)\\) are nonzero and they sum to 1, by symmetry one shows\n\n\\[\\mathbb{E}_x[w_i(x)w_j(x)] = \\frac{k-1}{E(E-1)k}.\\]\n\nSubstituting into the previous relation yields\n\n\\[\\mathbb{E}_x[w_i(x)^2] = \\frac1{E k}.\\]\n\nHence\n\n\\[\\mathrm{Var}_x(w_i(x)) = \\frac1{E k} - \\frac1{E^2} = \\frac{E-k}{E^2}.\\]\n\nThis completes the proof.", "C": "To derive the expectation and variance of the expert weight \\( w_i(x) \\) under the given assumptions, we leverage the symmetry and i.i.d. properties of the random variables \\( z_i(x) \\).\n\n**Expectation \\( \\mathbb{E}_x[w_i(x)] = \\frac{1}{E} \\):**\n\n1. **Symmetry:** Since \\( z_i(x) \\) are i.i.d., each expert \\( i \\) has an equal probability of being in the top-\\( k \\) set \\( \\Omega_k(x) \\). The probability that \\( i \\in \\Omega_k(x) \\) is \\( \\frac{k}{E} \\).\n\n2. **Conditional Expectation:** Given \\( i \\in \\Omega_k(x) \\), the weights \\( \\{w_j(x)\\}_{j \\in \\Omega_k(x)} \\) sum to 1. By symmetry, each \\( w_j(x) \\) (for \\( j \\in \\Omega_k(x) \\)) has an equal expectation:\n   \\[\n   \\mathbb{E}[w_j(x) \\mid j \\in \\Omega_k(x)] = \\frac{1}{k}.\n   \\]\n   \n3. **Total Expectation Law:**\n   \\[\n   \\mathbb{E}_x[w_i(x)] = \\mathbb{P}(i \\in \\Omega_k(x)) \\cdot \\mathbb{E}[w_i(x) \\mid i \\in \\Omega_k(x)] + \\mathbb{P}(i \\notin \\Omega_k(x)) \\cdot 0 = \\frac{k}{E} \\cdot \\frac{1}{k} = \\frac{1}{E}.\n   \\]\n\n**Variance \\( \\text{Var}_x(w_i(x)) = \\frac{E - k}{k E^2} \\):**\n\n1. **Second Moment:** Compute \\( \\mathbb{E}_x[w_i^2(x)] \\):\n   - When \\( i \\in \\Omega_k(x) \\), \\( w_i(x) = \\frac{\\exp(z_i(x))}{S} \\), where \\( S = \\sum_{j \\in \\Omega_k(x)} \\exp(z_j(x)) \\).\n   - By symmetry, all terms \\( \\mathbb{E}\\left[\\frac{\\exp(z_i(x))^2}{S^2} \\mid i \\in \\Omega_k(x)\\right] \\) are equal. Let this common value be \\( C \\).\n\n2. **Sum of Squared Weights:**\n   - For \\( \\Omega_k(x) \\), \\( \\sum_{j \\in \\Omega_k(x)} w_j^2(x) = \\frac{\\sum_{j \\in \\Omega_k(x)} \\exp(2z_j(x))}{S^2} \\).\n   - Taking expectation and using symmetry:\n     \\[\n     kC = \\mathbb{E}\\left[\\frac{\\sum_{j \\in \\Omega_k(x)} \\exp(2z_j(x))}{S^2} \\mid i \\in \\Omega_k(x)\\right].\n     \\]\n   - However, no closed-form expression for \\( C \\) is needed due to symmetry arguments.\n\n3. **Key Insight:** Using the linearity of expectation and the fact that \\( \\sum_{i=1}^E w_i(x) = 1 \\) almost surely:\n   - Expand \\( \\mathbb{E}\\left[\\left(\\sum_{i=1}^E w_i(x)\\right)^2\\right] = 1 \\):\n     \\[\n     \\sum_{i=1}^E \\mathbb{E}[w_i^2(x)] + 2\\sum_{i < j} \\mathbb{E}[w_i(x)w_j(x)] = 1.\n     \\]\n   - Let \\( A = \\mathbb{E}[w_i^2(x)] \\) and \\( B = \\mathbb{E}[w_i(x)w_j(x)] \\) for \\( i \\neq j \\). By symmetry:\n     \\[\n     E A + E(E - 1) B = 1.\n     \\]\n   - From the variance-covariance relationship and \\( \\text{Var}\\left(\\sum_{i=1}^E w_i(x)\\right) = 0 \\):\n     \\[\n     \\sum_{i=1}^E \\text{Var}(w_i(x)) + 2\\sum_{i < j} \\text{Cov}(w_i(x), w_j(x)) = 0.\n     \\]\n     Substitute \\( \\text{Var}(w_i(x)) = A - \\frac{1}{E^2} \\) and \\( \\text{Cov}(w_i(x), w_j(x)) = B - \\frac{1}{E^2} \\):\n     \\[\n     E\\left(A - \\frac{1}{E^2}\\right) + E(E - 1)\\left(B - \\frac{1}{E^2}\\right) = 0.\n     \\]\n   - Solve the system of equations to find:\n     \\[\n     A = \\frac{1}{k E}, \\quad B = \\frac{k - 1}{k E(E - 1)}.\n     \\]\n\n4. **Final Variance:**\n   \\[\n   \\text{Var}_x(w_i(x)) = \\mathbb{E}[w_i^2(x)] - \\left(\\mathbb{E}[w_i(x)]\\right)^2 = \\frac{1}{k E} - \\frac{1}{E^2} = \\frac{E - k}{k E^2}.\n   \\]\n\n**Conclusion:** Under the i.i.d. assumption and symmetry, the expert weights satisfy \\( \\mathbb{E}_x[w_i(x)] = \\frac{1}{E} \\) and \\( \\text{Var}_x(w_i(x)) = \\frac{E - k}{k E^2} \\).", "D": "Because the scores \\(z_1(x),...,z_E(x)\\) are i.i.d. random variables, any permutation of the indices leaves their joint distribution unchanged, and the Top-k operation is symmetric under permutation. Moreover, by definition, \\(\\sum_{i=1}^E w_i(x)=1\\). Taking expectation:\n\n\\[\\sum_{i=1}^E \\mathbb{E}_x[w_i(x)] = \\mathbb{E}_x\\bigl[\\sum_{i=1}^E w_i(x)\\bigr] = 1,\\]\n\nso by symmetry,\n\n\\[\\mathbb{E}_x[w_i(x)] = \\frac1E,\\quad\\forall i.\\]\n\nTo compute the variance, observe\n\n\\[\\mathrm{Var}_x(w_i(x)) = \\mathbb{E}_x[w_i(x)^2] - \\bigl(\\mathbb{E}_x[w_i(x)]\\bigr)^2 = \\mathbb{E}_x[w_i(x)^2] - \\frac1{E^2}.\\]\n\nWe use that\n\n\\[1 = \\bigl(\\sum_{i=1}^E w_i(x)\\bigr)^2 = \\sum_{i=1}^E w_i(x)^2 + 2\\sum_{i<j} w_i(x)w_j(x).\\]\n\nTaking expectation and using symmetry gives\n\n\\[1 = E\\,\\mathbb{E}_x[w_i(x)^2] + E(E-1)\\,\\mathbb{E}_x[w_i(x)w_j(x)],\\quad i\\neq j.\\]\n\nNext, since exactly k of the \\(w_i(x)\\) are nonzero and they sum to 1, by symmetry one shows\n\n\\[\\mathbb{E}_x[w_i(x)w_j(x)] = \\frac{k-1}{E(E-1)k}.\\]\n\nSubstituting into the previous relation yields\n\n\\[\\mathbb{E}_x[w_i(x)^2] = \\frac1{E k}.\\]\n\nHence\n\n\\[\\mathrm{Var}_x(w_i(x)) = \\frac1{E k} - \\frac1{E^2} = \\frac{E-k}{kE^2}.\\]\n\nThis completes the proof.", "ground-truth": "D"}
{"paper": "arxiv:2501.18922", "question": "The search space of logical forms is denoted by $F$ with size $|F| = k^L$, and the set of high-quality solutions is $F_{\\mathrm{optimal}} \\subseteq F$. For any search method $X$ with candidate set $F_X \\subseteq F$, its coverage is defined as\n$$C_X = \\frac{|F_X \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}.$$ \nThe Tree-of-Thoughts (ToT) method explores the full space $F$ with time complexity\n$$T_{\\mathrm{ToT}} = O\\bigl(k^L\\bigr),$$\nwhile the MCTS-based method, using $N$ rollouts and expanding $\\omega k$ candidates per step, has time complexity\n$$T_{\\mathrm{MCTS}} = O\\bigl(N \\cdot \\omega k \\cdot L\\bigr).$$\n\nPlease state and derive the inequalities in Proposition 4.2 showing that the MCTS-based heuristic method satisfies\n$$C_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}, \\quad T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}},$$\nand under effective strategy guidance,\n$$C_{\\mathrm{MCTS}} \\approx 1, \\quad C_{\\mathrm{ToT}} = 1?", "A": "To establish Proposition 4.2, we analyze the coverage \\( C_X \\) and time complexity \\( T_X \\) for the MCTS-based method and the ToT/CoT baselines under the following assumptions:\n\n1. **Search Space**: \\( |F| = k^L \\), where \\( k \\) is the branching factor and \\( L \\) is the depth.\n2. **High-Quality Solutions**: \\( |F_{\\text{optimal}}| = m \\), where \\( m \\ll k^L \\).\n3. **CoT Method**: Generates a single candidate solution (i.e., \\( |F_{\\text{CoT}}| = 1 \\)) with probability \\( p \\) of belonging to \\( F_{\\text{optimal}} \\).\n4. **MCTS Parameters**: Uses \\( N \\) rollouts, expands \\( \\omega k \\) candidates per step, and achieves a per-step success rate \\( \\alpha \\) under effective strategy guidance.\n\n### Derivation of Inequalities\n\n#### 1. **Coverage Comparison: \\( C_{\\text{MCTS}} \\gg C_{\\text{CoT}} \\)**\n- **CoT Coverage**:  \n  Since CoT generates one candidate, its coverage is bounded by:  \n  \\[\n  C_{\\text{CoT}} = \\frac{|F_{\\text{CoT}} \\cap F_{\\text{optimal}}|}{m} \\leq \\frac{1}{m}.\n  \\]\n  If \\( m \\geq 1 \\), \\( C_{\\text{CoT}} \\leq 1 \\), but typically \\( C_{\\text{CoT}} \\ll 1 \\) for large \\( m \\).\n\n- **MCTS Coverage**:  \n  MCTS explores \\( N \\cdot \\omega k \\cdot L \\) candidates. Under effective guidance, suppose it identifies \\( \\eta \\cdot m \\) solutions (\\( \\eta \\in [0, 1] \\)):  \n  \\[\n  C_{\\text{MCTS}} = \\frac{\\eta \\cdot m}{m} = \\eta.\n  \\]\n  For \\( \\eta \\approx 1 \\), \\( C_{\\text{MCTS}} \\approx 1 \\), which implies \\( C_{\\text{MCTS}} \\gg C_{\\text{CoT}} \\).\n\n#### 2. **Time Complexity: \\( T_{\\text{MCTS}} \\ll T_{\\text{ToT}} \\)**\n- **ToT Complexity**:  \n  \\[\n  T_{\\text{ToT}} = O(k^L),\n  \\]\n  which grows exponentially with \\( L \\).\n\n- **MCTS Complexity**:  \n  \\[\n  T_{\\text{MCTS}} = O(N \\cdot \\omega k \\cdot L),\n  \\]\n  which grows linearly with \\( L \\). For large \\( L \\), \\( N \\cdot \\omega k \\cdot L \\ll k^L \\), hence \\( T_{\\text{MCTS}} \\ll T_{\\text{ToT}} \\).\n\n#### 3. **Effective Strategy Guidance**\n- **MCTS Coverage**:  \n  With a high per-step success rate \\( \\alpha \\), the probability of finding a solution in \\( F_{\\text{optimal}} \\) is \\( \\alpha^L \\). For \\( N \\) rollouts:  \n  \\[\n  C_{\\text{MCTS}} = 1 - (1 - \\alpha^L)^N \\approx 1 \\quad \\text{when } N \\gg 1/\\alpha^L.\n  \\]\n- **ToT Coverage**:  \n  Since ToT exhaustively searches \\( F \\), \\( C_{\\text{ToT}} = \\frac{|F_{\\text{optimal}}|}{|F_{\\text{optimal}}|} = 1 \\).\n\n### Proposition 4.2 (Formal Statement)\nUnder the above assumptions:\n1. **Coverage**:  \n   \\[\n   C_{\\text{MCTS}} \\gg C_{\\text{CoT}}, \\quad \\text{and} \\quad C_{\\text{MCTS}} \\approx 1 \\quad \\text{(under effective guidance)}.\n   \\]\n2. **Time Complexity**:  \n   \\[\n   T_{\\text{MCTS}} \\ll T_{\\text{ToT}}.\n   \\]\n3. **ToT Baseline**:  \n   \\[\n   C_{\\text{ToT}} = 1 \\quad \\text{(exact coverage, but } T_{\\text{ToT}} \\text{ is intractable)}.\n   \\]\n\n### Summary\n- **MCTS** achieves near-optimal coverage \\( C_{\\text{MCTS}} \\approx 1 \\) with polynomial time \\( T_{\\text{MCTS}} \\), while **CoT** suffers from low coverage \\( C_{\\text{CoT}} \\ll 1 \\). \n- **ToT** guarantees \\( C_{\\text{ToT}} = 1 \\), but its exponential time complexity \\( T_{\\text{ToT}} \\) is infeasible for large \\( L \\).", "B": "Let us carefully derive the inequalities in Proposition 4.2 step by step.\n\n### Definitions Recap:\n1. **Search Space**: $|F| = k^L$ where $k$ is branching factor, $L$ is depth.\n2. **Optimal Solutions**: $F_{\\mathrm{optimal}} \\subseteq F$.\n3. **Coverage** for method $X$: $C_X = \\frac{|F_X \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}$.\n4. **Time Complexities**:\n   - ToT: $T_{\\mathrm{ToT}} = O(k^L)$ (exhaustive search).\n   - MCTS: $T_{\\mathrm{MCTS}} = O(N \\cdot \\omega k \\cdot L)$ ($N$ rollouts, $\\omega k$ expansions per step).\n\n---\n\n### Proposition 4.2:\nUnder reasonable assumptions, the MCTS-based method satisfies:\n1. **Coverage Superiority**: $C_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}$.\n2. **Time Efficiency**: $T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}$.\n3. **Effectiveness under Guidance**:\n   - $C_{\\mathrm{MCTS}} \\approx 1$ (near-optimal coverage),\n   - $C_{\\mathrm{ToT}} = 1$ (exhaustive coverage).\n\n---\n\n### Proof:\n\n#### 1. Time Complexity Comparison ($T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}$):\n- ToT explores the full space, so:\n  $$T_{\\mathrm{ToT}} = O(k^L).$$\n- MCTS scales polynomially:\n  $$T_{\\mathrm{MCTS}} = O(N \\cdot \\omega k \\cdot L).$$\n- For large $L$ and $k$, $k^L$ grows exponentially, while $N \\cdot \\omega k \\cdot L$ grows linearly in $L$ and $k$. Thus:\n  $$\\boxed{T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}}.$$\n\n#### 2. Coverage Comparison ($C_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}$):\n- **CoT (Chain-of-Thought)**: Typically generates a single path ($|F_{\\mathrm{CoT}}| = 1$), so:\n  $$C_{\\mathrm{CoT}} = \\frac{|F_{\\mathrm{CoT}} \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} \\leq \\frac{1}{|F_{\\mathrm{optimal}}|}.$$\n  Unless $|F_{\\mathrm{optimal}}| = 1$, $C_{\\mathrm{CoT}}$ is very small.\n- **MCTS**: Explores multiple paths ($|F_{\\mathrm{MCTS}}| = N \\cdot \\omega k \\cdot L$), so:\n  $$C_{\\mathrm{MCTS}} = \\frac{|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}.$$\n  With $N \\cdot \\omega k \\cdot L \\gg 1$, MCTS has higher probability of hitting $F_{\\mathrm{optimal}}$. Thus:\n  $$\\boxed{C_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}}.$$\n\n#### 3. Effectiveness under Guidance:\n- **MCTS with Heuristics**: If the rollout policy is well-guided (e.g., prioritizes high-reward branches), MCTS concentrates on promising regions. Assuming near-optimal guidance:\n  $$F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}} \\approx F_{\\mathrm{optimal}},$$\n  leading to:\n  $$\\boxed{C_{\\mathrm{MCTS}} \\approx 1}.$$\n- **ToT**: By design, ToT explores all $k^L$ possibilities, so:\n  $$F_{\\mathrm{ToT}} = F \\supseteq F_{\\mathrm{optimal}} \\implies F_{\\mathrm{ToT}} \\cap F_{\\mathrm{optimal}} = F_{\\mathrm{optimal}}.$$\n  Thus:\n  $$\\boxed{C_{\\mathrm{ToT}} = 1}.$$\n\n---\n\n### Summary:\n- MCTS achieves **exponentially faster time** ($T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}$) and **significantly better coverage** than CoT ($C_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}$).\n- Under effective guidance, MCTS approaches full coverage ($C_{\\mathrm{MCTS}} \\approx 1$), while ToT guarantees it ($C_{\\mathrm{ToT}} = 1$) at the cost of intractable search.", "C": "We denote the search space of logical forms by $F$ with $|F| = k^L$, where $k$ is the number of choices per step and $L$ is the maximum search depth. Let $F_{\\mathrm{optimal}} \\subseteq F$ be the set of high-quality solutions whose scores exceed a threshold. For any search method $X$ that generates a candidate set $F_X \\subseteq F$, define its coverage rate as\n\n$$\nC_X = \\frac{|F_X \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}.\n$$\n\n**1. Comparison with Chain-of-Thought (CoT):**\nCoT follows a single path $h_l$, so its candidate set is $F_{\\mathrm{CoT}} = \\{h_l\\}$.  Hence\n\n$$\nC_{\\mathrm{CoT}} = \\begin{cases}\n\\frac{1}{|F_{\\mathrm{optimal}}|}, & h_l \\in F_{\\mathrm{optimal}},\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n\nMCTS performs $N$ rollouts, producing the candidate set\n\n$$\nF_{\\mathrm{MCTS}} = \\bigcup_{n=1}^N \\{h_l^{(n)}\\}.\n$$\n\nUnder effective strategy and reward guidance, most of these paths fall into $F_{\\mathrm{optimal}}$, so\n\n$$\nC_{\\mathrm{MCTS}} = \\frac{|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} \\gg C_{\\mathrm{CoT}}.\n$$\n\n**2. Comparison with Tree-of-Thoughts (ToT):**\nToT exhaustively explores all paths in $F$, so its candidate set is $F_{\\mathrm{ToT}} = F$ and\n\n$$\nC_{\\mathrm{ToT}} = \\frac{|F \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} = 1,\n$$\n\nbut this requires time complexity\n\n$$\nT_{\\mathrm{ToT}} = O\\bigl(k^L\\bigr).\n$$\n\nMCTS only expands a fraction of the tree: if each rollout expands $\\omega k$ candidates (with $0<\\omega<1$) over depth $L$, then\n\n$$\nT_{\\mathrm{MCTS}} = O\\bigl(N \\cdot \\omega k \\cdot L\\bigr) \\ll T_{\\mathrm{ToT}} \\quad\\text{whenever }N\\,\\omega k\\,L \\ll k^L.\n$$\n\nMoreover, if the strategy and reward models focus the search on high-potential nodes, then almost all MCTS paths satisfy $h_l^{(n)}\\in F_{\\mathrm{optimal}}$, giving\n\n$$\n|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}| \\approx |F_{\\mathrm{optimal}}| \\quad\\Longrightarrow\\quad C_{\\mathrm{MCTS}} \\approx 1 = C_{\\mathrm{ToT}}.\n$$\n\n**Summary of Proposition 4.2 inequalities:**\n\n$$\nC_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}, \\quad T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}, \\quad C_{\\mathrm{MCTS}} \\approx 1, \\quad C_{\\mathrm{ToT}} = 1.\n$$", "D": "We denote the search space of logical forms by $F$ with $|F| = k^L$, where $k$ is the number of choices per step and $L$ is the maximum search depth. Let $F_{\\mathrm{optimal}} \\subseteq F$ be the set of high-quality solutions whose scores exceed a threshold. For any search method $X$ that generates a candidate set $F_X \\subseteq F$, define its coverage rate as\n\n$$\nC_X = \\frac{|F_X \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|}.\n$$\n\n**1. Comparison with Chain-of-Thought (CoT):**\nCoT follows a single path $h_l$, so its candidate set is $F_{\\mathrm{CoT}} = \\{h_l\\}$.  Hence\n\n$$\nC_{\\mathrm{CoT}} = \\begin{cases}\n\\frac{1}{|F_{\\mathrm{optimal}}|}, & h_l \\in F_{\\mathrm{optimal}},\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n\nMCTS performs $N$ rollouts, producing the candidate set\n\n$$\nF_{\\mathrm{MCTS}} = \\bigcup_{n=1}^N \\{h_l^{(n)}\\}.\n$$\n\nUnder effective strategy and reward guidance, most of these paths fall into $F_{\\mathrm{optimal}}$, so\n\n$$\nC_{\\mathrm{MCTS}} = \\frac{|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} \\gg C_{\\mathrm{CoT}}.\n$$\n\n**2. Comparison with Tree-of-Thoughts (ToT):**\nToT exhaustively explores all paths in $F$, so its candidate set is $F_{\\mathrm{ToT}} = F$ and\n\n$$\nC_{\\mathrm{ToT}} = \\frac{|F \\cap F_{\\mathrm{optimal}}|}{|F_{\\mathrm{optimal}}|} = 1,\n$$\n\nbut this requires time complexity\n\n$$\nT_{\\mathrm{ToT}} = O\\bigl(k^L\\bigr).\n$$\n\nMCTS only expands a fraction of the tree: if each rollout expands $\\omega k$ candidates (with $0<\\omega<1$) over depth $L$, then\n\n$$\nT_{\\mathrm{MCTS}} = O\\bigl(N \\cdot \\omega k \\cdot L\\bigr) \\ll T_{\\mathrm{ToT}} \\quad\\text{whenever }N\\,\\omega k\\,L \\ll k^L.\n$$\n\nMoreover, if the strategy and reward models focus the search on high-potential nodes, then almost all MCTS paths satisfy $h_l^{(n)}\\in F_{\\mathrm{optimal}}$, giving\n\n$$\n|F_{\\mathrm{MCTS}} \\cap F_{\\mathrm{optimal}}| \\approx |F_{\\mathrm{optimal}}| \\quad\\Longrightarrow\\quad C_{\\mathrm{MCTS}} \\approx C_{\\mathrm{ToT}}.\n$$\n\n**Summary of Proposition 4.2 inequalities:**\n\n$$\nC_{\\mathrm{MCTS}} \\gg C_{\\mathrm{CoT}}, \\quad T_{\\mathrm{MCTS}} \\ll T_{\\mathrm{ToT}}, \\quad C_{\\mathrm{MCTS}} \\approx C_{\\mathrm{ToT}}, \\quad C_{\\mathrm{ToT}} = 1.\n$$", "ground-truth": "C"}
{"paper": "arxiv:2501.18922", "question": "Let the model's initial performance on the annotated dataset $D_{a}$ be $E_{\\mathrm{base}}$, and let its performance after incremental fine-tuning be $E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^{*})$, where\n$$\\Delta E(\\gamma^{*}) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}\\bigl(D_{i}(\\gamma^{*})\\bigr).$$\nHere:\n- $\\lambda>0$ is a scaling factor,\n- $D_{i}(\\gamma^{*})$ is the set of logical-form-answer pairs $(\\widehat F_{Q},\\widehat A_{Q})$ drawn from the unannotated dataset $D_{n}$ of size $N_{n}=|D_{n}|$, filtered by the criteria $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_{Q})>\\gamma^{*}$ and $\\widehat A_{Q}\\neq\\emptyset$, \n- $N_{\\mathrm{high}}(\\gamma^{*}) = N_{n}\\cdot P_{\\mathrm{high}}(\\gamma^{*})$ with\n$$P_{\\mathrm{high}}(\\gamma^{*}) = \\int_{\\gamma^{*}}^{\\beta} P(R)\\,dR,\\quad\\text{where }\\beta\\text{ is the maximum reward score,}$$\n- $Q_{\\mathrm{avg}}\\bigl(D_{i}(\\gamma^{*})\\bigr)$ is the average quality of the incremental data.\n\nPlease state and derive the result in Proposition 4.3 showing that there exists some threshold $\\gamma^{*}<\\beta$ for which $\\Delta E(\\gamma^{*})$ grows significantly large.", "A": "First, we define the performance improvement of incremental fine-tuning, $\\Delta E(\\gamma^*)$. The model's initial performance on the annotated dataset $D_a$ is denoted as $E_{\\mathrm{base}}$, and its performance after incremental fine-tuning is\n> \\[E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^*).\\]\n> Here, $\\Delta E(\\gamma^*)$ represents the performance gain contributed by incremental data, which depends on the quantity and quality of the incremental data. \n\n> The incremental data originates from the unannotated dataset $D_n$, where the logical forms $\\widehat F_Q$ and answers $\\widehat A_Q$ are generated through a filtering mechanism. The filtering criteria are: 1) the reward score of the logical form $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_Q)>\\gamma^*$; 2) the answer set is non-empty, $\\widehat A_Q\\neq\\emptyset$. The logical forms that satisfy these criteria form the incremental dataset $D_i(\\gamma^*)$. The size and quality of this dataset are both dependent on the reward threshold $\\gamma^*$.\n\n> The performance improvement can be expressed as:\n> \\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^*) \\cdot Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr),\\]\n> where $\\lambda>0$ is a scaling factor; \n> \\[N_{\\mathrm{high}}(\\gamma^*) = N_n \\cdot P_{\\mathrm{high}}(\\gamma^*),\\quad P_{\\mathrm{high}}(\\gamma^*) = \\int_{\\gamma^*}^{\\beta} P(R)\\,dR;\\]\n> \\[Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr) = \\frac{\\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR}{\\int_{\\gamma^*}^{\\beta} P(R)\\,dR}.\\]\n\n> Substituting these into $\\Delta E(\\gamma^*)$ and simplifying, we get:\n> \\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_n \\cdot \\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR.\\]\n\n> To analyze the critical point, we take the derivative with respect to $\\gamma^*$:\n> \\[\\frac{\\partial \\Delta E(\\gamma^*)}{\\partial \\gamma^*} = -\\lambda \\cdot N_n \\cdot \\gamma^* \\cdot P(\\gamma^*).\\]\n> As $\\gamma^*\\to -\\infty$, this derivative is positive, indicating $\\Delta E(\\gamma^*)$ increases with $\\gamma^*$; as $\\gamma^*\\to \\beta$, the derivative is negative, indicating $\\Delta E(\\gamma^*)$ decreases. Thus, $\\Delta E(\\gamma^*)$ is a unimodal function with a maximum at some $\\gamma^*<\\beta$.\n\n**Conclusion:** There exists a reward threshold $\\gamma^*<\\beta$ such that the trade-off between the quantity and quality of incremental data is optimized and the performance improvement $\\Delta E(\\gamma^*)$ reaches its maximum.", "B": "To derive Proposition 4.3, we analyze the behavior of \\(\\Delta E(\\gamma^{*})\\) as a function of \\(\\gamma^{*}\\) and identify conditions under which it attains significant growth. \n\n### Proposition 4.3  \nThere exists a threshold \\(\\gamma^{*} < \\beta\\) such that the performance improvement \\(\\Delta E(\\gamma^{*})\\) grows significantly large, provided:  \n1. The reward distribution \\(P(R)\\) has sufficient density in the interval \\([\\gamma^{*}, \\beta]\\).  \n2. The average quality \\(Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*}))\\) does not decay faster than \\(P_{\\mathrm{high}}(\\gamma^{*})\\) as \\(\\gamma^{*}\\) increases.  \n\n### Derivation  \n\n1. **Expression for \\(\\Delta E(\\gamma^{*})\\):**  \n   \\[\n   \\Delta E(\\gamma^{*}) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*})).\n   \\]\n   Substituting \\(N_{\\mathrm{high}}(\\gamma^{*}) = N_{n} \\cdot P_{\\mathrm{high}}(\\gamma^{*})\\), we get:  \n   \\[\n   \\Delta E(\\gamma^{*}) = \\lambda N_{n} \\cdot P_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*})).\n   \\]\n\n2. **Behavior of \\(P_{\\mathrm{high}}(\\gamma^{*})\\):**  \n   \\[\n   P_{\\mathrm{high}}(\\gamma^{*}) = \\int_{\\gamma^{*}}^{\\beta} P(R) \\, dR.\n   \\]\n   - As \\(\\gamma^{*} \\to \\beta\\), \\(P_{\\mathrm{high}}(\\gamma^{*}) \\to 0\\).  \n   - For \\(\\gamma^{*} < \\beta\\), if \\(P(R)\\) is non-negligible in \\([\\gamma^{*}, \\beta]\\), then \\(P_{\\mathrm{high}}(\\gamma^{*})\\) remains substantial.  \n\n3. **Assumption on \\(Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*}))\\):**  \n   - Let \\(Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*}))\\) be lower-bounded by a function \\(Q_{\\mathrm{min}}(\\gamma^{*})\\) such that:  \n     \\[\n     Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*})) \\geq Q_{\\mathrm{min}}(\\gamma^{*}).\n     \\]\n   - Assume \\(Q_{\\mathrm{min}}(\\gamma^{*})\\) does not decay faster than \\(P_{\\mathrm{high}}(\\gamma^{*})\\), i.e.,  \n     \\[\n     \\lim_{\\gamma^{*} \\to \\beta} \\frac{Q_{\\mathrm{min}}(\\gamma^{*})}{P_{\\mathrm{high}}(\\gamma^{*})} \\geq c > 0.\n     \\]\n\n4. **Lower Bound for \\(\\Delta E(\\gamma^{*})\\):**  \n   \\[\n   \\Delta E(\\gamma^{*}) \\geq \\lambda N_{n} \\cdot P_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{min}}(\\gamma^{*}).\n   \\]\n   If \\(Q_{\\mathrm{min}}(\\gamma^{*}) = \\kappa \\cdot P_{\\mathrm{high}}(\\gamma^{*})\\) for some \\(\\kappa > 0\\), then:  \n   \\[\n   \\Delta E(\\gamma^{*}) \\geq \\lambda N_{n} \\kappa \\cdot [P_{\\mathrm{high}}(\\gamma^{*})]^{2}.\n   \\]\n   Thus, \\(\\Delta E(\\gamma^{*})\\) grows quadratically with \\(P_{\\mathrm{high}}(\\gamma^{*})\\).\n\n5. **Existence of \\(\\gamma^{*}\\):**  \n   - Since \\(P_{\\mathrm{high}}(\\gamma^{*})\\) is continuous and monotonically decreasing in \\(\\gamma^{*}\\), by the intermediate value theorem, there exists \\(\\gamma^{*} < \\beta\\) such that \\(P_{\\mathrm{high}}(\\gamma^{*})\\) is sufficiently large to make \\(\\Delta E(\\gamma^{*})\\) significant.  \n   - The optimal \\(\\gamma^{*}\\) balances the trade-off between selectivity (high \\(\\gamma^{*}\\)) and data volume (high \\(P_{\\mathrm{high}}(\\gamma^{*})\\)).  \n\n### Conclusion  \nUnder reasonable assumptions on \\(P(R)\\) and \\(Q_{\\mathrm{avg}}\\), there exists a threshold \\(\\gamma^{*} < \\beta\\) where \\(\\Delta E(\\gamma^{*})\\) attains significant growth, justifying the proposition.  \n\n\\[\n\\boxed{\\exists \\gamma^{*} < \\beta \\text{ such that } \\Delta E(\\gamma^{*}) \\text{ grows significantly large.}}\n\\]", "C": "To establish the existence of a threshold $\\gamma^* < \\beta$ where $\\Delta E(\\gamma^*)$ grows significantly large, we analyze the trade-off between $P_{\\mathrm{high}}(\\gamma)$ (the quantity of filtered data) and $Q_{\\mathrm{avg}}(D_i(\\gamma))$ (the quality of filtered data). The key steps are as follows:\n\n**Proposition 4.3:**  \n*Under the assumptions that:*\n1. $Q_{\\mathrm{avg}}(\\gamma)$ *is non-decreasing and differentiable in* $\\gamma$,\n2. $P(R)$ *is a continuous probability density function over* $[\\alpha, \\beta]$,\n\n*there exists a threshold* $\\gamma^* \\in (\\alpha, \\beta)$ *at which* $\\Delta E(\\gamma^*)$ *attains a local maximum, making it significantly large compared to endpoints.*\n\n**Proof:**  \nDefine the unnormalized performance gain component:  \n$$\nf(\\gamma) = P_{\\mathrm{high}}(\\gamma) \\cdot Q_{\\mathrm{avg}}(\\gamma), \\quad \\gamma \\in [\\alpha, \\beta].\n$$  \nSince $\\Delta E(\\gamma) = \\lambda N_n f(\\gamma)$, maximizing $f(\\gamma)$ suffices.\n\n1. **Boundary Behavior:**  \n   - At $\\gamma = \\beta$:  \n     $P_{\\mathrm{high}}(\\beta) = 0$ (integral over zero measure) $\\implies f(\\beta) = 0$.  \n   - At $\\gamma = \\alpha$:  \n     $P_{\\mathrm{high}}(\\alpha) = \\int_{\\alpha}^\\beta P(R)dR = 1$ (total probability). Assuming $Q_{\\mathrm{avg}}(\\alpha) = Q_{\\mathrm{min}} > 0$, $f(\\alpha) = Q_{\\mathrm{min}}$.  \n\n2. **Intermediate Value Analysis:**  \n   - Since $Q_{\\mathrm{avg}}(\\gamma)$ is non-decreasing, $Q_{\\mathrm{avg}}(\\gamma) \\geq Q_{\\mathrm{min}}$ for $\\gamma > \\alpha$.  \n   - $P_{\\mathrm{high}}(\\gamma)$ decreases continuously from $1$ to $0$ as $\\gamma$ increases from $\\alpha$ to $\\beta$.  \n\n3. **Differentiability and Critical Points:**  \n   Compute the derivative:  \n   $$\n   f'(\\gamma) = \\underbrace{\\frac{dP_{\\mathrm{high}}}{d\\gamma}}_{-P(\\gamma)} Q_{\\mathrm{avg}}(\\gamma) + P_{\\mathrm{high}}(\\gamma) \\underbrace{\\frac{dQ_{\\mathrm{avg}}}{d\\gamma}}_{Q'(\\gamma)}.\n   $$  \n   Setting $f'(\\gamma) = 0$ yields:  \n   $$\n   Q_{\\mathrm{avg}}(\\gamma) \\cdot P(\\gamma) = P_{\\mathrm{high}}(\\gamma) \\cdot Q'(\\gamma).\n   $$  \n\n4. **Existence of $\\gamma^*$:**  \n   - At $\\gamma = \\alpha$:  \n     $Q'(\\alpha) \\geq 0$ (non-decreasing), so $f'(\\alpha) = -P(\\alpha)Q_{\\mathrm{min}} + 1 \\cdot Q'(\\alpha)$.  \n     If $Q'(\\alpha) > P(\\alpha)Q_{\\mathrm{min}}$, then $f'(\\alpha) > 0$.  \n   - At $\\gamma \\to \\beta^-$:  \n     $P_{\\mathrm{high}}(\\gamma) \\to 0$, $Q_{\\mathrm{avg}}(\\gamma) \\to Q_{\\mathrm{max}}$, so $f'(\\gamma) \\approx -P(\\gamma)Q_{\\mathrm{max}} < 0$.  \n\n   By the Intermediate Value Theorem, if $f'(\\gamma)$ transitions from positive to negative and is continuous, there exists $\\gamma^* \\in (\\alpha, \\beta)$ where $f'(\\gamma^*) = 0$. At this point, $f(\\gamma)$ attains a local maximum, making $\\Delta E(\\gamma^*)$ significantly large compared to $\\gamma = \\alpha$ or $\\beta$.  \n\n5. **Conditions for Significant Growth:**  \n   - If $Q_{\\mathrm{avg}}(\\gamma)$ increases sufficiently rapidly (e.g., high-quality data concentrates above moderate $\\gamma$), the product $P_{\\mathrm{high}}(\\gamma)Q_{\\mathrm{avg}}(\\gamma)$ peaks at $\\gamma^*$.  \n   - The scaling factor $\\lambda N_n$ amplifies this peak, ensuring $\\Delta E(\\gamma^*)$ is non-trivial.  \n\nThus, under mild assumptions on $Q_{\\mathrm{avg}}(\\gamma)$ and $P(R)$, such a threshold $\\gamma^* < \\beta$ exists.  \n$\\blacksquare$", "D": "First, we define the performance improvement of incremental fine-tuning, $\\Delta E(\\gamma^*)$. The model's initial performance on the annotated dataset $D_a$ is denoted as $E_{\\mathrm{base}}$, and its performance after incremental fine-tuning is\n\\[E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^*).\\]\nHere, $\\Delta E(\\gamma^*)$ represents the performance gain contributed by incremental data, which depends on the quantity and quality of the incremental data. \n\nThe incremental data originates from the unannotated dataset $D_n$, where the logical forms $\\widehat F_Q$ and answers $\\widehat A_Q$ are generated through a filtering mechanism. The filtering criteria are: 1) the reward score of the logical form $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_Q)>\\gamma^*$; 2) the answer set is non-empty, $\\widehat A_Q\\neq\\emptyset$. The logical forms that satisfy these criteria form the incremental dataset $D_i(\\gamma^*)$. The size and quality of this dataset are both dependent on the reward threshold $\\gamma^*$.\n\nThe performance improvement can be expressed as:\n\\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^*) \\cdot Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr),\\]\nwhere $\\lambda>0$ is a scaling factor; \n\\[N_{\\mathrm{high}}(\\gamma^*) = N_n \\cdot P_{\\mathrm{high}}(\\gamma^*),\\quad P_{\\mathrm{high}}(\\gamma^*) = \\int_{\\gamma^*}^{\\beta} P(R)\\,dR;\\]\n\\[Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr) = \\frac{\\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR}{\\int_{\\gamma^*}^{\\beta} P(R)\\,dR}.\\]\n\nSubstituting these into $\\Delta E(\\gamma^*)$ and simplifying, we get:\n\\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_n \\cdot \\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR.\\]\n\nTo analyze the critical point, we take the derivative with respect to $\\gamma^*$:\n\\[\\frac{\\partial \\Delta E(\\gamma^*)}{\\partial \\gamma^*} = -\\lambda \\cdot N_n \\cdot \\gamma^* \\cdot P(\\gamma^*).\\]\nAs $\\gamma^*\\to -\\infty$, this derivative is positive, indicating $\\Delta E(\\gamma^*)$ increases with $\\gamma^*$; as $\\gamma^*\\to \\beta$, the derivative is negative, indicating $\\Delta E(\\gamma^*)$ decreases. Thus, $\\Delta E(\\gamma^*)$ is a unimodal function with a maximum at some $\\gamma^*<\\beta$.\n\n**Conclusion:** There exists a reward threshold $\\gamma^*<\\beta$ such that the trade-off between the quantity and quality of incremental data is optimized and the performance improvement $\\Delta E(\\gamma^*)$ reaches its maximum. However, this maximum is only guaranteed if $P(\\gamma^*)>0$ for all $\\gamma^*<\\beta$, which may not hold if the reward distribution $P(R)$ has gaps.", "ground-truth": "A"}
{"paper": "arxiv:2501.18922", "question": "A discrete dynamic system is defined by the state update \\(h_{t+1} = f(h_t, G)\\) with \\(h_t \\in H\\) and environment feedback \\(G\\), and the transition probability is \\(P(h_{t+1} \\mid h_t, G)\\). A Lyapunov function is given by \\(V(h_t) = -\\log P(h_t \\mid h^*, G)\\), with increment \\(\\Delta V(h_t) = V(h_{t+1}) - V(h_t)\\). Under the agent's environment awareness it holds that \\(P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 1\\) and \\(0 < P(h_{t+1} \\mid h_t, G) < 1\\), whereas for end-to-end methods either \\(P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 0\\) or \\(P(h_{t+1} \\mid h_t, \\mathrm{Exp}(h_t, G)) = 0\\). Please state and derive the inequalities in Proposition 4.1 showing that under agent environment awareness \\(\\Delta V(h_t) < 0\\) and bounded, but under end-to-end methods \\(P(h_{t+1} \\mid h_t, G) = 0\\) leads to unbounded \\(\\Delta V(h_t)\\).", "A": "Certainly! Below is the derivation of the inequalities in **Proposition 4.1**, comparing the behavior of \\(\\Delta V(h_t)\\) under **agent environment awareness** and **end-to-end methods**.\n\n---\n\n### **Proposition 4.1**  \nGiven the Lyapunov function \\(V(h_t) = -\\log P(h_t \\mid h^*, G)\\) and its increment \\(\\Delta V(h_t) = V(h_{t+1}) - V(h_t)\\), the following holds:  \n\n1. **Under agent environment awareness:**  \n   \\[\n   \\Delta V(h_t) < 0 \\quad \\text{and bounded.}\n   \\]  \n2. **Under end-to-end methods:**  \n   If \\(P(h_{t+1} \\mid h_t, G) = 0\\), then \\(\\Delta V(h_t)\\) becomes unbounded.  \n\n---\n\n### **Derivation**  \n\n#### **1. Agent Environment Awareness**  \nUnder agent environment awareness, we have:  \n\\[\nP(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 1,\n\\]  \nand \\(0 < P(h_{t+1} \\mid h_t, G) < 1\\).  \n\nThe Lyapunov increment is:  \n\\[\n\\Delta V(h_t) = -\\log P(h_{t+1} \\mid h^*, G) + \\log P(h_t \\mid h^*, G).\n\\]  \n\nSince the agent follows the expected transition \\(\\mathrm{Exp}(h_t, G)\\) with probability 1, we can write:  \n\\[\nP(h_{t+1} \\mid h^*, G) = P(\\mathrm{Exp}(h_t, G) \\mid h^*, G) \\geq P(h_t \\mid h^*, G) \\cdot P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = P(h_t \\mid h^*, G).\n\\]  \n\nThus:  \n\\[\n\\Delta V(h_t) = -\\log \\left( \\frac{P(h_{t+1} \\mid h^*, G)}{P(h_t \\mid h^*, G)} \\right) \\leq -\\log 1 = 0.\n\\]  \n\nHowever, since \\(P(h_{t+1} \\mid h_t, G) < 1\\), the inequality is strict:  \n\\[\n\\Delta V(h_t) < 0.\n\\]  \n\nAdditionally, since \\(P(h_{t+1} \\mid h_t, G)\\) is bounded away from 0, \\(\\Delta V(h_t)\\) remains bounded.  \n\n---\n\n#### **2. End-to-End Methods**  \nUnder end-to-end methods, either:  \n\\[\nP(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 0 \\quad \\text{or} \\quad P(h_{t+1} \\mid h_t, \\mathrm{Exp}(h_t, G)) = 0.\n\\]  \n\nIf \\(P(h_{t+1} \\mid h_t, G) = 0\\), then:  \n\\[\n\\Delta V(h_t) = -\\log P(h_{t+1} \\mid h^*, G) + \\log P(h_t \\mid h^*, G).\n\\]  \n\nSince \\(P(h_{t+1} \\mid h^*, G) \\leq P(h_{t+1} \\mid h_t, G) P(h_t \\mid h^*, G) = 0\\), we have:  \n\\[\n\\Delta V(h_t) = -\\log 0 + \\log P(h_t \\mid h^*, G) \\to +\\infty,\n\\]  \nbecause \\(-\\log 0 = +\\infty\\).  \n\nThus, \\(\\Delta V(h_t)\\) becomes **unbounded** when \\(P(h_{t+1} \\mid h_t, G) = 0\\).  \n\n---\n\n### **Conclusion**  \n- **Agent environment awareness** ensures \\(\\Delta V(h_t) < 0\\) (strict decrease) and bounded.  \n- **End-to-end methods** can lead to unbounded \\(\\Delta V(h_t)\\) when \\(P(h_{t+1} \\mid h_t, G) = 0\\).  \n\nThis highlights the stability advantage of agent environment awareness over end-to-end methods in this setting.  \n\n--- \n\nLet me know if you'd like any refinements or additional explanations!", "B": "For a dynamic system $\\dot{x}=f(x)$, where $x$ is the state and $f(x)$ is the nonlinear function describing state evolution. Define a non-negative scalar function $V(x)$ such that: $V(x)>0$ for all $x\\neq 0$ and $V(0)=0$ (positive definiteness). Its time derivative along the system trajectory satisfies $\\dot{V}(x)<0$ and is bounded (negative definiteness). Then $V(x)$ is called a Lyapunov function candidate, and the system (from Lyapunov's perspective) is asymptotically stable.\nFor a discrete dynamic system $x_{t+1}=f(x_t)$, if a Lyapunov function $V(x)$ can be defined, and the Lyapunov increment $\\Delta V(x_t)=V(x_{t+1})-V(x_t)$, then $V(x_t)>0$ and $\\Delta V(x_t)<0$ with boundedness implies asymptotic stability.\n\nOur KBQA-o1 system is modeled as a discrete dynamic system:\n$$h_{t+1}=f(h_t, G),$$\nwhere $h_t\\in H$ is the system state at step $t$; $G$ is environment feedback. The state transition probability is defined as $P(h_{t+1}\\mid h_t, G)$, which represents the probability of transitioning to the next state $h_{t+1}$, given the current state $h_t$ and the environment feedback $G$. The goal is for the system state $h_t$ to converge asymptotically to the target state $h^*$, satisfying $P(h^*\\mid h_t, G)=1$.\n\nWe define a positive-definite Lyapunov function $V(h_t)$ as:\n$$V(h_t)=-\\log P(h_t\\mid h^*, G),$$\nwhere $P(h_t\\mid h^*, G)$ is the posterior probability of state $h_t$ relative to the target state $h^*$; $V(h_t)$ quantifies the deviation of $h_t$ from $h^*$.\n\nIn KBQA-o1, our environment can provide all possible explorations under the prior state, and the correct exploration step is guaranteed to be included. Thus,\n$$P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)=1.$$ \nTherefore,\n$$P(h_{t+1}\\mid h_t,G)=P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr),$$\nand given a set of explorations containing the correct one, the probability of selecting it satisfies\n$$0<P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)<1.$$ \nConsequently,\n$$0<P(h_{t+1}\\mid h_t,G)<1.$$ \n\nSince $0<P(h_{t+1}\\mid h_t,G)<1$, we conclude\n$$\\Delta V(h_t)=V(h_{t+1})-V(h_t)=-\\log P(h_{t+1}\\mid h_t,G)>0$$\nand is bounded. Thus, the Lyapunov function change is strictly negative definite. Based on the Lyapunov Stability Second Theorem, $V(h_t)=-\\log P(h_t\\mid h^*,G)$ satisfies positive definiteness ($V(h_t)\\ge0$, with equality only when $h_t=h^*$) and negative definiteness ($\\Delta V(h_t)<0$), ensuring $V(h_t)$ strictly decreases. Hence, the agent system is asymptotically stable in the Lyapunov sense, and $h_t$ converges to $h^*$.\n\nHowever, for end-to-end methods, although both pre-retrieval and post-retrieval approaches can incorporate environmental information, pre-retrieval may yield an exploration space that does not include the target, giving\n$$P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)=0.$$ \nPost-retrieval may generate an incorrect logical form framework, yielding\n$$P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)=0.$$ \nIn such cases,\n$$P(h_{t+1}\\mid h_t,G)=P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)\\cdot P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)=0,$$\ncausing $\\Delta V(h_t)$ to be unbounded and leading to instability.\n\nTherefore, we conclude that the agent's awareness of the environment makes it more effective in generating optimal logical forms compared to end-to-end methods.", "C": "To address Proposition 4.1, we analyze the Lyapunov function increment \\(\\Delta V(h_t) = V(h_{t+1}) - V(h_t)\\) under two scenarios: agent environment awareness and end-to-end methods.\n\n---\n\n### **Agent Environment Awareness**\n**Conditions**:\n1. \\(P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 1\\), ensuring the agent always takes the intended action.\n2. \\(0 < P(h_{t+1} \\mid h_t, G) < 1\\), guaranteeing bounded transitions.\n\n**Derivation**:\nThe Lyapunov function is \\(V(h_t) = -\\log P(h_t \\mid h^*, G)\\). The increment becomes:\n\\[\n\\Delta V(h_t) = -\\log P(h_{t+1} \\mid h^*, G) + \\log P(h_t \\mid h^*, G) = \\log \\frac{P(h_t \\mid h^*, G)}{P(h_{t+1} \\mid h^*, G)}.\n\\]\nUnder environment awareness, the agent’s policy ensures \\(P(h_{t+1} \\mid h^*, G) > P(h_t \\mid h^*, G)\\) (i.e., the system progresses toward \\(h^*\\)). Thus:\n\\[\n\\Delta V(h_t) = \\log \\frac{P(h_t \\mid h^*, G)}{P(h_{t+1} \\mid h^*, G)} < 0.\n\\]\nBoundedness follows from \\(0 < P(h_{t+1} \\mid h_t, G) < 1\\). Let \\(\\eta \\in (0, 1)\\) be the minimal improvement ratio such that:\n\\[\nP(h_{t+1} \\mid h^*, G) \\geq \\eta P(h_t \\mid h^*, G).\n\\]\nThen:\n\\[\n\\Delta V(h_t) \\leq \\log \\frac{1}{\\eta} = -\\log \\eta,\n\\]\nwhich is bounded below by a negative constant. Hence, \\(\\Delta V(h_t) < 0\\) and bounded.\n\n---\n\n### **End-to-End Methods**\n**Conditions**:\nEither \\(P(\\mathrm{Exp}(h_t, G) \\mid h_t, G) = 0\\) (agent cannot take correct action) or \\(P(h_{t+1} \\mid h_t, \\mathrm{Exp}(h_t, G)) = 0\\) (invalid transition).\n\n**Derivation**:\nIf \\(P(h_{t+1} \\mid h_t, G) = 0\\), the state \\(h_{t+1}\\) becomes unreachable. Consequently:\n\\[\nP(h_{t+1} \\mid h^*, G) = 0 \\implies V(h_{t+1}) = -\\log 0 = +\\infty.\n\\]\nThus:\n\\[\n\\Delta V(h_t) = V(h_{t+1}) - V(h_t) = +\\infty - (-\\log P(h_t \\mid h^*, G)) = +\\infty,\n\\]\nwhich is unbounded. This divergence occurs because the Lyapunov function \\(V(h_{t+1})\\) catastrophically fails to assign finite value to an invalid state.\n\n---\n\n### **Proposition 4.1 (Inequalities)**\n1. **Agent Environment Awareness**:\n   \\[\n   \\Delta V(h_t) \\leq -\\log \\eta < 0 \\quad \\text{(bounded)}.\n   \\]\n2. **End-to-End Methods**:\n   If \\(P(h_{t+1} \\mid h_t, G) = 0\\), then:\n   \\[\n   \\Delta V(h_t) = +\\infty \\quad \\text{(unbounded)}.\n   \\]\n\n**Conclusion**:\nAgent environment awareness ensures stable convergence (\\(\\Delta V < 0\\)), while end-to-end methods risk unbounded divergence due to invalid transitions.", "D": "For a dynamic system $\\dot{x}=f(x)$, where $x$ is the state and $f(x)$ is the nonlinear function describing state evolution. Define a non-negative scalar function $V(x)$ such that: $V(x)>0$ for all $x\\neq 0$ and $V(0)=0$ (positive definiteness). Its time derivative along the system trajectory satisfies $\\dot{V}(x)<0$ and is bounded (negative definiteness). Then $V(x)$ is called a Lyapunov function candidate, and the system (from Lyapunov's perspective) is asymptotically stable.\nFor a discrete dynamic system $x_{t+1}=f(x_t)$, if a Lyapunov function $V(x)$ can be defined, and the Lyapunov increment $\\Delta V(x_t)=V(x_{t+1})-V(x_t)$, then $V(x_t)>0$ and $\\Delta V(x_t)<0$ with boundedness implies asymptotic stability.\n\nOur KBQA-o1 system is modeled as a discrete dynamic system:\n$$h_{t+1}=f(h_t, G),$$\nwhere $h_t\\in H$ is the system state at step $t$; $G$ is environment feedback. The state transition probability is defined as $P(h_{t+1}\\mid h_t, G)$, which represents the probability of transitioning to the next state $h_{t+1}$, given the current state $h_t$ and the environment feedback $G$. The goal is for the system state $h_t$ to converge asymptotically to the target state $h^*$, satisfying $P(h^*\\mid h_t, G)=1$.\n\nWe define a positive-definite Lyapunov function $V(h_t)$ as:\n$$V(h_t)=-\\log P(h_t\\mid h^*, G),$$\nwhere $P(h_t\\mid h^*, G)$ is the posterior probability of state $h_t$ relative to the target state $h^*$; $V(h_t)$ quantifies the deviation of $h_t$ from $h^*$.\n\nIn KBQA-o1, our environment can provide all possible explorations under the prior state, and the correct exploration step is guaranteed to be included. Thus,\n$$P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)=1.$$ \nTherefore,\n$$P(h_{t+1}\\mid h_t,G)=P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr),$$\nand given a set of explorations containing the correct one, the probability of selecting it satisfies\n$$0<P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)<1.$$ \nConsequently,\n$$0<P(h_{t+1}\\mid h_t,G)<1.$$ \n\nSince $0<P(h_{t+1}\\mid h_t,G)<1$, we conclude\n$$\\Delta V(h_t)=V(h_{t+1})-V(h_t)=-\\log P(h_{t+1}\\mid h_t,G)<0$$\nand is bounded. Thus, the Lyapunov function change is strictly negative definite. Based on the Lyapunov Stability Second Theorem, $V(h_t)=-\\log P(h_t\\mid h^*,G)$ satisfies positive definiteness ($V(h_t)\\ge0$, with equality only when $h_t=h^*$) and negative definiteness ($\\Delta V(h_t)<0$), ensuring $V(h_t)$ strictly decreases. Hence, the agent system is asymptotically stable in the Lyapunov sense, and $h_t$ converges to $h^*$.\n\nHowever, for end-to-end methods, although both pre-retrieval and post-retrieval approaches can incorporate environmental information, pre-retrieval may yield an exploration space that does not include the target, giving\n$$P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)=0.$$ \nPost-retrieval may generate an incorrect logical form framework, yielding\n$$P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)=0.$$ \nIn such cases,\n$$P(h_{t+1}\\mid h_t,G)=P\\bigl(\\mathrm{Exp}(h_t,G)\\mid h_t,G\\bigr)\\cdot P\\bigl(h_{t+1}\\mid h_t,\\mathrm{Exp}(h_t,G)\\bigr)=0,$$\ncausing $\\Delta V(h_t)$ to be unbounded and leading to instability.\n\nTherefore, we conclude that the agent's awareness of the environment makes it more effective in generating optimal logical forms compared to end-to-end methods.", "ground-truth": "D"}
{"paper": "arxiv:2309.06657", "question": "\n\nConsider a target probability distribution $\\pirpsi(y|x)$ related to a proposal distribution $\\pisft(y|x)$ and a reward function $r(x,y)$ by $\\pirpsi(y|x) \\propto \\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)$.\nA rejection sampling algorithm uses $\\pisft$ to propose candidates $y$. A candidate $y$ not already in the accepted set $D_x$ is accepted if $u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$, where $u \\sim U[0,1]$ and $M_{D_x} = \\max_{y' \\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}$.\n\nIt can be shown from the relationship between $\\pirpsi$, $\\pisft$, and $r(x,y)$ that:\n\\[\n\\frac{1}{M_{D_x}} = \\E_{y\\sim\\pisft(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta}\\cdot \\left(r(x,y)-\\max_{y'\\notin D_x} r(x,y')\\right)\\right)\\right] \\quad (*)\n\\]\n(This relationship is derived via steps analogous to Equations 11-14 in the source material).\n\nProve the following properties:\n\\begin{enumerate}\n    \\item The distribution of accepted samples is $\\pirpsi(y|x)$.\n    \\item The expected acceptance rate equals the right-hand side of equation (*).\n\\end{enumerate}\n\n", "A": "\n\n\\textit{Proof.} Let the process of generation be the one described and the accepted sequence set be $D_x$ at the current step, we have\n\\begin{align*}\n\\sP(\\text{sample }y\\text{ and get accepted}|x) &= \\sP\\left(u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right)\\pisft(y|x) \\\\\n&= \\frac{1}{M_{D_x}}\\pirpsi(y|x),\n\\end{align*}\nwhere $M_{D_x} \\triangleq \\min \\{m\\mid m\\cdot\\pisft(y|x) \\geq \\pirpsi(y|x) \\text{ for all } y\\notin D_x\\}$.\n\n\\begin{align*}\n\\sP(y \\text{ get accepted}|x) &= \\sP\\left(u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right) \\\\\n&= \\E \\mathbfone\\left[u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] \\\\\n&= \\E_{\\pisft}\\left[\\E \\mathbfone\\left[u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\middle\\vert y\\right]\\right] \\\\\n&= \\E_{\\pisft}\\left[\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] \\\\\n&= \\frac{1}{M_{D_x}}\n\\end{align*}\n\n\\begin{align*}\n\\sP(y|\\text{$y$ is accepted}, x) &= \\frac{\\sP(\\text{sample }y\\text{ and get accepted}|x)}{\\sP(y\\text{ get accepted}|x)} = \\pirpsi(y|x).\n\\end{align*}\n\nBy relationship (*), established from the definitions (analogous to deriving Eq. 18 from Eq. 13/14 and 16 in the source material), we have the acceptance rate\n\\[\n\\frac{1}{M_{D_x}} = \\E_{y\\sim\\pisft(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta}\\cdot \\left(r(x,y)-\\max_{y'\\notin D_x} r(x,y')\\right)\\right)\\right]\n\\]\nSince the expected acceptance rate is $\\sP(y \\text{ get accepted}|x) = 1/M_{D_x}$, this confirms the second property. \\qed\n\n", "B": "We will prove the two properties step by step.\n\n### 1. The distribution of accepted samples is \\(\\pi_{\\text{rw}}(y|x)\\)\n\n**Proof:**\n\nThe rejection sampling algorithm accepts a candidate \\(y\\) with probability:\n\\[\n\\text{Acceptance probability} = \\min\\left(1, \\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x} \\pi_{\\text{sf}}(y|x)}\\right).\n\\]\n\nThe joint probability of proposing \\(y\\) and accepting it is:\n\\[\n\\pi_{\\text{sf}}(y|x) \\cdot \\min\\left(1, \\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x} \\pi_{\\text{sf}}(y|x)}\\right) = \\min\\left(\\pi_{\\text{sf}}(y|x), \\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x}}\\right).\n\\]\n\nSince \\(M_{D_x} = \\max_{y' \\notin D_x} \\frac{\\pi_{\\text{rw}}(y'|x)}{\\pi_{\\text{sf}}(y'|x)}\\), we have \\(\\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x} \\pi_{\\text{sf}}(y|x)} \\leq 1\\) for all \\(y \\notin D_x\\). Thus, the acceptance probability simplifies to:\n\\[\n\\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x} \\pi_{\\text{sf}}(y|x)}.\n\\]\n\nThe probability density of accepted samples is proportional to:\n\\[\n\\pi_{\\text{sf}}(y|x) \\cdot \\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x} \\pi_{\\text{sf}}(y|x)} = \\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x}}.\n\\]\n\nSince \\(\\pi_{\\text{rw}}(y|x)\\) is already normalized, the distribution of accepted samples is indeed \\(\\pi_{\\text{rw}}(y|x)\\).\n\n### 2. The expected acceptance rate equals the right-hand side of equation (*)\n\n**Proof:**\n\nThe expected acceptance rate is the probability that a proposed sample \\(y\\) is accepted. This is given by:\n\\[\n\\mathbb{E}_{y \\sim \\pi_{\\text{sf}}(y|x)}\\left[\\frac{\\pi_{\\text{rw}}(y|x)}{M_{D_x} \\pi_{\\text{sf}}(y|x)}\\right] = \\frac{1}{M_{D_x}} \\mathbb{E}_{y \\sim \\pi_{\\text{sf}}(y|x)}\\left[\\frac{\\pi_{\\text{rw}}(y|x)}{\\pi_{\\text{sf}}(y|x)}\\right].\n\\]\n\nFrom the definition of \\(\\pi_{\\text{rw}}(y|x) \\propto \\pi_{\\text{sf}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)\\), we have:\n\\[\n\\frac{\\pi_{\\text{rw}}(y|x)}{\\pi_{\\text{sf}}(y|x)} \\propto \\exp\\left(\\frac{1}{\\beta} r(x,y)\\right).\n\\]\n\nLet \\(Z = \\mathbb{E}_{y \\sim \\pi_{\\text{sf}}(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)\\right]\\) be the normalization constant. Then:\n\\[\n\\frac{\\pi_{\\text{rw}}(y|x)}{\\pi_{\\text{sf}}(y|x)} = \\frac{\\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)}{Z}.\n\\]\n\nSubstituting back into the expected acceptance rate:\n\\[\n\\frac{1}{M_{D_x}} \\mathbb{E}_{y \\sim \\pi_{\\text{sf}}(y|x)}\\left[\\frac{\\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)}{Z}\\right] = \\frac{1}{M_{D_x} Z} \\mathbb{E}_{y \\sim \\pi_{\\text{sf}}(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)\\right] = \\frac{1}{M_{D_x}}.\n\\]\n\nFrom equation (*), we have:\n\\[\n\\frac{1}{M_{D_x}} = \\mathbb{E}_{y \\sim \\pi_{\\text{sf}}(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta} \\left(r(x,y) - \\max_{y' \\notin D_x} r(x,y')\\right)\\right)\\right].\n\\]\n\nThus, the expected acceptance rate equals the right-hand side of equation (*). \n\n### Summary:\n1. The distribution of accepted samples is \\(\\pi_{\\text{rw}}(y|x)\\) because the acceptance probability corrects the proposal distribution to match the target.\n2. The expected acceptance rate is \\(\\frac{1}{M_{D_x}}\\), which is equal to the expression derived in equation (*).", "C": "\\textit{Proof.} Let the process of generation be the one described and the accepted sequence set be $D_x$ at the current step, we have\n\\begin{align*}\n\\sP(\\text{sample }y\\text{ and get accepted}|x) &= \\sP\\left(u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right)\\pisft(y|x) \\\\\n&= \\frac{1}{M_{D_x}}\\pirpsi(y|x),\n\\end{align*}\nwhere $M_{D_x} \\triangleq \\min \\{m\\mid m\\cdot\\pisft(y|x) \\geq \\pirpsi(y|x) \\text{ for all } y\\notin D_x\\}$.\n\n\\begin{align*}\n\\sP(y \\text{ get accepted}|x) &= \\sP\\left(u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right) \\\\\n&= \\E \\mathbfone\\left[u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] \\\\\n&= \\E_{\\pisft}\\left[\\E \\mathbfone\\left[u < \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\middle\\vert y\\right]\\right] \\\\\n&= \\E_{\\pisft}\\left[\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] \\\\\n&= \\frac{1}{M_{D_x}}\n\\end{align*}\n\n\\begin{align*}\n\\sP(y|\\text{$y$ is accepted}, x) &= \\frac{\\sP(\\text{sample }y\\text{ and get accepted}|x)}{\\sP(y\\text{ get accepted}|x)} = \\frac{\\pirpsi(y|x)}{M_{D_x}}.\n\\end{align*}\n\nBy relationship (*), established from the definitions (analogous to deriving Eq. 18 from Eq. 13/14 and 16 in the source material), we have the acceptance rate\n\\[\n\\frac{1}{M_{D_x}} = \\E_{y\\sim\\pisft(y|x)}\\left[\\exp\\left(\\frac{1}{\\beta}\\cdot \\left(r(x,y)-\\max_{y'\\notin D_x} r(x,y')\\right)\\right)\\right]\n\\]\nSince the expected acceptance rate is $\\sP(y \\text{ get accepted}|x) = 1/M_{D_x}$, this confirms the second property. \\qed", "D": "1. **Distribution of Accepted Samples is $\\pirpsi(y|x)$:**\n\n   When $D_x$ is empty, $M_{D_x} = \\max_y \\frac{\\pirpsi(y|x)}{\\pisft(y|x)}$. The acceptance probability for a candidate $y$ is:\n   \\[\n   \\alpha(y) = \\frac{\\pirpsi(y|x)}{M_{D_x} \\pisft(y|x)}.\n   \\]\n   The joint distribution of accepted samples is proportional to:\n   \\[\n   \\pisft(y|x) \\cdot \\alpha(y) = \\frac{\\pirpsi(y|x)}{M_{D_x}}.\n   \\]\n   Normalizing over all $y$, the density becomes $\\pirpsi(y|x)$, as the normalization constant $Z = \\frac{1}{M_{D_x}} \\sum_y \\pirpsi(y|x) = \\frac{1}{M_{D_x}}$ cancels out. Hence, the distribution of accepted samples is $\\pirpsi(y|x)$.\n\n2. **Expected Acceptance Rate Equals RHS of (*):**\n\n   The expected acceptance rate is:\n   \\[\n   \\mathbb{E}_{y\\sim\\pisft}\\left[\\mathbb{I}(y \\notin D_x) \\cdot \\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right].\n   \\]\n   When $D_x$ is empty, this simplifies to:\n   \\[\n   \\mathbb{E}_{y\\sim\\pisft}\\left[\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}\\right] = \\frac{1}{M_{D_x}} \\underbrace{\\mathbb{E}_{y\\sim\\pisft}\\left[\\frac{\\pirpsi(y|x)}{\\pisft(y|x)}\\right]}_{=1},\n   \\]\n   where the expectation equals $1$ because $\\pirpsi$ is normalized. By equation (*), $\\frac{1}{M_{D_x}}$ equals the RHS of (*), proving the result.", "ground-truth": "A"}
{"paper": "arxiv:2309.06657", "question": "Consider a target probability distribution $\\pirpsi(y|x)$ which is related to a proposal distribution $\\pisft(y|x)$ and a reward function $r_\\psi(x,y)$ via the relationship:\n\\begin{equation}\n\\pirpsi(y|x) = \\frac{1}{\\Zpsi(x)}\\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\quad (1)\n\\end{equation}\nwhere $\\beta$ is a positive constant and $\\Zpsi(x)$ is the partition function defined as:\n\\[\n\\Zpsi(x) = \\sum_{y'} \\pisft(y'|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)\n\\]\n\nIn a rejection sampling scheme using $\\pisft$ to sample from $\\pirpsi$, the acceptance probability for a candidate $y$ depends on the ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$. Here, $D_x$ represents the set of samples already accepted for context $x$, and $M_{D_x}$ is the rejection sampling constant defined as the maximum value of the ratio $\\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}$ over all candidates $y'$ not yet accepted ($y' \\notin D_x$):\n\\[\nM_{D_x} \\triangleq \\max_{y'\\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}\n\\]\n\nDerive an expression for the acceptance ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$ purely in terms of the reward function $r_\\psi(x,y)$ and the constant $\\beta$.\n", "A": "According to the given relationship (1), we have:\n\\[\n\\pirpsi(y|x) = \\frac{1}{\\Zpsi(x)}\\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)\n\\]\nwhere $\\Zpsi(x) = \\sum_{y'} \\pisft(y'|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)$.\n\nFrom (1), we can express the ratio of the target distribution to the proposal distribution:\n\\begin{equation}\n\\frac{\\pirpsi(y|x)}{\\pisft(y|x)} = \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\quad (2)\n\\end{equation}\n\nThe rejection sampling constant $M_{D_x}$ is defined as:\n\\[\nM_{D_x} \\triangleq \\max_{y'\\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}\n\\]\nSubstituting the expression (2) into the definition of $M_{D_x}$:\n\\begin{align}\nM_{D_x} &= \\max_{y'\\notin D_x} \\left[ \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right) \\right] \\nonumber \\\\\n&= \\frac{1}{\\Zpsi(x)}\\max_{y'\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)\\right] \\quad (3)\n\\end{align}\n\nNow, we derive the required acceptance ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$. We can rewrite this as $\\frac{1}{M_{D_x}} \\cdot \\frac{\\pirpsi(y|x)}{\\pisft(y|x)}$ and substitute using (2) and (3):\n\\begin{align*}\n\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)} &= \\frac{1}{\\frac{1}{\\Zpsi(x)}\\max_{y''\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y'')\\right)\\right]} \\cdot \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\\\\n&= \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\max_{y''\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y'')\\right)\\right]} \\\\\n&= \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\exp\\left(\\max_{y''\\notin D_x} \\frac{1}{\\beta}r_\\psi(x,y'')\\right)} \\quad (\\text{Since } \\max(\\exp(a)) = \\exp(\\max(a)) ) \\\\\n&= \\exp\\left( \\frac{1}{\\beta}r_\\psi(x,y) - \\max_{y''\\notin D_x} \\frac{1}{\\beta}r_\\psi(x,y'') \\right) \\\\\n&= \\exp\\left( \\frac{1}{\\beta} \\left( r_\\psi(x,y) - \\max_{y'\\notin D_x} r_\\psi(x,y') \\right) \\right) \\cdot \\frac{1}{\\Zpsi(x)} \\quad \\text{(Incorrectly introduced factor)}\n\\end{align*}", "B": "The acceptance ratio $\\frac{\\pi^{r_\\psi}(y|x)}{M_{D_x}\\pi^{\\text{sft}}(y|x)}$ can be derived as follows:\n\n1. **Substitute $\\pi^{r_\\psi}(y|x)$ from Eq. (1):**  \n   \\[\n   \\frac{\\pi^{r_\\psi}(y|x)}{M_{D_x}\\pi^{\\text{sft}}(y|x)} = \\frac{1}{M_{D_x}} \\cdot \\frac{\\pi^{\\text{sft}}(y|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)/Z_\\psi(x)}{\\pi^{\\text{sft}}(y|x)} = \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{Z_\\psi(x) M_{D_x}}\n   \\]\n\n2. **Express $M_{D_x}$ using its definition:**  \n   \\[\n   M_{D_x} = \\max_{y' \\notin D_x} \\frac{\\pi^{r_\\psi}(y'|x)}{\\pi^{\\text{sft}}(y'|x)} = \\max_{y' \\notin D_x} \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)}{Z_\\psi(x)}\n   \\]\n   Hence:\n   \\[\n   Z_\\psi(x) M_{D_x} = \\max_{y' \\notin D_x} \\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)\n   \\]\n\n3. **Substitute $Z_\\psi(x) M_{D_x}$ into the acceptance ratio:**  \n   \\[\n   \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{Z_\\psi(x) M_{D_x}} = \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\max_{y' \\notin D_x} \\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)}\n   \\]\n\n4. **Simplify using properties of exponentials:**  \n   \\[\n   \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\exp\\left(\\frac{1}{\\beta} \\max_{y' \\notin D_x} r_\\psi(x,y')\\right)} = \\exp\\left(\\frac{1}{\\beta}\\left[r_\\psi(x,y) - \\max_{y' \\notin D_x} r_\\psi(x,y')\\right]\\right)\n   \\]\n\n**Final Result:**  \nThe acceptance ratio is:\n\\[\n\\boxed{\\exp\\left(\\frac{1}{\\beta}\\left[r_\\psi(x,y) - \\max_{y' \\notin D_x} r_\\psi(x,y')\\right]\\right)}\n\\]", "C": "To derive the acceptance ratio \\(\\frac{\\pi_{\\psi}(y|x)}{M_{D_x}\\pi_{\\text{ft}}(y|x)}\\) purely in terms of the reward function \\(r_\\psi(x,y)\\) and the constant \\(\\beta\\), let's proceed step-by-step.\n\n### Step 1: Express \\(\\pi_{\\psi}(y|x)\\) in Terms of \\(\\pi_{\\text{ft}}(y|x)\\) and \\(r_\\psi(x,y)\\)\nFrom equation (1), we have:\n\\[\n\\pi_{\\psi}(y|x) = \\frac{1}{Z_{\\psi}(x)} \\pi_{\\text{ft}}(y|x) \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y)\\right)\n\\]\nwhere the partition function \\(Z_{\\psi}(x)\\) is:\n\\[\nZ_{\\psi}(x) = \\sum_{y'} \\pi_{\\text{ft}}(y'|x) \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y')\\right).\n\\]\n\n### Step 2: Compute the Ratio \\(\\frac{\\pi_{\\psi}(y|x)}{\\pi_{\\text{ft}}(y|x)}\\)\nDivide both sides of the expression for \\(\\pi_{\\psi}(y|x)\\) by \\(\\pi_{\\text{ft}}(y|x)\\):\n\\[\n\\frac{\\pi_{\\psi}(y|x)}{\\pi_{\\text{ft}}(y|x)} = \\frac{1}{Z_{\\psi}(x)} \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y)\\right).\n\\]\n\n### Step 3: Express \\(M_{D_x}\\) in Terms of the Reward Function\nThe rejection sampling constant \\(M_{D_x}\\) is defined as:\n\\[\nM_{D_x} \\triangleq \\max_{y' \\notin D_x} \\frac{\\pi_{\\psi}(y'|x)}{\\pi_{\\text{ft}}(y'|x)} = \\max_{y' \\notin D_x} \\left[ \\frac{1}{Z_{\\psi}(x)} \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y')\\right) \\right].\n\\]\nSince \\(Z_{\\psi}(x)\\) is the same for all \\(y'\\), we can write:\n\\[\nM_{D_x} = \\frac{1}{Z_{\\psi}(x)} \\max_{y' \\notin D_x} \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y')\\right).\n\\]\n\n### Step 4: Derive the Acceptance Ratio\nThe acceptance ratio is:\n\\[\n\\frac{\\pi_{\\psi}(y|x)}{M_{D_x} \\pi_{\\text{ft}}(y|x)} = \\frac{\\frac{1}{Z_{\\psi}(x)} \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y)\\right)}{\\frac{1}{Z_{\\psi}(x)} \\max_{y' \\notin D_x} \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y')\\right)}.\n\\]\nThe \\(Z_{\\psi}(x)\\) terms cancel out, leaving:\n\\[\n\\frac{\\pi_{\\psi}(y|x)}{M_{D_x} \\pi_{\\text{ft}}(y|x)} = \\frac{\\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y)\\right)}{\\max_{y' \\notin D_x} \\exp\\left(\\frac{1}{\\beta} r_\\psi(x,y')\\right)}.\n\\]\nUsing the property of exponentials, this simplifies to:\n\\[\n\\frac{\\pi_{\\psi}(y|x)}{M_{D_x} \\pi_{\\text{ft}}(y|x)} = \\exp\\left(\\frac{1}{\\beta} \\left( r_\\psi(x,y) - \\max_{y' \\notin D_x} r_\\psi(x,y') \\right) \\right).\n\\]\n\n### Final Expression\nThus, the acceptance ratio is given purely in terms of the reward function \\(r_\\psi(x,y)\\) and the constant \\(\\beta\\) as:\n\\[\n\\boxed{ \\frac{\\pi_{\\psi}(y|x)}{M_{D_x} \\pi_{\\text{ft}}(y|x)} = \\exp\\left( \\frac{1}{\\beta} \\left( r_\\psi(x,y) - \\max_{y' \\notin D_x} r_\\psi(x,y') \\right) \\right) }\n\\]", "D": "According to the given relationship (1), we have:\n\\[\n\\pirpsi(y|x) = \\frac{1}{\\Zpsi(x)}\\pisft(y|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)\n\\]\nwhere $\\Zpsi(x) = \\sum_{y'} \\pisft(y'|x)\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)$.\n\nFrom (1), we can express the ratio of the target distribution to the proposal distribution:\n\\begin{equation}\n\\frac{\\pirpsi(y|x)}{\\pisft(y|x)} = \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\quad (2)\n\\end{equation}\n\nThe rejection sampling constant $M_{D_x}$ is defined as:\n\\[\nM_{D_x} \\triangleq \\max_{y'\\notin D_x} \\frac{\\pirpsi(y'|x)}{\\pisft(y'|x)}\n\\]\nSubstituting the expression (2) into the definition of $M_{D_x}$:\n\\begin{align}\nM_{D_x} &= \\max_{y'\\notin D_x} \\left[ \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right) \\right] \\nonumber \\\\\n&= \\frac{1}{\\Zpsi(x)}\\max_{y'\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y')\\right)\\right] \\quad (3)\n\\end{align}\n\nNow, we derive the required acceptance ratio $\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)}$. We can rewrite this as $\\frac{1}{M_{D_x}} \\cdot \\frac{\\pirpsi(y|x)}{\\pisft(y|x)}$ and substitute using (2) and (3):\n\\begin{align*}\n\\frac{\\pirpsi(y|x)}{M_{D_x}\\pisft(y|x)} &= \\frac{1}{\\frac{1}{\\Zpsi(x)}\\max_{y''\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y'')\\right)\\right]} \\cdot \\frac{1}{\\Zpsi(x)}\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right) \\\\\n&= \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\max_{y''\\notin D_x} \\left[\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y'')\\right)\\right]} \\\\\n&= \\frac{\\exp\\left(\\frac{1}{\\beta}r_\\psi(x,y)\\right)}{\\exp\\left(\\max_{y''\\notin D_x} \\frac{1}{\\beta}r_\\psi(x,y'')\\right)} \\quad (\\text{Since } \\max(\\exp(a)) = \\exp(\\max(a)) ) \\\\\n&= \\exp\\left( \\frac{1}{\\beta}r_\\psi(x,y) - \\max_{y''\\notin D_x} \\frac{1}{\\beta}r_\\psi(x,y'') \\right) \\\\\n&= \\exp\\left( \\frac{1}{\\beta} \\left( r_\\psi(x,y) - \\max_{y'\\notin D_x} r_\\psi(x,y') \\right) \\right)\n\\end{align*}\nThis provides the expression for the acceptance ratio purely in terms of the reward function $r_\\psi$ and the constant $\\beta$.\n", "ground-truth": "D"}
{"paper": "arxiv:2404.05868", "question": "Proposition 1 asserts that\n\n$$\nF(\\theta) = \\int_{\\mathcal{X}} k(x,\\theta)\\,\\mathrm{d}\\mu(x) \\le C\\,\\|\\theta\\|^2.\n$$\n\nIn this statement:\n- $\\mathcal{X}$ is a measurable space,\n- $k\\colon\\mathcal{X}\\times\\mathbb{R}^d\\to\\mathbb{R}$ is a continuous kernel,\n- $\\mu$ is a probability measure on $\\mathcal{X}$,\n- $C>0$ is a constant,\n- $\\|\\theta\\|$ denotes the Euclidean norm of $\\theta\\in\\mathbb{R}^d$,\n- and it is assumed that $k$ satisfies the Lipschitz condition\n  $$|k(x,\\theta_1)-k(x,\\theta_2)| \\le L\\,\\|\\theta_1-\\theta_2\\|$$\n  for all $x\\in\\mathcal{X}$ and some $L>0$.\n\nHow can one derive and prove Proposition 1?", "A": "Adopt the shorthand\n\n$$R_i = \\log\\frac{\\pi_\\theta(y_i\\mid x_i)}{\\pi_{\\mathrm{ref}}(y_i\\mid x_i)}.\n$$\n\n1. **First part.** For any $i\\in[n_f]$, observe that\n\n$$\n\\lim_{\\beta\\to0}\\Bigl(-\\tfrac{2}{\\beta}\\log\\sigma(-\\beta R_i) - \\tfrac{2}{\\beta}\\log2\\Bigr)\n= \\lim_{\\beta\\to0}\\tfrac{2}{\\beta}\\log\\frac{1+e^{\\beta R_i}}{2}\n= R_i.\n$$\n\nAveraging over $i=1,\\dots,n_f$ and noting\n\n$$\n\\frac{1}{n_f}\\sum_{i=1}^{n_f}R_i\n= L_{\\mathrm{GA}}(\\theta) - \\mathbb{E}_{(x,y)\\sim D_{\\mathrm{FG}}}\\bigl[\\log\\pi_{\\mathrm{ref}}(y\\mid x)\\bigr]\n$$\n\nyields the first claimed limit. :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}\n\n2. **Second part.** By definition,\n\n$$\n\\nabla_\\theta L_{\\mathrm{NPO},\\beta}(\\theta)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\Bigl(-\\tfrac{2}{\\beta}\\nabla_\\theta\\log\\sigma(-\\beta R_i)\\Bigr)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\frac{2}{\\beta}\\nabla_\\theta\\log(1+e^{\\beta R_i})\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\frac{2e^{\\beta R_i}}{1+e^{\\beta R_i}}\\,\\nabla_\\theta R_i.\n$$\n\nTaking the limit as $\\beta\\to0$ gives\n\n$$\n\\lim_{\\beta\\to0}\\nabla_\\theta L_{\\mathrm{NPO},\\beta}(\\theta)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\nabla_\\theta R_i\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\nabla_\\theta\\log\\pi_\\theta(y_i\\mid x_i)\n= \\nabla_\\theta L_{\\mathrm{GA}}(\\theta).\n$$\n\nThis completes the proof of Proposition 1.", "B": "To derive and prove Proposition 1, we will proceed step-by-step, leveraging the given conditions, particularly the Lipschitz continuity of the kernel \\( k \\). Here's the detailed proof:\n\n---\n\n### **Proof of Proposition 1**\n\n**Given:**\n- \\( \\mathcal{X} \\) is a measurable space,\n- \\( k: \\mathcal{X} \\times \\mathbb{R}^d \\to \\mathbb{R} \\) is a continuous kernel,\n- \\( \\mu \\) is a probability measure on \\( \\mathcal{X} \\),\n- \\( C > 0 \\) is a constant,\n- \\( \\|\\theta\\| \\) is the Euclidean norm of \\( \\theta \\in \\mathbb{R}^d \\),\n- \\( k \\) satisfies the Lipschitz condition:\n  \\[\n  |k(x, \\theta_1) - k(x, \\theta_2)| \\le L \\|\\theta_1 - \\theta_2\\|,\n  \\]\n  for all \\( x \\in \\mathcal{X} \\) and some \\( L > 0 \\).\n\n**To show:**\n\\[\nF(\\theta) = \\int_{\\mathcal{X}} k(x, \\theta) \\, \\mathrm{d}\\mu(x) \\le C \\|\\theta\\|^2.\n\\]\n\n---\n\n#### **Step 1: Analyze the Lipschitz Condition**\n\nThe Lipschitz condition implies that \\( k(x, \\theta) \\) grows at most linearly in \\( \\|\\theta\\| \\). Specifically, fix \\( \\theta_2 = 0 \\) (the origin in \\( \\mathbb{R}^d \\)), then:\n\\[\n|k(x, \\theta) - k(x, 0)| \\le L \\|\\theta\\|.\n\\]\nBy the triangle inequality,\n\\[\n|k(x, \\theta)| \\le |k(x, 0)| + L \\|\\theta\\|.\n\\]\nLet \\( M = \\sup_{x \\in \\mathcal{X}} |k(x, 0)| \\). Since \\( k \\) is continuous and \\( \\mathcal{X} \\) is measurable, \\( M \\) is finite (assuming \\( \\mathcal{X} \\) is compact or \\( k(x, 0) \\) is bounded). Thus:\n\\[\n|k(x, \\theta)| \\le M + L \\|\\theta\\|.\n\\]\n\n---\n\n#### **Step 2: Bound the Integral \\( F(\\theta) \\)**\n\nSince \\( \\mu \\) is a probability measure (\\( \\int_{\\mathcal{X}} \\mathrm{d}\\mu(x) = 1 \\)), we have:\n\\[\nF(\\theta) = \\int_{\\mathcal{X}} k(x, \\theta) \\, \\mathrm{d}\\mu(x) \\le \\int_{\\mathcal{X}} |k(x, \\theta)| \\, \\mathrm{d}\\mu(x).\n\\]\nUsing the bound from Step 1:\n\\[\nF(\\theta) \\le \\int_{\\mathcal{X}} (M + L \\|\\theta\\|) \\, \\mathrm{d}\\mu(x) = M + L \\|\\theta\\|.\n\\]\nHowever, this only gives a linear bound in \\( \\|\\theta\\| \\), whereas we need a quadratic bound \\( C \\|\\theta\\|^2 \\).\n\n---\n\n#### **Step 3: Strengthen the Bound Using Quadratic Growth**\n\nTo obtain a quadratic bound, we assume additional structure on \\( k \\). A common assumption (not explicitly given but often implied in such contexts) is that \\( k(x, 0) = 0 \\) for all \\( x \\in \\mathcal{X} \\). If this holds, then \\( M = 0 \\), and:\n\\[\n|k(x, \\theta)| \\le L \\|\\theta\\|.\n\\]\nNow, suppose \\( k(x, \\theta) \\) is also *positively homogeneous* in \\( \\theta \\), i.e., \\( k(x, c \\theta) = c^2 k(x, \\theta) \\) for \\( c > 0 \\). Then, for \\( \\theta \\neq 0 \\), let \\( \\theta = \\|\\theta\\| \\cdot \\frac{\\theta}{\\|\\theta\\|} \\), and:\n\\[\nk(x, \\theta) = \\|\\theta\\|^2 k\\left(x, \\frac{\\theta}{\\|\\theta\\|}\\right).\n\\]\nSince \\( \\frac{\\theta}{\\|\\theta\\|} \\) has unit norm, let:\n\\[\nK = \\sup_{x \\in \\mathcal{X}, \\|\\theta\\| = 1} |k(x, \\theta)|.\n\\]\nThen:\n\\[\n|k(x, \\theta)| \\le K \\|\\theta\\|^2.\n\\]\nIntegrating over \\( \\mathcal{X} \\):\n\\[\nF(\\theta) \\le \\int_{\\mathcal{X}} K \\|\\theta\\|^2 \\, \\mathrm{d}\\mu(x) = K \\|\\theta\\|^2.\n\\]\nThus, \\( C = K \\) satisfies the proposition.\n\n---\n\n#### **Step 4: Relaxing the Assumptions**\n\nIf \\( k(x, 0) \\neq 0 \\) or \\( k \\) is not homogeneous, we can still derive a quadratic bound by combining the linear bound with a growth condition. For large \\( \\|\\theta\\| \\), the quadratic term dominates. Specifically, suppose:\n\\[\n|k(x, \\theta)| \\le C_1 \\|\\theta\\|^2 + C_2,\n\\]\nfor constants \\( C_1, C_2 > 0 \\). Then:\n\\[\nF(\\theta) \\le C_1 \\|\\theta\\|^2 + C_2.\n\\]\nFor \\( \\|\\theta\\| \\ge 1 \\), \\( \\|\\theta\\|^2 \\ge \\|\\theta\\| \\), so:\n\\[\nF(\\theta) \\le (C_1 + C_2) \\|\\theta\\|^2.\n\\]\nFor \\( \\|\\theta\\| < 1 \\), the linear bound \\( F(\\theta) \\le M + L \\|\\theta\\| \\) can be adjusted to a quadratic bound by choosing \\( C \\) large enough. Thus, in all cases, there exists \\( C > 0 \\) such that:\n\\[\nF(\\theta) \\le C \\|\\theta\\|^2.\n\\]\n\n---\n\n#### **Conclusion**\n\nUnder the given Lipschitz condition and reasonable assumptions on the growth of \\( k(x, \\theta) \\), Proposition 1 holds with some constant \\( C > 0 \\). The exact value of \\( C \\) depends on the specific properties of \\( k \\), but the quadratic bound is achievable.\n\n\\[\n\\boxed{F(\\theta) \\le C \\|\\theta\\|^2}\n\\]\n\n--- \n\n### **Key Observations:**\n1. The Lipschitz condition ensures linear growth in \\( \\|\\theta\\| \\), but additional structure (e.g., homogeneity or \\( k(x, 0) = 0 \\)) is needed for a clean quadratic bound.\n2. If no such structure is given, a combined linear-quadratic bound can still imply the result for large \\( \\|\\theta\\| \\), with adjustments for small \\( \\|\\theta\\| \\).\n3. The constant \\( C \\) depends on \\( L \\), \\( M \\), and the behavior of \\( k \\) for unit-norm \\( \\theta \\).", "C": "Adopt the shorthand\n\n$$R_i = \\log\\frac{\\pi_\\theta(y_i\\mid x_i)}{\\pi_{\\mathrm{ref}}(y_i\\mid x_i)}.\n$$\n\n1. **First part.** For any $i\\in[n_f]$, observe that\n\n$$\n\\lim_{\\beta\\to0}\\Bigl(-\\tfrac{2}{\\beta}\\log\\sigma(-\\beta R_i) - \\tfrac{2}{\\beta}\\log2\\Bigr)\n= \\lim_{\\beta\\to0}\\tfrac{2}{\\beta}\\log\\frac{1+e^{\\beta R_i}}{2}\n= R_i.\n$$\n\nAveraging over $i=1,\\dots,n_f$ and noting\n\n$$\n\\frac{1}{n_f}\\sum_{i=1}^{n_f}R_i\n= L_{\\mathrm{GA}}(\\theta) - \\mathbb{E}_{(x,y)\\sim D_{\\mathrm{FG}}}\\bigl[\\log\\pi_{\\mathrm{ref}}(y\\mid x)\\bigr]\n$$\n\nyields the first claimed limit. :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}\n\n2. **Second part.** By definition,\n\n$$\n\\nabla_\\theta L_{\\mathrm{NPO},\\beta}(\\theta)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\Bigl(-\\tfrac{2}{\\beta}\\nabla_\\theta\\log\\sigma(-\\beta R_i)\\Bigr)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\frac{2}{\\beta}\\nabla_\\theta\\log(1+e^{\\beta R_i})\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\frac{2e^{\\beta R_i}}{1+e^{\\beta R_i}}\\,\\nabla_\\theta R_i.\n$$\n\nTaking the limit as $\\beta\\to0$ gives\n\n$$\n\\lim_{\\beta\\to0}\\nabla_\\theta L_{\\mathrm{NPO},\\beta}(\\theta)\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\nabla_\\theta R_i\n= \\frac{1}{n_f}\\sum_{i=1}^{n_f}\\nabla_\\theta\\log\\pi_\\theta(y_i\\mid x_i)\n= \\nabla_\\theta L_{\\mathrm{GA}}(\\theta).\n$$\n\nThis completes the proof of Proposition 1. \n\n\\boxed{\\text{Note: The limit in the second part should be } \\frac{1}{2}\\nabla_\\theta L_{\\mathrm{GA}}(\\theta) \\text{ due to the factor of 2 in the original expression.}}", "D": "To derive and prove Proposition 1, we leverage the Lipschitz condition on \\( k(x, \\theta) \\) and properties of the integral over a probability measure. Here's the step-by-step proof:\n\n**Step 1: Apply the Lipschitz condition with \\( \\theta_2 = 0 \\)**  \nFor any \\( \\theta \\in \\mathbb{R}^d \\), the Lipschitz condition gives:\n\\[\n|k(x, \\theta) - k(x, 0)| \\le L \\|\\theta - 0\\| = L \\|\\theta\\|.\n\\]\nThis implies:\n\\[\nk(x, \\theta) \\le k(x, 0) + L \\|\\theta\\|.\n\\]\n\n**Step 2: Integrate both sides over \\( \\mathcal{X} \\)**  \nUsing the probability measure \\( \\mu \\):\n\\[\nF(\\theta) = \\int_{\\mathcal{X}} k(x, \\theta) \\, d\\mu(x) \\le \\int_{\\mathcal{X}} \\left(k(x, 0) + L \\|\\theta\\|\\right) d\\mu(x).\n\\]\nSplit the integral:\n\\[\nF(\\theta) \\le \\underbrace{\\int_{\\mathcal{X}} k(x, 0) \\, d\\mu(x)}_{C_0} + L \\|\\theta\\| \\int_{\\mathcal{X}} d\\mu(x).\n\\]\nSince \\( \\mu \\) is a probability measure, \\( \\int_{\\mathcal{X}} d\\mu(x) = 1 \\), so:\n\\[\nF(\\theta) \\le C_0 + L \\|\\theta\\|.\n\\]\nThis provides a linear bound, but Proposition 1 requires a quadratic bound. To resolve this, we hypothesize an additional implicit structure.\n\n**Step 3: Assume quadratic structure via Taylor expansion**  \nAssume \\( k(x, 0) = 0 \\) and \\( \\nabla_\\theta k(x, 0) = 0 \\) for all \\( x \\). Expand \\( k(x, \\theta) \\) to second order:\n\\[\nk(x, \\theta) = \\frac{1}{2} \\theta^\\top H(x, \\xi) \\theta,\n\\]\nwhere \\( H(x, \\xi) \\) is the Hessian of \\( k \\) at some \\( \\xi \\) between \\( 0 \\) and \\( \\theta \\).\n\n**Step 4: Bound the Hessian using Lipschitz continuity**  \nThe Lipschitz condition on \\( \\nabla_\\theta k \\) implies:\n\\[\n\\|H(x, \\xi)\\| \\le L \\quad \\forall x, \\xi.\n\\]\nThus:\n\\[\n|k(x, \\theta)| \\le \\frac{L}{2} \\|\\theta\\|^2.\n\\]\n\n**Step 5: Integrate to get the final bound**  \n\\[\nF(\\theta) \\le \\int_{\\mathcal{X}} \\frac{L}{2} \\|\\theta\\|^2 \\, d\\mu(x) = \\frac{L}{2} \\|\\theta\\|^2 \\underbrace{\\int_{\\mathcal{X}} d\\mu(x)}_{1}.\n\\]\nThus:\n\\[\nF(\\theta) \\le C \\|\\theta\\|^2 \\quad \\text{with } C = \\frac{L}{2}.\n\\]\n\n**Conclusion**  \nUnder the assumption that \\( k(x, 0) = 0 \\) and \\( \\nabla_\\theta k(x, 0) = 0 \\), the quadratic bound \\( F(\\theta) \\le C \\|\\theta\\|^2 \\) follows from the Lipschitz condition and integration over the probability measure. The constant \\( C \\) depends on the Lipschitz constant \\( L \\).", "ground-truth": "A"}
{"paper": "arxiv:2404.05868", "question": "Lemma 3 states that\n\n$$\nC_{1}\\,\\eta_{0}\\,t \\;\\le\\; b^{(t)}_{GA,i} \\;\\le\\; C_{2}\\,\\eta_{0}\\,t,\n\\quad\\text{if }y_{i}=0,\\\\\n-\\,C_{2}\\,\\eta_{0}\\,t \\;\\le\\; b^{(t)}_{GA,i} \\;\\le\\; -\\,C_{1}\\,\\eta_{0}\\,t,\n\\quad\\text{if }y_{i}=1,\n$$\n\nfor all $i\\in[n_{f}]$ and $t\\ge1$, under the condition\n$$\n\\max_{i\\neq j}\\lvert\\gamma_{i,j}\\rvert \\le \\frac{C_{0}}{n_{f}}.\n$$\n\nHere:\n- $b^{(t)}_{GA,i}$ denotes the $i$-th component of the GA update vector at iteration $t$,\n- $\\eta_{0}$ is the normalized learning rate,\n- $C_{0},C_{1},C_{2}>0$ are constants depending on $(B_{\\theta},b_{x},B_{x})$,\n- $\\gamma_{i,j}=\\langle x_{i},x_{j}\\rangle$ are inner products of feature vectors $x_{i},x_{j}$,\n- $y_{i}\\in\\{0,1\\}$ is the binary label for sample $i$, and\n- $n_{f}$ is the total number of samples.\n\nHow can one derive and prove Lemma 3?", "A": "nWe prove Lemma 3 by induction.\n\nCase 1: $t=1$\nWhen $t=1$, since $|c_{\\mathrm{init},i}|\\le\\|x_i\\|_2\\cdot\\|\\theta_{\\mathrm{init}}\\|_2\\le B_xB_\\theta$, it follows from the definition of $\\mathrm{pred}_i(\\cdot)$ that\n$$C_3\\le\\Delta^{(0)}_{GA,i}\\le1\\quad\\text{when }y_i=1,\\quad-1\\le\\Delta^{(0)}_{GA,i}\\le-C_4\\quad\\text{when }y_i=0,$$\nfor all $i\\in[n_f]$, for some constants $C_3,C_4\\in(0,1)$. Moreover, there exists $C_0>0$ such that\n$$\\sum_{j\\neq i}\\gamma_{i,j}\\Delta^{(0)}_{GA,j}\\le\\frac{\\gamma_{i,i}}2\\Delta^{(0)}_{GA,i}$$\nfor all $i$ when $\\max_{i\\neq j}|\\gamma_{i,j}|\\le C_0/n_f$. It follows that\n$$-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[-\\tfrac32\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|,-\\tfrac12\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|]=[-C_2\\eta_0,-C_1\\eta_0]\\text{ when }y_i=1,$$\n$$-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[\\tfrac12\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|,\\tfrac32\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|]=[C_1\\eta_0,C_2\\eta_0]\\text{ when }y_i=0.$$\nTherefore,\n$$b^{(1)}_{GA,i}=b^{(0)}_{GA,i}-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[-C_2\\eta_0,-C_1\\eta_0]\\text{ when }y_i=1,$$\n$$b^{(1)}_{GA,i}\\in[C_1\\eta_0,C_2\\eta_0]\\text{ when }y_i=0.$$\n\nCase 2: $t=K+1$\nSuppose for $t=1,\\dots,K$ we have\n$$b^{(t)}_{GA,i}\\in[-C_2\\eta_0t,-C_1\\eta_0t]\\text{ when }y_i=1,$$\n$$b^{(t)}_{GA,i}\\in[C_1\\eta_0t,C_2\\eta_0t]\\text{ when }y_i=0.$$\nBy monotonicity of $\\Delta^{(t)}_{GA,i}$, we also have\n$$C_3\\le\\Delta^{(K)}_{GA,i}\\le1\\quad\\text{when }y_i=1,\\quad-1\\le\\Delta^{(K)}_{GA,i}\\le-C_4\\quad\\text{when }y_i=0.$$\nThen\n$$b^{(K+1)}_{GA,i}=b^{(K)}_{GA,i}-\\eta_0\\gamma_i^\\top\\Delta^{(K)}_{GA}\\in[-C_2\\eta_0(K+1),-C_1\\eta_0(K+1)]\\text{ when }y_i=1,$$\n$$b^{(K+1)}_{GA,i}\\in[C_1\\eta_0(K+1),C_2\\eta_0(K+1)]\\text{ when }y_i=0.$$\nThis completes the induction and thus the proof of Lemma 3.", "B": "We prove Lemma 3 by induction.\n\nCase 1: $t=1$\nWhen $t=1$, since $|c_{\\mathrm{init},i}|\\le\\|x_i\\|_2\\cdot\\|\\theta_{\\mathrm{init}}\\|_2\\le B_xB_\\theta$, it follows from the definition of $\\mathrm{pred}_i(\\cdot)$ that\n$$C_3\\le\\Delta^{(0)}_{GA,i}\\le1\\quad\\text{when }y_i=1,\\quad-1\\le\\Delta^{(0)}_{GA,i}\\le-C_4\\quad\\text{when }y_i=0,$$\nfor all $i\\in[n_f]$, for some constants $C_3,C_4\\in(0,1)$. Moreover, there exists $C_0>0$ such that\n$$\\sum_{j\\neq i}\\gamma_{i,j}\\Delta^{(0)}_{GA,j}\\le\\frac{\\gamma_{i,i}}2\\Delta^{(0)}_{GA,i}$$\nfor all $i$ when $\\max_{i\\neq j}|\\gamma_{i,j}|\\le C_0/n_f$. It follows that\n$$-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[-\\tfrac32\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|,-\\tfrac12\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|]=[-C_2\\eta_0,-C_1\\eta_0]\\text{ when }y_i=1,$$\n$$-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[\\tfrac12\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|,\\tfrac32\\gamma_{i,i}\\eta_0|\\Delta^{(0)}_{GA,i}|]=[C_1\\eta_0,C_2\\eta_0]\\text{ when }y_i=0.$$\nTherefore,\n$$b^{(1)}_{GA,i}=b^{(0)}_{GA,i}-\\eta_0\\gamma_i^\\top\\Delta^{(0)}_{GA}\\in[-C_2\\eta_0,-C_1\\eta_0]\\text{ when }y_i=1,$$\n$$b^{(1)}_{GA,i}\\in[C_1\\eta_0,C_2\\eta_0]\\text{ when }y_i=0.$$\n\nCase 2: $t=K+1$\nSuppose for $t=1,\\dots,K$ we have\n$$b^{(t)}_{GA,i}\\in[-C_2\\eta_0t,-C_1\\eta_0t]\\text{ when }y_i=1,$$\n$$b^{(t)}_{GA,i}\\in[C_1\\eta_0t,C_2\\eta_0t]\\text{ when }y_i=0.$$\nBy monotonicity of $\\Delta^{(t)}_{GA,i}$, we also have\n$$C_3\\le\\Delta^{(K)}_{GA,i}\\le1\\quad\\text{when }y_i=1,\\quad-1\\le\\Delta^{(K)}_{GA,i}\\le-C_4\\quad\\text{when }y_i=0.$$\nThen\n$$b^{(K+1)}_{GA,i}=b^{(K)}_{GA,i}-\\eta_0\\gamma_i^\\top\\Delta^{(K)}_{GA}\\in[-C_2\\eta_0(K+1),-C_1\\eta_0(K+1)]\\text{ when }y_i=0,$$\n$$b^{(K+1)}_{GA,i}\\in[C_1\\eta_0(K+1),C_2\\eta_0(K+1)]\\text{ when }y_i=1.$$\nThis completes the induction and thus the proof of Lemma 3.", "C": "To derive and prove **Lemma 3**, we need to analyze the behavior of the GA (Gradient Accumulation) update vector \\( b^{(t)}_{GA,i} \\) under the given conditions. Below is a step-by-step derivation and proof.\n\n---\n\n### **Notation Recap**\n- \\( b^{(t)}_{GA,i} \\): The \\(i\\)-th component of the GA update vector at iteration \\(t\\).\n- \\( \\eta_0 \\): Normalized learning rate.\n- \\( C_0, C_1, C_2 > 0 \\): Constants depending on \\( (B_\\theta, b_x, B_x) \\).\n- \\( \\gamma_{i,j} = \\langle x_i, x_j \\rangle \\): Inner product between feature vectors \\(x_i\\) and \\(x_j\\).\n- \\( y_i \\in \\{0,1\\} \\): Binary label for sample \\(i\\).\n- \\( n_f \\): Total number of samples.\n\n### **Assumptions & Conditions**\n1. **Small Correlation Condition**:\n   \\[\n   \\max_{i \\neq j} |\\gamma_{i,j}| \\le \\frac{C_0}{n_f}.\n   \\]\n   This ensures that feature vectors are weakly correlated.\n\n2. **Boundedness of Features & Parameters**:\n   - \\( \\|x_i\\| \\le B_x \\) (bounded feature vectors).\n   - \\( \\|\\theta\\| \\le B_\\theta \\) (bounded model parameters).\n   - \\( b_x \\le \\|x_i\\| \\) (lower bound on feature norms).\n\n---\n\n### **Derivation of \\( b^{(t)}_{GA,i} \\)**\nThe GA update vector accumulates gradients over iterations. For a binary classification problem (e.g., logistic regression), the gradient for sample \\(i\\) at step \\(t\\) is:\n\\[\n\\nabla_\\theta \\mathcal{L}_i = (p_i - y_i) x_i,\n\\]\nwhere \\( p_i = \\sigma(\\theta^T x_i) \\) is the predicted probability (\\( \\sigma \\) is the sigmoid function).\n\nThe GA update accumulates these gradients:\n\\[\nb^{(t)}_{GA,i} = \\eta_0 \\sum_{s=1}^t (p_i^{(s)} - y_i) \\|x_i\\|^2 + \\eta_0 \\sum_{s=1}^t \\sum_{j \\neq i} (p_j^{(s)} - y_j) \\gamma_{i,j}.\n\\]\n\n#### **Key Observations**\n1. **Dominant Term**: The first term \\( \\eta_0 \\sum_{s=1}^t (p_i^{(s)} - y_i) \\|x_i\\|^2 \\) dominates because:\n   - \\( \\|x_i\\|^2 \\ge b_x^2 \\) (lower bounded).\n   - The second term is small due to the **small correlation condition** (\\( |\\gamma_{i,j}| \\le \\frac{C_0}{n_f} \\)).\n\n2. **Label Dependence**:\n   - If \\( y_i = 0 \\), \\( p_i^{(s)} \\) tends to be small (since the model learns to predict \\( 0 \\)), so \\( (p_i^{(s)} - y_i) \\approx p_i^{(s)} \\ge 0 \\).\n   - If \\( y_i = 1 \\), \\( p_i^{(s)} \\) tends to be large (since the model learns to predict \\( 1 \\)), so \\( (p_i^{(s)} - y_i) \\approx p_i^{(s)} - 1 \\le 0 \\).\n\n3. **Bounding the Accumulated Updates**:\n   - For \\( y_i = 0 \\), the first term grows linearly with \\( t \\), while the second term is \\( O\\left(\\frac{C_0 t}{n_f}\\right) \\), which is negligible for large \\( n_f \\).\n   - Similarly for \\( y_i = 1 \\), the first term decreases linearly, while the second term remains small.\n\n---\n\n### **Proof of Lemma 3**\nWe now formalize the bounds.\n\n#### **Case 1: \\( y_i = 0 \\)**\nThe GA update simplifies to:\n\\[\nb^{(t)}_{GA,i} = \\eta_0 \\sum_{s=1}^t p_i^{(s)} \\|x_i\\|^2 + \\eta_0 \\sum_{s=1}^t \\sum_{j \\neq i} (p_j^{(s)} - y_j) \\gamma_{i,j}.\n\\]\n\n1. **Lower Bound**:\n   - Since \\( p_i^{(s)} \\ge \\sigma(-B_\\theta B_x) \\ge c_1 > 0 \\) (bounded away from 0),\n   - \\( \\|x_i\\|^2 \\ge b_x^2 \\),\n   - The second term is \\( \\ge -\\eta_0 t \\cdot n_f \\cdot \\frac{C_0}{n_f} = -\\eta_0 C_0 t \\).\n\n   Thus:\n   \\[\n   b^{(t)}_{GA,i} \\ge \\eta_0 t (c_1 b_x^2 - C_0) \\ge C_1 \\eta_0 t,\n   \\]\n   where \\( C_1 = c_1 b_x^2 - C_0 > 0 \\) (by choosing \\( C_0 \\) sufficiently small).\n\n2. **Upper Bound**:\n   - \\( p_i^{(s)} \\le 1 \\),\n   - The second term is \\( \\le \\eta_0 t \\cdot n_f \\cdot \\frac{C_0}{n_f} = \\eta_0 C_0 t \\).\n\n   Thus:\n   \\[\n   b^{(t)}_{GA,i} \\le \\eta_0 t (B_x^2 + C_0) \\le C_2 \\eta_0 t,\n   \\]\n   where \\( C_2 = B_x^2 + C_0 \\).\n\n#### **Case 2: \\( y_i = 1 \\)**\nThe GA update becomes:\n\\[\nb^{(t)}_{GA,i} = \\eta_0 \\sum_{s=1}^t (p_i^{(s)} - 1) \\|x_i\\|^2 + \\eta_0 \\sum_{s=1}^t \\sum_{j \\neq i} (p_j^{(s)} - y_j) \\gamma_{i,j}.\n\\]\n\n1. **Lower Bound**:\n   - \\( (p_i^{(s)} - 1) \\ge -1 \\),\n   - The second term is \\( \\ge -\\eta_0 C_0 t \\).\n\n   Thus:\n   \\[\n   b^{(t)}_{GA,i} \\ge -\\eta_0 t (B_x^2 + C_0) \\ge -C_2 \\eta_0 t.\n   \\]\n\n2. **Upper Bound**:\n   - \\( (p_i^{(s)} - 1) \\le -\\sigma(B_\\theta b_x) \\le -c_2 < 0 \\),\n   - The second term is \\( \\le \\eta_0 C_0 t \\).\n\n   Thus:\n   \\[\n   b^{(t)}_{GA,i} \\le -\\eta_0 t (c_2 b_x^2 - C_0) \\le -C_1 \\eta_0 t,\n   \\]\n   where \\( C_1 = c_2 b_x^2 - C_0 > 0 \\).\n\n---\n\n### **Conclusion**\nUnder the given conditions, we have derived the bounds:\n\\[\n\\begin{cases}\nC_1 \\eta_0 t \\le b^{(t)}_{GA,i} \\le C_2 \\eta_0 t, & \\text{if } y_i = 0, \\\\\n-C_2 \\eta_0 t \\le b^{(t)}_{GA,i} \\le -C_1 \\eta_0 t, & \\text{if } y_i = 1,\n\\end{cases}\n\\]\nfor all \\( i \\in [n_f] \\) and \\( t \\ge 1 \\). This completes the proof of **Lemma 3**. ∎", "D": "To derive and prove Lemma 3, we analyze the iterative updates of \\( b^{(t)}_{GA,i} \\) under the given conditions. The key steps are as follows:\n\n---\n\n**Step 1: Model the GA Update Dynamics**\n\nAssume the GA update for \\( b^{(t)}_{GA,i} \\) follows a gradient-based rule. For binary labels \\( y_i \\in \\{0,1\\} \\), the update at iteration \\( t \\) depends on:\n- The direct term from sample \\( i \\), scaled by \\( \\eta_0 \\).\n- Cross-terms from other samples \\( j \\neq i \\), weighted by \\( \\gamma_{i,j} = \\langle x_i, x_j \\rangle \\).\n\nFor simplicity, let the update for \\( b^{(t)}_{GA,i} \\) be:\n\\[\n\\Delta b^{(t)}_{GA,i} = \\eta_0 \\left[ \\underbrace{\\alpha \\cdot (1 - y_i) - \\alpha \\cdot y_i}_{\\text{主项}} + \\underbrace{\\sum_{j \\neq i} \\gamma_{i,j} \\cdot \\beta^{(t)}_j}_{\\text{干扰项}} \\right],\n\\]\nwhere \\( \\alpha > 0 \\) is a constant from the loss gradient, and \\( \\beta^{(t)}_j \\) is bounded by parameters \\( B_\\theta, B_x \\).\n\n---\n\n**Step 2: Bound the Cross-Terms**\n\nGiven \\( \\max_{i \\neq j} |\\gamma_{i,j}| \\leq \\frac{C_0}{n_f} \\), the cross-term sum is bounded:\n\\[\n\\left| \\sum_{j \\neq i} \\gamma_{i,j} \\beta^{(t)}_j \\right| \\leq \\sum_{j \\neq i} |\\gamma_{i,j}| \\cdot |\\beta^{(t)}_j| \\leq \\frac{C_0}{n_f} \\cdot (n_f - 1) \\cdot B_\\beta \\leq C_0 B_\\beta,\n\\]\nwhere \\( |\\beta^{(t)}_j| \\leq B_\\beta \\) (dependent on \\( B_\\theta, B_x \\)). Thus, the cross-term magnitude is \\( \\mathcal{O}(1) \\), independent of \\( n_f \\).\n\n---\n\n**Step 3: Establish Per-Iteration Bounds**\n\nFor \\( y_i = 0 \\):\n\\[\n\\Delta b^{(t)}_{GA,i} \\geq \\eta_0 \\left( \\alpha - C_0 B_\\beta \\right), \\quad \\Delta b^{(t)}_{GA,i} \\leq \\eta_0 \\left( \\alpha + C_0 B_\\beta \\right).\n\\]\nFor \\( y_i = 1 \\):\n\\[\n\\Delta b^{(t)}_{GA,i} \\geq \\eta_0 \\left( -\\alpha - C_0 B_\\beta \\right), \\quad \\Delta b^{(t)}_{GA,i} \\leq \\eta_0 \\left( -\\alpha + C_0 B_\\beta \\right).\n\\]\n\n---\n\n**Step 4: Accumulate Over Iterations**\n\nSumming from \\( t = 1 \\) to \\( t \\), we get:\n- If \\( y_i = 0 \\):\n\\[\nC_1 \\eta_0 t \\leq b^{(t)}_{GA,i} = \\sum_{k=1}^t \\Delta b^{(k)}_{GA,i} \\leq C_2 \\eta_0 t,\n\\]\nwhere \\( C_1 = \\alpha - C_0 B_\\beta \\), \\( C_2 = \\alpha + C_0 B_\\beta \\).\n\n- If \\( y_i = 1 \\):\n\\[\n-C_2 \\eta_0 t \\leq b^{(t)}_{GA,i} \\leq -C_1 \\eta_0 t.\n\\]\n\n---\n\n**Step 5: Ensure Consistency of Constants**\n\nConstants \\( C_1, C_2 > 0 \\) must satisfy:\n\\[\n\\alpha > C_0 B_\\beta \\quad \\text{(to guarantee positivity for \\( y_i = 0 \\))}.\n\\]\nThis holds under the lemma's dependency on \\( (B_\\theta, b_x, B_x) \\), which bounds \\( B_\\beta \\).\n\n---\n\n**Proof Summary**\n\nBy induction:\n1. **Base case**: At \\( t = 1 \\), bounds hold as \\( \\Delta b^{(1)}_{GA,i} \\) satisfies Step 3.\n2. **Inductive step**: Assume bounds hold for \\( t \\). For \\( t+1 \\), the update \\( \\Delta b^{(t+1)}_{GA,i} \\) remains within the same bounds due to controlled cross-terms, preserving the linear accumulation.\n\nThus, Lemma 3 is proven under the given condition \\( \\max_{i \\neq j} |\\gamma_{i,j}| \\leq \\frac{C_0}{n_f} \\).", "ground-truth": "A"}
