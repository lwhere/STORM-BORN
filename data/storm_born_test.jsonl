{"paper":"arxiv:2305.18290", "question":"Given the following equations:\n\nEquation 1 (Bradley-Terry model):\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\nEquation 2 (Reward function):\n\\begin{equation}\n    r(x,y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\nHow can we derive Equation 3 from Equations 1 and 2?\n\nEquation 3 (Objective):\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)}\\right)}\n\\end{equation}", "whole_label":"To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. Start with Equation 1, which models the probability that $y_1$ is preferred over $y_2$ given $x$:\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\n2. Substitute the reward function $r^*(x, y)$ from Equation 2 into Equation 1. Equation 2 expresses the reward in terms of the optimal policy $\\pi^*$ and the reference policy $\\pi_{\\text{ref}}$:\n\\begin{equation}\n    r^*(x,y) = \\beta \\log \\frac{\\pi^*(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\n3. Substitute $r^*(x, y_1)$ and $r^*(x, y_2)$ into Equation 1:\n\\begin{align*}\n    p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} + \\beta \\log Z(x)\\right)} \\\\\n    &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)}\\right)}.\n\\end{align*}\n\n4. The final expression is Equation 3, which simplifies the preference probability in terms of the ratio of the optimal policy to the reference policy for the two options $y_1$ and $y_2$.\n\nThis derivation shows how the preference probability can be expressed directly in terms of the policies, without explicitly computing the reward function, by leveraging the relationship between the reward and the policies as given in Equation 2."}
{"paper":"arxiv:2407.18242", "question":"Prove the following theorem:\n\\begin{theorem}\nWhen updating matrices $A$ and $B$ using the closed-form solution, we proceed as follows:\n\\begin{gather}\n    A\\leftarrow A - \\gamma g^A \\\\\n    B\\leftarrow B - \\gamma g^B, \n\\end{gather}\nwhere $\\gamma \\ge 0$ denotes the learning rate.\nOur method ensures a decrease in the loss, akin to the standard gradient descent algorithm, expressed by:\n\\begin{equation}\n    \\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\} \\le 0.\n\\end{equation}\n\\end{theorem}", "whole_label":"The proof of the theorem is divided into two parts.\n\nPart I.\nWe first show that the differential change in the loss function, $\\dd L$, can be expressed as:\n\\begin{equation}\n\\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\}.\n\\end{equation}\nThis is derived from the differentials $\\dd A = -\\gamma g^A$ and $\\dd B = -\\gamma g^B$, and the fact that $\\frac{\\partial L}{\\partial A} = g^A_{lora}$ and $\\frac{\\partial L}{\\partial B} = g^B_{lora}$.\n\nPart II.\nWe then prove that $\\dd L \\le 0$ by showing:\n\\begin{gather}\n\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F\\ge 0, \\\\\n\\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\ge 0.\n\\end{gather}\n\n1. For the first inequality, we show $(B^TB)^{-1}$ is positive definite by demonstrating $B^TB$ is positive definite, hence $(B^TB)^{-1} = UU^T$ for some matrix $U$, leading to $\\frac{1}{s^2}\\|U^Tg^A_{lora}\\|_F^2\\ge 0$.\n\n2. For the second inequality, we show $(AA^T)^{-1}$ is positive-definite and $P = I - B(B^TB)^{-1}B^T$ is a projection matrix (hence positive semi-definite), leading to $\\frac{1}{s^2}\\|V^Tg^B_{lora}U\\|_F^2 \\ge 0$ where $P = VV^T$ and $(AA^T)^{-1} = UU^T$.\n\nIn conclusion, combining these results confirms $\\dd L \\le 0$, ensuring the loss decreases with each update."}
{"paper":"arxiv:2405.20304", "question":"Prove the proposition: Suppose that the loss $l(\\cdot;(x_g,y,y'))$ is non-negative, convex, $B_{\\nabla}-$Lipschitz continuous, and bounded by $B_l$ for all $(x_g,y,y')\\in\\gX\\oplus\\gG\\times\\gY\\times\\gY$ and $\\|\\theta\\|_2\\leq B_{\\Theta}$ for all $\\theta\\in\\Theta$ with convex $\\Theta\\subset\\R^d$. The error of the average iterate of the algorithm, i.e., $\\pi_{\\avtheta} = \\tfrac{1}{T}\\sum_{t=1}^T \\theta^t$, satisfies $\\mathbb{E}[\\rdpol(\\pi_{\\avtheta})]-\\min_{\\theta\\in\\Theta}\\rdpol(\\pi_{\\theta})= \\mathcal{O}\\big(T^{-1/2}\\big)$.", "whole_label":"We begin by considering the saddle-point stochastic optimization problem: $\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\E[F_{g}(\\theta,\\xi)]\\Big\\}$. Here, $\\xi:=(x_g,y,y')\\sim \\sum_{g=1}^{K}\\frac{N_g}{N}\\train_g$, and we define $F_{g'}(\\theta,(x_g,y,y')):= \\frac{N}{N_g}\\mathbf{1}_{[g = g']}l(\\theta; (x_g, y,y'))$. The expectation $\\E_{\\xi}[F_{g}(\\theta,\\xi)]$ simplifies to $f_{g}(\\theta) = \\E_{\\train_{g}}[l(\\theta; (x_g, y,y'))|g=g']$.\n\nNext, we map the algorithm to the general mirror descent framework. The gradients of the objective $\\phi(\\theta,\\alpha)$ with respect to $\\theta$ and $\\alpha$ are given by $\\partial \\phi(\\theta,\\alpha)=(\\partial_\\theta \\phi(\\theta,\\alpha),-\\partial_\\alpha \\phi(\\theta,\\alpha))$. For a given $\\xi$, the stochastic subgradients for $\\phi(\\theta,\\alpha)$ are $\\partial \\Phi(\\theta,\\alpha,\\xi)=(\\partial_\\theta \\Phi(\\theta,\\alpha,\\xi),-\\partial_\\alpha \\Phi(\\theta,\\alpha,\\xi))$, which simplifies to $\\begin{bmatrix}\\frac{N\\alpha_g}{N_g}\\nabla_{\\theta} l(\\theta;(x_{g},y,y')) \\\\-\\Big(0,\\cdots,\\frac{N}{N_g}l(\\theta;(x_{g},y,y')),\\cdots,0\\Big)\\end{bmatrix}$ for $\\xi=(x_g,y,y')$.\n\nThe updates of $\\theta^t$ and $\\alpha^t$ in the algorithm can be mapped to the general mirror descent update rule: $\\zeta^{t+1}=(\\theta^{t+1},\\alpha^{t+1})=P_{(\\zeta^t)}(\\gamma^t \\partial \\Phi(\\theta^t,\\alpha^t,\\xi))$, where $P_{\\zeta}(\\nu) = \\argmin_{\\zeta' \\in (\\Theta \\times\\Delta_K)} \\left\\{ \\nu^\\top (\\zeta' - \\zeta) + V(\\zeta, \\zeta') \\right\\}$ is the prox-mapping.\n\nTo bound the error, we define the approximation error $\\epsilon_T$ as $\\max_{\\alpha \\in \\Delta_K}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\phi(\\theta,\\alpha)$, which can be bounded using the result from mirror-descent convergence for max-min problems. The error bound is given by $\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10\\frac{R_{\\theta}^2 M_{\\theta}^{2}+\\ln(K) M_{\\alpha}^{2}}{T}}$, where $M_{\\theta}^2 = B_{\\nabla}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$ and $M_{\\alpha}^2 = KB_{l}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$. The strong convexity parameters $\\lambda_{\\theta}$ and $\\lambda_{\\alpha}$ are both 1, leading to the overall error bound $\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10(\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g})\\frac{B_{\\Theta}^2 B_{\\nabla}^2+KB_{l}^2\\ln K}{T}}$."}
{"paper":"arxiv:2405.16577", "question":"Prove the following theorem:\n\\begin{theorem}\nAssume that $p_t(\\bm{x})>0$ for all $\\bm{x}\\in\\Omega^o$ and $t\\in[0,1]$. Then $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$.\n\\end{theorem}", "whole_label":"To prove the theorem, we start by expanding the squared norms of the differences between the velocity fields:\n\\begin{subequations}\n\\begin{align}\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x})\\|^2&= \\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})+\\|\\bm{v}_t(\\bm{x})\\|^2\\\\\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2&=\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)+\\|\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2\n\\end{align}\n\\end{subequations}\n\nNext, we observe that the expectation of the squared norm of $\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)$ under the joint distribution $p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)$ is equal to its expectation under the marginal distribution $p_t(\\bm{x})$:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2=\\mathbb{E}_{p_t(\\bm{x})}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2\n\\end{equation}\n\nTo complete the proof, we need to show that the expectations of the dot products are equal:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}).\n\\end{equation}\n\nBy the definition of $\\bm{v}_t(\\bm{x})$, we can express the expectation under $p_t(\\bm{x})$ as:\n\\begin{align*}\n\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})\\dif\\bm{x}\\\\\\n&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\int_\\Omega \\bm{v}_t(\\bm{x}|\\bm{x}_1)\\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\int_\\Omega\\int_\\Omega  \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\bm{v}_t(\\bm{x}|\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1).\n\\end{align*}\nThis completes the proof, showing that $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$ under the given assumptions."}
{"paper":"arxiv:2405.16577", "question":"Prove the following theorem:\n\\begin{theorem}\nLet $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\bm{x})$ be a standard Gaussian distribution. Assume the OT conditional velocity field for FM, i.e., $\\bm{\\phi}_{t}(\\bm{x}|\\bm{x}_1)=t\\bm{x}_1+(1-(1-\\sigma_{\\min})t)\\bm{x}$.\nThen\n\\begin{equation}\n\\bm{v}_t(\\bm{x})=\\frac{1}{t}\\bm{x}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x}).\n\\end{equation}\n\\end{theorem}", "whole_label":"Let $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\cdot)$ be the standard Gaussian distribution.\nUnder the OT conditional flow, the conditional velocity field is $\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}$, and the conditional probability path is $p_{t}(\\bm{x}|\\bm{x}_1)=\\mathcal{N}(t\\bm{x}_1,(1-(1-\\sigma_{\\min})t)^2\\bm{I})$.\nDefine\n\\[\n\\bar{p}_t(\\bm{x}_1|\\bm{x}) = \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}.\n\\]\nBy the definition of $\\bm{v}_t(\\bm{x})$,\nwe have\n\\begin{align*}\n\\bm{v}_t(\\bm{x})&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\\\\n&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}\\\\\n&=\\frac{\\bm{x}}{t}-\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}-t\\bm{x}_1}{(1-(1-\\sigma_{\\min})t)^2}\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\nabla\\log p_{t}(\\bm{x}|\\bm{x}_1)\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x})\n\\end{align*}\nwhere we use the fact\n\\[\n\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})} \\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}|\\bm{x}_1)=\\int \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_{t}(\\bm{x})}\\frac{\\nabla_{\\bm{x}} p_{t}(\\bm{x}|\\bm{x}_1)}{p_{t}(\\bm{x}|\\bm{x}_1)}\\dif \\bm{x}_1=\\frac{1}{p_{t}(\\bm{x})}\\nabla_{\\bm{x}} \\int p_{t}(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1=\\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}).\n\\]\nThis finishes the proof."}
{"paper":"arxiv:2402.01306", "question":"Prove the following theorem:\n\\begin{theorem}\n    DPO and PPO-Clip are human-aware losses.\n\\end{theorem}. It is required to prove that DPO and PPO-Clip satisfy the definition of HALO, which means they can be expressed in the following form:\n\\[f(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}[a_{x,y}v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])] + C_D\\]\n\nThe DPO loss function is:\n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\left(\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}\\right) - \\beta \\log\\left(\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right]\\]\n\nThe PPO-Clip loss function is:\n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere \\[q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})}\\]", "whole_label":"\\begin{proof}\nFor a loss $f$ to be a HALO, we need to first construct the human value\n\\[v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])\\]\nwhere $r_\\theta(x,y) = l(x,y) \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$ is the implied reward (normalized by factor $l(y)$), $Q(Y'|x)$ is an input-conditioned reference point distribution, and $v: \\mathbb{R} \\to \\mathbb{R}$ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in $(0, \\infty)$.\n\nThe DPO loss is\n\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x, y_w, y_l} \\left[  -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]\\]\nwhere $\\beta > 0$ is a hyperparameter.\nDPO meets the criteria with the following construction: $l(y) = \\beta$; $r_\\theta = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$; $v(\\cdot) = \\log \\sigma(\\cdot)$ is increasing and concave everywhere; $Q$ places all mass on $(x, y_l)$, where $y_l$ is a dispreferred output for $x$ such that $y \\succ y_l$; and $a_{x,y} = -1$.\n\nThe PPO-Clip loss is\n\\[\\mathcal{L}_{\\text{PPO (offline)}} = -\\mathbb{E}_{x,y,t \\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta, 1 - \\epsilon, 1 + \\epsilon) A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere $q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{ref}}(y_t|x:y_{<t})}$ are the token-level probability ratios (where $y_{< t}$ denotes the output sequence up to the $t$-th token), $A$ denotes the token-level advantages, and $\\epsilon \\in (0,1)$ is a hyperparameter.\n\nSince this is a token-level objective, let $x:y_{<t}$ denote the actual input and the token $y_i$ the actual output for the purpose of framing this as a HALO.\nThe advantage function $A(x:y_{<t}, y_t)$ can be expressed as $Q^\\pi(x:y_{<t}, y_t) - V^\\pi(x:y_{<t})$, the difference between the action-value and value functions.\nBecause $V^\\pi(x:y_{<t}) = \\mathbb{E}_{y \\sim \\pi}Q^\\pi(x:y_{<t}, y)$, the reference point distribution is simply the policy.\n\nThe HALO-defined reward $r_\\theta$ is then implied by the product $q_\\theta Q^\\pi(x:y_{<t}, y)$.\nAssume without loss of generality that $Q^\\pi$ is non-negative, since a constant can be added to $Q^\\pi$ without changing the advantage.\nThen means $\\exists u \\geq 1, q_\\theta Q^\\pi(x:y_{<t}, y) = \\log u = \\log \\hat{\\pi}_\\theta(x:y_{<t}, y) / \\hat{\\pi}_{\\text{ref}}(x:y_{<t}, y)$, where $\\hat{\\pi}_\\theta, \\hat{\\pi}_{\\text{ref}}$ are some implied policy and reference distributions.\nIt is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise.\nWhere $q_\\theta A = r_\\theta - z_0$ in the HALO notation:\n\\[v(q_\\theta A) = \\begin{cases} A \\min(q_\\theta, 1 + \\epsilon)& \\text{if } A(x:y_{<t}, y_t) \\geq 0 \\\\\nA \\max(q_\\theta, 1 - \\epsilon)& \\text{if } A(x:y_{<t}, y_t) < 0 \\\\\n\\end{cases}\\]\nwhich we can combine as $v(q_\\theta A) = \\min(q_\\theta A, A(1 + \\text{sign}(q_\\theta A) \\epsilon))$.\n$a_{x,y} = -1$ completes the construction.\n\\end{proof}"}
{"paper":"arxiv:2402.01306", "question":"Prove the following proposition:\n\\begin{proposition}\n    As the reward implied by the current policy tends to $\\pm \\infty$, the KTO update of $\\pi_\\theta$ tends to zero.\n\\end{proposition}\n\nThe KTO loss function is:\n\\[ L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}[\\lambda_y - v(x,y)] \\]\nwhere $v(x,y)$ is defined as:\n\\[ v(x,y) = \\begin{cases} \n\\lambda_D \\sigma(\\beta(r_\\theta(x,y) - z_0)) & \\text{if } y \\text{ is desirable given } x \\\\\n\\lambda_U \\sigma(\\beta(z_0 - r_\\theta(x,y))) & \\text{if } y \\text{ is undesirable given } x \n\\end{cases} \\]", "whole_label":"\\begin{proof}\nWhere $d(y)$ is $-1$ ($+1$) when $y$ is desirable (undesirable), $\\lambda_y$ is $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable (undesirable), and $z = r_\\theta(x,y) - z_0$, the derivative of the KTO loss is\n\\begin{equation}\n\\nabla_\\theta L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}\\left[ d(y) \\lambda_y \\sigma(\\beta z) (1 - \\sigma(\\beta z)) \\beta \\nabla_\\theta \\log \\pi_\\theta(y|x) \\right]\n\\label{eq:kto_derivative}\n\\end{equation}\nNote that we do not backpropagate through the KL term in the KTO loss and $\\beta, \\lambda_y > 0$.\nThis gradient is simple to interpret: if $y$ is desirable, then $d(y)$ is negative and we push up the probability of $\\pi_\\theta(y|x)$ to minimize the loss; if $y$ is undesirable, then $d(y)$ is positive and we push down the probability of $\\pi_\\theta(y|x)$ to minimize the loss.\nAs $r_\\theta$ tends to $\\pm \\infty$, the gradient will tend to zero since either $(1 - \\sigma(\\beta z))$ or $\\sigma(\\beta z)$ will tend to zero.\n\\end{proof}"}
{"paper":"arxiv:2304.02835", "question":"How can we prove Theorem 1: (Traditional Influence Functions) is valid? The theorem relies on (i) the definition $L_{0} = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i})$, (ii) the Hessian matrix $H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})$, and (iii) the notion of a node unlearning request $\\Delta G = \\{V^{(rm)}, \\emptyset, \\emptyset\\}$. In particular, Theorem 1 states that $\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$. Based on these formulas and definitions, how do we derive and prove that relationship?", "whole_label":"The validity of Theorem 1, which involves traditional influence functions, can be proven by understanding the relationship between the parameter change and the influence of unlearning a node. The proof leverages the convexity of the loss function and the properties of the Hessian matrix. Here's a step-by-step derivation:\n\n1. Initial Setup: Start with the original loss function $L_0$ and its Hessian $H_{\\theta_0}$ at the parameter $\\theta_0$.\n\n2. Perturbation Introduction: Introduce a small perturbation $\\Delta G$ representing the node unlearning request. This affects the loss function by $L_{\\Delta G}$, the sum of losses over the nodes to be unlearned.\n\n3. Parameter Change Approximation: The change in parameters $\\hat{\\theta} - \\theta_0$ due to the perturbation can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of $L_{\\Delta G}$ with respect to $\\theta_0$.\n\n4. Hessian Inverse Role: The Hessian matrix $H_{\\theta_0}^{-1}$ plays a crucial role in scaling the gradient of the perturbation loss, providing the direction and magnitude of the parameter change.\n\n5. Final Relationship: Combining these elements, the relationship $\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla_{\\theta_0} L_{\\Delta G}$ is derived, showing how the parameter change is influenced by the unlearning request through the inverse Hessian and the gradient of the perturbation loss.\n\nThis derivation assumes the loss function's convexity and differentiability, ensuring the Hessian is positive definite and invertible, which are critical for the approximation's validity."}
{"paper":"arxiv:2304.02835", "question":"After unlearning, the objective $L$ is redefined over the remaining data $D_{0} \\setminus \\Delta D$ using the modified graph $G \\setminus \\Delta G$. How can we derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ with $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$ from the idea that we re-optimize the model parameters on the reduced dataset?", "whole_label":"To derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ where $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$, we follow these steps:\n\n1. Identify the Remaining Data: After unlearning, the dataset is reduced to $D_{0} \\setminus \\Delta D$. This means we only consider the data points that are not part of the unlearning request.\n\n2. Modify the Graph: The graph $G$ is modified to $G \\setminus \\Delta G$ to reflect the removal of the unlearned data. This modification affects how the function $f$ computes its outputs.\n\n3. Define the Loss Function: The loss function $L$ is defined over the remaining data, where for each data point $z_{i}$ in $D_{0} \\setminus \\Delta D$, we compute the loss $l$ between the predicted output $f_{G \\setminus \\Delta G}(z_{i})$ and the actual output $y_{i}$.\n\n4. Re-optimize the Model Parameters: The goal is to find the parameters $\\theta$ that minimize the loss function $L$. This is achieved by solving the optimization problem $\\hat{\\theta} = \\arg\\min_{\\theta} L$.\n\n5. Derive $\\hat{L}$: The solution to the optimization problem gives us $\\hat{L}$, which is the minimized loss over the remaining data with the modified graph.\n\nThe key idea is that by re-optimizing the model parameters on the reduced dataset and modified graph, we ensure that the model performs well on the remaining data while disregarding the unlearned data. This process is encapsulated in the equation $\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$."}
{"paper":"arxiv:2304.02835", "question":"Given that $\\epsilon = -1$, derive the relationship $\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$ from the approximate linearization in terms of the Hessian $H_{\\theta_{0}}$ and the influence of $L_{\\Delta G}$.", "whole_label":"To derive the relationship when $\\epsilon = -1$, we start by considering the approximate linearization of the parameter update. The key steps involve the Hessian matrix $H_{\\theta_{0}}$ and the gradient of the loss function $L_{\\Delta G}$ with respect to the parameters $\\theta_0$. The derivation proceeds as follows:\n\n1. Approximate Linearization: The change in parameters $\\hat{\\theta} - \\theta_0$ can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of the loss function and the inverse of the Hessian matrix.\n\n2. Setting $\\epsilon = -1$: This specific value of $\\epsilon$ implies that we are considering the effect of removing the influence of $L_{\\Delta G}$ on the parameter update.\n\n3. Resulting Relationship: Combining these elements, we arrive at the relationship\n   $$\\hat{\\theta} - \\theta_0 = \\theta_{\\epsilon=-1} - \\theta_0 \\approx H_{\\theta_0}^{-1}\\nabla_{\\theta_0} L_{\\Delta G},$$\n   which shows that the parameter update is dominated by the removal of $L_{\\Delta G}$'s influence, scaled by the inverse of the Hessian matrix at $\\theta_0$."}
{"paper":"arxiv:2309.04475", "question":"Given the assumption of G-invariance, derive the chain of equalities leading from $p(gx_0)$ to $p(x_0)$. Explain the step where substituting $gx_t$ back to $x_t$ preserves the integral and results in $p(x_0)$.", "whole_label":"To derive the chain of equalities under the assumption of G-invariance, we start with the expression for $p(gx_0)$ and systematically apply the properties of G-invariance and probability measures:\n\n1. Initial Expression: We begin with $p(gx_0)$, which represents the probability density at the transformed point $gx_0$.\n\n2. Application of G-invariance: Due to G-invariance, the probability density is unchanged under group action. This allows us to express $p(gx_0)$ in terms of $p(gx_T)$ and the conditional probabilities $p(gx_{t-1} \\mid gx_t)$ for $t = 1$ to $T$.\n\n3. Substitution and Integral Preservation: The key step involves substituting $gx_t$ back to $x_t$ within the integral. This substitution is valid because the probability measure is invariant under the group action, meaning the integral's value remains unchanged. This invariance ensures that the integral over the transformed variables is equivalent to the integral over the original variables.\n\n4. Final Equality: After substitution, the expression simplifies to $p(x_T)$ multiplied by the integral of the product of conditional probabilities $p(x_{t-1} \\mid x_t)$, which, by definition, equals $p(x_0)$. This final step relies on the consistency of the probability measure and the Markov property of the process.\n\nThe preservation of the integral when substituting $gx_t$ back to $x_t$ is a direct consequence of the measure's invariance under the group action, ensuring that the transformation does not alter the integral's value. This property is crucial for maintaining the equality throughout the chain and ultimately arriving at $p(x_0)$."}
{"paper":"arxiv:2309.04475", "question":"How can we demonstrate the sequence of equalities in the expression involving $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$ and its transformations? Specifically, how does the application of $Q$ and the property of O(3)-equivariance, expressed as $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$, ensure that the distribution for $L_{t-1}$ remains unchanged?", "whole_label":"To understand the sequence of equalities, let's break down the process step by step:\n\n1. Initial Distribution: We start with the conditional distribution $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$, which describes the probability of $L_{t-1}$ given $L_t$, $F_t$, and $A$ under a normal distribution framework.\n\n2. Application of $Q$: By applying the orthogonal transformation $Q$ to both $L_{t-1}$ and $L_t$, we consider the transformed variables $QL_{t-1}$ and $QL_t$. The equality $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}(QL_{t-1}\\mid QL_t,F_t,A)$ holds because the normal distribution is invariant under orthogonal transformations. This means the probability density remains the same before and after the transformation.\n\n3. O(3)-Equivariance Property: The key to understanding the next equality lies in the O(3)-equivariance of $\\hat{\\epsilon}_L$, which is given by $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$. This property ensures that the transformation $Q$ commutes with the function $\\hat{\\epsilon}_L$, meaning that applying $Q$ to the input is equivalent to applying $Q$ to the output of $\\hat{\\epsilon}_L$.\n\n4. Recovery of Original Distribution: Due to the invariance of the normal distribution under orthogonal transformations and the O(3)-equivariance of $\\hat{\\epsilon}_L$, the distribution of $L_{t-1}$ conditioned on $L_t$, $F_t$, and $A$ remains unchanged. This leads us back to the original distribution $p(L_{t-1}\\mid L_t,F_t,A)$, completing the chain of equalities.\n\nIn summary, the combination of the normal distribution's invariance under orthogonal transformations and the O(3)-equivariance property of $\\hat{\\epsilon}_L$ ensures that the distribution for $L_{t-1}$ is preserved throughout the transformations, validating the sequence of equalities presented."}
{"paper":"arxiv:2309.04475", "question":"Given the definition of the wrapped normal distribution, how do we prove that $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$? Please illustrate how the index shift in the sum over $k$ preserves the same distribution.", "whole_label":"To prove the equality $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$, we start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution is defined as a sum of normal distributions shifted by integer multiples of $2\\pi$, which can be written as: $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Now, consider the distribution of $x + t$ given $\\mu + t$ and $\\sigma^2$: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2).$$ By simplifying the argument of the normal distribution, we get: $$\\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2) = \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Substituting this back into the sum, we obtain: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2) = \\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$ This shows that shifting both the variable $x$ and the mean $\\mu$ by the same amount $t$ leaves the wrapped normal distribution unchanged. The key step is the index shift in the sum over $k$, which does not affect the infinite sum because it runs over all integers, ensuring that the distribution remains invariant under such shifts."}
{"paper":"arxiv:2309.04475", "question":"How can we prove that the score $\\hat{\\epsilon}_L$ by Eq. (9) is O(3)-equivariant, and the score $\\hat{\\epsilon}_F$ from Eq. (10) is periodic translation invariant, thereby making the generated distribution periodic E(3) invariant? In particular, use:\n\n1. Eq. (9) for the lattice-denoising score:\n$$\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t),\\ \\forall\\ Q \\in \\mathbb{R}^{3\\times 3},\\ Q^T Q = I.$$\n\n2. Eq. (10) for the coordinate-denoising score $\\hat{\\epsilon}_F$, which is shown to be periodic translation invariant.\n\n3. The orthonormal-transformation identity:\n$$(QL)^T(QL) = L^T Q^T Q L = L^T I L = L^T L, \\quad \\text{for any orthogonal } Q \\text{ with } Q^T Q = I.$$\n\n4. The periodic-Fourier relation (used to show translation invariance) of the form:\n$$\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k].$$", "whole_label":"To prove Proposition 3, we analyze the properties of the scores $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$ step by step.\n\n1. O(3)-equivariance of $\\hat{\\epsilon}_L$:\n   - Given the equation $\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t)$ for any orthogonal matrix $Q$, this directly shows that $\\hat{\\epsilon}_L$ is O(3)-equivariant. This means that applying an orthogonal transformation $Q$ to the lattice $L$ before computing the score is equivalent to applying $Q$ to the score computed from the original lattice $L$.\n   - The orthonormal-transformation identity $(QL)^T(QL) = L^T L$ further supports this by showing that the inner product structure of the lattice is preserved under orthogonal transformations, which is a key property for O(3)-equivariance.\n\n2. Periodic translation invariance of $\\hat{\\epsilon}_F$:\n   - The periodic-Fourier relation $\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]$ demonstrates that the Fourier transform of the differences between coordinates is invariant under periodic translations $t$. This implies that $\\hat{\\epsilon}_F$ is periodic translation invariant, as the score depends only on the relative positions of the coordinates, not on their absolute positions modulo the periodic boundary.\n\n3. Conclusion:\n   - Combining the O(3)-equivariance of $\\hat{\\epsilon}_L$ and the periodic translation invariance of $\\hat{\\epsilon}_F$, the generated distribution is invariant under the combined actions of orthogonal transformations and periodic translations, i.e., it is periodic E(3) invariant. This completes the proof of Proposition 3."}
{"paper":"arxiv:2303.10876", "question":"How can we prove Theorem 1, which states the network s modules satisfy the listed equivariance and invariance properties for any translation vector $t$ and rotation (or reflection) matrix $R$? Specifically, how do Equations (1a) through (1e) ensure that $G^{(0)}R + t, H^{(0)} = F_{IL}(X_R + t)$, $\\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)})$, $G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\})$, $H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)})$, and $\\hat{Y}R + t = F_{EOL}(G^{(L)}R + t)$ collectively prove the claim?", "whole_label":"To prove Theorem 1, we systematically verify the equivariance and invariance properties of each module in the network under any translation vector $t$ and rotation (or reflection) matrix $R$. The proof is structured around the following key points:\n\n1. Initialization Layer ($F_{IL}(\\cdot)$): The initial geometric feature $G^{(0)}$ is designed to be equivariant, meaning that applying a transformation $R$ and translation $t$ to the input $X$ results in a corresponding transformation of $G^{(0)}$. The initial pattern feature $H^{(0)}$ is invariant, remaining unchanged under such transformations. This is encapsulated by the equation:\n   \\[ G^{(0)}R + t, \\quad H^{(0)} = F_{IL}(X_R + t). \\]\n\n2. Interaction Reasoning Module ($F_{IRM}(\\cdot)$): This module processes the transformed geometric features and invariant pattern features to produce interaction categorical vectors $\\{c_{ij}\\}$ that are invariant to transformations, as shown by:\n   \\[ \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}). \\]\n\n3. Geometric Feature Learning Layer ($F_{EGFL}^{(l)}(\\cdot)$): At each layer $l$, the geometric features are updated in a manner that preserves equivariance. The transformation properties are maintained through the layer's operations, ensuring that:\n   \\[ G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\}). \\]\n\n4. Pattern Feature Learning Layer ($F_{IPFL}^{(l)}(\\cdot)$): Similarly, the pattern features are updated in a way that maintains their invariance under transformations, as described by:\n   \\[ H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}). \\]\n\n5. Output Layer ($F_{EOL}(\\cdot)$): Finally, the output layer produces predictions that are equivariant to the input transformations, completing the proof with:\n   \\[ \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t). \\]\n\nBy demonstrating that each component of the network adheres to these transformation properties, we collectively establish the overall equivariance and invariance claims of Theorem 1. The detailed operations within each module, including attention mechanisms and non-linear functions, are carefully designed to preserve these properties, ensuring the network's behavior is consistent under Euclidean transformations."}
{"paper":"arxiv:2303.10876", "question":"Starting from the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features, how do we obtain $G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})R + t$? Show how the transformation $R$ and translation $t$ commute with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$ to yield $G_i^{(l+1)} R + t$ again.", "whole_label":"To understand how $G_i^{(l+1)} R + t$ is obtained from the given equation, let's break down the process step by step, focusing on how the transformation $R$ and translation $t$ interact with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$. Here's the reasoning:\n\n1. Initial Definition: We start with the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features:\n   $$G_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})$$\n\n2. Apply Transformation and Translation: We then apply the transformation $R$ and translation $t$ to $G_i^{(l+1)}$:\n   $$G_i^{(l+1)} R + t = \\left(G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\\right) R + t$$\n\n3. Distribute $R$ and $t$: Distribute the transformation $R$ and the translation $t$ inside the parentheses:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R + t$$\n   Here, we see that $R$ is distributed over the difference $(G_j^{(l)} - G_i^{(l)})$, and $t$ is added to each term.\n\n4. Simplify the Expression: Notice that the translation $t$ appears twice in the equation. However, since $t$ is a constant translation vector, adding it once is sufficient. Thus, the equation simplifies to:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This simplification shows that the transformation $R$ commutes with the difference operation, and the translation $t$ is applied once to the entire expression.\n\n5. Final Form: The final form demonstrates that the transformation and translation preserve the structure of the neighbor aggregation, leading to the equivariance property:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This confirms that $G_i^{(l+1)} R + t$ is indeed obtained as described, with $R$ and $t$ commuting appropriately with the operations involved."}
{"paper":"A Connection Between Score Matching and Denoising Autoencoders", "question":"Starting from the expression:\n\\[ S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right], \\]\nhow can we rewrite it using the definition \n\\[ q_\\sigma(x) = \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}? \\]", "whole_label":"To rewrite the expression for \\( S(\\theta) \\), we start by substituting the definition of \\( q_\\sigma(x) \\) into the original equation. This gives us:\n\\[ S(\\theta) = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} q_\\sigma(x) \\right\\rangle dx = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx. \\]\nHere, we have interchanged the derivative and integral operations, assuming the conditions for doing so are met. This step is crucial for moving forward with the derivation, as it allows us to work directly with the conditional probability density function \\( q_\\sigma(x|\\tilde{x}) \\) and the marginal density \\( q(\\tilde{x}) \\)."}
{"paper":"arxiv:2405.16577", "question":"Prove the following theorem:\n\\begin{theorem}[Wasserstein Bound]\nAssume $\\Omega$ is convex and the velocity model $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$ (uniformly on $t$). \nLet $p_{\\bm{\\theta},t}(\\bm{x})$ be the probability path of the reflected CNF induced by $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ starting from the same prior distribution $p_0(\\bm{x})$.\nThen the squared Wasserstein-2 distance between $p_{1}(\\bm{x})$ and $p_{\\bm{\\theta},1}(\\bm{x})$ is bounded by\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\n\\end{theorem}", "whole_label":"We first give an alternative expression of the reflected ODE\n\\begin{subequations}\n\\arraycolsep=1.8pt\n\\def\\arraystretch{1.5}\n\\begin{align}\n\\mathrm{d}\\bm{\\phi}_t(\\bm{x}) &= \\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t,\\\\\n\\bm{\\phi}_0(\\bm{x}) &= \\bm{x},\n\\end{align}\n\\end{subequations}\nwhere the initial point $\\bm{x}\\sim p_0(\\bm{x})$, $\\bm{n}(\\bm{x})$ is the inward unit normal vector at $\\bm{x}$ on $\\partial\\Omega$, and $l_t$ is non-decreasing in $t$ with $l_0=0$.\nMoreover, $l_t$ satisfies $\\int_{0}^t \\mathbb{I}(\\bm{\\phi}_s(\\bm{x})\\notin\\partial\\Omega)\\dif l_s=0$ for $t>0$, i.e., $\\dif l_t$ vanishes in the interior and will push the velocity along $\\bm{n}(\\bm{x})$ when the trajectory hits the boundary.\n\nLet $\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})$ (with a reflection term $\\bar{l}_t$) be the solution to the reflected ODE with velocity field $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$.\nBy the definition of Wasserstein-2 distance, we have\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq \\int_{\\Omega}\\|\\bm{\\phi}_1(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},1}(\\bm{x})\\|^2 p_0(\\bm{x})\\dif \\bm{x}:= \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=1}.\n\\end{equation}\nTake the derivative of $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)$ w.r.t. $t$ gives\n\\begin{equation}\n\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\n=2\\int_{\\Omega}\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right) p_0(\\bm{x})\\dif \\bm{x}\n\\end{equation}\nNote that\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left[(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t-\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\right]\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t \\\\\n&\\quad\\quad\\quad\\quad\\quad+ \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t - \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t,\n\\end{align*}\nThe first term can be bounded by\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n=&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t))+\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n\\leq & \\frac{1}{2}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2+\\frac{1}{2}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2 + M\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\n\\end{align*}\nwhere in the last inequality we use the mean inequality and the fact that $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$.\nTo handle the second term, note that $dl_t>0$ only if $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$. When $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$, we know\n$\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\leq 0$ due to the convexity of $\\Omega$.\nTherefore, the second term satisfies\n\\[\n\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t\\leq 0.\n\\]\nUsing the same argument, we know that the third term satisfies\n\\[\n-\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\leq 0\n\\]\nTherefore, we have the following bound for $\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t$\n\\begin{align*}\n&\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t\\\\\n\\leq & \\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2p_0(\\bm{x})\\dif \\bm{x}+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x} + 2M\\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\np_0(\\bm{x})\\dif \\bm{x}\\\\\n=&(1+2M)\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}.\n\\end{align*}\nBy Gronwall's inequality and $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=0}=0$, we have\n\\begin{equation}\n\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right) \\leq e^{1+2M}\\int_0^1\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}\\dif t=e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\nBy combining the above equations, we finally conclude\n\\[\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nThis finishes the proof."}
{"paper":"arxiv:2407.10490", "question":"Consider a classification model with V classes. For a given input x, assume that a fixed feature extractor \\(\\phi(x) \\in \\mathbb{R}^d\\) produces a feature vector, and the logits are computed as \\(z_t = w_t^\\top \\phi(x)\\). The predicted probability vector at time step t is given by \\(p_t = \\text{Softmax}(z_t)\\). Suppose that the model parameters are updated via stochastic gradient descent with learning rate \\(\\eta\\) (with an effective learning rate defined as \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\)) based on a single training example with ground truth label \\(y\\). This update results in the following change in the logits:\n\n\\[\nz_{t+1} = z_t - \\eta' (p_t - e_y),\n\\]\n\nwhere \\(e_y\\) is the one-hot vector corresponding to label \\(y\\). Define the ratio of the confidence change for each class \\(i\\) as\n\n\\[\n\\alpha_i \\equiv \\frac{p_{t+1,i}}{p_{t,i}},\n\\]\n\nLemma 1 asserts that\n\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^{V} e^{z_t^j}}{\\sum_{j=1}^{V} \\beta_j e^{z_t^j}},\n\\]\n\nwith the coefficients \\(\\beta_j\\) specified according to two cases:\n\n1. When \\(i = y\\) (Case 1):\n\n\\[\n\\beta_j = \\begin{cases} e^{-\\eta' (1 + p_t^j - p_t^y)} & \\text{if } j \\neq y, \\\\ 1 & \\text{if } j = y, \\end{cases}\n\\]\n\n2. When \\(i \\neq y\\) (Case 2):\n\n\\[\n\\beta_j = \\begin{cases} e^{-\\eta' (p_t^j - p_t^i)} & \\text{if } j \\neq y, \\\\ e^{-\\eta' (p_t^y - p_t^i - 1)} & \\text{if } j = y. \\end{cases}\n\\]\n\nCould you please provide a complete derivation of Lemma 1 using the above definitions and assumptions, ensuring that the meaning of each symbol and condition is clear?", "whole_label":"To derive Equation (34), we need to have the analytical expression of each \\(p_{t+1,i}\\) and \\(p_{t,i}\\). As \\(p = \\text{Softmax}(z)\\), we need to link \\(z_{t+1}\\) and \\(z_t\\) first. With Equations (32) and (33), \\(z_{t+1}\\) can be recursively written down as:\n\n\\[\nz_{t+1} = w_{t+1}^\\top \\phi(x) = \\Bigl( w_t - \\eta\\, \\phi(x)(p_t - e_y)^\\top \\Bigr)^\\top \\phi(x) = z_t - \\eta \\|\\phi(x)\\|_2^2 (p_t - e_y) = z_t - \\eta'(p_t - e_y),\n\\] \\(36\\)\n\nwhere \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\) is the equivalent learning rate that depends on the norm of the feature representation. Note that \\(z\\), \\(p\\), and \\(e_y\\) are all length-\\(V\\) vectors and \\(y\\) is an integer ranging from 1 to \\(V\\). Then we can write down each \\(z_{t+1,i}\\) as:\n\n\\[\nz_{t+1,i} = \\begin{cases}\n z_{t,i} - \\eta' p_{t,i} + \\eta', & \\text{if } i = y, \\\\\n z_{t,i} - \\eta' p_{t,i}, & \\text{if } i \\neq y.\n\\end{cases}\n\\] \\(37\\)\n\nThen, we can combine the definition of the Softmax function and write down different \\(p_{t+1,i}\\) case-by-case. For Case 1 where \\(i = y\\), we have:\n\n\\[\np_{t+1,y} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j=1}^{V} e^{z_{t+1,j}}} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (38) \n\\]\n\nCombining the fact that \\(p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^{V} e^{z_{t,j}}}\\), we can derive \\(\\alpha_i\\) and \\(\\beta_j\\) as in Equation (35). Similarly, when \\(i \\neq y\\), we have:\n\n\\[\np_{t+1,i} = \\frac{e^{z_{t,i} - \\eta' p_{t,i}}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (39)\n\\]\n\nwhich leads to the right part of Equation (35)."}
{"paper":"arxiv:2312.11456", "question":"Let \\(\\Pi\\) be a policy class and let \\(\\pi, \\hat{\\pi} \\in \\Pi\\) be two policies that, along with a reference policy \\(\\pi_0\\), share the same support. Let \\(d_0\\) denote the distribution over states (or prompts), and let \\(\\hat{r}: X \\times A \\to \\mathbb{R}\\) be an estimated reward function. Suppose that \\(\\hat{\\pi}\\) is generated by applying Oracle 1 with \\(\\hat{r}\\). Under these conditions, Lemma 2 states that\n\n\\[\n\\mathbb{E}_{x\\sim d_0}\\Big[\\mathbb{E}_{a\\sim \\pi}\\big[\\hat{r}(x,a)\\big] - \\mathbb{E}_{a\\sim \\hat{\\pi}}\\big[\\hat{r}(x,a)\\big] + \\eta\\, D_{KL}\\big(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big) - \\eta\\, D_{KL}\\big(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big)\\Big] = -\\eta\\, \\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\big(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x)\\big)\\Big].\n\\]\n\nCould you provide a complete derivation or proof of this lemma?", "whole_label":"Since \\(\\hat{\\pi}\\) is induced by calling Oracle 1 with \\(\\hat{r}\\), we know that for any \\(x \\in X\\),\n\n\\[\n\\hat{\\pi}(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\, \\hat{r}(x,a)\\Big),\n\\]\n\nwhere \\(Z(x)=\\sum_{a \\in A}\\pi_0(a|x)\\exp\\Big(\\frac{1}{\\eta}\\hat{r}(x,a)\\Big)\\) is the normalization constant. We can rewrite the reward function as\n\n\\[\n\\hat{r}(x,a)=\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}+\\eta\\log Z(x).\n\\]\n\nPlugging this reward reparameterization into the policy optimization error under \\(\\hat{r}\\), we have\n\n\\[\n\\mathbb{E}_{\\pi}[\\hat{r}(x,a)]-\\mathbb{E}_{\\hat{\\pi}}[\\hat{r}(x,a)] = \\mathbb{E}_{\\pi}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big] - \\mathbb{E}_{\\hat{\\pi}}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big]\n\\]\n\n\\[\n= \\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x))-\\eta\\,D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)).\n\\]\n\nPlugging the above equality into the left-hand side of Lemma 2 completes the proof."}
{"paper":"arxiv:2402.05749", "question":"Consider a discrete set of outcomes $Y$ and a parametric distribution (policy) $\\pi_{\\theta}(y)$ with parameters $\\theta\\in\\mathbb{R}^d$, and a fixed reference distribution $\\pi_{\\mathrm{ref}}(y)$ on $Y$. Define the Kullback-Leibler divergence\n\n$$\n\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\n$$\n\nIntroduce the log-density ratio for each $y\\in Y$:\n\n$$\n\\rho_{\\theta}(y) := \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}.\n$$\n\nShow how to derive the following gradient identity:\n\n$$\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n$$\n\nYou may assume that you can interchange gradient and expectation and that\n$\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)]=0$ under the shared support of the two distributions.", "whole_label":"By definition we have\\n\\n$$\\n\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\\n$$\\n\\nIts gradient contains two terms:\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr]\n+\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\bigr].\\n$$\\n\\nThe second term vanishes because it is the expectation of the score function under its own distribution. Meanwhile, rewriting the \\mu-weighted squared loss with \\mu=\\pi_{\\theta} gives\\n\\n$$\\n\\frac12\\,\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[\\rho_{\\theta}^2]\n=\\frac12\\,\\mathbb{E}_{(y_1,y_2)\\sim\\mu}\\Bigl[\\Bigl(\\log\\frac{\\pi_{\\theta}(y_1)}{\\pi_{\\mathrm{ref}}(y_1)}-\\log\\frac{\\pi_{\\theta}(y_2)}{\\pi_{\\mathrm{ref}}(y_2)}\\Bigr)^2\\Bigr],\\n$$\\n\\nand taking its gradient under \\mu=\\pi_{\\theta} yields\\n\\n$$\\n\\mathbb{E}_{(y_1,y_2)\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}^2)\\bigr]\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr],\\n$$\\n\\nwhere the cross term and exchange of expectations use independence and identical distribution of y_1,y_2. Equating the two expressions proves\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}(y)^2)\\bigr].\\n$$"}
{"paper":"arxiv:2403.03419", "question":"Could you please provide a complete derivation and proof of the following lemma, making use only of the accompanying definitions and assumptions? \n\nDefinitions and assumptions\n  \\(\\pi_r(y)\\): the probability distribution over responses y produced by the original (pre-trained) language model.\n  \\(\\pi_{\\theta}(y)\\): a trainable policy over responses.\n  \\(r^{*}(y)\\): the ground-truth scalar reward assigned to an individual response.\n  \\(\\beta>0\\): a fixed temperature (scaling) constant; \\(Z>0\\) is a finite normalising factor.\n  For any distribution \\(\\pi\\), let \\(\\varphi(\\pi)\\) be its distributional preference value.\n  Let \\(\\mu\\) denote a fixed distribution of dispreferred responses.\n  A Generation-with-Distributional-Control (GDC) problem is solved under the moment constraint \\(\\varphi(\\pi) > \\varphi(\\mu)\\).\n\nLemma 1\nIf the policy \\(\\pi_{\\theta}(y)\\) obtained from the GDC optimisation satisfies the constraint above, then\n\\[\n r^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\qquad\n \\varphi^{*}(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\Big[\\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)}\\Big].\n\\]\n\nPlease present a rigorous, step-by-step derivation and proof of Lemma 1.", "whole_label":"The Generation-with-Distributional-Control (GDC) can be formalized as\n\\[\\theta^{*}=\\arg\\min_{\\theta} \\mathrm{KL}\\big[p(x)\\,\\|\\,\\pi_{\\theta}(x)\\big]-\\big[\\varphi(p)-\\varphi(\\mu)\\big].\\tag{5}\\]\nThe KL term is minimized when \\(p(x)=\\pi_{\\theta}(x)\\), that is, \\(\\pi_r(x)e^{r^{*}(x)/\\beta}/Z=\\pi_{\\theta}(x)\\). Hence we obtain \\(r^{*}(x)=\\beta\\,\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}+\\beta\\log Z\\). We then need only maximize \\(\\varphi(p)-\\varphi(\\mu)\\). \\(\\varphi\\) is defined as the expectation of reward over the given distribution, yielding\n\\[\\begin{aligned}\n\\arg\\max_{\\pi} \\varphi(p)-\\varphi(\\mu)\n&=\\mathbb{E}_{p}[r(x)]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{p(x)}{\\pi_{\\theta}(x)}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{\\pi_r(x)}{\\pi_{\\theta}(x)}\\,\\frac{\\pi_r(x)e^{r^{*}(x)/\\beta}}{Z}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\beta\\,\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big]-\\beta\\,\\mathbb{E}_{\\mu}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big].\\tag{6}\n\\end{aligned}\\]\nThus the GDC problem can be solved by maximizing Eq. (6). Consequently, we may directly define the distributional reward (a constant for a fixed \\(\\pi\\)) as \\(\\varphi^{*}(\\pi)=\\mathbb{E}_{\\pi}\\big[\\beta\\,\\log\\tfrac{\\pi^{*}(x)}{\\pi_r(x)}\\big]\\). Therefore, learning a distributionally controlled LLM is equivalent to maximizing the gap \\(\\varphi(\\pi)-\\varphi(\\mu)\\) when the constraint reflects a human preference relation."}
{"paper":"arxiv:2308.02585", "question":"How can the norm of the second-order mixed Hessian term of the lower-level value function, $\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|$, be bounded?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. Here $H_\\ell$ is the lower-level horizon length and $\\gamma \\in (0,1)$ is the discount factor.\n    \\item Approximate policy parameter at iteration $t$: $\\theta^K(\\nu_t)$.\n    \\item The mixed Hessian term $\\nabla^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))$ (representing second derivatives w.r.t. $\\nu$ and $\\theta$) is defined via expectation over trajectories $\\tau$ generated by the policy $\\pi_{\\theta^K(\\nu_t)}$ with distribution $\\rho(\\tau;\\theta^K(\\nu_t))$:\n    \\[\n        {\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t)) = \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg]\n    \\]\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Reward Gradient Bound:} The reward function $r_\\nu(s,a)$ has a bounded gradient w.r.t. $\\nu$: $\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r$ for all $s, a$.\n    \\item \\textbf{Policy Score Bound:} The score function (log-policy gradient) is bounded: $\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B$ for all $s, a$.\n\\end{itemize}\n\n**Goal:** Using these definitions and assumptions, derive the upper bound:\n\\[\n    \\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq H_\\ell^2 L_r B\n\\]", "whole_label":"We start with the definition:\n\\begin{align}\n    \\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t)) &=  \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right)^T \\Bigg] \\nonumber \\\\\n    & \\leq \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\nonumber \\\\\n    & = \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \n\\end{align}\nwhere $\\tau' = [(s_0', a_0') \\cdot (s_h', a_h') \\cdots]$ represents the trajectory for which the term inside the expectation is maximum and thereby upper-bounding with that leads to the expression above. Next we upper-bound the norm of the $\\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\|$ as\n\\begin{align}\n    \\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\| &\\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T\\Bigg\\| \\nonumber \\\\\n    & \\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\Bigg\\|\\cdot\\Bigg\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\Bigg\\| \\nonumber\\\\\n    & \\leq H_\\ell^2 L_r B \n\\end{align}\nwhere we apply the Cauchy-Schwarz inequality to get the inequality in the second line. Subsequently, we apply triangle inequality with the reward gradient bound assumption ($L_r$) and the bounded score function assumption ($B$) to get the final bound for the mixed hessian term."}
{"paper":"arxiv:2410.11443", "question":"Let $x_{is}, x_{jt}$ be difference between node positions and $Y$ be the harmonics. Why there exists a bijection between the set of inner products $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ and angles $\\{\\arccos \\langle x_{is}, x_{jt}\\rangle\\}$?", "whole_label":"First, the output of inner product $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ is Legendre polynomials, which span a space isomorphic to power polynomials. Moreover, through Newton's identities and Vieta's theorem, we can get the set of all angle cosines. Also note that trigonometric functions directly form bijections to angles, so this bijection exists."}
{"paper":"Real Analysis: Modern Techniques and Their Applications", "question":"For any measurable functions $f$ and $g$ on a measure space $(X, \\mu)$, let $p, q > 1$ be conjugate exponents, meaning they satisfy \n \\[\\frac{1}{p} + \\frac{1}{q} = 1.\\] \n Holder's inequality states:\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\left( \\int_X |g(x)|^q d\\mu(x) \\right)^{\\frac{1}{q}}.\n\\]", "whole_label":"Define the function\n\\[\n    h(x) = \\frac{|f(x)|}{\\|f\\|_p} \\quad \\text{and} \\quad k(x) = \\frac{|g(x)|}{\\|g\\|_q}.\n\\]\n\nBy Young's inequality, which states that for nonnegative $a, b$,\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]\n\nwe apply this to $h(x)^p$ and $k(x)^q$:\n\\[\n    |f(x) g(x)| \\leq \\frac{|f(x)|^p}{p \\|f\\|_p^p} + \\frac{|g(x)|^q}{q \\|g\\|_q^q}.\n\\]\n\nIntegrating both sides over $X$ gives\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\nMultiplying both sides by $\\|f\\|_p \\|g\\|_q$ completes the proof.\n"}
{"paper":"Numerical Optimization", "question":"Let the function \\( f(x) \\) be twice continuously differentiable in a neighborhood of the minimum point \\( x^* \\). Suppose there exist \\(\\varepsilon > 0\\) and \\( M > m > 0 \\) such that when \\(\\|x - x^*\\| < \\varepsilon\\),\n\\[\nm \\|y\\|^2 \\leq y^T G(x) y \\leq M \\|y\\|^2, \\quad \\forall y \\in \\mathbb{R}^n \n\\]\nholds. \n\nAdditionally, given the expansion:\n\\[\nf(x + \\alpha d) = f(x) + \\alpha g^T d + \\alpha^2 \\int_{0}^{1} (1 - t) \\left[ d^T G(x + t\\alpha d) \\right] dt,\n\\]\n\nwe aim to prove:\n\\begin{align*}\n   \\|g(x)\\| \\geq m \\|x - x^*\\|. \n\\end{align*}", "whole_label":"By the expansion,\n\\begin{align*}\nf(x) - f(x^*) &= g(x^*)^T (x - x^*) +\\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\mathrm{d}t \\\\ \n&= \\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\, \\mathrm{d}t,\n\\end{align*}\n\nPerforming a Taylor expansion on \\( g(x) \\):\n\\[\ng(x) = g(x) - g(x^*) = \\int_0^1 G(tx + (1-t)x^*) (x - x^*) dt.\n\\]\n\nThus, we have:\n\\begin{align*}\n\\|g(x)\\|\\|x - x^*\\| &\\geqslant (x - x^*)^\\top g(x) \\\\\n&= \\int_0^1 (x - x^*)^\\top G(tx + (1 - t)x^*)(x - x^*) \\, \\mathrm{d}t \\\\\n&\\geqslant m \\|x - x^*\\|^2\n\\end{align*}"}
{"paper":"arxiv:2501.18922", "question":"Let the model's initial performance on the annotated dataset $D_{a}$ be $E_{\\mathrm{base}}$, and let its performance after incremental fine-tuning be $E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^{*})$, where\n$$\\Delta E(\\gamma^{*}) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}\\bigl(D_{i}(\\gamma^{*})\\bigr).$$\nHere:\n- $\\lambda>0$ is a scaling factor,\n- $D_{i}(\\gamma^{*})$ is the set of logical-form-answer pairs $(\\widehat F_{Q},\\widehat A_{Q})$ drawn from the unannotated dataset $D_{n}$ of size $N_{n}=|D_{n}|$, filtered by the criteria $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_{Q})>\\gamma^{*}$ and $\\widehat A_{Q}\\neq\\emptyset$, \n- $N_{\\mathrm{high}}(\\gamma^{*}) = N_{n}\\cdot P_{\\mathrm{high}}(\\gamma^{*})$ with\n$$P_{\\mathrm{high}}(\\gamma^{*}) = \\int_{\\gamma^{*}}^{\\beta} P(R)\\,dR,\\quad\\text{where }\\beta\\text{ is the maximum reward score,}$$\n- $Q_{\\mathrm{avg}}\\bigl(D_{i}(\\gamma^{*})\\bigr)$ is the average quality of the incremental data.\n\nPlease state and derive the result in Proposition 4.3 showing that there exists some threshold $\\gamma^{*}<\\beta$ for which $\\Delta E(\\gamma^{*})$ grows significantly large.", "whole_label":"First, we define the performance improvement of incremental fine-tuning, $\\Delta E(\\gamma^*)$. The model's initial performance on the annotated dataset $D_a$ is denoted as $E_{\\mathrm{base}}$, and its performance after incremental fine-tuning is\n> \\[E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^*).\\]\n> Here, $\\Delta E(\\gamma^*)$ represents the performance gain contributed by incremental data, which depends on the quantity and quality of the incremental data. \n\n> The incremental data originates from the unannotated dataset $D_n$, where the logical forms $\\widehat F_Q$ and answers $\\widehat A_Q$ are generated through a filtering mechanism. The filtering criteria are: 1) the reward score of the logical form $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_Q)>\\gamma^*$; 2) the answer set is non-empty, $\\widehat A_Q\\neq\\emptyset$. The logical forms that satisfy these criteria form the incremental dataset $D_i(\\gamma^*)$. The size and quality of this dataset are both dependent on the reward threshold $\\gamma^*$.\n\n> The performance improvement can be expressed as:\n> \\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^*) \\cdot Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr),\\]\n> where $\\lambda>0$ is a scaling factor; \n> \\[N_{\\mathrm{high}}(\\gamma^*) = N_n \\cdot P_{\\mathrm{high}}(\\gamma^*),\\quad P_{\\mathrm{high}}(\\gamma^*) = \\int_{\\gamma^*}^{\\beta} P(R)\\,dR;\\]\n> \\[Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr) = \\frac{\\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR}{\\int_{\\gamma^*}^{\\beta} P(R)\\,dR}.\\]\n\n> Substituting these into $\\Delta E(\\gamma^*)$ and simplifying, we get:\n> \\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_n \\cdot \\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR.\\]\n\n> To analyze the critical point, we take the derivative with respect to $\\gamma^*$:\n> \\[\\frac{\\partial \\Delta E(\\gamma^*)}{\\partial \\gamma^*} = -\\lambda \\cdot N_n \\cdot \\gamma^* \\cdot P(\\gamma^*).\\]\n> As $\\gamma^*\\to -\\infty$, this derivative is positive, indicating $\\Delta E(\\gamma^*)$ increases with $\\gamma^*$; as $\\gamma^*\\to \\beta$, the derivative is negative, indicating $\\Delta E(\\gamma^*)$ decreases. Thus, $\\Delta E(\\gamma^*)$ is a unimodal function with a maximum at some $\\gamma^*<\\beta$.\n\n**Conclusion:** There exists a reward threshold $\\gamma^*<\\beta$ such that the trade-off between the quantity and quality of incremental data is optimized and the performance improvement $\\Delta E(\\gamma^*)$ reaches its maximum."}
