{"paper": "arxiv:2305.18290", "question": "Given the following equations:\n\nEquation 1 (Bradley-Terry model):\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\nEquation 2 (Reward function):\n\\begin{equation}\n    r(x,y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\nHow can we derive Equation 3 from Equations 1 and 2?\n\nEquation 3 (Objective):\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)}\\right)}\n\\end{equation}", "A": "To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. Start with Equation 1, which models the probability that $y_1$ is preferred over $y_2$ given $x$:\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\n2. Substitute the reward function $r^*(x, y)$ from Equation 2 into Equation 1. Equation 2 expresses the reward in terms of the optimal policy $\\pi^*$ and the reference policy $\\pi_{\\text{ref}}$:\n\\begin{equation}\n    r^*(x,y) = \\beta \\log \\frac{\\pi^*(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\n3. Substitute $r^*(x, y_1)$ and $r^*(x, y_2)$ into Equation 1:\n\\begin{align*}\n    p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} + \\beta \\log Z(x)\\right)} \\\\\n    &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} - \\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)}\\right)}.\n\\end{align*}\n\n4. The final expression is Equation 3, which simplifies the preference probability in terms of the ratio of the optimal policy to the reference policy for the two options $y_1$ and $y_2$.\n\nThis derivation shows how the preference probability can be expressed directly in terms of the policies, without explicitly computing the reward function, by leveraging the relationship between the reward and the policies as given in Equation 2.", "B": "To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. Start with Equation 1, which models the probability that $y_1$ is preferred over $y_2$ given $x$:\n\\begin{equation}\n    p^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\n\n2. Substitute the reward function $r^*(x, y)$ from Equation 2 into Equation 1. Equation 2 expresses the reward in terms of the optimal policy $\\pi^*$ and the reference policy $\\pi_{\\text{ref}}$:\n\\begin{equation}\n    r^*(x,y) = \\beta \\log \\frac{\\pi^*(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\n\n3. Substitute $r^*(x, y_1)$ and $r^*(x, y_2)$ into Equation 1:\n\\begin{align*}\n    p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} + \\beta \\log Z(x)\\right)} \\\\\n    &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_{\\text{ref}}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_{\\text{ref}}(y_1\\mid x)}\\right)}.\n\\end{align*}\n\n4. The final expression is Equation 3, which simplifies the preference probability in terms of the ratio of the optimal policy to the reference policy for the two options $y_1$ and $y_2$.\n\nThis derivation shows how the preference probability can be expressed directly in terms of the policies, without explicitly computing the reward function, by leveraging the relationship between the reward and the policies as given in Equation 2.", "C": "To derive Equation 3 from Equations 1 and 2, follow these steps:\n\n1. **Substitute the reward function (Equation 2) into the Bradley-Terry model (Equation 1):**\n   \\begin{align*}\n     p^*(y_1 \\succ y_2 \\mid x) &= \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)} \\\\\n     &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} + \\beta \\log Z(x)\\right)}.\n   \\end{align*}\n\n2. **Factor out \\(\\beta \\log Z(x)\\) from the exponents:**\n   \\begin{align*}\n     &= \\frac{\\exp\\left(\\beta \\log Z(x)\\right) \\cdot \\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp\\left(\\beta \\log Z(x)\\right) \\left[\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)\\right]}.\n   \\end{align*}\n\n3. **Cancel the common factor \\(\\exp\\left(\\beta \\log Z(x)\\right)\\):**\n   \\begin{align*}\n     &= \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)}.\n   \\end{align*}\n\n4. **Simplify the exponentials using \\( \\exp(\\beta \\log a) = a^\\beta \\):**\n   \\begin{align*}\n     &= \\frac{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta}{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta + \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)^\\beta}.\n   \\end{align*}\n\n5. **Factor out \\(\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta\\) from the denominator:**\n   \\begin{align*}\n     &= \\frac{1}{1 + \\left(\\frac{\\pi^*(y_2 \\mid x) \\cdot \\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x) \\cdot \\pi_{\\text{ref}}(y_2 \\mid x)}\\right)^\\beta}.\n   \\end{align*}\n\n6. **Rewrite the ratio using logarithmic properties:**\n   \\begin{align*}\n     &= \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} - \\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}.\n   \\end{align*}\n\nThis matches **Equation 3**, completing the derivation. The normalization term \\(Z(x)\\) cancels out naturally during simplification.", "D": "To derive Equation 3 from Equations 1 and 2, we will substitute the reward function \\( r^*(x, y) \\) from Equation 2 into the Bradley-Terry model in Equation 1. Here is the step-by-step derivation:\n\n---\n\n**Step 1: Start with the Bradley-Terry model (Equation 1):**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\]\n\n**Step 2: Substitute the reward function \\( r^*(x, y) \\) from Equation 2:**\n\\[\nr^*(x, y) = \\beta \\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + \\beta \\log Z(x).\n\\]\n\nSubstitute \\( r^*(x, y_1) \\) and \\( r^*(x, y_2) \\) into Equation 1:\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} + \\beta \\log Z(x)\\right)}.\n\\]\n\n**Step 3: Simplify the exponents:**\nFactor out \\( \\exp(\\beta \\log Z(x)) \\) from the numerator and denominator:\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp(\\beta \\log Z(x)) \\cdot \\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp(\\beta \\log Z(x)) \\cdot \\left[\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)\\right]}.\n\\]\n\nCancel \\( \\exp(\\beta \\log Z(x)) \\) from numerator and denominator:\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)}.\n\\]\n\n**Step 4: Rewrite the expression using the property \\( \\exp(\\beta \\log a) = a^\\beta \\):**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta}{\\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta + \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)}\\right)^\\beta}.\n\\]\n\n**Step 5: Divide numerator and denominator by \\( \\left(\\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)^\\beta \\):**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right)^\\beta}.\n\\]\n\n**Step 6: Rewrite the denominator using logarithmic properties:**\n\\[\n\\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right)^\\beta = \\exp\\left(\\beta \\log \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right)\\right).\n\\]\n\nSimplify the logarithm:\n\\[\n\\beta \\log \\left(\\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} \\cdot \\frac{\\pi_{\\text{ref}}(y_1 \\mid x)}{\\pi^*(y_1 \\mid x)}\\right) = \\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} - \\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}.\n\\]\n\n**Step 7: Substitute back into the expression:**\n\\[\np^*(y_1 \\succ y_2 \\mid x) = \\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} - \\beta \\log \\frac{\\pi^*(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)}\\right)}.\n\\]\n\n---\n\nThis is the desired **Equation 3**. The derivation shows how the Bradley-Terry model, combined with the reward function, leads to the final objective function for preference modeling.", "ground-truth": "B"}
{"paper": "arxiv:2407.18242", "question": "Prove the following theorem:\n\\begin{theorem}\nWhen updating matrices $A$ and $B$ using the closed-form solution, we proceed as follows:\n\\begin{gather}\n    A\\leftarrow A - \\gamma g^A \\\\\n    B\\leftarrow B - \\gamma g^B, \n\\end{gather}\nwhere $\\gamma \\ge 0$ denotes the learning rate.\nOur method ensures a decrease in the loss, akin to the standard gradient descent algorithm, expressed by:\n\\begin{equation}\n    \\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\} \\le 0.\n\\end{equation}\n\\end{theorem}", "A": "The proof of the theorem is divided into two parts.\n\nPart I.\nWe first show that the differential change in the loss function, $\\dd L$, can be expressed as:\n\\begin{equation}\n\\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\}.\n\\end{equation}\nThis is derived from the differentials $\\dd A = -\\gamma g^A$ and $\\dd B = -\\gamma g^B$, and the fact that $\\frac{\\partial L}{\\partial A} = g^A_{lora}$ and $\\frac{\\partial L}{\\partial B} = g^B_{lora}$.\n\nPart II.\nWe then prove that $\\dd L \\le 0$ by showing:\n\\begin{gather}\n\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F\\ge 0, \\\\\n\\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\ge 0.\n\\end{gather}\n\n1. For the first inequality, we show $(B^TB)^{-1}$ is positive definite by demonstrating $B^TB$ is positive definite, hence $(B^TB)^{-1} = UU^T$ for some matrix $U$, leading to $\\frac{1}{s^2}\\|U^Tg^A_{lora}\\|_F^2\\ge 0$.\n\n2. For the second inequality, we show $(AA^T)^{-1}$ is positive-definite and $P = I - B(B^TB)^{-1}B^T$ is a projection matrix (hence positive semi-definite), leading to $\\frac{1}{s^2}\\|V^Tg^B_{lora}U\\|_F^2 \\ge 0$ where $P = VV^T$ and $(AA^T)^{-1} = UU^T$.\n\nIn conclusion, combining these results confirms $\\dd L \\le 0$, ensuring the loss decreases with each update.", "B": "To prove the theorem, we need to show that the loss \\( L \\) decreases when updating matrices \\( A \\) and \\( B \\) using the given closed-form solution. The key idea is to analyze the change in the loss \\( \\dd L \\) due to the updates and demonstrate that it is non-positive.\n\n### Step 1: Loss Function and Gradients\nAssume the loss \\( L \\) is a function of \\( A \\) and \\( B \\), and let \\( g^A \\) and \\( g^B \\) be the gradients of \\( L \\) with respect to \\( A \\) and \\( B \\), respectively. The updates are:\n\\[\nA \\leftarrow A - \\gamma g^A, \\quad B \\leftarrow B - \\gamma g^B.\n\\]\n\n### Step 2: Change in Loss\nThe first-order approximation of the change in the loss \\( \\dd L \\) due to the updates is:\n\\[\n\\dd L = \\langle \\nabla_A L, \\dd A \\rangle_F + \\langle \\nabla_B L, \\dd B \\rangle_F,\n\\]\nwhere \\( \\dd A = -\\gamma g^A \\) and \\( \\dd B = -\\gamma g^B \\). Substituting these, we get:\n\\[\n\\dd L = -\\gamma \\left( \\langle g^A, g^A \\rangle_F + \\langle g^B, g^B \\rangle_F \\right).\n\\]\nHowever, this is the standard gradient descent result, which always ensures \\( \\dd L \\leq 0 \\).\n\n### Step 3: Special Form of Gradients\nThe theorem specifies a particular form for the gradients \\( g^A \\) and \\( g^B \\), which are derived from the LoRA (Low-Rank Adaptation) framework. Let:\n\\[\ng^A = \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora}, \\quad g^B = \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1}.\n\\]\nHere, \\( s \\) is a scaling factor, and \\( g^A_{lora} \\) and \\( g^B_{lora} \\) are the LoRA-specific gradients.\n\n### Step 4: Substitute Gradients into \\( \\dd L \\)\nSubstituting \\( g^A \\) and \\( g^B \\) into the expression for \\( \\dd L \\):\n\\[\n\\dd L = -\\gamma \\left( \\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F \\right).\n\\]\n\n### Step 5: Non-Positivity of \\( \\dd L \\)\nTo show \\( \\dd L \\leq 0 \\), we analyze each term:\n1. The first term is \\( \\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F \\). Since \\( (B^TB)^{-1} \\) is positive definite (assuming \\( B \\) has full column rank), this term is non-negative.\n2. The second term is \\( \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F \\). The matrix \\( [I - B(B^TB)^{-1}B^T] \\) is a projection matrix (idempotent and symmetric), and \\( (AA^T)^{-1} \\) is positive definite (assuming \\( A \\) has full row rank). Thus, this term is also non-negative.\n\nSince both terms are non-negative and multiplied by \\( -\\gamma \\) (where \\( \\gamma \\geq 0 \\)), we conclude:\n\\[\n\\dd L \\leq 0.\n\\]\n\n### Conclusion\nThe updates for \\( A \\) and \\( B \\) ensure that the loss \\( L \\) decreases (or remains unchanged) at each step, analogous to standard gradient descent. This completes the proof.", "C": "To prove the theorem, we demonstrate that the closed-form updates for matrices \\(A\\) and \\(B\\) guarantee a non-positive loss change \\(\\dd L \\leq 0\\), analogous to standard gradient descent.\n\n### Key Steps:\n1. **First-Order Loss Approximation**:  \n   The loss change under parameter updates \\(\\Delta A = -\\gamma g^A\\) and \\(\\Delta B = -\\gamma g^B\\) is approximated as:\n   \\[\n   \\dd L \\approx -\\gamma \\left( \\left\\langle \\frac{\\partial L}{\\partial A},\\, g^A \\right\\rangle_F + \\left\\langle \\frac{\\partial L}{\\partial B},\\, g^B \\right\\rangle_F \\right).\n   \\]\n\n2. **Preconditioned Gradients**:  \n   The update directions \\(g^A\\) and \\(g^B\\) are derived from preconditioned gradients:\n   \\[\n   g^A = \\frac{1}{s^2}(B^\\top B)^{-1} g^A_{\\text{lora}}, \\quad \n   g^B = \\frac{1}{s^2}\\left[I - B(B^\\top B)^{-1}B^\\top \\right] g^B_{\\text{lora}} (AA^\\top)^{-1}.\n   \\]\n   Here, \\(g^A_{\\text{lora}} = \\partial L / \\partial A\\) and \\(g^B_{\\text{lora}} = \\partial L / \\partial B\\).\n\n3. **Substitute into Loss Change**:  \n   Plugging \\(g^A\\) and \\(g^B\\) into \\(\\dd L\\):\n   \\[\n   \\dd L = -\\gamma \\left( \\left\\langle g^A_{\\text{lora}},\\, \\frac{1}{s^2}(B^\\top B)^{-1}g^A_{\\text{lora}} \\right\\rangle_F + \\left\\langle g^B_{\\text{lora}},\\, \\frac{1}{s^2}\\left[I - B(B^\\top B)^{-1}B^\\top \\right]g^B_{\\text{lora}}(AA^\\top)^{-1} \\right\\rangle_F \\right).\n   \\]\n\n4. **Non-Negativity of Terms**:  \n   - **First Term**: \\((B^\\top B)^{-1}\\) is positive definite (assuming \\(B\\) is full column rank), so:\n     \\[\n     \\left\\langle g^A_{\\text{lora}},\\, (B^\\top B)^{-1}g^A_{\\text{lora}} \\right\\rangle_F \\geq 0.\n     \\]\n   - **Second Term**: The operator \\(\\left[I - B(B^\\top B)^{-1}B^\\top \\right]\\) is a projection matrix (symmetric, idempotent, and positive semi-definite), and \\((AA^\\top)^{-1}\\) is positive definite (assuming \\(A\\) is full row rank). The combined term is positive semi-definite, ensuring:\n     \\[\n     \\left\\langle g^B_{\\text{lora}},\\, \\left[I - B(B^\\top B)^{-1}B^\\top \\right]g^B_{\\text{lora}}(AA^\\top)^{-1} \\right\\rangle_F \\geq 0.\n     \\]\n\n5. **Conclusion**:  \n   Both inner products are non-negative. Thus, their sum scaled by \\(-\\gamma\\) guarantees \\(\\dd L \\leq 0\\), proving the theorem.\n\n### Final Expression:\n\\[\n\\dd L = -\\gamma \\left( \\text{Non-negative Term}_A + \\text{Non-negative Term}_B \\right) \\leq 0.\n\\]\nThis ensures the loss decreases monotonically with updates, analogous to gradient descent.", "D": "The proof of the theorem is divided into two parts.\n\nPart I.\nWe first show that the differential change in the loss function, $\\dd L$, can be expressed as:\n\\begin{equation}\n\\dd L = -\\gamma \\{\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F + \\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\}.\n\\end{equation}\nThis is derived from the differentials $\\dd A = -\\gamma g^A$ and $\\dd B = -\\gamma g^B$, and the fact that $\\frac{\\partial L}{\\partial A} = g^A_{lora}$ and $\\frac{\\partial L}{\\partial B} = g^B_{lora}$.\n\nPart II.\nWe then prove that $\\dd L \\le 0$ by showing:\n\\begin{gather}\n\\langle g^A_{lora}, \\frac{1}{s^2}(B^TB)^{-1}g^A_{lora} \\rangle_F\\ge 0, \\\\\n\\langle g^B_{lora}, \\frac{1}{s^2}[I - B(B^TB)^{-1}B^T]g^B_{lora}(AA^T)^{-1} \\rangle_F\\ge 0.\n\\end{gather}\n\n1. For the first inequality, we show $(B^TB)^{-1}$ is positive definite by demonstrating $B^TB$ is positive definite, hence $(B^TB)^{-1} = UU^T$ for some matrix $U$, leading to $\\frac{1}{s^2}\\|U^Tg^A_{lora}\\|_F^2\\ge 0$.\n\n2. For the second inequality, we show $(AA^T)^{-1}$ is positive-definite and $P = I - B(B^TB)^{-1}B^T$ is a projection matrix (hence positive semi-definite), leading to $\\frac{1}{s^2}\\|V^Tg^B_{lora}U\\|_F^2 \\ge 0$ where $P = VV^T$ and $(AA^T)^{-1} = U^TU$.\n\nIn conclusion, combining these results confirms $\\dd L \\le 0$, ensuring the loss decreases with each update.", "ground-truth": "A"}
{"paper": "arxiv:2405.20304", "question": "Prove the proposition: Suppose that the loss $l(\\cdot;(x_g,y,y'))$ is non-negative, convex, $B_{\\nabla}-$Lipschitz continuous, and bounded by $B_l$ for all $(x_g,y,y')\\in\\gX\\oplus\\gG\\times\\gY\\times\\gY$ and $\\|\\theta\\|_2\\leq B_{\\Theta}$ for all $\\theta\\in\\Theta$ with convex $\\Theta\\subset\\R^d$. The error of the average iterate of the algorithm, i.e., $\\pi_{\\avtheta} = \\tfrac{1}{T}\\sum_{t=1}^T \\theta^t$, satisfies $\\mathbb{E}[\\rdpol(\\pi_{\\avtheta})]-\\min_{\\theta\\in\\Theta}\\rdpol(\\pi_{\\theta})= \\mathcal{O}\\big(T^{-1/2}\\big)$.", "A": "To prove the proposition, we analyze the convergence of the average iterate $\\pi_{\\avtheta} = \\frac{1}{T}\\sum_{t=1}^T \\theta^t$ under the given conditions. The key steps are as follows:\n\n**1. Problem Setup and Assumptions:**\n- Let $\\rdpol(\\pi_\\theta) := \\mathbb{E}[l(\\theta; (x_g, y, y'))]$ be the expected loss.\n- $\\Theta \\subset \\mathbb{R}^d$ is convex and bounded: $\\|\\theta\\|_2 \\leq B_\\Theta$ for all $\\theta \\in \\Theta$.\n- $l(\\cdot; (x_g, y, y'))$ is convex, $B_\\nabla$-Lipschitz (i.e., $\\|\\nabla l(\\theta)\\|_2 \\leq B_\\nabla$), and bounded by $B_l$.\n\n**2. Online Gradient Descent (OGD) Analysis:**\nAssume the algorithm performs projected OGD with step size $\\eta$:\n$$\n\\theta^{t+1} = \\text{Proj}_\\Theta\\left(\\theta^t - \\eta g^t\\right),\n$$\nwhere $g^t$ is a (sub)gradient of $l$ at $\\theta^t$. By convexity:\n$$\nl(\\theta^t) - l(\\theta^*) \\leq \\langle g^t, \\theta^t - \\theta^* \\rangle \\quad \\forall \\theta^* \\in \\Theta.\n$$\nSumming over $t=1,\\dots,T$:\n$$\n\\sum_{t=1}^T \\left[l(\\theta^t) - l(\\theta^*)\\right] \\leq \\sum_{t=1}^T \\langle g^t, \\theta^t - \\theta^* \\rangle.\n$$\n\n**3. Bounding the Cumulative Gradient Term:**\nUsing the OGD update and the non-expansiveness of projection:\n$$\n\\|\\theta^{t+1} - \\theta^*\\|_2^2 \\leq \\|\\theta^t - \\theta^*\\|_2^2 - 2\\eta \\langle g^t, \\theta^t - \\theta^* \\rangle + \\eta^2 \\|g^t\\|_2^2.\n$$\nRearranging and summing over $t$:\n$$\n\\sum_{t=1}^T \\langle g^t, \\theta^t - \\theta^* \\rangle \\leq \\frac{\\|\\theta^1 - \\theta^*\\|_2^2}{2\\eta} + \\frac{\\eta}{2} \\sum_{t=1}^T \\|g^t\\|_2^2.\n$$\nGiven $\\|\\theta^1 - \\theta^*\\|_2 \\leq 2B_\\Theta$ (diameter of $\\Theta$) and $\\|g^t\\|_2 \\leq B_\\nabla$:\n$$\n\\sum_{t=1}^T \\langle g^t, \\theta^t - \\theta^* \\rangle \\leq \\frac{2B_\\Theta^2}{\\eta} + \\frac{\\eta T B_\\nabla^2}{2}.\n$$\n\n**4. Optimizing the Step Size:**\nChoose $\\eta = \\frac{2B_\\Theta}{B_\\nabla \\sqrt{T}}$ to minimize the bound. Substituting:\n$$\n\\sum_{t=1}^T \\left[l(\\theta^t) - l(\\theta^*)\\right] \\leq 2B_\\Theta B_\\nabla \\sqrt{T}.\n$$\nDividing by $T$ for the average iterate:\n$$\n\\mathbb{E}[\\rdpol(\\pi_{\\avtheta})] - \\rdpol(\\pi_{\\theta^*}) \\leq \\frac{2B_\\Theta B_\\nabla}{\\sqrt{T}} = \\mathcal{O}\\left(T^{-1/2}\\right).\n$$\n\n**5. Conclusion:**\nThe average iterate $\\pi_{\\avtheta}$ achieves the optimal $\\mathcal{O}(T^{-1/2})$ convergence rate under the given assumptions, matching the lower bound for convex stochastic optimization.", "B": "We begin by considering the saddle-point stochastic optimization problem: $\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\E[F_{g}(\\theta,\\xi)]\\Big\\}$. Here, $\\xi:=(x_g,y,y')\\sim \\sum_{g=1}^{K}\\frac{N_g}{N}\\train_g$, and we define $F_{g'}(\\theta,(x_g,y,y')):= \\frac{N}{N_g}\\mathbf{1}_{[g = g']}l(\\theta; (x_g, y,y'))$. The expectation $\\E_{\\xi}[F_{g}(\\theta,\\xi)]$ simplifies to $f_{g}(\\theta) = \\E_{\\train_{g}}[l(\\theta; (x_g, y,y'))|g=g']$.\n\nNext, we map the algorithm to the general mirror descent framework. The gradients of the objective $\\phi(\\theta,\\alpha)$ with respect to $\\theta$ and $\\alpha$ are given by $\\partial \\phi(\\theta,\\alpha)=(\\partial_\\theta \\phi(\\theta,\\alpha),-\\partial_\\alpha \\phi(\\theta,\\alpha))$. For a given $\\xi$, the stochastic subgradients for $\\phi(\\theta,\\alpha)$ are $\\partial \\Phi(\\theta,\\alpha,\\xi)=(\\partial_\\theta \\Phi(\\theta,\\alpha,\\xi),-\\partial_\\alpha \\Phi(\\theta,\\alpha,\\xi))$, which simplifies to $\\begin{bmatrix}\\frac{N\\alpha_g}{N_g}\\nabla_{\\theta} l(\\theta;(x_{g},y,y')) \\\\-\\Big(0,\\cdots,\\frac{N}{N_g}l(\\theta;(x_{g},y,y')),\\cdots,0\\Big)\\end{bmatrix}$ for $\\xi=(x_g,y,y')$.\n\nThe updates of $\\theta^t$ and $\\alpha^t$ in the algorithm can be mapped to the general mirror descent update rule: $\\zeta^{t+1}=(\\theta^{t+1},\\alpha^{t+1})=P_{(\\zeta^t)}(\\gamma^t \\partial \\Phi(\\theta^t,\\alpha^t,\\xi))$, where $P_{\\zeta}(\\nu) = \\argmin_{\\zeta' \\in (\\Theta \\times\\Delta_K)} \\left\\{ \\nu^\\top (\\zeta' - \\zeta) + V(\\zeta, \\zeta') \\right\\}$ is the prox-mapping.\n\nTo bound the error, we define the approximation error $\\epsilon_T$ as $\\max_{\\alpha \\in \\Delta_K}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\phi(\\theta,\\alpha)$, which can be bounded using the result from mirror-descent convergence for max-min problems. The error bound is given by $\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10\\frac{R_{\\theta}^2 M_{\\theta}^{2}+\\ln(K) M_{\\alpha}^{2}}{T}}$, where $M_{\\theta}^2 = B_{\\nabla}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$ and $M_{\\alpha}^2 = KB_{l}^2\\frac{N^2}{\\min_{g\\in \\mathcal{G}}N_g}$. The strong convexity parameters $\\lambda_{\\theta}$ and $\\lambda_{\\alpha}$ are both 1, leading to the overall error bound $\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10(\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g})\\frac{B_{\\Theta}^2 B_{\\nabla}^2+KB_{l}^2\\ln K}{T}}$.", "C": "To prove the proposition, we will analyze the convergence rate of the average iterate \\(\\pi_{\\avtheta} = \\tfrac{1}{T}\\sum_{t=1}^T \\theta^t\\) for a convex optimization problem under the given assumptions. The proof follows standard techniques in online convex optimization, particularly using the Online Gradient Descent (OGD) framework.\n\n### **Assumptions Recap:**\n1. **Loss Function:** \\(l(\\cdot; (x_g, y, y'))\\) is non-negative, convex, \\(B_{\\nabla}\\)-Lipschitz continuous, and bounded by \\(B_l\\).\n2. **Parameter Space:** \\(\\Theta \\subset \\R^d\\) is convex, and \\(\\|\\theta\\|_2 \\leq B_{\\Theta}\\) for all \\(\\theta \\in \\Theta\\).\n3. **Objective:** The expected risk \\(\\rdpol(\\pi_\\theta)\\) is convex in \\(\\theta\\).\n\n### **Proof:**\n\n#### **Step 1: Online Gradient Descent Setup**\nConsider the online gradient descent updates:\n\\[\n\\theta^{t+1} = \\text{Proj}_{\\Theta}\\left(\\theta^t - \\eta_t \\nabla l(\\theta^t; (x_g^t, y^t, y'^t))\\right),\n\\]\nwhere \\(\\text{Proj}_{\\Theta}\\) is the Euclidean projection onto \\(\\Theta\\), and \\(\\eta_t\\) is the step size.\n\n#### **Step 2: Regret Bound for OGD**\nUnder convexity and \\(B_{\\nabla}\\)-Lipschitz continuity, the regret of OGD after \\(T\\) steps with a constant step size \\(\\eta = \\frac{B_{\\Theta}}{B_{\\nabla}\\sqrt{T}}\\) is bounded by:\n\\[\n\\sum_{t=1}^T l(\\theta^t; (x_g^t, y^t, y'^t)) - \\min_{\\theta \\in \\Theta} \\sum_{t=1}^T l(\\theta; (x_g^t, y^t, y'^t)) \\leq B_{\\nabla} B_{\\Theta} \\sqrt{T}.\n\\]\nDividing by \\(T\\), we get:\n\\[\n\\frac{1}{T} \\sum_{t=1}^T l(\\theta^t; (x_g^t, y^t, y'^t)) - \\min_{\\theta \\in \\Theta} \\frac{1}{T} \\sum_{t=1}^T l(\\theta; (x_g^t, y^t, y'^t)) \\leq \\frac{B_{\\nabla} B_{\\Theta}}{\\sqrt{T}}.\n\\]\n\n#### **Step 3: Expectation over Data**\nTaking expectation over the data distribution (assuming i.i.d. samples), we have:\n\\[\n\\mathbb{E}\\left[\\frac{1}{T} \\sum_{t=1}^T l(\\theta^t; (x_g^t, y^t, y'^t))\\right] = \\mathbb{E}\\left[\\rdpol(\\pi_{\\avtheta})\\right],\n\\]\nand\n\\[\n\\min_{\\theta \\in \\Theta} \\mathbb{E}\\left[\\frac{1}{T} \\sum_{t=1}^T l(\\theta; (x_g^t, y^t, y'^t))\\right] = \\min_{\\theta \\in \\Theta} \\rdpol(\\pi_\\theta).\n\\]\nThus, the bound becomes:\n\\[\n\\mathbb{E}\\left[\\rdpol(\\pi_{\\avtheta})\\right] - \\min_{\\theta \\in \\Theta} \\rdpol(\\pi_\\theta) \\leq \\frac{B_{\\nabla} B_{\\Theta}}{\\sqrt{T}}.\n\\]\n\n#### **Step 4: Final Convergence Rate**\nThis shows that the expected suboptimality gap decays as \\(\\mathcal{O}(T^{-1/2})\\):\n\\[\n\\mathbb{E}\\left[\\rdpol(\\pi_{\\avtheta})\\right] - \\min_{\\theta \\in \\Theta} \\rdpol(\\pi_\\theta) = \\mathcal{O}\\left(T^{-1/2}\\right).\n\\]\n\n### **Conclusion:**\nUnder the given assumptions, the average iterate \\(\\pi_{\\avtheta}\\) of the online gradient descent algorithm achieves an expected excess risk of order \\(\\mathcal{O}(T^{-1/2})\\), matching the standard convergence rate for convex Lipschitz problems in stochastic optimization. \\(\\square\\)", "D": "We begin by considering the saddle-point stochastic optimization problem: $\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\E[F_{g}(\\theta,\\xi)]\\Big\\}$. Here, $\\xi:=(x_g,y,y')\\sim \\sum_{g=1}^{K}\\frac{N_g}{N}\\train_g$, and we define $F_{g'}(\\theta,(x_g,y,y')):= \\frac{N}{N_g}\\mathbf{1}_{[g = g']}l(\\theta; (x_g, y,y'))$. The expectation $\\E_{\\xi}[F_{g}(\\theta,\\xi)]$ simplifies to $f_{g}(\\theta) = \\E_{\\train_{g}}[l(\\theta; (x_g, y,y'))|g=g']$.\n\nNext, we map the algorithm to the general mirror descent framework. The gradients of the objective $\\phi(\\theta,\\alpha)$ with respect to $\\theta$ and $\\alpha$ are given by $\\partial \\phi(\\theta,\\alpha)=(\\partial_\\theta \\phi(\\theta,\\alpha),-\\partial_\\alpha \\phi(\\theta,\\alpha))$. For a given $\\xi$, the stochastic subgradients for $\\phi(\\theta,\\alpha)$ are $\\partial \\Phi(\\theta,\\alpha,\\xi)=(\\partial_\\theta \\Phi(\\theta,\\alpha,\\xi),-\\partial_\\alpha \\Phi(\\theta,\\alpha,\\xi))$, which simplifies to $\\begin{bmatrix}\\frac{N\\alpha_g}{N_g}\\nabla_{\\theta} l(\\theta;(x_{g},y,y')) \\\\-\\Big(0,\\cdots,\\frac{N}{N_g}l(\\theta;(x_{g},y,y')),\\cdots,0\\Big)\\end{bmatrix}$ for $\\xi=(x_g,y,y')$.\n\nThe updates of $\\theta^t$ and $\\alpha^t$ in the algorithm can be mapped to the general mirror descent update rule: $\\zeta^{t+1}=(\\theta^{t+1},\\alpha^{t+1})=P_{(\\zeta^t)}(\\gamma^t \\partial \\Phi(\\theta^t,\\alpha^t,\\xi))$, where $P_{\\zeta}(\\nu) = \\argmin_{\\zeta' \\in (\\Theta \\times\\Delta_K)} \\left\\{ \\nu^\\top (\\zeta' - \\zeta) + V(\\zeta, \\zeta') \\right\\}$ is the prox-mapping.\n\nTo bound the error, we define the approximation error $\\epsilon_T$ as $\\max_{\\alpha \\in \\Delta_K}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\min_{\\theta\\in\\Theta}\\max_{\\alpha\\in\\Delta_K}\\phi(\\theta,\\alpha)$, which can be bounded using the result from mirror-descent convergence for max-min problems. The error bound is given by $\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10\\frac{R_{\\theta}^2 M_{\\theta}^{2}+\\ln(K) M_{\\alpha}^{2}}{T}}$, where $M_{\\theta}^2 = B_{\\nabla}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$ and $M_{\\alpha}^2 = KB_{l}^2\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g}$. The strong convexity parameters $\\lambda_{\\theta}$ and $\\lambda_{\\alpha}$ are both 1, leading to the overall error bound $\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T}) \\leq 2\\sqrt{10(\\frac{N}{\\min_{g\\in \\mathcal{G}}N_g})\\frac{B_{\\Theta}^2 B_{\\nabla}^2+KB_{l}^2\\ln K}{T}}$.", "ground-truth": "D"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}\nAssume that $p_t(\\bm{x})>0$ for all $\\bm{x}\\in\\Omega^o$ and $t\\in[0,1]$. Then $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$.\n\\end{theorem}", "A": "To prove the theorem, we analyze the gradients of the RFM and CRFM objectives under the given conditions. Let $\\mathcal{L}_{\\text{RFM}}(\\bm{\\theta})$ and $\\mathcal{L}_{\\text{CRFM}}(\\bm{\\theta})$ be defined as path integrals over $\\Omega^o \\times [0,1]$. The key steps are as follows:\n\n1. **Express the Gradient Difference**:\n   The gradients differ by an integral involving the divergence of a vector field $\\bm{F}(\\bm{\\theta},\\bm{x},t)$:\n   \\[\n   \\nabla \\mathcal{L}_{\\text{CRFM}}(\\bm{\\theta}) - \\nabla \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}) = \\int_0^1 \\int_{\\Omega^o} \\nabla_{\\bm{\\theta}} \\left[ \\nabla_{\\bm{x}} \\cdot \\big(p_t(\\bm{x}) \\bm{F}(\\bm{\\theta},\\bm{x},t)\\big) \\right] d\\bm{x} dt\n   \\]\n   This arises from the constraint structure in CRFM.\n\n2. **Apply Integration by Parts**:\n   Use the divergence theorem to convert the volume integral to a boundary integral:\n   \\[\n   \\int_{\\Omega^o} \\nabla_{\\bm{x}} \\cdot \\big(p_t(\\bm{x}) \\bm{F}\\big) d\\bm{x} = \\int_{\\partial \\Omega} p_t(\\bm{x}) \\bm{F} \\cdot \\bm{n} dS\n   \\]\n   where $\\bm{n}$ is the outward normal vector on $\\partial\\Omega$.\n\n3. **Boundary Term Vanishes**:\n   By assumption, $p_t(\\bm{x}) > 0$ in $\\Omega^o$ but $p_t(\\bm{x})$ naturally decays to zero on $\\partial\\Omega$ (common in probability densities to ensure integrability). Hence:\n   \\[\n   \\int_{\\partial \\Omega} p_t(\\bm{x}) \\bm{F} \\cdot \\bm{n} dS = 0 \\quad \\forall t \\in [0,1].\n   \\]\n\n4. **Conclude Equality**:\n   The remaining terms in $\\nabla \\mathcal{L}_{\\text{CRFM}}$ and $\\nabla \\mathcal{L}_{\\text{RFM}}$ are identical after the boundary cancellation. Thus:\n   \\[\n   \\nabla \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\text{CRFM}}(\\bm{\\theta}).\n   \\]\n\nThis completes the proof under the given positivity condition on $p_t(\\bm{x})$.", "B": "To prove the theorem, we start by expanding the squared norms of the differences between the velocity fields:\n\\begin{subequations}\n\\begin{align}\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x})\\|^2&= \\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})+\\|\\bm{v}_t(\\bm{x})\\|^2\\\\\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2&=\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)+\\|\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2\n\\end{align}\n\\end{subequations}\n\nNext, we observe that the expectation of the squared norm of $\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)$ under the joint distribution $p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)$ is equal to its expectation under the marginal distribution $p_t(\\bm{x})$:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2=\\mathbb{E}_{p_t(\\bm{x})}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2\n\\end{equation}\n\nTo complete the proof, we need to show that the expectations of the dot products are equal:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}).\n\\end{equation}\n\nBy the definition of $\\bm{v}_t(\\bm{x})$, we can express the expectation under $p_t(\\bm{x})$ as:\n\\begin{align*}\n\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})\\dif\\bm{x}\\\\\\n&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\int_\\Omega \\bm{v}_t(\\bm{x}|\\bm{x}_1)\\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\int_\\Omega\\int_\\Omega  \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\bm{v}_t(\\bm{x}|\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1).\n\\end{align*}\nThis completes the proof, showing that $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$ under the given assumptions.", "C": "**Proof:**\n\nWe aim to show that under the given conditions, the gradients of the Risk Functional Minimization (RFM) loss \\(\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\\) and the Conditional Risk Functional Minimization (CRFM) loss \\(\\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})\\) are equal. \n\n### Definitions:\n1. **RFM Loss:**\n   \\[\n   \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\right],\n   \\]\n   where \\(\\ell(\\bm{x}; \\bm{\\theta})\\) is the loss function for input \\(\\bm{x}\\) with parameters \\(\\bm{\\theta}\\), and \\(p_t(\\bm{x})\\) is the probability density over \\(\\Omega^o\\) at time \\(t\\).\n\n2. **CRFM Loss:**\n   \\[\n   \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}) = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\mid \\bm{x} \\in \\Omega^o \\right],\n   \\]\n   where the expectation is conditioned on \\(\\bm{x}\\) being in the interior \\(\\Omega^o\\).\n\n### Gradient of RFM Loss:\n\\[\n\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\right] = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\nabla \\ell(\\bm{x}; \\bm{\\theta}) \\right].\n\\]\n\n### Gradient of CRFM Loss:\nSince \\(p_t(\\bm{x}) > 0\\) for all \\(\\bm{x} \\in \\Omega^o\\) and \\(t \\in [0,1]\\), the conditional distribution \\(p_t(\\bm{x} \\mid \\bm{x} \\in \\Omega^o)\\) is identical to \\(p_t(\\bm{x})\\) because \\(\\Omega^o\\) is the full support of \\(p_t(\\bm{x})\\). Thus:\n\\[\n\\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}) = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\mid \\bm{x} \\in \\Omega^o \\right] = \\mathbb{E}_{\\bm{x} \\sim p_t(\\bm{x})} \\left[ \\ell(\\bm{x}; \\bm{\\theta}) \\right] = \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nTaking the gradient:\n\\[\n\\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\n\n### Conclusion:\nSince the expectations (and thus the losses) are identical under the given conditions, their gradients must also be equal:\n\\[\n\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta}). \\quad \\square\n\\]\n\n**Remark:** The key assumption \\(p_t(\\bm{x}) > 0\\) for all \\(\\bm{x} \\in \\Omega^o\\) ensures that the conditional distribution does not alter the expectation, making the two losses equivalent. If \\(p_t(\\bm{x})\\) had zero mass in some region of \\(\\Omega^o\\), the conditional expectation would differ, and the gradients might not be equal.", "D": "To prove the theorem, we start by expanding the squared norms of the differences between the velocity fields:\n\\begin{subequations}\n\\begin{align}\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x})\\|^2&= \\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})+\\|\\bm{v}_t(\\bm{x})\\|^2\\\\\n\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)-\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2&=\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2-2\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)+\\|\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\|^2\n\\end{align}\n\\end{subequations}\n\nNext, we observe that the expectation of the squared norm of $\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)$ under the joint distribution $p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)$ is equal to its expectation under the marginal distribution $p_t(\\bm{x})$:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2=\\mathbb{E}_{p_t(\\bm{x})}\\|\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2\n\\end{equation}\n\nTo complete the proof, we need to show that the expectations of the dot products are equal:\n\\begin{equation}\n\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}).\n\\end{equation}\n\nBy the definition of $\\bm{v}_t(\\bm{x})$, we can express the expectation under $p_t(\\bm{x})$ as:\n\\begin{align*}\n\\mathbb{E}_{p_t(\\bm{x})}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x})\\dif\\bm{x}\\\\\\n&=\\int_\\Omega p_t(\\bm{x})\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\int_\\Omega \\bm{v}_t(\\bm{x}|\\bm{x}_1)\\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\int_\\Omega\\int_\\Omega  \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot \\bm{v}_t(\\bm{x}|\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1\\dif\\bm{x}\\\\\n&=\\mathbb{E}_{p_{\\mathrm{data}}(\\bm{x}_1)p_t(\\bm{x}|\\bm{x}_1)}\\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\cdot\\bm{v}_t(\\bm{x}|\\bm{x}_1).\n\\end{align*}\nThis completes the proof, showing that $\\nabla \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\nabla \\mathcal{L}_{\\textrm{CRFM}}(\\bm{\\theta})$ under the given assumptions.", "ground-truth": "D"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}\nLet $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\bm{x})$ be a standard Gaussian distribution. Assume the OT conditional velocity field for FM, i.e., $\\bm{\\phi}_{t}(\\bm{x}|\\bm{x}_1)=t\\bm{x}_1+(1-(1-\\sigma_{\\min})t)\\bm{x}$.\nThen\n\\begin{equation}\n\\bm{v}_t(\\bm{x})=\\frac{1}{t}\\bm{x}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x}).\n\\end{equation}\n\\end{theorem}", "A": "Let $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\cdot)$ be the standard Gaussian distribution.\nUnder the OT conditional flow, the conditional velocity field is $\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}$, and the conditional probability path is $p_{t}(\\bm{x}|\\bm{x}_1)=\\mathcal{N}(t\\bm{x}_1,(1-(1-\\sigma_{\\min})t)^2\\bm{I})$.\nDefine\n\\[\n\\bar{p}_t(\\bm{x}_1|\\bm{x}) = \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}.\n\\]\nBy the definition of $\\bm{v}_t(\\bm{x})$,\nwe have\n\\begin{align*}\n\\bm{v}_t(\\bm{x})&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\\\\n&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}\\\\\n&=\\frac{\\bm{x}}{t}-\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}-t\\bm{x}_1}{(1-(1-\\sigma_{\\min})t)^2}\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\nabla\\log p_{t}(\\bm{x}|\\bm{x}_1)\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x})\n\\end{align*}\nwhere we use the fact\n\\[\n\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})} \\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}|\\bm{x}_1)=\\int \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_{t}(\\bm{x})}\\frac{\\nabla_{\\bm{x}} p_{t}(\\bm{x}|\\bm{x}_1)}{p_{t}(\\bm{x}|\\bm{x}_1)}\\dif \\bm{x}_1=\\frac{1}{p_{t}(\\bm{x})}\\nabla_{\\bm{x}} \\int p_{t}(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1=\\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}).\n\\]\nThis finishes the proof.", "B": "To prove the given theorem, we will derive the velocity field \\(\\bm{v}_t(\\bm{x})\\) from the given OT conditional velocity field \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) and the prior distribution \\(p_0(\\bm{x})\\). Here is the step-by-step derivation:\n\n### Given:\n1. The prior distribution is a standard Gaussian:\n   \\[\n   p_0(\\bm{x}) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{\\|\\bm{x}\\|^2}{2}\\right).\n   \\]\n2. The OT conditional velocity field is:\n   \\[\n   \\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t)\\bm{x}.\n   \\]\n3. The marginal distribution \\(p_t(\\bm{x})\\) is obtained by pushing \\(p_0(\\bm{x})\\) through the flow defined by \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\).\n\n### Step 1: Express the marginal distribution \\(p_t(\\bm{x})\\)\nThe flow \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) defines a deterministic transformation from \\(\\bm{x}_0\\) to \\(\\bm{x}_t\\):\n\\[\n\\bm{x}_t = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t)\\bm{x}_0.\n\\]\nSince \\(\\bm{x}_1\\) is fixed, the transformation is linear in \\(\\bm{x}_0\\). The inverse transformation is:\n\\[\n\\bm{x}_0 = \\frac{\\bm{x}_t - t\\bm{x}_1}{1 - (1 - \\sigma_{\\min})t}.\n\\]\nThe Jacobian determinant of this transformation is:\n\\[\nJ = \\left(\\frac{1}{1 - (1 - \\sigma_{\\min})t}\\right)^d.\n\\]\nThus, the marginal distribution \\(p_t(\\bm{x})\\) is:\n\\[\np_t(\\bm{x}) = p_0\\left(\\frac{\\bm{x} - t\\bm{x}_1}{1 - (1 - \\sigma_{\\min})t}\\right) \\cdot J.\n\\]\nSubstituting \\(p_0(\\bm{x})\\):\n\\[\np_t(\\bm{x}) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{\\|\\bm{x} - t\\bm{x}_1\\|^2}{2(1 - (1 - \\sigma_{\\min})t)^2}\\right) \\cdot \\left(\\frac{1}{1 - (1 - \\sigma_{\\min})t}\\right)^d.\n\\]\n\n### Step 2: Compute \\(\\nabla \\log p_t(\\bm{x})\\)\nTaking the logarithm of \\(p_t(\\bm{x})\\):\n\\[\n\\log p_t(\\bm{x}) = -\\frac{\\|\\bm{x} - t\\bm{x}_1\\|^2}{2(1 - (1 - \\sigma_{\\min})t)^2} - \\frac{d}{2} \\log(2\\pi) + d \\log\\left(\\frac{1}{1 - (1 - \\sigma_{\\min})t}\\right).\n\\]\nThe gradient is:\n\\[\n\\nabla \\log p_t(\\bm{x}) = -\\frac{\\bm{x} - t\\bm{x}_1}{(1 - (1 - \\sigma_{\\min})t)^2}.\n\\]\n\n### Step 3: Relate \\(\\bm{v}_t(\\bm{x})\\) to \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\)\nThe velocity field \\(\\bm{v}_t(\\bm{x})\\) is the conditional expectation of \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) given \\(\\bm{x}_t = \\bm{x}\\):\n\\[\n\\bm{v}_t(\\bm{x}) = \\mathbb{E}[\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) | \\bm{x}_t = \\bm{x}].\n\\]\nFrom the definition of \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\):\n\\[\n\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t)\\bm{x}_0.\n\\]\nSubstitute \\(\\bm{x}_0\\) from the inverse transformation:\n\\[\n\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + (1 - (1 - \\sigma_{\\min})t) \\cdot \\frac{\\bm{x} - t\\bm{x}_1}{1 - (1 - \\sigma_{\\min})t} = t\\bm{x}_1 + \\bm{x} - t\\bm{x}_1 = \\bm{x}.\n\\]\nThis suggests that \\(\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\) is deterministic given \\(\\bm{x}_t = \\bm{x}\\), so:\n\\[\n\\bm{v}_t(\\bm{x}) = \\bm{x}.\n\\]\nHowever, this seems inconsistent with the given formula. Let us instead derive \\(\\bm{v}_t(\\bm{x})\\) from the continuity equation.\n\n### Step 4: Derive \\(\\bm{v}_t(\\bm{x})\\) from the continuity equation\nThe continuity equation for the marginal distribution is:\n\\[\n\\frac{\\partial p_t(\\bm{x})}{\\partial t} + \\nabla \\cdot (p_t(\\bm{x}) \\bm{v}_t(\\bm{x})) = 0.\n\\]\nFrom Step 1, we can compute \\(\\frac{\\partial p_t(\\bm{x})}{\\partial t}\\):\n\\[\n\\frac{\\partial p_t(\\bm{x})}{\\partial t} = p_t(\\bm{x}) \\cdot \\left(\\frac{\\|\\bm{x} - t\\bm{x}_1\\|^2 (1 - \\sigma_{\\min})}{(1 - (1 - \\sigma_{\\min})t)^3} - \\frac{(\\bm{x} - t\\bm{x}_1) \\cdot \\bm{x}_1}{(1 - (1 - \\sigma_{\\min})t)^2} + \\frac{d(1 - \\sigma_{\\min})}{1 - (1 - \\sigma_{\\min})t}\\right).\n\\]\nThe divergence term is:\n\\[\n\\nabla \\cdot (p_t(\\bm{x}) \\bm{v}_t(\\bm{x})) = \\nabla p_t(\\bm{x}) \\cdot \\bm{v}_t(\\bm{x}) + p_t(\\bm{x}) \\nabla \\cdot \\bm{v}_t(\\bm{x}).\n\\]\nAssuming \\(\\bm{v}_t(\\bm{x})\\) is proportional to \\(\\nabla \\log p_t(\\bm{x})\\), we can match terms to find:\n\\[\n\\bm{v}_t(\\bm{x}) = \\frac{1}{t}\\bm{x} + \\frac{1 - (1 - \\sigma_{\\min})t}{t} \\nabla \\log p_t(\\bm{x}).\n\\]\n\n### Final Result:\nCombining the above steps, we have derived the velocity field as:\n\\[\n\\bm{v}_t(\\bm{x}) = \\frac{1}{t}\\bm{x} + \\frac{1 - (1 - \\sigma_{\\min})t}{t} \\nabla \\log p_t(\\bm{x}).\n\\]\nThis completes the proof. \\(\\square\\)", "C": "To prove the theorem, we leverage the relationship between the conditional velocity field and the marginal probability flow in Flow Matching (FM). Given the conditional velocity field:\n\\[\n\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = t\\bm{x}_1 + \\big[1 - (1-\\sigma_{\\min})t\\big]\\bm{x},\n\\]\nwe derive the marginal velocity field \\(\\bm{v}_t(\\bm{x})\\) by taking the expectation over the conditional distribution \\(p_t(\\bm{x}_1|\\bm{x})\\).\n\n### Key Steps:\n1. **Express the conditional velocity**:  \n   The conditional velocity field can be rewritten as:\n   \\[\n   \\bm{\\phi}_t(\\bm{x}|\\bm{x}_1) = \\frac{\\bm{x}_1 - (1-\\sigma_{\\min})\\bm{x}}{1 - (1-\\sigma_{\\min})t}.\n   \\]\n\n2. **Marginal velocity via expectation**:  \n   The marginal velocity field is the conditional expectation:\n   \\[\n   \\bm{v}_t(\\bm{x}) = \\mathbb{E}_{p_t(\\bm{x}_1|\\bm{x})}\\big[\\bm{\\phi}_t(\\bm{x}|\\bm{x}_1)\\big] = \\frac{\\mathbb{E}[\\bm{x}_1|\\bm{x}] - (1-\\sigma_{\\min})\\bm{x}}{1 - (1-\\sigma_{\\min})t}.\n   \\]\n\n3. **Relate \\(\\mathbb{E}[\\bm{x}_1|\\bm{x}]\\) to the score function**:  \n   From the structure of the conditional Gaussian path \\(p_t(\\bm{x}|\\bm{x}_1) = \\mathcal{N}\\big(t\\bm{x}_1, [1 - (1-\\sigma_{\\min})t]^2\\bm{I}\\big)\\), the score function is:\n   \\[\n   \\nabla\\log p_t(\\bm{x}) = \\frac{\\mathbb{E}[\\bm{x}_1|\\bm{x}] - t\\bm{x}}{[1 - (1-\\sigma_{\\min})t]^2}.\n   \\]\n   Solving for \\(\\mathbb{E}[\\bm{x}_1|\\bm{x}]\\) gives:\n   \\[\n   \\mathbb{E}[\\bm{x}_1|\\bm{x}] = t\\bm{x} + [1 - (1-\\sigma_{\\min})t]^2\\nabla\\log p_t(\\bm{x}).\n   \\]\n\n4. **Substitute into \\(\\bm{v}_t(\\bm{x})\\)**:  \n   Plugging \\(\\mathbb{E}[\\bm{x}_1|\\bm{x}]\\) back:\n   \\[\n   \\bm{v}_t(\\bm{x}) = \\frac{1}{t}\\bm{x} + \\frac{1 - (1-\\sigma_{\\min})t}{t}\\nabla\\log p_t(\\bm{x}).\n   \\]\n\n### Conclusion:\nThe derived velocity field \\(\\bm{v}_t(\\bm{x})\\) explicitly links the deterministic drift and the score function of the marginal distribution, thereby proving the theorem.", "D": "Let $\\Omega=\\mathbb{R}^d$ and the prior distribution $p_0(\\cdot)$ be the standard Gaussian distribution.\nUnder the OT conditional flow, the conditional velocity field is $\\bm{v}_t(\\bm{x}|\\bm{x}_1)=\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}$, and the conditional probability path is $p_{t}(\\bm{x}|\\bm{x}_1)=\\mathcal{N}(t\\bm{x}_1,(1-(1-\\sigma_{\\min})t)^2\\bm{I})$.\nDefine\n\\[\n\\bar{p}_t(\\bm{x}_1|\\bm{x}) = \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_t(\\bm{x})}.\n\\]\nBy the definition of $\\bm{v}_t(\\bm{x})$,\nwe have\n\\begin{align*}\n\\bm{v}_t(\\bm{x})&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\bm{v}_t(\\bm{x}|\\bm{x}_1)\\\\\n&=\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}_1-(1-\\sigma_{\\min})\\bm{x}}{1-(1-\\sigma_{\\min})t}\\\\\n&=\\frac{\\bm{x}}{t}-\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\frac{\\bm{x}-t\\bm{x}_1}{(1-(1-\\sigma_{\\min})t)^2}\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})}\\nabla\\log p_{t}(\\bm{x}|\\bm{x}_1)\\\\\n&=\\frac{\\bm{x}}{t}+\\frac{1-(1-\\sigma_{\\min})t}{t}\\nabla\\log p_{t}(\\bm{x})\n\\end{align*}\nwhere we use the fact\n\\[\n\\mathbb{E}_{\\bar{p}_t(\\bm{x}_1|\\bm{x})} \\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}|\\bm{x}_1)=\\int \\frac{p_t(\\bm{x}|\\bm{x}_1)p_{\\textrm{data}}(\\bm{x}_1)}{p_{t}(\\bm{x})}\\frac{\\nabla_{\\bm{x}} p_{t}(\\bm{x}|\\bm{x}_1)}{p_{t}(\\bm{x}|\\bm{x}_1)}\\dif \\bm{x}_1=\\frac{1}{p_{t}(\\bm{x})}\\nabla_{\\bm{x}} \\int p_{t}(\\bm{x}|\\bm{x}_1)p_{\\mathrm{data}}(\\bm{x}_1)\\dif \\bm{x}_1=\\nabla_{\\bm{x}}\\log p_{t}(\\bm{x}).\n\\]\nThis finishes the proof.", "ground-truth": "D"}
{"paper": "arxiv:2402.01306", "question": "Prove the following theorem:\n\\begin{theorem}\n    DPO and PPO-Clip are human-aware losses.\n\\end{theorem}. It is required to prove that DPO and PPO-Clip satisfy the definition of HALO, which means they can be expressed in the following form:\n\\[f(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}[a_{x,y}v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])] + C_D\\]\n\nThe DPO loss function is:\n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\left(\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}\\right) - \\beta \\log\\left(\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right]\\]\n\nThe PPO-Clip loss function is:\n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere \\[q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})}\\]", "A": "\\begin{proof}\nFor a loss $f$ to be a HALO, we need to first construct the human value\n\\[v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])\\]\nwhere $r_\\theta(x,y) = l(x,y) \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$ is the implied reward (normalized by factor $l(y)$), $Q(Y'|x)$ is an input-conditioned reference point distribution, and $v: \\mathbb{R} \\to \\mathbb{R}$ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in $(0, \\infty)$.\n\nThe DPO loss is\n\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x, y_w, y_l} \\left[  -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]\\]\nwhere $\\beta > 0$ is a hyperparameter.\nDPO meets the criteria with the following construction: $l(y) = \\beta$; $r_\\theta = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$; $v(\\cdot) = \\log \\sigma(\\cdot)$ is increasing and concave everywhere; $Q$ places all mass on $(x, y_l)$, where $y_l$ is a dispreferred output for $x$ such that $y \\succ y_l$; and $a_{x,y} = -1$.\n\nThe PPO-Clip loss is\n\\[\\mathcal{L}_{\\text{PPO (offline)}} = -\\mathbb{E}_{x,y,t \\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta, 1 - \\epsilon, 1 + \\epsilon) A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere $q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{ref}}(y_t|x:y_{<t})}$ are the token-level probability ratios (where $y_{< t}$ denotes the output sequence up to the $t$-th token), $A$ denotes the token-level advantages, and $\\epsilon \\in (0,1)$ is a hyperparameter.\n\nSince this is a token-level objective, let $x:y_{<t}$ denote the actual input and the token $y_i$ the actual output for the purpose of framing this as a HALO.\nThe advantage function $A(x:y_{<t}, y_t)$ can be expressed as $Q^\\pi(x:y_{<t}, y_t) - V^\\pi(x:y_{<t})$, the difference between the action-value and value functions.\nBecause $V^\\pi(x:y_{<t}) = \\mathbb{E}_{y \\sim \\pi}Q^\\pi(x:y_{<t}, y)$, the reference point distribution is simply the policy.\n\nThe HALO-defined reward $r_\\theta$ is then implied by the product $q_\\theta Q^\\pi(x:y_{<t}, y)$.\nAssume without loss of generality that $Q^\\pi$ is non-negative, since a constant can be added to $Q^\\pi$ without changing the advantage.\nThen means $\\exists u \\geq 1, q_\\theta Q^\\pi(x:y_{<t}, y) = \\log u = \\log \\hat{\\pi}_\\theta(x:y_{<t}, y) / \\hat{\\pi}_{\\text{ref}}(x:y_{<t}, y)$, where $\\hat{\\pi}_\\theta, \\hat{\\pi}_{\\text{ref}}$ are some implied policy and reference distributions.\nIt is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise.\nWhere $q_\\theta A = r_\\theta - z_0$ in the HALO notation:\n\\[v(q_\\theta A) = \\begin{cases} A \\min(q_\\theta, 1 + \\epsilon)& \\text{if } A(x:y_{<t}, y_t) \\geq 0 \\\\\nA \\max(q_\\theta, 1 - \\epsilon)& \\text{if } A(x:y_{<t}, y_t) < 0 \\\\\n\\end{cases}\\]\nwhich we can combine as $v(q_\\theta A) = \\min(q_\\theta A, A(1 + \\text{sign}(q_\\theta A) \\epsilon))$.\n$a_{x,y} = -1$ completes the construction.\n\\end{proof}", "B": "\\begin{proof}\nFor a loss $f$ to be a HALO, we need to first construct the human value\n\\[v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])\\]\nwhere $r_\\theta(x,y) = l(x,y) \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$ is the implied reward (normalized by factor $l(y)$), $Q(Y'|x)$ is an input-conditioned reference point distribution, and $v: \\mathbb{R} \\to \\mathbb{R}$ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in $(0, \\infty)$.\n\nThe DPO loss is\n\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x, y_w, y_l} \\left[  -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]\\]\nwhere $\\beta > 0$ is a hyperparameter.\nDPO meets the criteria with the following construction: $l(y) = \\beta$; $r_\\theta = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$; $v(\\cdot) = \\log \\sigma(\\cdot)$ is increasing and concave everywhere; $Q$ places all mass on $(x, y_l)$, where $y_l$ is a dispreferred output for $x$ such that $y \\succ y_l$; and $a_{x,y} = -1$.\n\nThe PPO-Clip loss is\n\\[\\mathcal{L}_{\\text{PPO (offline)}} = -\\mathbb{E}_{x,y,t \\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta, 1 - \\epsilon, 1 + \\epsilon) A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere $q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{ref}}(y_t|x:y_{<t})}$ are the token-level probability ratios (where $y_{< t}$ denotes the output sequence up to the $t$-th token), $A$ denotes the token-level advantages, and $\\epsilon \\in (0,1)$ is a hyperparameter.\n\nSince this is a token-level objective, let $x:y_{<t}$ denote the actual input and the token $y_i$ the actual output for the purpose of framing this as a HALO.\nThe advantage function $A(x:y_{<t}, y_t)$ can be expressed as $Q^\\pi(x:y_{<t}, y_t) - V^\\pi(x:y_{<t})$, the difference between the action-value and value functions.\nBecause $V^\\pi(x:y_{<t}) = \\mathbb{E}_{y \\sim \\pi}Q^\\pi(x:y_{<t}, y)$, the reference point distribution is simply the policy.\n\nThe HALO-defined reward $r_\\theta$ is then implied by the product $q_\\theta Q^\\pi(x:y_{<t}, y)$.\nAssume without loss of generality that $Q^\\pi$ is non-negative, since a constant can be added to $Q^\\pi$ without changing the advantage.\nThen means $\\exists u \\geq 1, q_\\theta Q^\\pi(x:y_{<t}, y) = \\log u = \\log \\hat{\\pi}_\\theta(x:y_{<t}, y) / \\hat{\\pi}_{\\text{ref}}(x:y_{<t}, y)$, where $\\hat{\\pi}_\\theta, \\hat{\\pi}_{\\text{ref}}$ are some implied policy and reference distributions.\nIt is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise.\nWhere $q_\\theta A = r_\\theta - z_0$ in the HALO notation:\n\\[v(q_\\theta A) = \\begin{cases} A \\min(q_\\theta, 1 + \\epsilon)& \\text{if } A(x:y_{<t}, y_t) \\geq 0 \\\\\nA \\max(q_\\theta, 1 - \\epsilon)& \\text{if } A(x:y_{<t}, y_t) < 0 \\\\\n\\end{cases}\\]\nwhich we can combine as $v(q_\\theta A) = \\min(q_\\theta A, A(1 + \\text{sign}(A) \\epsilon))$.\n$a_{x,y} = -1$ completes the construction.\n\\end{proof}", "C": "To prove that DPO and PPO-Clip are human-aware losses (HALO), we need to show that they can be expressed in the form:\n\n\\[\nf(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}\\left[a_{x,y}v\\left(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')]\\right)\\right] + C_D\n\\]\n\nwhere:\n- \\(a_{x,y}\\) is a weighting term,\n- \\(v\\) is a scalar function,\n- \\(r_\\theta(x,y)\\) is a reward or scoring function,\n- \\(Q\\) is a distribution over \\(y'\\) (e.g., reference policy \\(\\pi_{\\text{ref}}\\)),\n- \\(C_D\\) is a constant depending on the dataset \\(D\\).\n\n---\n\n### **1. Proof for DPO**\n\nThe DPO loss is:\n\n\\[\nL_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\left(\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}\\right) - \\beta \\log\\left(\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right]\n\\]\n\nLet \\(r_\\theta(x,y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\\). Then the loss can be rewritten as:\n\n\\[\nL_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(r_\\theta(x,y_w) - r_\\theta(x,y_l)\\right)\\right]\n\\]\n\nThis resembles the HALO form:\n- The term \\(r_\\theta(x,y_w) - r_\\theta(x,y_l)\\) can be seen as \\(r_\\theta(x,y_w) - \\mathbb{E}_{y' \\sim \\pi_{\\text{ref}}}[r_\\theta(x,y')]\\), where the expectation is approximated by a single sample \\(y_l\\) from the dataset.\n- The weighting term \\(a_{x,y} = 1\\) (implicitly, since all pairs are equally weighted).\n- The function \\(v(z) = -\\log \\sigma(z)\\).\n- The constant \\(C_D\\) is absorbed into the expectation (since the dataset is fixed).\n\nThus, DPO fits the HALO framework.\n\n---\n\n### **2. Proof for PPO-Clip**\n\nThe PPO-Clip loss is:\n\n\\[\nL_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\n\\]\n\nwhere:\n\\[\nq_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})}\n\\]\n\nLet \\(r_\\theta(x,y) = \\log \\pi_\\theta(y|x)\\) (the log-probability as a reward). The advantage \\(A(x:y_{<t}, y_t)\\) can be seen as a weighting term \\(a_{x,y}\\). The clipped ratio ensures that updates are conservative, which can be interpreted as a form of human-aware regularization.\n\nRewriting the loss:\n\n\\[\nL_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[A(x:y_{<t}, y_t) \\cdot \\min\\left(q_\\theta, \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)\\right)\\right]\n\\]\n\nThis can be mapped to the HALO form:\n- The term \\(\\min(q_\\theta, \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon))\\) acts as a constraint on the reward difference \\(r_\\theta(x,y) - r_{\\text{old}}(x,y)\\).\n- The weighting term \\(a_{x,y} = A(x:y_{<t}, y_t)\\).\n- The function \\(v(z)\\) is implicitly the clipping operation, ensuring bounded updates.\n- The expectation over \\(y_t\\) is analogous to \\(\\mathbb{E}_Q[r_\\theta(x,y')]\\), where \\(Q\\) is the old policy \\(\\pi_{\\text{old}}\\).\n\nThus, PPO-Clip also fits the HALO framework.\n\n---\n\n### **Conclusion**\nBoth DPO and PPO-Clip can be expressed in the HALO form, where:\n- The reward \\(r_\\theta(x,y)\\) is derived from policy probabilities,\n- The weighting \\(a_{x,y}\\) and function \\(v\\) enforce human-like constraints (e.g., pairwise comparisons in DPO, clipping in PPO),\n- The expectation over \\(y'\\) is approximated by the reference policy or old policy.\n\nTherefore, **DPO and PPO-Clip are human-aware losses**. \\(\\square\\)", "D": "To prove that DPO and PPO-Clip satisfy the HALO definition, we demonstrate their equivalence to the form: \n\\[f(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y\\sim D}\\left[a_{x,y}v\\left(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')]\\right)\\right] + C_D\\]\n\n---\n\n### **Proof for DPO**  \n**DPO Loss**:  \n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l\\sim D}\\left[-\\log \\sigma\\left(\\beta \\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right]\\]\n\n**Key Observations**:  \n1. **Reward Parametrization**:  \n   Define \\( r_\\theta(x,y) = \\beta \\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} \\). This aligns with DPOs implicit reward model.  \n\n2. **Contrastive Structure**:  \n   The term inside \\(\\sigma\\) becomes \\( r_\\theta(x,y_w) - r_\\theta(x,y_l) \\).  \n\n3. **HALO Mapping**:  \n   - Let \\( Q \\) be the empirical distribution of \\( y_l \\) conditioned on \\( x \\), i.e., \\( \\mathbb{E}_Q[r_\\theta(x,y')] = r_\\theta(x,y_l) \\).  \n   - Set \\( a_{x,y} = \\mathbb{I}[y=y_w] \\) (indicator for preferred completions).  \n   - Define \\( v(z) = -\\log \\sigma(z) = \\log(1 + e^{-z}) \\).  \n\n**Rewriting DPO Loss**:  \n\\[L_{\\text{DPO}} = \\mathbb{E}_{x,y_w,y_l}\\left[a_{x,y_w}v\\left(r_\\theta(x,y_w) - \\mathbb{E}_Q[r_\\theta(x,y')]\\right)\\right]\\]\nSince \\( \\mathbb{E}_Q[r_\\theta(x,y')] = r_\\theta(x,y_l) \\), this matches HALOs form with \\( C_D = 0 \\).  \n\n---\n\n### **Proof for PPO-Clip**  \n**PPO-Clip Loss**:  \n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t\\sim D}\\left[\\min\\left(q_\\theta A(x:y_{<t}, y_t), \\text{clip}(q_\\theta,1-\\epsilon,1+\\epsilon)A(x:y_{<t}, y_t)\\right)\\right]\\]\nwhere \\( q_\\theta = \\frac{\\pi_\\theta(y_t|x:y_{<t})}{\\pi_{\\text{old}}(y_t|x:y_{<t})} \\).  \n\n**Key Observations**:  \n1. **Reward Parametrization**:  \n   Define \\( r_\\theta(x,y) = \\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)} \\). Then \\( q_\\theta = e^{r_\\theta(x,y)} \\).  \n\n2. **Advantage as Baseline Shift**:  \n   The advantage \\( A(x:y_{<t}, y_t) \\) typically estimates \\( A = r_\\theta(x,y) - \\mathbb{E}_{y'\\sim \\pi_{\\text{old}}}[r_\\theta(x,y')] \\), where \\( \\mathbb{E}_{Q}[r_\\theta(x,y')] = \\mathbb{E}_{y'\\sim \\pi_{\\text{old}}}[r_\\theta(x,y')] \\).  \n\n3. **HALO Mapping**:  \n   - Let \\( Q = \\pi_{\\text{old}} \\).  \n   - Set \\( a_{x,y} = A(x:y_{<t}, y_t) \\).  \n   - Define \\( v(z) = \\min(e^z, \\text{clip}(e^z,1-\\epsilon,1+\\epsilon)) \\).  \n\n**Rewriting PPO-Clip Loss**:  \n\\[L_{\\text{PPO}} = -\\mathbb{E}_{x,y,t}\\left[a_{x,y} \\cdot v\\left(r_\\theta(x,y) - \\mathbb{E}_Q[r_\\theta(x,y')]\\right)\\right]\\]\nThis matches HALOs form with \\( C_D = 0 \\).  \n\n---\n\n### **Conclusion**  \nBoth DPO and PPO-Clip can be expressed in the HALO framework by:  \n1. Defining \\( r_\\theta(x,y) \\) as a policy-derived reward.  \n2. Setting \\( a_{x,y} \\) and \\( Q \\) to capture human preference or baseline expectations.  \n3. Choosing \\( v(\\cdot) \\) to enforce alignment constraints (contrastive or clipping).  \n\nThus, they satisfy the definition of human-aware losses. \\(\\square\\)", "ground-truth": "A"}
{"paper": "arxiv:2402.01306", "question": "Prove the following proposition:\n\\begin{proposition}\n    As the reward implied by the current policy tends to $\\pm \\infty$, the KTO update of $\\pi_\\theta$ tends to zero.\n\\end{proposition}\n\nThe KTO loss function is:\n\\[ L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}[\\lambda_y - v(x,y)] \\]\nwhere $v(x,y)$ is defined as:\n\\[ v(x,y) = \\begin{cases} \n\\lambda_D \\sigma(\\beta(r_\\theta(x,y) - z_0)) & \\text{if } y \\text{ is desirable given } x \\\\\n\\lambda_U \\sigma(\\beta(z_0 - r_\\theta(x,y))) & \\text{if } y \\text{ is undesirable given } x \n\\end{cases} \\]", "A": "To prove the proposition, we analyze the gradient of the KTO loss function with respect to policy parameters $\\theta$ as the implied reward $r_\\theta(x,y)$ tends to $\\pm\\infty$. The update magnitude depends on $\\nabla_\\theta L_{\\text{KTO}} = \\mathbb{E}[\\nabla_\\theta(\\lambda_y - v(x,y))] = -\\mathbb{E}[\\nabla_\\theta v(x,y)]$. We examine the behavior of $\\nabla_\\theta v(x,y)$ in all cases:\n\n---\n\n### Case 1: $y$ is desirable\nHere, $v(x,y) = \\lambda_D \\sigma(\\beta(r_\\theta - z_0))$.  \nThe gradient is:  \n$$\n\\nabla_\\theta v = \\lambda_D \\beta \\sigma'(\\beta(r_\\theta - z_0)) \\nabla_\\theta r_\\theta\n$$\nwhere $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$.\n\n- **As $r_\\theta \\to +\\infty$**:  \n  $\\sigma(\\beta(r_\\theta - z_0)) \\to 1$ $\\implies \\sigma' \\to 1 \\cdot (1 - 1) = 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n- **As $r_\\theta \\to -\\infty$**:  \n  $\\sigma(\\beta(r_\\theta - z_0)) \\to 0$ $\\implies \\sigma' \\to 0 \\cdot (1 - 0) = 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n---\n\n### Case 2: $y$ is undesirable\nHere, $v(x,y) = \\lambda_U \\sigma(\\beta(z_0 - r_\\theta))$.  \nThe gradient is:  \n$$\n\\nabla_\\theta v = -\\lambda_U \\beta \\sigma'(\\beta(z_0 - r_\\theta)) \\nabla_\\theta r_\\theta\n$$\n\n- **As $r_\\theta \\to +\\infty$**:  \n  $\\sigma(\\beta(z_0 - r_\\theta)) \\to \\sigma(-\\infty) = 0$ $\\implies \\sigma' \\to 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n- **As $r_\\theta \\to -\\infty$**:  \n  $\\sigma(\\beta(z_0 - r_\\theta)) \\to \\sigma(+\\infty) = 1$ $\\implies \\sigma' \\to 1 \\cdot (1 - 1) = 0$.  \n  Thus, $\\nabla_\\theta v \\to 0$.\n\n---\n\n### Conclusion\nIn all cases, as $|r_\\theta| \\to \\infty$, $\\nabla_\\theta v(x,y) \\to 0$. Therefore:  \n$$\n\\nabla_\\theta L_{\\text{KTO}} = -\\mathbb{E}[\\nabla_\\theta v(x,y)] \\to 0\n$$\n\nThis implies that the KTO update of $\\pi_\\theta$ (proportional to $\\nabla_\\theta L_{\\text{KTO}}$) tends to zero when the implied reward diverges to $\\pm\\infty$. $\\blacksquare$", "B": "\\begin{proof}\nWhere $d(y)$ is $-1$ ($+1$) when $y$ is desirable (undesirable), $\\lambda_y$ is $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable (undesirable), and $z = r_\\theta(x,y) - z_0$, the derivative of the KTO loss is\n\\begin{equation}\n\\nabla_\\theta L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}\\left[ d(y) \\lambda_y \\sigma(\\beta z) (1 - \\sigma(\\beta z)) \\beta \\nabla_\\theta \\log \\pi_\\theta(y|x) \\right]\n\\label{eq:kto_derivative}\n\\end{equation}\nNote that we do not backpropagate through the KL term in the KTO loss and $\\beta, \\lambda_y > 0$.\nThis gradient is simple to interpret: if $y$ is desirable, then $d(y)$ is negative and we push up the probability of $\\pi_\\theta(y|x)$ to minimize the loss; if $y$ is undesirable, then $d(y)$ is positive and we push down the probability of $\\pi_\\theta(y|x)$ to minimize the loss.\nAs $r_\\theta$ tends to $\\pm \\infty$, the gradient will tend to zero since either $(1 - \\sigma(\\beta z))$ or $\\sigma(\\beta z)$ will tend to zero.\n\\end{proof}", "C": "To prove the proposition, we need to analyze the behavior of the KTO loss function \\( L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) \\) as the implied reward \\( r_\\theta(x,y) \\) tends to \\( \\pm \\infty \\). Specifically, we will show that the gradient of \\( L_{\\text{KTO}} \\) with respect to \\( \\pi_\\theta \\) tends to zero in these limits, implying that the update to \\( \\pi_\\theta \\) tends to zero.\n\n### Step 1: Analyze \\( v(x,y) \\) as \\( r_\\theta(x,y) \\to \\infty \\)\nConsider the case where \\( y \\) is desirable given \\( x \\). The term \\( v(x,y) \\) is:\n\\[\nv(x,y) = \\lambda_D \\sigma(\\beta(r_\\theta(x,y) - z_0))\n\\]\nwhere \\( \\sigma \\) is the sigmoid function. As \\( r_\\theta(x,y) \\to \\infty \\), \\( \\beta(r_\\theta(x,y) - z_0) \\to \\infty \\), and thus:\n\\[\n\\sigma(\\beta(r_\\theta(x,y) - z_0)) \\to 1\n\\]\nTherefore:\n\\[\nv(x,y) \\to \\lambda_D\n\\]\nSimilarly, for \\( y \\) undesirable given \\( x \\), \\( v(x,y) = \\lambda_U \\sigma(\\beta(z_0 - r_\\theta(x,y))) \\). As \\( r_\\theta(x,y) \\to \\infty \\), \\( \\beta(z_0 - r_\\theta(x,y)) \\to -\\infty \\), and thus:\n\\[\n\\sigma(\\beta(z_0 - r_\\theta(x,y))) \\to 0\n\\]\nTherefore:\n\\[\nv(x,y) \\to 0\n\\]\n\n### Step 2: Analyze \\( v(x,y) \\) as \\( r_\\theta(x,y) \\to -\\infty \\)\nFor \\( y \\) desirable given \\( x \\), as \\( r_\\theta(x,y) \\to -\\infty \\), \\( \\beta(r_\\theta(x,y) - z_0) \\to -\\infty \\), and thus:\n\\[\n\\sigma(\\beta(r_\\theta(x,y) - z_0)) \\to 0\n\\]\nTherefore:\n\\[\nv(x,y) \\to 0\n\\]\nFor \\( y \\) undesirable given \\( x \\), as \\( r_\\theta(x,y) \\to -\\infty \\), \\( \\beta(z_0 - r_\\theta(x,y)) \\to \\infty \\), and thus:\n\\[\n\\sigma(\\beta(z_0 - r_\\theta(x,y))) \\to 1\n\\]\nTherefore:\n\\[\nv(x,y) \\to \\lambda_U\n\\]\n\n### Step 3: Behavior of \\( L_{\\text{KTO}} \\) in the Limits\nThe KTO loss is:\n\\[\nL_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}[\\lambda_y - v(x,y)]\n\\]\nwhere \\( \\lambda_y \\) is a constant (either \\( \\lambda_D \\) or \\( \\lambda_U \\)) depending on whether \\( y \\) is desirable or undesirable. \n\n- As \\( r_\\theta(x,y) \\to \\infty \\):\n  - For desirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_D - \\lambda_D = 0 \\).\n  - For undesirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_U - 0 = \\lambda_U \\).\n  However, the gradient of \\( L_{\\text{KTO}} \\) with respect to \\( \\pi_\\theta \\) depends on the derivative of \\( v(x,y) \\), which tends to zero because \\( \\sigma'(\\cdot) \\to 0 \\) as its input tends to \\( \\pm \\infty \\).\n\n- As \\( r_\\theta(x,y) \\to -\\infty \\):\n  - For desirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_D - 0 = \\lambda_D \\).\n  - For undesirable \\( y \\), \\( \\lambda_y - v(x,y) \\to \\lambda_U - \\lambda_U = 0 \\).\n  Again, the gradient tends to zero because \\( \\sigma'(\\cdot) \\to 0 \\).\n\n### Step 4: Gradient of \\( L_{\\text{KTO}} \\)\nThe gradient of \\( L_{\\text{KTO}} \\) with respect to \\( \\pi_\\theta \\) is:\n\\[\n\\nabla_{\\pi_\\theta} L_{\\text{KTO}} = -\\mathbb{E}_{x,y \\sim D}[\\nabla_{\\pi_\\theta} v(x,y)]\n\\]\nFor desirable \\( y \\):\n\\[\n\\nabla_{\\pi_\\theta} v(x,y) = \\lambda_D \\beta \\sigma'(\\beta(r_\\theta(x,y) - z_0)) \\nabla_{\\pi_\\theta} r_\\theta(x,y)\n\\]\nFor undesirable \\( y \\):\n\\[\n\\nabla_{\\pi_\\theta} v(x,y) = -\\lambda_U \\beta \\sigma'(\\beta(z_0 - r_\\theta(x,y))) \\nabla_{\\pi_\\theta} r_\\theta(x,y)\n\\]\nSince \\( \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\), and \\( \\sigma(z) \\to 0 \\) or \\( 1 \\) as \\( z \\to \\pm \\infty \\), we have \\( \\sigma'(z) \\to 0 \\) in both cases. Thus:\n\\[\n\\nabla_{\\pi_\\theta} v(x,y) \\to 0 \\quad \\text{as} \\quad r_\\theta(x,y) \\to \\pm \\infty\n\\]\nThis implies:\n\\[\n\\nabla_{\\pi_\\theta} L_{\\text{KTO}} \\to 0\n\\]\n\n### Conclusion\nAs the implied reward \\( r_\\theta(x,y) \\) tends to \\( \\pm \\infty \\), the gradient of the KTO loss with respect to \\( \\pi_\\theta \\) tends to zero, meaning the update to \\( \\pi_\\theta \\) tends to zero. This completes the proof.", "D": "\\begin{proof}\nWhere $d(y)$ is $-1$ ($+1$) when $y$ is desirable (undesirable), $\\lambda_y$ is $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable (undesirable), and $z = r_\\theta(x,y) - z_0$, the derivative of the KTO loss is\n\\begin{equation}\n\\nabla_\\theta L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim D}\\left[ d(y) \\lambda_y \\sigma(\\beta z) (1 - \\sigma(\\beta z)) \\beta \\nabla_\\theta \\log \\pi_\\theta(y|x) \\right]\n\\label{eq:kto_derivative}\n\\end{equation}\nNote that we do not backpropagate through the KL term in the KTO loss and $\\beta, \\lambda_y > 0$.\nThis gradient is simple to interpret: if $y$ is desirable, then $d(y)$ is negative and we push up the probability of $\\pi_\\theta(y|x)$ to minimize the loss; if $y$ is undesirable, then $d(y)$ is positive and we push down the probability of $\\pi_\\theta(y|x)$ to minimize the loss.\nAs $r_\\theta$ tends to $\\pm \\infty$, the gradient will tend to zero since either $(1 - \\sigma(\\beta z))$ or $\\sigma(\\beta z)$ will tend to one.\n\\end{proof}", "ground-truth": "B"}
{"paper": "arxiv:2304.02835", "question": "How can we prove Theorem 1: (Traditional Influence Functions) is valid? The theorem relies on (i) the definition $L_{0} = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i})$, (ii) the Hessian matrix $H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i})$, and (iii) the notion of a node unlearning request $\\Delta G = \\{V^{(rm)}, \\emptyset, \\emptyset\\}$. In particular, Theorem 1 states that $\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$. Based on these formulas and definitions, how do we derive and prove that relationship?", "A": "To prove Theorem 1, which states that the parameter difference \\(\\hat{\\theta} - \\theta_{0}\\) is approximately given by \\(\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}\\), we will derive this relationship step-by-step using influence functions. Here's the detailed proof:\n\n---\n\n### **1. Setup and Definitions**\n- **Original Loss Function**:  \n  The loss over the original dataset \\(D_{0}\\) is:\n  \\[\n  L_{0}(\\theta) = \\sum_{z_{i} \\in D_{0}} l(f_{G}(z_{i}), y_{i}),\n  \\]\n  where \\(f_{G}\\) is the model parameterized by \\(\\theta\\), and \\(l\\) is the per-instance loss.\n\n- **Optimal Parameters \\(\\theta_{0}\\):**  \n  The original parameters \\(\\theta_{0}\\) minimize \\(L_{0}(\\theta)\\):\n  \\[\n  \\theta_{0} = \\arg\\min_{\\theta} L_{0}(\\theta).\n  \\]\n  At \\(\\theta = \\theta_{0}\\), the gradient is zero:\n  \\[\n  \\nabla L_{0}(\\theta_{0}) = 0.\n  \\]\n\n- **Hessian Matrix \\(H_{\\theta_{0}}\\):**  \n  The Hessian of \\(L_{0}\\) at \\(\\theta_{0}\\) is:\n  \\[\n  H_{\\theta_{0}} = \\sum_{z_{i} \\in D_{0}} \\frac{\\partial^2}{\\partial \\theta^2} l(f_{G}(z_{i}), y_{i}) \\bigg|_{\\theta = \\theta_{0}}.\n  \\]\n  We assume \\(H_{\\theta_{0}}\\) is positive definite (i.e., the loss is locally convex at \\(\\theta_{0}\\)).\n\n- **Unlearning Request \\(\\Delta G\\):**  \n  The unlearning request removes nodes \\(V^{(rm)}\\) from the graph, leading to a modified dataset \\(D_{0} \\setminus \\Delta G\\). The loss after unlearning is:\n  \\[\n  \\hat{L}(\\theta) = L_{0}(\\theta) - L_{\\Delta G}(\\theta),\n  \\]\n  where \\(L_{\\Delta G}(\\theta) = \\sum_{z_{i} \\in \\Delta G} l(f_{G}(z_{i}), y_{i})\\) is the loss contribution of the removed data.\n\n---\n\n### **2. Taylor Approximation of the Updated Loss**\nAfter unlearning, the new optimal parameters \\(\\hat{\\theta}\\) minimize \\(\\hat{L}(\\theta)\\):\n\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} \\hat{L}(\\theta).\n\\]\nTo approximate \\(\\hat{\\theta}\\), we consider a first-order Taylor expansion of \\(\\nabla \\hat{L}(\\theta)\\) around \\(\\theta_{0}\\):\n\\[\n\\nabla \\hat{L}(\\theta) \\approx \\nabla \\hat{L}(\\theta_{0}) + H_{\\theta_{0}} (\\theta - \\theta_{0}).\n\\]\nAt \\(\\theta = \\hat{\\theta}\\), the gradient is zero:\n\\[\n0 \\approx \\nabla \\hat{L}(\\theta_{0}) + H_{\\theta_{0}} (\\hat{\\theta} - \\theta_{0}).\n\\]\nRearranging gives:\n\\[\n\\hat{\\theta} - \\theta_{0} \\approx -H_{\\theta_{0}}^{-1} \\nabla \\hat{L}(\\theta_{0}).\n\\]\n\n---\n\n### **3. Computing \\(\\nabla \\hat{L}(\\theta_{0})\\)**\nSince \\(\\hat{L}(\\theta) = L_{0}(\\theta) - L_{\\Delta G}(\\theta)\\), its gradient is:\n\\[\n\\nabla \\hat{L}(\\theta_{0}) = \\nabla L_{0}(\\theta_{0}) - \\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\nBut \\(\\nabla L_{0}(\\theta_{0}) = 0\\) (by definition of \\(\\theta_{0}\\)), so:\n\\[\n\\nabla \\hat{L}(\\theta_{0}) = -\\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\nThus:\n\\[\n\\hat{\\theta} - \\theta_{0} \\approx H_{\\theta_{0}}^{-1} \\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\n\n---\n\n### **4. Influence Function Interpretation**\nThe term \\(\\nabla L_{\\Delta G}(\\theta_{0})\\) is the gradient of the loss for the removed data \\(\\Delta G\\):\n\\[\n\\nabla L_{\\Delta G}(\\theta_{0}) = \\sum_{z_{i} \\in \\Delta G} \\frac{\\partial}{\\partial \\theta} l(f_{G}(z_{i}), y_{i}) \\bigg|_{\\theta = \\theta_{0}}.\n\\]\nThis represents the \"influence\" of \\(\\Delta G\\) on the parameters. The Hessian inverse \\(H_{\\theta_{0}}^{-1}\\) scales this influence to approximate the parameter change.\n\n---\n\n### **5. Final Result**\nCombining these steps, we obtain:\n\\[\n\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} \\nabla L_{\\Delta G}(\\theta_{0}).\n\\]\nIf we denote \\(L_{\\Delta G} = \\nabla L_{\\Delta G}(\\theta_{0})\\) (as in the theorem statement), this simplifies to:\n\\[\n\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}.\n\\]\n\n---\n\n### **Key Assumptions:**\n1. The loss \\(L_{0}(\\theta)\\) is twice-differentiable and locally convex near \\(\\theta_{0}\\).\n2. The removed data \\(\\Delta G\\) is small relative to \\(D_{0}\\), so the Taylor approximation holds.\n3. The Hessian \\(H_{\\theta_{0}}\\) is invertible (i.e., the model is not at a saddle point).\n\nThis completes the proof of Theorem 1. The result shows that the parameter change due to unlearning can be approximated using influence functions, where the Hessian captures the local curvature of the loss landscape.", "B": "The validity of Theorem 1, which involves traditional influence functions, can be proven by understanding the relationship between the parameter change and the influence of unlearning a node. The proof leverages the convexity of the loss function and the properties of the Hessian matrix. Here's a step-by-step derivation:\n\n1. Initial Setup: Start with the original loss function $L_0$ and its Hessian $H_{\\theta_0}$ at the parameter $\\theta_0$.\n\n2. Perturbation Introduction: Introduce a small perturbation $\\Delta G$ representing the node unlearning request. This affects the loss function by $L_{\\Delta G}$, the sum of losses over the nodes to be unlearned.\n\n3. Parameter Change Approximation: The change in parameters $\\hat{\\theta} - \\theta_0$ due to the perturbation can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of $L_{\\Delta G}$ with respect to $\\theta_0$.\n\n4. Hessian Inverse Role: The Hessian matrix $H_{\\theta_0}^{-1}$ plays a crucial role in scaling the gradient of the perturbation loss, providing the direction and magnitude of the parameter change.\n\n5. Final Relationship: Combining these elements, the relationship $\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0} \\nabla_{\\theta_0} L_{\\Delta G}$ is derived, showing how the parameter change is influenced by the unlearning request through the inverse Hessian and the gradient of the perturbation loss.\n\nThis derivation assumes the loss function's convexity and differentiability, ensuring the Hessian is positive definite and invertible, which are critical for the approximation's validity.", "C": "The validity of Theorem 1, which involves traditional influence functions, can be proven by understanding the relationship between the parameter change and the influence of unlearning a node. The proof leverages the convexity of the loss function and the properties of the Hessian matrix. Here's a step-by-step derivation:\n\n1. Initial Setup: Start with the original loss function $L_0$ and its Hessian $H_{\\theta_0}$ at the parameter $\\theta_0$.\n\n2. Perturbation Introduction: Introduce a small perturbation $\\Delta G$ representing the node unlearning request. This affects the loss function by $L_{\\Delta G}$, the sum of losses over the nodes to be unlearned.\n\n3. Parameter Change Approximation: The change in parameters $\\hat{\\theta} - \\theta_0$ due to the perturbation can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of $L_{\\Delta G}$ with respect to $\\theta_0$.\n\n4. Hessian Inverse Role: The Hessian matrix $H_{\\theta_0}^{-1}$ plays a crucial role in scaling the gradient of the perturbation loss, providing the direction and magnitude of the parameter change.\n\n5. Final Relationship: Combining these elements, the relationship $\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla_{\\theta_0} L_{\\Delta G}$ is derived, showing how the parameter change is influenced by the unlearning request through the inverse Hessian and the gradient of the perturbation loss.\n\nThis derivation assumes the loss function's convexity and differentiability, ensuring the Hessian is positive definite and invertible, which are critical for the approximation's validity.", "D": "To prove Theorem 1: $\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$, we proceed as follows:\n\n**Step 1: Optimality Conditions**  \nThe original parameter $\\theta_0$ minimizes $L_0$, so:  \n$$\\nabla L_0(\\theta_0) = 0.$$  \nAfter unlearning $\\Delta G$, the new loss becomes $L_{\\text{new}}(\\theta) = L_0(\\theta) - L_{\\Delta G}(\\theta)$, where $L_{\\Delta G}(\\theta) = \\sum_{z_i \\in \\Delta G} l(f_G(z_i), y_i)$. The new optimal parameter $\\hat{\\theta}$ satisfies:  \n$$\\nabla L_{\\text{new}}(\\hat{\\theta}) = \\nabla L_0(\\hat{\\theta}) - \\nabla L_{\\Delta G}(\\hat{\\theta}) = 0.$$\n\n**Step 2: Taylor Expansion**  \nExpand $\\nabla L_0(\\hat{\\theta})$ around $\\theta_0$:  \n$$\\nabla L_0(\\hat{\\theta}) \\approx \\nabla L_0(\\theta_0) + H_{\\theta_0}(\\hat{\\theta} - \\theta_0) = H_{\\theta_0}(\\hat{\\theta} - \\theta_0).$$  \nSimilarly, expand $\\nabla L_{\\Delta G}(\\hat{\\theta})$ around $\\theta_0$:  \n$$\\nabla L_{\\Delta G}(\\hat{\\theta}) \\approx \\nabla L_{\\Delta G}(\\theta_0) + H_{\\Delta G}(\\hat{\\theta} - \\theta_0),$$  \nwhere $H_{\\Delta G} = \\sum_{z_i \\in \\Delta G} \\frac{\\partial^2}{\\partial \\theta^2} l(f_G(z_i), y_i)$.\n\n**Step 3: Approximation**  \nSubstitute into the optimality condition:  \n$$H_{\\theta_0}(\\hat{\\theta} - \\theta_0) \\approx \\nabla L_{\\Delta G}(\\theta_0) + H_{\\Delta G}(\\hat{\\theta} - \\theta_0).$$  \nAssuming $L_{\\Delta G}$ is small relative to $L_0$, the term $H_{\\Delta G}(\\hat{\\theta} - \\theta_0)$ is negligible. Thus:  \n$$H_{\\theta_0}(\\hat{\\theta} - \\theta_0) \\approx \\nabla L_{\\Delta G}(\\theta_0).$$\n\n**Step 4: Inverting the Hessian**  \nSolve for $\\hat{\\theta} - \\theta_0$:  \n$$\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla L_{\\Delta G}(\\theta_0).$$  \nHere, $\\nabla L_{\\Delta G}(\\theta_0) = \\sum_{z_i \\in \\Delta G} \\frac{\\partial}{\\partial \\theta} l(f_G(z_i), y_i)\\big|_{\\theta_0}$, which corresponds to $L_{\\Delta G}$ **if $L_{\\Delta G}$ is interpreted as the gradient sum** (not the loss sum). For consistency with Theorem 1, redefine $L_{\\Delta G}$ as the gradient term:  \n$$L_{\\Delta G} \\triangleq \\nabla L_{\\Delta G}(\\theta_0).$$  \nThen the result $\\hat{\\theta} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}$ follows directly.  \n\n**Conclusion**  \nThe validity of Theorem 1 hinges on interpreting $L_{\\Delta G}$ as the **gradient contribution** of the unlearned nodes at $\\theta_0$, not their raw loss values. This aligns with standard influence function derivations where parameter shifts are proportional to $H^{-1}\\nabla$. Clarifying the definition of $L_{\\Delta G}$ resolves the apparent discrepancy.", "ground-truth": "C"}
{"paper": "arxiv:2304.02835", "question": "After unlearning, the objective $L$ is redefined over the remaining data $D_{0} \\setminus \\Delta D$ using the modified graph $G \\setminus \\Delta G$. How can we derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ with $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$ from the idea that we re-optimize the model parameters on the reduced dataset?", "A": "To derive the re-optimized objective \\(\\hat{L}\\) after unlearning, we follow these steps based on the idea of re-optimizing the model parameters \\(\\theta\\) on the reduced dataset \\(D_0 \\setminus \\Delta D\\) and the modified graph \\(G \\setminus \\Delta G\\):\n\n### 1. **Original Objective Before Unlearning:**\nThe original objective \\(L_{\\text{original}}\\) is typically defined over the full dataset \\(D_0\\) and the full graph \\(G\\):\n\\[\nL_{\\text{original}} = \\sum_{z_i \\in D_0} l(f_G(z_i), y_i),\n\\]\nwhere:\n- \\(f_G(z_i)\\) is the model's prediction for data point \\(z_i\\) using the graph \\(G\\).\n- \\(l(\\cdot, \\cdot)\\) is the loss function.\n- \\(\\theta\\) are the model parameters.\n\n### 2. **After Unlearning:**\nAfter unlearning the subset \\(\\Delta D\\) (and correspondingly modifying the graph by removing \\(\\Delta G\\)), the remaining data is \\(D_0 \\setminus \\Delta D\\), and the modified graph is \\(G \\setminus \\Delta G\\). The new objective \\(L\\) is defined as:\n\\[\nL = \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i).\n\\]\n\n### 3. **Re-optimization:**\nTo derive the re-optimized parameters \\(\\hat{\\theta}\\), we minimize the new objective \\(L\\) with respect to \\(\\theta\\):\n\\[\n\\hat{L} = \\arg\\min_{\\theta} L = \\arg\\min_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i).\n\\]\n\n### 4. **Key Observations:**\n- The re-optimization is performed **only on the remaining data** \\(D_0 \\setminus \\Delta D\\).\n- The model's predictions \\(f_{G \\setminus \\Delta G}(z_i)\\) now depend on the modified graph \\(G \\setminus \\Delta G\\), which may alter the relationships or features used for prediction.\n- The loss function \\(l(\\cdot, \\cdot)\\) remains unchanged, but its inputs are now based on the reduced data and graph.\n\n### 5. **Derivation of \\(\\hat{L}\\):**\nThe derivation involves solving the optimization problem:\n\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} L(\\theta),\n\\]\nwhere \\(L(\\theta)\\) is the new objective. This can be done using standard optimization techniques (e.g., gradient descent) by:\n1. Computing the gradient of \\(L\\) with respect to \\(\\theta\\):\n   \\[\n   \\nabla_\\theta L = \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\nabla_\\theta l(f_{G \\setminus \\Delta G}(z_i), y_i).\n   \\]\n2. Updating \\(\\theta\\) iteratively to minimize \\(L\\).\n\n### 6. **Final Expression:**\nThe re-optimized objective \\(\\hat{L}\\) is the minimized value of \\(L\\):\n\\[\n\\hat{L} = \\min_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i),\n\\]\nand the corresponding optimal parameters are:\n\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_i), y_i).\n\\]\n\n### Summary:\nThe re-optimized objective \\(\\hat{L}\\) is derived by minimizing the loss over the remaining data \\(D_0 \\setminus \\Delta D\\) using the modified graph \\(G \\setminus \\Delta G\\). This ensures that the model parameters \\(\\theta\\) are adjusted to fit the new data and graph structure after unlearning.", "B": "To derive $\\hat{L} = \\mathop{\\text{arg min}}_{\\theta} L$ under the redefined objective over $D_0 \\setminus \\Delta D$ and modified graph $G \\setminus \\Delta G$, we follow these steps:\n\n1. **Re-optimization Objective**: After unlearning, the model parameters $\\theta$ are re-optimized on the remaining data $D_0 \\setminus \\Delta D$ using the modified graph structure $G \\setminus \\Delta G$. The loss function becomes:\n   $$\n   L(\\theta) = \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\ell\\left(f_{G \\setminus \\Delta G}(z_i; \\theta), y_i\\right),\n   $$\n   where $f_{G \\setminus \\Delta G}(z_i; \\theta)$ denotes the model output parameterized by $\\theta$ under the pruned graph.\n\n2. **Optimality Condition**: To minimize $L(\\theta)$, we compute its gradient with respect to $\\theta$ and set it to zero:\n   $$\n   \\nabla_\\theta L(\\theta) = \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\nabla_\\theta \\ell\\left(f_{G \\setminus \\Delta G}(z_i; \\theta), y_i\\right) = 0.\n   $$\n   This ensures $\\theta$ is a stationary point of the loss landscape.\n\n3. **Explicit Minimizer**: The optimal parameters $\\hat{\\theta}$ satisfy:\n   $$\n   \\hat{\\theta} = \\mathop{\\text{arg min}}_{\\theta} \\sum_{z_i \\in D_0 \\setminus \\Delta D} \\ell\\left(f_{G \\setminus \\Delta G}(z_i; \\theta), y_i\\right).\n   $$\n   This is equivalent to training the model **from scratch** on $D_0 \\setminus \\Delta D$ with the updated graph $G \\setminus \\Delta G$, ensuring $\\hat{\\theta}$ adapts to both the reduced data and altered model structure.\n\n**Conclusion**: The derivation directly follows the empirical risk minimization principle, where $\\hat{\\theta}$ minimizes the loss over the post-unlearning data and computational graph. No implicit assumptions about convexity or parameter dependencies are required for this formulation.", "C": "To derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ where $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$, we follow these steps:\n\n1. Identify the Remaining Data: After unlearning, the dataset is reduced to $D_{0} \\setminus \\Delta D$. This means we only consider the data points that are not part of the unlearning request.\n\n2. Modify the Graph: The graph $G$ is modified to $G \\setminus \\Delta G$ to reflect the removal of the unlearned data. This modification affects how the function $f$ computes its outputs.\n\n3. Define the Loss Function: The loss function $L$ is defined over the remaining data, where for each data point $z_{i}$ in $D_{0} \\setminus \\Delta D$, we compute the loss $l$ between the predicted output $f_{G \\setminus \\Delta G}(z_{i})$ and the actual output $y_{i}$.\n\n4. Re-optimize the Model Parameters: The goal is to find the parameters $\\theta$ that minimize the loss function $L$. This is achieved by solving the optimization problem $\\hat{\\theta} = \\arg\\min_{\\theta} L$.\n\n5. Derive $\\hat{L}$: The solution to the optimization problem gives us $\\hat{L}$, which is the minimized loss over the remaining data with the modified graph.\n\nThe key idea is that by re-optimizing the model parameters on the reduced dataset and modified graph, we ensure that the model performs well on the remaining data while disregarding the unlearned data. This process is encapsulated in the equation $\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$. Note that this optimization assumes the graph modification $\\Delta G$ does not affect the parameters $\\theta$ of the model, which may not hold in general.", "D": "To derive $\\hat{L} = \\text{arg} \\text{min}_{\\theta} L$ where $L = \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$, we follow these steps:\n\n1. Identify the Remaining Data: After unlearning, the dataset is reduced to $D_{0} \\setminus \\Delta D$. This means we only consider the data points that are not part of the unlearning request.\n\n2. Modify the Graph: The graph $G$ is modified to $G \\setminus \\Delta G$ to reflect the removal of the unlearned data. This modification affects how the function $f$ computes its outputs.\n\n3. Define the Loss Function: The loss function $L$ is defined over the remaining data, where for each data point $z_{i}$ in $D_{0} \\setminus \\Delta D$, we compute the loss $l$ between the predicted output $f_{G \\setminus \\Delta G}(z_{i})$ and the actual output $y_{i}$.\n\n4. Re-optimize the Model Parameters: The goal is to find the parameters $\\theta$ that minimize the loss function $L$. This is achieved by solving the optimization problem $\\hat{\\theta} = \\arg\\min_{\\theta} L$.\n\n5. Derive $\\hat{L}$: The solution to the optimization problem gives us $\\hat{L}$, which is the minimized loss over the remaining data with the modified graph.\n\nThe key idea is that by re-optimizing the model parameters on the reduced dataset and modified graph, we ensure that the model performs well on the remaining data while disregarding the unlearned data. This process is encapsulated in the equation $\\hat{\\theta} = \\arg\\min_{\\theta} \\sum_{z_{i} \\in D_{0} \\setminus \\Delta D} l(f_{G \\setminus \\Delta G}(z_{i}), y_{i})$.", "ground-truth": "D"}
{"paper": "arxiv:2304.02835", "question": "Given that $\\epsilon = -1$, derive the relationship $\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}$ from the approximate linearization in terms of the Hessian $H_{\\theta_{0}}$ and the influence of $L_{\\Delta G}$.", "A": "To derive the relationship when $\\epsilon = -1$, we start by considering the approximate linearization of the parameter update. The key steps involve the Hessian matrix $H_{\\theta_{0}}$ and the gradient of the loss function $L_{\\Delta G}$ with respect to the parameters $\\theta_0$. The derivation proceeds as follows:\n\n1. Approximate Linearization: The change in parameters $\\hat{\\theta} - \\theta_0$ can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of the loss function and the inverse of the Hessian matrix.\n\n2. Setting $\\epsilon = -1$: This specific value of $\\epsilon$ implies that we are considering the effect of removing the influence of $L_{\\Delta G}$ on the parameter update.\n\n3. Resulting Relationship: Combining these elements, we arrive at the relationship\n   $$\\hat{\\theta} - \\theta_0 = \\theta_{\\epsilon=-1} - \\theta_0 \\approx H_{\\theta_0}^{-1}\\nabla_{\\theta_0} L_{\\Delta G},$$\n   which shows that the parameter update is dominated by the removal of $L_{\\Delta G}$'s influence, scaled by the inverse of the Hessian matrix at $\\theta_0$.", "B": "To derive the relationship \\(\\hat{\\theta} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}\\) when \\(\\epsilon = -1\\), we proceed as follows:\n\n---\n\n**1. Perturbed Optimization Setup**  \nAssume the original optimization problem minimizes \\(L(\\theta)\\), with \\(\\theta_0\\) as the optimal solution (\\(\\nabla L(\\theta_0) = 0\\)). When perturbed by \\(\\epsilon \\Delta G(\\theta)\\), the new objective becomes \\(L(\\theta) + \\epsilon \\Delta G(\\theta)\\). The perturbed solution \\(\\hat{\\theta}\\) satisfies:\n\\[\n\\nabla L(\\hat{\\theta}) + \\epsilon \\nabla \\Delta G(\\hat{\\theta}) = 0.\n\\]\n\n**2. Linear Approximation at \\(\\theta_0\\)**  \nExpand \\(\\nabla L(\\hat{\\theta})\\) and \\(\\nabla \\Delta G(\\hat{\\theta})\\) around \\(\\theta_0\\) (using Taylor series):\n\\[\n\\nabla L(\\theta_0) + H_{\\theta_0}(\\hat{\\theta} - \\theta_0) + \\epsilon \\left[\\nabla \\Delta G(\\theta_0) + \\nabla^2 \\Delta G(\\theta_0)(\\hat{\\theta} - \\theta_0)\\right] \\approx 0.\n\\]\nSince \\(\\nabla L(\\theta_0) = 0\\) and higher-order terms (e.g., \\(\\nabla^2 \\Delta G\\)) are negligible:\n\\[\nH_{\\theta_0}(\\hat{\\theta} - \\theta_0) + \\epsilon \\nabla \\Delta G(\\theta_0) \\approx 0.\n\\]\n\n**3. Solve for \\(\\hat{\\theta} - \\theta_0\\)**  \nRearranging gives:\n\\[\n\\hat{\\theta} - \\theta_0 \\approx -\\epsilon H_{\\theta_0}^{-1} \\nabla \\Delta G(\\theta_0).\n\\]\nLet \\(L_{\\Delta G} \\equiv \\nabla \\Delta G(\\theta_0)\\). Substituting \\(\\epsilon = -1\\):\n\\[\n\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} L_{\\Delta G}.\n\\]\n\n**4. Final Relationship**  \nThus, under linear approximation:\n\\[\n\\boxed{\\hat{\\theta} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}.}\n\\]\n\n---\n\n**Key Steps**:\n- The perturbation introduces a gradient term \\(\\nabla \\Delta G(\\theta_0)\\).\n- The Hessian \\(H_{\\theta_0}\\) governs the local curvature of \\(L(\\theta)\\).\n- Inverting \\(H_{\\theta_0}\\) scales the perturbation effect to estimate the parameter shift.", "C": "To derive the relationship \\(\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}\\) from the approximate linearization, we proceed with the following steps:\n\n### 1. **Approximate Linearization of the Loss Function**\nConsider a loss function \\(L(\\theta)\\) that depends on parameters \\(\\theta\\). Around a reference point \\(\\theta_0\\), the second-order Taylor approximation is:\n\\[\nL(\\theta) \\approx L(\\theta_0) + \\nabla L(\\theta_0)^T (\\theta - \\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H_{\\theta_0} (\\theta - \\theta_0),\n\\]\nwhere:\n- \\(\\nabla L(\\theta_0)\\) is the gradient of \\(L\\) at \\(\\theta_0\\),\n- \\(H_{\\theta_0}\\) is the Hessian matrix of \\(L\\) at \\(\\theta_0\\).\n\n### 2. **Perturbation Analysis with \\(\\epsilon = -1\\)**\nIntroduce a perturbation \\(\\Delta G\\) to the loss function, scaled by \\(\\epsilon\\):\n\\[\nL_{\\text{pert}}(\\theta) = L(\\theta) + \\epsilon \\cdot L_{\\Delta G}(\\theta).\n\\]\nFor \\(\\epsilon = -1\\), this becomes:\n\\[\nL_{\\text{pert}}(\\theta) = L(\\theta) - L_{\\Delta G}(\\theta).\n\\]\n\nThe optimal parameter \\(\\hat{\\theta}\\) under the perturbed loss satisfies \\(\\nabla L_{\\text{pert}}(\\hat{\\theta}) = 0\\). Linearizing around \\(\\theta_0\\):\n\\[\n\\nabla L(\\theta_0) + H_{\\theta_0} (\\hat{\\theta} - \\theta_0) - \\nabla L_{\\Delta G}(\\theta_0) \\approx 0.\n\\]\nAssuming \\(\\theta_0\\) is a critical point of \\(L\\) (i.e., \\(\\nabla L(\\theta_0) = 0\\)), this simplifies to:\n\\[\nH_{\\theta_0} (\\hat{\\theta} - \\theta_0) \\approx \\nabla L_{\\Delta G}(\\theta_0).\n\\]\n\n### 3. **Solving for the Parameter Shift**\nThe parameter shift is then:\n\\[\n\\hat{\\theta} - \\theta_0 \\approx H_{\\theta_0}^{-1} \\nabla L_{\\Delta G}(\\theta_0).\n\\]\nHere, \\(\\nabla L_{\\Delta G}(\\theta_0)\\) represents the influence of the perturbation \\(\\Delta G\\) on the gradient, which we denote as \\(L_{\\Delta G}\\) for brevity. Thus:\n\\[\n\\hat{\\theta} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}.\n\\]\n\n### 4. **Special Case for \\(\\epsilon = -1\\)**\nWhen \\(\\epsilon = -1\\), the perturbed parameter \\(\\theta_{\\epsilon=-1}\\) coincides with \\(\\hat{\\theta}\\), yielding:\n\\[\n\\theta_{\\epsilon=-1} - \\theta_0 \\sim H_{\\theta_0}^{-1} L_{\\Delta G}.\n\\]\nThis completes the derivation. \n\n### **Final Relationship**\n\\[\n\\boxed{\\hat{\\theta} - \\theta_{0} = \\theta_{\\epsilon=-1} - \\theta_{0} \\sim H_{\\theta_{0}}^{-1} L_{\\Delta G}}\n\\]", "D": "To derive the relationship when $\\epsilon = -1$, we start by considering the approximate linearization of the parameter update. The key steps involve the Hessian matrix $H_{\\theta_{0}}$ and the gradient of the loss function $L_{\\Delta G}$ with respect to the parameters $\\theta_0$. The derivation proceeds as follows:\n\n1. Approximate Linearization: The change in parameters $\\hat{\\theta} - \\theta_0$ can be approximated using the first-order Taylor expansion around $\\theta_0$. This involves the gradient of the loss function and the inverse of the Hessian matrix.\n\n2. Setting $\\epsilon = -1$: This specific value of $\\epsilon$ implies that we are considering the effect of removing the influence of $L_{\\Delta G}$ on the parameter update.\n\n3. Resulting Relationship: Combining these elements, we arrive at the relationship\n   $$\\hat{\\theta} - \\theta_0 = \\theta_{\\epsilon=-1} - \\theta_0 \\approx -H_{\\theta_0}^{-1}\\nabla_{\\theta_0} L_{\\Delta G},$$\n   which shows that the parameter update is dominated by the removal of $L_{\\Delta G}$'s influence, scaled by the inverse of the Hessian matrix at $\\theta_0$.", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "Given the assumption of G-invariance, derive the chain of equalities leading from $p(gx_0)$ to $p(x_0)$. Explain the step where substituting $gx_t$ back to $x_t$ preserves the integral and results in $p(x_0)$.", "A": "To derive the chain of equalities under the assumption of G-invariance, we start with the expression for $p(gx_0)$ and systematically apply the properties of G-invariance and probability measures:\n\n1. Initial Expression: We begin with $p(gx_0)$, which represents the probability density at the transformed point $gx_0$.\n\n2. Application of G-invariance: Due to G-invariance, the probability density is unchanged under group action. This allows us to express $p(gx_0)$ in terms of $p(gx_T)$ and the conditional probabilities $p(gx_{t-1} \\mid gx_t)$ for $t = 1$ to $T$.\n\n3. Substitution and Integral Preservation: The key step involves substituting $gx_t$ back to $x_t$ within the integral. This substitution is valid because the probability measure is invariant under the group action, meaning the integral's value remains unchanged. This invariance ensures that the integral over the transformed variables is equivalent to the integral over the original variables.\n\n4. Final Equality: After substitution, the expression simplifies to $p(x_T)$ multiplied by the integral of the product of conditional probabilities $p(x_{t-1} \\mid x_t)$, which, by definition, equals $p(x_T)$. This final step relies on the consistency of the probability measure and the Markov property of the process.\n\nThe preservation of the integral when substituting $gx_t$ back to $x_t$ is a direct consequence of the measure's invariance under the group action, ensuring that the transformation does not alter the integral's value. This property is crucial for maintaining the equality throughout the chain and ultimately arriving at $p(x_T)$.", "B": "To derive the chain of equalities under the assumption of G-invariance, we start with the expression for $p(gx_0)$ and systematically apply the properties of G-invariance and probability measures:\n\n1. Initial Expression: We begin with $p(gx_0)$, which represents the probability density at the transformed point $gx_0$.\n\n2. Application of G-invariance: Due to G-invariance, the probability density is unchanged under group action. This allows us to express $p(gx_0)$ in terms of $p(gx_T)$ and the conditional probabilities $p(gx_{t-1} \\mid gx_t)$ for $t = 1$ to $T$.\n\n3. Substitution and Integral Preservation: The key step involves substituting $gx_t$ back to $x_t$ within the integral. This substitution is valid because the probability measure is invariant under the group action, meaning the integral's value remains unchanged. This invariance ensures that the integral over the transformed variables is equivalent to the integral over the original variables.\n\n4. Final Equality: After substitution, the expression simplifies to $p(x_T)$ multiplied by the integral of the product of conditional probabilities $p(x_{t-1} \\mid x_t)$, which, by definition, equals $p(x_0)$. This final step relies on the consistency of the probability measure and the Markov property of the process.\n\nThe preservation of the integral when substituting $gx_t$ back to $x_t$ is a direct consequence of the measure's invariance under the group action, ensuring that the transformation does not alter the integral's value. This property is crucial for maintaining the equality throughout the chain and ultimately arriving at $p(x_0)$.", "C": "Under the assumption of \\( G \\)-invariance, we aim to derive the chain of equalities leading from \\( p(gx_0) \\) to \\( p(x_0) \\). Here is the step-by-step derivation with explanations:\n\n1. **\\( G \\)-Invariance Assumption**:  \n   The probability density \\( p(x) \\) is \\( G \\)-invariant, meaning it satisfies:\n   \\[\n   p(gx) = p(x), \\quad \\forall g \\in G.\n   \\]\n   This implies that the density is unchanged under group actions.\n\n2. **Starting Point**:  \n   We begin with \\( p(gx_0) \\). By the \\( G \\)-invariance of \\( p \\), we immediately have:\n   \\[\n   p(gx_0) = p(x_0).\n   \\]\n   This is the simplest case where the equality holds directly due to the invariance property.\n\n3. **General Case with Integral Preservation**:  \n   Now, consider a scenario where \\( p(gx_0) \\) arises from an integral or expectation over group transformations. For example, suppose:\n   \\[\n   p(gx_0) = \\int p(gx_t) \\, d\\mu(t),\n   \\]\n   where \\( \\mu \\) is a measure over some parameter space (e.g., time or group elements). \n\n   - **Substitution Step**:  \n     Using the \\( G \\)-invariance \\( p(gx_t) = p(x_t) \\), we substitute inside the integral:\n     \\[\n     p(gx_0) = \\int p(gx_t) \\, d\\mu(t) = \\int p(x_t) \\, d\\mu(t).\n     \\]\n     The substitution preserves the integral because the measure \\( d\\mu(t) \\) is invariant under the group action (or the substitution is measure-preserving). This is a common assumption in such derivations, ensuring that the integral's value remains unchanged.\n\n   - **Resulting Equality**:  \n     If the integral \\( \\int p(x_t) \\, d\\mu(t) \\) evaluates to \\( p(x_0) \\) (e.g., when \\( x_t \\) is a path or process with \\( x_0 \\) as its initial or reference point), we obtain:\n     \\[\n     p(gx_0) = p(x_0).\n     \\]\n\n4. **Key Explanation**:  \n   The critical step is the substitution \\( p(gx_t) = p(x_t) \\) under the integral. This is valid because:\n   - The \\( G \\)-invariance of \\( p \\) ensures \\( p(gx_t) = p(x_t) \\) pointwise.\n   - The measure \\( d\\mu(t) \\) (or the underlying space) is assumed to be compatible with the group action, meaning the substitution does not introduce any Jacobian factors or alter the integral's value. For example, if \\( G \\) is a compact group with Haar measure \\( dg \\), then \\( \\int f(gx) \\, dg = \\int f(x) \\, dg \\) due to the invariance of the Haar measure.\n\nThus, under \\( G \\)-invariance and appropriate measure-preserving conditions, the chain of equalities simplifies \\( p(gx_0) \\) to \\( p(x_0) \\). The substitution step is justified by the invariance of both \\( p \\) and the measure under group actions.", "D": "Under the assumption of $G$-invariance, the probability density $p(x)$ satisfies $p(gx) = p(x)$ for all $g \\in G$. To derive $p(x_0)$ from $p(gx_0)$, consider the following steps:\n\n1. **Express $p(gx_0)$ via marginalization**:  \n   \\begin{align*}\n   p(gx_0) &= \\int p(gx_0 | x_t) p(x_t) \\, dx_t\n   \\end{align*}\n   Here, $x_t$ represents intermediate states (e.g., in a diffusion process or stochastic trajectory).\n\n2. **Exploit $G$-invariance of the conditional distribution**:  \n   By $G$-equivariance of the dynamics or symmetry of the transition kernel, we have  \n   \\begin{align*}\n   p(gx_0 | x_t) = p(x_0 | g^{-1}x_t).\n   \\end{align*}\n\n3. **Substitute $x_t \\to gx_t$ in the integral**:  \n   Change variables by letting $x'_t = g^{-1}x_t$. Since the measure $dx_t$ is $G$-invariant (e.g., under Haar measure), $dx'_t = dx_t$. The integral becomes:  \n   \\begin{align*}\n   p(gx_0) &= \\int p(x_0 | x'_t) \\cdot p(gx'_t) \\, dx'_t.\n   \\end{align*}\n\n4. **Apply $G$-invariance of $p(x_t)$**:  \n   By assumption, $p(gx'_t) = p(x'_t)$. Thus,  \n   \\begin{align*}\n   p(gx_0) &= \\int p(x_0 | x'_t) p(x'_t) \\, dx'_t.\n   \\end{align*}\n\n5. **Re-express as $p(x_0)$**:  \n   The right-hand side is the definition of $p(x_0)$ via marginalization:  \n   \\begin{align*}\n   p(gx_0) &= p(x_0).\n   \\end{align*}\n\n**Critical explanation for Step 3**:  \nThe substitution $x_t \\to gx_t$ preserves the integral because:  \n- The measure $dx_t$ is $G$-invariant, so $dx_t = d(gx_t)$.  \n- The conditional distribution $p(x_0 | x_t)$ transforms covariantly under $G$-action (equivariance), ensuring $p(gx_0 | x_t) = p(x_0 | g^{-1}x_t)$.  \n- The $G$-invariance of $p(x_t)$ ensures $p(gx_t) = p(x_t)$.  \n\nThese symmetries collectively guarantee that the integral evaluates to $p(x_0)$, independent of $g$. Thus, $p(gx_0) = p(x_0)$ holds universally under $G$-invariance.", "ground-truth": "B"}
{"paper": "arxiv:2309.04475", "question": "How can we demonstrate the sequence of equalities in the expression involving $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$ and its transformations? Specifically, how does the application of $Q$ and the property of O(3)-equivariance, expressed as $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$, ensure that the distribution for $L_{t-1}$ remains unchanged?", "A": "To understand the sequence of equalities, let's break down the process step by step:\n\n1. Initial Distribution: We start with the conditional distribution $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$, which describes the probability of $L_{t-1}$ given $L_t$, $F_t$, and $A$ under a normal distribution framework.\n\n2. Application of $Q$: By applying the orthogonal transformation $Q$ to both $L_{t-1}$ and $L_t$, we consider the transformed variables $QL_{t-1}$ and $QL_t$. The equality $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}(QL_{t-1}\\mid QL_t,F_t,A)$ holds because the normal distribution is invariant under orthogonal transformations. This means the probability density remains the same before and after the transformation.\n\n3. O(3)-Equivariance Property: The key to understanding the next equality lies in the O(3)-equivariance of $\\hat{\\epsilon}_L$, which is given by $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$. This property ensures that the transformation $Q$ commutes with the function $\\hat{\\epsilon}_L$, meaning that applying $Q$ to the input is equivalent to applying $Q$ to the output of $\\hat{\\epsilon}_L$.\n\n4. Recovery of Original Distribution: Due to the invariance of the normal distribution under orthogonal transformations and the O(3)-equivariance of $\\hat{\\epsilon}_L$, the distribution of $L_{t-1}$ conditioned on $L_t$, $F_t$, and $A$ remains unchanged. This leads us back to the original distribution $p(L_{t-1}\\mid L_t,F_t,A)$, completing the chain of equalities.\n\nIn summary, the combination of the normal distribution's invariance under orthogonal transformations and the O(3)-equivariance property of $\\hat{\\epsilon}_L$ ensures that the distribution for $L_{t-1}$ is preserved throughout the transformations, validating the sequence of equalities presented.", "B": "To demonstrate the sequence of equalities involving the distribution \\(\\mathcal{N}(L_{t-1} \\mid L_t, F_t, A)\\) and its transformations under the application of an orthogonal transformation \\(Q \\in O(3)\\), we will carefully analyze the equivariance property and its implications on the distribution.\n\n### **1. Initial Distribution and Transformation**\nThe distribution is given by:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(\\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right),\n\\]\nwhere \\(\\hat{\\epsilon}_L\\) is the noise predictor, and \\(\\alpha_t, \\beta_t, \\bar{\\alpha}_t\\) are diffusion process parameters.\n\n### **2. O(3)-Equivariance of \\(\\hat{\\epsilon}_L\\)**\nThe key property is the \\(O(3)\\)-equivariance of \\(\\hat{\\epsilon}_L\\):\n\\[\n\\hat{\\epsilon}_L(Q L_t, F_t, A, t) = Q \\hat{\\epsilon}_L(L_t, F_t, A, t).\n\\]\nThis means that rotating the input \\(L_t\\) by \\(Q\\) results in the same rotation applied to the output noise prediction.\n\n### **3. Distribution Under Transformation \\(Q\\)**\nNow, consider applying \\(Q\\) to \\(L_t\\) and examining the transformed distribution for \\(L_{t-1}\\). Let \\(L'_t = Q L_t\\). The distribution for \\(L_{t-1}\\) becomes:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(\\frac{1}{\\sqrt{\\alpha_t}} \\left(Q L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(Q L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right).\n\\]\nUsing the equivariance property:\n\\[\n\\hat{\\epsilon}_L(Q L_t, F_t, A, t) = Q \\hat{\\epsilon}_L(L_t, F_t, A, t),\n\\]\nwe substitute back:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(\\frac{1}{\\sqrt{\\alpha_t}} \\left(Q L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} Q \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right).\n\\]\nFactor out \\(Q\\) from the mean:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(Q \\cdot \\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right), \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right).\n\\]\n\n### **4. Rotational Invariance of the Distribution**\nThe covariance matrix \\(\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\) is isotropic (proportional to the identity matrix), and thus invariant under orthogonal transformations:\n\\[\nQ \\left(\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\right) Q^T = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I.\n\\]\nTherefore, the distribution can be rewritten as:\n\\[\nL_{t-1} \\sim \\mathcal{N}\\left(Q \\mu, \\Sigma\\right),\n\\]\nwhere \\(\\mu = \\frac{1}{\\sqrt{\\alpha_t}} \\left(L_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_L(L_t, F_t, A, t)\\right)\\) and \\(\\Sigma = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t I\\).\n\n### **5. Conclusion: Unchanged Distribution**\nSince the distribution of \\(L_{t-1}\\) is rotationally equivariant (i.e., applying \\(Q\\) to \\(L_t\\) results in \\(Q\\) being applied to the mean while the covariance remains unchanged), the overall distribution remains consistent under \\(O(3)\\) transformations. Thus, the distribution for \\(L_{t-1}\\) is unchanged up to the orthogonal transformation \\(Q\\), ensuring the desired invariance property.\n\n\\[\n\\boxed{\\mathcal{N}(L_{t-1} \\mid Q L_t, F_t, A) = \\mathcal{N}(Q L_{t-1} \\mid L_t, F_t, A)}\n\\]", "C": "To understand the sequence of equalities, let's break down the process step by step:\n\n1. Initial Distribution: We start with the conditional distribution $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A)$, which describes the probability of $L_{t-1}$ given $L_t$, $F_t$, and $A$ under a normal distribution framework.\n\n2. Application of $Q$: By applying the orthogonal transformation $Q$ to both $L_{t-1}$ and $L_t$, we consider the transformed variables $QL_{t-1}$ and $QL_t$. The equality $\\mathcal{N}(L_{t-1}\\mid L_t,F_t,A) = \\mathcal{N}(QL_{t-1}\\mid QL_t,F_t,A)$ holds because the normal distribution is invariant under orthogonal transformations. This means the probability density remains the same before and after the transformation.\n\n3. O(3)-Equivariance Property: The key to understanding the next equality lies in the O(3)-equivariance of $\\hat{\\epsilon}_L$, which is given by $\\hat{\\epsilon}_L(QL_t,F_t,A,t) = Q\\hat{\\epsilon}_L(L_t,F_t,A,t)$. This property ensures that the transformation $Q$ commutes with the function $\\hat{\\epsilon}_L$, meaning that applying $Q$ to the input is equivalent to applying $Q$ to the output of $\\hat{\\epsilon}_L$.\n\n4. Recovery of Original Distribution: Due to the invariance of the normal distribution under orthogonal transformations and the O(3)-equivariance of $\\hat{\\epsilon}_L$, the distribution of $L_{t-1}$ conditioned on $L_t$, $F_t$, and $A$ remains unchanged. This leads us back to the original distribution $p(L_{t-1}\\mid L_t,F_t,A)$, completing the chain of equalities.\n\nIn summary, the combination of the normal distribution's invariance under orthogonal transformations and the O(3)-equivariance property of $\\hat{\\epsilon}_L$ ensures that the distribution for $L_{t-1}$ is preserved throughout the transformations, validating the sequence of equalities presented. However, this conclusion implicitly assumes that $Q$ is invertible, which is not guaranteed for all orthogonal transformations.", "D": "To demonstrate the sequence of equalities and show that the distribution for $L_{t-1}$ remains unchanged under the application of $Q \\in \\text{O}(3)$, we leverage the equivariance property of $\\hat{\\epsilon}_L$ and the structure of the Gaussian distribution. Here's the step-by-step reasoning:\n\n---\n\n**1. Define the conditional distribution:**  \nAssume the reverse process is parameterized as:  \n$$\nL_{t-1} \\sim \\mathcal{N}\\left(\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right),\n$$  \nwhere the **mean** $\\mu(L_t, F_t, A, t)$ depends on the learned noise-prediction model $\\hat{\\epsilon}_L$ via:  \n$$\n\\mu(L_t, F_t, A, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(L_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\hat{\\epsilon}_L(L_t, F_t, A, t)\\right).\n$$  \nHere, $\\alpha_t$, $\\beta_t$, and $\\bar{\\alpha}_t$ are diffusion scheduling hyperparameters.\n\n---\n\n**2. Apply $Q \\in \\text{O}(3)$ to $L_t$:**  \nUnder the transformation $L_t \\to QL_t$, the equivariance property of $\\hat{\\epsilon}_L$ implies:  \n$$\n\\hat{\\epsilon}_L(QL_t, F_t, A, t) = Q\\hat{\\epsilon}_L(L_t, F_t, A, t).\n$$  \nSubstituting this into the mean $\\mu(QL_t, F_t, A, t)$:  \n$$\n\\mu(QL_t, F_t, A, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(QL_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}Q\\hat{\\epsilon}_L(L_t, F_t, A, t)\\right) = Q\\mu(L_t, F_t, A, t).\n$$  \nThus, **the mean transforms equivariantly** under $Q$.\n\n---\n\n**3. Analyze the covariance matrix:**  \nThe covariance $\\sigma_t^2 I$ is **isotropic** (invariant under $\\text{O}(3)$ transformations). For any $Q \\in \\text{O}(3)$:  \n$$\nQ\\left(\\sigma_t^2 I\\right)Q^\\top = \\sigma_t^2 QQ^\\top = \\sigma_t^2 I.\n$$  \nHence, the covariance structure is preserved.\n\n---\n\n**4. Show distributional equivalence:**  \nThe transformed distribution becomes:  \n$$\n\\mathcal{N}\\left(Q\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right).\n$$  \nThis is equivalent to applying $Q$ to samples from the original distribution:  \n$$\nQ\\mathcal{N}\\left(\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right) \\equiv \\mathcal{N}\\left(Q\\mu(L_t, F_t, A, t),\\, \\sigma_t^2 I\\right).\n$$  \nThus, the distribution for $L_{t-1}$ under the transformation $L_t \\to QL_t$ is **identical** to applying $Q$ to $L_{t-1}$ directly.\n\n---\n\n**5. Conclusion:**  \nThe $\\text{O}(3)$-equivariance of $\\hat{\\epsilon}_L$ ensures the mean transforms as $\\mu(QL_t, \\cdot) = Q\\mu(L_t, \\cdot)$, while the isotropic covariance remains invariant. Consequently, the distribution of $L_{t-1}$ satisfies:  \n$$\n\\mathcal{N}(L_{t-1} \\mid QL_t, F_t, A) = Q \\cdot \\mathcal{N}(L_{t-1} \\mid L_t, F_t, A),\n$$  \nmeaning the distribution is **equivariant** under $Q$. If the prior distribution at the final timestep is $\\text{O}(3)$-invariant (e.g., standard Gaussian), the overall generative process becomes **invariant** to $\\text{O}(3)$ transformations.", "ground-truth": "A"}
{"paper": "arxiv:2309.04475", "question": "Given the definition of the wrapped normal distribution, how do we prove that $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$? Please illustrate how the index shift in the sum over $k$ preserves the same distribution.", "A": "To prove that the wrapped normal distribution satisfies $$\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2)$$ for any shift \\( t \\), we analyze its definition and the summation over periodic replicas.\n\n**Definition of Wrapped Normal Distribution:**\nThe wrapped normal density is constructed by summing over all \"wrapped\" (periodic) replicas of the normal distribution:\n$$\n\\mathcal{N}_w(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum_{k=-\\infty}^{\\infty} \\exp\\left(-\\frac{(x - \\mu + 2\\pi k)^2}{2\\sigma^2}\\right).\n$$\n\n**Left-Hand Side (LHS) Expansion:**\nSubstitute \\( x \\to x + t \\) and \\( \\mu \\to \\mu + t \\):\n$$\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum_{k=-\\infty}^{\\infty} \\exp\\left(-\\frac{(x + t - (\\mu + t) + 2\\pi k)^2}{2\\sigma^2}\\right).\n$$\nSimplify the numerator:\n$$\n(x + t) - (\\mu + t) = x - \\mu \\implies \\text{LHS} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum_{k=-\\infty}^{\\infty} \\exp\\left(-\\frac{(x - \\mu + 2\\pi k)^2}{2\\sigma^2}\\right).\n$$\n\n**Equivalence to Right-Hand Side (RHS):**\nThe simplified LHS matches the definition of \\( \\mathcal{N}_w(x \\mid \\mu, \\sigma^2) \\). Thus:\n$$\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n$$\n\n**Role of the Summation Index \\( k \\):**\nThe shift \\( t \\) cancels out in the exponent, leaving the summation over \\( k \\) unchanged. Since the sum includes **all integers** \\( k \\in \\mathbb{Z} \\), it inherently accounts for all periodic replicas of \\( x \\) and \\( \\mu \\). A shift \\( t \\) (even non-integer multiples of \\( 2\\pi \\)) does not affect the infinite summations coverage over the circle. Hence, the distribution remains invariant under translation.\n\n**Conclusion:**\nThe wrapped normal distribution is translationally invariant because the shift \\( t \\) cancels in the exponent, and the infinite sum over \\( k \\) ensures all periodic replicas are preserved.", "B": "To prove the equality $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$, we start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution is defined as a sum of normal distributions shifted by integer multiples of $2\\pi$, which can be written as: $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Now, consider the distribution of $x + t$ given $\\mu + t$ and $\\sigma^2$: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2).$$ By simplifying the argument of the normal distribution, we get: $$\\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2) = \\mathcal{N}(x\\mid\\mu + 2\\pi (k + 1), \\sigma^2).$$ Substituting this back into the sum, we obtain: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi (k + 1), \\sigma^2) = \\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$ This shows that shifting both the variable $x$ and the mean $\\mu$ by the same amount $t$ leaves the wrapped normal distribution unchanged. The key step is the index shift in the sum over $k$, which does not affect the infinite sum because it runs over all integers, ensuring that the distribution remains invariant under such shifts.", "C": "To prove that the wrapped normal distribution satisfies the shift invariance property:\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2),\n\\]\nwe start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution \\(\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)\\) is defined as:\n\\[\n\\mathcal{N}_w(x \\mid \\mu, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2),\n\\]\nwhere \\(\\mathcal{N}(x \\mid \\mu, \\sigma^2)\\) is the standard normal distribution:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\n\\]\n\n### Proof of Shift Invariance:\nConsider the wrapped normal distribution evaluated at \\(x + t\\) with mean \\(\\mu + t\\):\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t + 2\\pi k \\mid \\mu + t, \\sigma^2).\n\\]\n\nSubstitute \\(y = x + t\\) and \\(\\nu = \\mu + t\\) into the standard normal distribution:\n\\[\n\\mathcal{N}(y + 2\\pi k \\mid \\nu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y + 2\\pi k - \\nu)^2}{2\\sigma^2}\\right).\n\\]\nSubstituting back \\(y = x + t\\) and \\(\\nu = \\mu + t\\):\n\\[\n\\mathcal{N}(x + t + 2\\pi k \\mid \\mu + t, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x + t + 2\\pi k - \\mu - t)^2}{2\\sigma^2}\\right) = \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2).\n\\]\n\nThus, the sum becomes:\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n\\]\n\n### Index Shift in the Sum:\nThe key observation is that the sum over \\(k\\) is invariant under any integer shift in the index. For example, let \\(k' = k + m\\) for some integer \\(m\\). Then:\n\\[\n\\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k \\mid \\mu, \\sigma^2) = \\sum_{k'=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi (k' - m) \\mid \\mu, \\sigma^2) = \\sum_{k'=-\\infty}^{\\infty} \\mathcal{N}(x + 2\\pi k' \\mid \\mu, \\sigma^2),\n\\]\nsince the sum over all integers \\(k'\\) is identical to the sum over all integers \\(k\\). This demonstrates that the wrapped normal distribution is periodic and invariant under shifts \\(t\\). \n\n### Conclusion:\nThe wrapped normal distribution \\(\\mathcal{N}_w(x \\mid \\mu, \\sigma^2)\\) is shift-invariant because the infinite sum over \\(k\\) ensures that any shift \\(t\\) in the argument \\(x\\) or the mean \\(\\mu\\) cancels out, leaving the distribution unchanged. Thus:\n\\[\n\\mathcal{N}_w(x + t \\mid \\mu + t, \\sigma^2) = \\mathcal{N}_w(x \\mid \\mu, \\sigma^2).\n\\]", "D": "To prove the equality $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2)=\\mathcal{N}_w(x\\mid\\mu,\\sigma^2)$$ for any shift $t$, we start by recalling the definition of the wrapped normal distribution. The wrapped normal distribution is defined as a sum of normal distributions shifted by integer multiples of $2\\pi$, which can be written as: $$\\mathcal{N}_w(x\\mid\\mu,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Now, consider the distribution of $x + t$ given $\\mu + t$ and $\\sigma^2$: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2).$$ By simplifying the argument of the normal distribution, we get: $$\\mathcal{N}(x + t\\mid\\mu + t + 2\\pi k, \\sigma^2) = \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2).$$ Substituting this back into the sum, we obtain: $$\\mathcal{N}_w(x + t\\mid\\mu + t,\\sigma^2) = \\sum_{k=-\\infty}^{\\infty} \\mathcal{N}(x\\mid\\mu + 2\\pi k, \\sigma^2) = \\mathcal{N}_w(x\\mid\\mu,\\sigma^2).$$ This shows that shifting both the variable $x$ and the mean $\\mu$ by the same amount $t$ leaves the wrapped normal distribution unchanged. The key step is the index shift in the sum over $k$, which does not affect the infinite sum because it runs over all integers, ensuring that the distribution remains invariant under such shifts.", "ground-truth": "D"}
{"paper": "arxiv:2309.04475", "question": "How can we prove that the score $\\hat{\\epsilon}_L$ by Eq. (9) is O(3)-equivariant, and the score $\\hat{\\epsilon}_F$ from Eq. (10) is periodic translation invariant, thereby making the generated distribution periodic E(3) invariant? In particular, use:\n\n1. Eq. (9) for the lattice-denoising score:\n$$\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t),\\ \\forall\\ Q \\in \\mathbb{R}^{3\\times 3},\\ Q^T Q = I.$$\n\n2. Eq. (10) for the coordinate-denoising score $\\hat{\\epsilon}_F$, which is shown to be periodic translation invariant.\n\n3. The orthonormal-transformation identity:\n$$(QL)^T(QL) = L^T Q^T Q L = L^T I L = L^T L, \\quad \\text{for any orthogonal } Q \\text{ with } Q^T Q = I.$$\n\n4. The periodic-Fourier relation (used to show translation invariance) of the form:\n$$\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k].$$", "A": "To prove that the score $\\hat{\\epsilon}_L$ is O(3)-equivariant and $\\hat{\\epsilon}_F$ is periodic translation invariant, thereby ensuring the generated distribution is periodic E(3) invariant, we proceed as follows:\n\n---\n\n### **1. O(3)-Equivariance of $\\hat{\\epsilon}_L$ (Eq. 9)**  \nGiven the identity $(QL)^T(QL) = L^T L$ for any orthogonal $Q \\in \\mathbb{R}^{3 \\times 3}$ with $Q^T Q = I$, the lattice score $\\hat{\\epsilon}_L$ must depend on $L$ through invariants preserved under orthogonal transformations. Suppose $\\hat{\\epsilon}_L$ is derived from a function of $L^T L$ (e.g., via matrix decomposition or gradient descent). Since $L^T L$ remains unchanged under $Q$, the functional form of $\\hat{\\epsilon}_L$ inherits equivariance:  \n$$\n\\hat{\\epsilon}_L(QL, F, A, t) = Q \\cdot \\hat{\\epsilon}_L(L, F, A, t).\n$$  \nThis holds because $Q$ acts linearly on $L$, and $\\hat{\\epsilon}_L$ transforms covariantly with $L$ under $Q$. For instance, if $\\hat{\\epsilon}_L$ computes a gradient of a rotation-invariant energy function, the gradient direction rotates with $Q$.\n\n---\n\n### **2. Periodic Translation Invariance of $\\hat{\\epsilon}_F$ (Eq. 10)**  \nThe score $\\hat{\\epsilon}_F$ depends on periodicized coordinates via the Fourier relation:  \n$$\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k],\n$$  \nwhere $w$ is a periodic wrapping function (e.g., $w(x) = x - \\lfloor x/T \\rfloor T$). Translating all coordinates by $t$ modifies $f_j \\to f_j + t$, but the wrapped differences $w(f_j + t) - w(f_i + t)$ reduce to $f_j - f_i$ modulo periodic boundaries. The Fourier transform $\\mathcal{F}_T$ then eliminates phase shifts caused by $t$, leaving only translation-invariant spectral magnitudes. Thus, $\\hat{\\epsilon}_F$ depends solely on relative positions modulo $T$, making it invariant to periodic translations:  \n$$\n\\hat{\\epsilon}_F(L, F + t\\mathbf{1}^T, A, t) = \\hat{\\epsilon}_F(L, F, A, t).\n$$\n\n---\n\n### **3. Periodic E(3) Invariance of the Generated Distribution**  \nThe full denoising process combines $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$. For any $Q \\in \\text{O}(3)$ and periodic translation $t \\in \\mathbb{R}^3$, the scores satisfy:  \n- **Lattice**: $\\hat{\\epsilon}_L(Q L, Q F + t, A, t) = Q \\hat{\\epsilon}_L(L, F, A, t)$ (equivariance).  \n- **Coordinates**: $\\hat{\\epsilon}_F(Q L, Q F + t, A, t) = \\hat{\\epsilon}_F(L, F, A, t)$ (invariance to $Q$ due to O(3)-invariant input processing, and invariance to $t$ via periodicity).  \n\nUnder the diffusion process, the learned score $\\nabla \\log p(F, L)$ transforms as:  \n$$\n\\nabla \\log p(Q L + t, Q F + t) = Q \\nabla \\log p(L, F),\n$$  \nwhich matches the transformation of the noise terms. By the uniqueness of the score-based diffusion, the generated distribution inherits periodic E(3) invariance:  \n$$\np(Q L + t, Q F + t) = p(L, F) \\quad \\forall Q \\in \\text{O}(3), t \\in \\mathbb{R}^3.\n$$\n\n---\n\n### **Conclusion**  \nThe O(3)-equivariance of $\\hat{\\epsilon}_L$ and periodic translation invariance of $\\hat{\\epsilon}_F$ jointly ensure that the denoising process respects all transformations in the periodic E(3) group, resulting in a generated distribution invariant to periodic Euclidean symmetries.", "B": "To prove Proposition 3, we analyze the properties of the scores $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$ step by step.\n\n1. O(3)-equivariance of $\\hat{\\epsilon}_L$:\n   - Given the equation $\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t)$ for any orthogonal matrix $Q$, this directly shows that $\\hat{\\epsilon}_L$ is O(3)-equivariant. This means that applying an orthogonal transformation $Q$ to the lattice $L$ before computing the score is equivalent to applying $Q$ to the score computed from the original lattice $L$.\n   - The orthonormal-transformation identity $(QL)^T(QL) = L^T L$ further supports this by showing that the inner product structure of the lattice is preserved under orthogonal transformations, which is a key property for O(3)-equivariance.\n\n2. Periodic translation invariance of $\\hat{\\epsilon}_F$:\n   - The periodic-Fourier relation $\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]$ demonstrates that the Fourier transform of the differences between coordinates is invariant under periodic translations $t$. This implies that $\\hat{\\epsilon}_F$ is periodic translation invariant, as the score depends only on the relative positions of the coordinates, not on their absolute positions modulo the periodic boundary.\n\n3. Conclusion:\n   - Combining the O(3)-equivariance of $\\hat{\\epsilon}_L$ and the periodic translation invariance of $\\hat{\\epsilon}_F$, the generated distribution is invariant under the combined actions of orthogonal transformations and periodic translations, i.e., it is periodic E(3) invariant. This completes the proof of Proposition 3.", "C": "To prove Proposition 3, we analyze the properties of the scores $\\hat{\\epsilon}_L$ and $\\hat{\\epsilon}_F$ step by step.\n\n1. O(3)-equivariance of $\\hat{\\epsilon}_L$:\n   - Given the equation $\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t)$ for any orthogonal matrix $Q$, this directly shows that $\\hat{\\epsilon}_L$ is O(3)-equivariant. This means that applying an orthogonal transformation $Q$ to the lattice $L$ before computing the score is equivalent to applying $Q$ to the score computed from the original lattice $L$.\n   - The orthonormal-transformation identity $(QL)^T(QL) = L^T L$ further supports this by showing that the inner product structure of the lattice is preserved under orthogonal transformations, which is a key property for O(3)-equivariance.\n\n2. Periodic translation invariance of $\\hat{\\epsilon}_F$:\n   - The periodic-Fourier relation $\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k]$ demonstrates that the Fourier transform of the differences between coordinates is invariant under periodic translations $t$. This implies that $\\hat{\\epsilon}_F$ is periodic translation invariant, as the score depends only on the relative positions of the coordinates, not on their absolute positions modulo the periodic boundary.\n\n3. Conclusion:\n   - Combining the O(3)-equivariance of $\\hat{\\epsilon}_L$ and the periodic translation invariance of $\\hat{\\epsilon}_F$, the generated distribution is invariant under the combined actions of orthogonal transformations and periodic translations, i.e., it is periodic E(3) invariant. However, this only holds when the lattice $L$ is invertible, which was not explicitly assumed in the proposition. This completes the proof of Proposition 3.", "D": "To prove that the generated distribution is periodic E(3) invariant, we need to show that the lattice-denoising score \\(\\hat{\\epsilon}_L\\) is O(3)-equivariant and the coordinate-denoising score \\(\\hat{\\epsilon}_F\\) is periodic translation invariant. Below is the step-by-step proof:\n\n---\n\n### 1. **O(3)-Equivariance of \\(\\hat{\\epsilon}_L\\) (Eq. 9)**\nGiven:\n\\[\n\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t), \\quad \\forall Q \\in \\mathbb{R}^{3 \\times 3}, Q^T Q = I.\n\\]\nThis is the definition of O(3)-equivariance for the lattice-denoising score. To verify this, we rely on the orthonormal-transformation identity:\n\\[\n(QL)^T(QL) = L^T Q^T Q L = L^T I L = L^T L.\n\\]\nThis shows that the Gram matrix \\(L^T L\\) (which encodes the lattice geometry) is invariant under orthogonal transformations. Since \\(\\hat{\\epsilon}_L\\) depends only on \\(L^T L\\) (and other O(3)-invariant quantities like \\(F\\) and \\(A\\)), the score must transform as:\n\\[\n\\hat{\\epsilon}_L(QL, F, A, t) = Q\\hat{\\epsilon}_L(L, F, A, t).\n\\]\nThus, \\(\\hat{\\epsilon}_L\\) is O(3)-equivariant.\n\n---\n\n### 2. **Periodic Translation Invariance of \\(\\hat{\\epsilon}_F\\) (Eq. 10)**\nThe coordinate-denoising score \\(\\hat{\\epsilon}_F\\) is constructed to be invariant under periodic translations. This is ensured by the Fourier-based formulation, where the score depends on relative positions in a periodic manner. \n\nGiven the periodic-Fourier relation:\n\\[\n\\mathcal{F}_T\\bigl(w(f_j + t) - w(f_i + t)\\bigr)[c, k] = \\mathcal{F}_T\\bigl(f_j - f_i\\bigr)[c, k],\n\\]\nwhere \\(w(\\cdot)\\) is a periodic wrapping function and \\(\\mathcal{F}_T\\) denotes the Fourier transform over the periodic domain. This shows that the relative positions \\(f_j - f_i\\) are preserved under translation \\(t\\), making the score invariant to periodic shifts:\n\\[\n\\hat{\\epsilon}_F(L, F + t, A, t) = \\hat{\\epsilon}_F(L, F, A, t).\n\\]\n\n---\n\n### 3. **Combined Periodic E(3) Invariance**\nThe full denoising process combines:\n- **O(3)-equivariance** in the lattice score \\(\\hat{\\epsilon}_L\\) (handling rotations/reflections).\n- **Periodic translation invariance** in the coordinate score \\(\\hat{\\epsilon}_F\\) (handling shifts modulo the lattice periodicity).\n\nSince E(3) is generated by orthogonal transformations (O(3)) and translations, and the periodic setting restricts translations to those modulo the lattice, the generated distribution is invariant under the **periodic E(3) group**.\n\n---\n\n### Summary:\n1. \\(\\hat{\\epsilon}_L\\) is O(3)-equivariant due to the invariance of \\(L^T L\\) under orthogonal transformations.\n2. \\(\\hat{\\epsilon}_F\\) is periodic translation invariant due to the Fourier-based preservation of relative positions.\n3. Together, they ensure the generated distribution is periodic E(3) invariant. \n\nThus, the proof is complete. \\(\\square\\)", "ground-truth": "B"}
{"paper": "arxiv:2303.10876", "question": "How can we prove Theorem 1, which states the network s modules satisfy the listed equivariance and invariance properties for any translation vector $t$ and rotation (or reflection) matrix $R$? Specifically, how do Equations (1a) through (1e) ensure that $G^{(0)}R + t, H^{(0)} = F_{IL}(X_R + t)$, $\\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)})$, $G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\})$, $H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)})$, and $\\hat{Y}R + t = F_{EOL}(G^{(L)}R + t)$ collectively prove the claim?", "A": "To prove Theorem 1, we demonstrate that each module in the network adheres to the equivariance or invariance properties under translation \\( t \\) and rotation/reflection \\( R \\), as formalized by Equations (1a)(1e). The proof follows by induction over the network layers, leveraging the structure of these equations.\n\n### **1. Initial Layer (\\( F_{IL} \\)): Invariance of \\( H^{(0)} \\)**\nEquation (1a): \\( H^{(0)} = F_{IL}(X_R + t) \\).  \n- **Input**: \\( X_R + t = XR + t \\) (transformed coordinates).  \n- **Output**: \\( H^{(0)} \\) must be invariant.  \n- **Mechanism**: \\( F_{IL} \\) computes invariant features (e.g., pairwise distances, angles, or mean-centered coordinates) that are unaffected by \\( R \\) and \\( t \\). For example, if \\( F_{IL} \\) subtracts the centroid \\( \\mu = \\frac{1}{N}\\sum_i (XR + t)_i = R\\mu_X + t \\), the centered coordinates become \\( (XR + t) - (R\\mu_X + t) = R(X - \\mu_X) \\). To ensure invariance, \\( F_{IL} \\) further processes these into rotation-invariant scalars (e.g., \\( \\|X_i - \\mu_X\\| \\)).  \n- **Result**: \\( H^{(0)} \\) is invariant: \\( H^{(0)}(XR + t) = H^{(0)}(X) \\).\n\n### **2. Interaction Relation Module (\\( F_{IRM} \\)): Invariance of \\( \\{c_{ij}\\} \\)**\nEquation (1b): \\( \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}) \\).  \n- **Inputs**:  \n  - Transformed geometric features \\( G^{(0)}R + t \\).  \n  - Invariant features \\( H^{(0)} \\).  \n- **Output**: \\( \\{c_{ij}\\} \\) must be invariant.  \n- **Mechanism**: \\( F_{IRM} \\) uses invariant operations on \\( G^{(0)}R + t \\), such as computing relative displacements \\( (G^{(0)}_iR + t) - (G^{(0)}_jR + t) = R(G^{(0)}_i - G^{(0)}_j) \\), and then extracting invariants (e.g., \\( \\|G^{(0)}_i - G^{(0)}_j\\| \\)) or using \\( H^{(0)} \\).  \n- **Result**: \\( \\{c_{ij}\\} \\) is invariant: \\( \\{c_{ij}\\}(G^{(0)}R + t, H^{(0)}) = \\{c_{ij}\\}(G^{(0)}, H^{(0)}) \\).\n\n### **3. Equivariant Geometric Feature Layer (\\( F_{EGFL}^{(l)} \\)): Equivariance of \\( G^{(l)} \\)**\nEquation (1c): \\( G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\}) \\).  \n- **Inductive Hypothesis**: Assume \\( G^{(l)}R + t \\) is equivariant at layer \\( l \\).  \n- **Mechanism**: \\( F_{EGFL}^{(l)} \\) uses equivariant operations (e.g., vector neurons, steerable convolutions) that commute with \\( R \\) and \\( t \\). For example, updating \\( G^{(l)} \\) via:  \n  \\[\n  G^{(l+1)}_i = \\sum_j c_{ij} \\cdot (G^{(l)}_j - G^{(l)}_i),\n  \\]  \n  where \\( c_{ij} \\) are invariant. Applying \\( R \\) and \\( t \\):  \n  \\[\n  G^{(l+1)}_iR + t = \\sum_j c_{ij} \\cdot \\left(R(G^{(l)}_j - G^{(l)}_i) + t - t\\right) = R\\left(\\sum_j c_{ij} \\cdot (G^{(l)}_j - G^{(l)}_i)\\right) + t.\n  \\]  \n- **Result**: \\( G^{(l+1)} \\) is equivariant: \\( G^{(l+1)}(G^{(l)}R + t) = G^{(l+1)}(G^{(l)})R + t \\).\n\n### **4. Invariant Pointwise Feature Layer (\\( F_{IPFL}^{(l)} \\)): Invariance of \\( H^{(l)} \\)**\nEquation (1d): \\( H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}) \\).  \n- **Inputs**:  \n  - Transformed geometric features \\( G^{(l)}R + t \\).  \n  - Invariant features \\( H^{(l)} \\).  \n- **Mechanism**: \\( F_{IPFL}^{(l)} \\) uses invariant operations on \\( G^{(l)}R + t \\), such as computing \\( \\|G^{(l)}_iR + t - G^{(l)}_jR + t\\| = \\|G^{(l)}_i - G^{(l)}_j\\| \\), or aggregating \\( H^{(l)} \\) with invariant functions (e.g., sum/max over neighbors).  \n- **Result**: \\( H^{(l+1)} \\) is invariant: \\( H^{(l+1)}(G^{(l)}R + t, H^{(l)}) = H^{(l+1)}(G^{(l)}, H^{(l)}) \\).\n\n### **5. Equivariant Output Layer (\\( F_{EOL} \\)): Equivariance of \\( \\hat{Y} \\)**\nEquation (1e): \\( \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t) \\).  \n- **Mechanism**: \\( F_{EOL} \\) maps the final equivariant features \\( G^{(L)}R + t \\) to the output space using equivariant operations. For example, a linear projection \\( \\hat{Y} = WG^{(L)} \\) satisfies:  \n  \\[\n  F_{EOL}(G^{(L)}R + t) = W(G^{(L)}R + t) = (WG^{(L)})R + Wt.\n  \\]  \n  If \\( Wt = t \\) (e.g., \\( W \\) is identity for translations), equivariance holds.  \n- **Result**: \\( \\hat{Y} \\) is equivariant: \\( \\hat{Y}(G^{(L)}R + t) = \\hat{Y}(G^{(L)})R + t \\).\n\n### **Conclusion**\nBy induction:  \n- **Base Case**: \\( H^{(0)} \\) is invariant (Equation 1a).  \n- **Inductive Step**: If \\( G^{(l)} \\) is equivariant and \\( H^{(l)} \\) is invariant at layer \\( l \\), then \\( G^{(l+1)} \\) remains equivariant (Equation 1c) and \\( H^{(l+1)} \\) remains invariant (Equation 1d).  \n- **Final Output**: \\( \\hat{Y} \\) is equivariant (Equation 1e).  \n\nThus, the networks modules collectively ensure the claimed equivariance and invariance properties.", "B": "To prove Theorem 1, we systematically verify the equivariance and invariance properties of each module in the network under any translation vector $t$ and rotation (or reflection) matrix $R$. The proof is structured around the following key points:\n\n1. Initialization Layer ($F_{IL}(\\cdot)$): The initial geometric feature $G^{(0)}$ is designed to be equivariant, meaning that applying a transformation $R$ and translation $t$ to the input $X$ results in a corresponding transformation of $G^{(0)}$. The initial pattern feature $H^{(0)}$ is invariant, remaining unchanged under such transformations. This is encapsulated by the equation:\n   \\[ G^{(0)}R + t, \\quad H^{(0)} = F_{IL}(X_R + t). \\]\n\n2. Interaction Reasoning Module ($F_{IRM}(\\cdot)$): This module processes the transformed geometric features and invariant pattern features to produce interaction categorical vectors $\\{c_{ij}\\}$ that are invariant to transformations, as shown by:\n   \\[ \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}). \\]\n\n3. Geometric Feature Learning Layer ($F_{EGFL}^{(l)}(\\cdot)$): At each layer $l$, the geometric features are updated in a manner that preserves equivariance. The transformation properties are maintained through the layer's operations, ensuring that:\n   \\[ G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R - t, H^{(l)}, \\{c_{ij}\\}). \\]\n\n4. Pattern Feature Learning Layer ($F_{IPFL}^{(l)}(\\cdot)$): Similarly, the pattern features are updated in a way that maintains their invariance under transformations, as described by:\n   \\[ H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}). \\]\n\n5. Output Layer ($F_{EOL}(\\cdot)$): Finally, the output layer produces predictions that are equivariant to the input transformations, completing the proof with:\n   \\[ \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t). \\]\n\nBy demonstrating that each component of the network adheres to these transformation properties, we collectively establish the overall equivariance and invariance claims of Theorem 1. The detailed operations within each module, including attention mechanisms and non-linear functions, are carefully designed to preserve these properties, ensuring the network's behavior is consistent under Euclidean transformations.", "C": "To prove Theorem 1, we systematically verify the equivariance and invariance properties of each module in the network under any translation vector $t$ and rotation (or reflection) matrix $R$. The proof is structured around the following key points:\n\n1. Initialization Layer ($F_{IL}(\\cdot)$): The initial geometric feature $G^{(0)}$ is designed to be equivariant, meaning that applying a transformation $R$ and translation $t$ to the input $X$ results in a corresponding transformation of $G^{(0)}$. The initial pattern feature $H^{(0)}$ is invariant, remaining unchanged under such transformations. This is encapsulated by the equation:\n   \\[ G^{(0)}R + t, \\quad H^{(0)} = F_{IL}(X_R + t). \\]\n\n2. Interaction Reasoning Module ($F_{IRM}(\\cdot)$): This module processes the transformed geometric features and invariant pattern features to produce interaction categorical vectors $\\{c_{ij}\\}$ that are invariant to transformations, as shown by:\n   \\[ \\{c_{ij}\\} = F_{IRM}(G^{(0)}R + t, H^{(0)}). \\]\n\n3. Geometric Feature Learning Layer ($F_{EGFL}^{(l)}(\\cdot)$): At each layer $l$, the geometric features are updated in a manner that preserves equivariance. The transformation properties are maintained through the layer's operations, ensuring that:\n   \\[ G^{(l+1)}R + t = F_{EGFL}^{(l)}(G^{(l)}R + t, H^{(l)}, \\{c_{ij}\\}). \\]\n\n4. Pattern Feature Learning Layer ($F_{IPFL}^{(l)}(\\cdot)$): Similarly, the pattern features are updated in a way that maintains their invariance under transformations, as described by:\n   \\[ H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)}R + t, H^{(l)}). \\]\n\n5. Output Layer ($F_{EOL}(\\cdot)$): Finally, the output layer produces predictions that are equivariant to the input transformations, completing the proof with:\n   \\[ \\hat{Y}R + t = F_{EOL}(G^{(L)}R + t). \\]\n\nBy demonstrating that each component of the network adheres to these transformation properties, we collectively establish the overall equivariance and invariance claims of Theorem 1. The detailed operations within each module, including attention mechanisms and non-linear functions, are carefully designed to preserve these properties, ensuring the network's behavior is consistent under Euclidean transformations.", "D": "To prove Theorem 1, which asserts that the network's modules satisfy the listed equivariance and invariance properties under translations \\( t \\) and rotations/reflections \\( R \\), we need to carefully analyze how each equation (1a) through (1e) contributes to the overall equivariance or invariance of the network's transformations. Below is a step-by-step proof:\n\n---\n\n### **Proof of Theorem 1**\n\nWe proceed by induction over the layers of the network, showing that each module preserves the claimed properties.\n\n#### **1. Base Case: Input Layer (\\( l = 0 \\))**\n- **Equation (1a):**  \n  The input layer \\( F_{IL} \\) processes the input point cloud \\( X \\). The claim states:\n  \\[\n  G^{(0)} R + t, H^{(0)} = F_{IL}(X R + t).\n  \\]\n  - If \\( F_{IL} \\) is **equivariant** to \\( R \\) and \\( t \\), then processing the transformed input \\( X R + t \\) yields transformed features \\( G^{(0)} R + t \\) (equivariant) and invariant features \\( H^{(0)} \\).  \n  - This ensures that the initial features respect the transformation.\n\n#### **2. Invariant Relation Module (\\( F_{IRM} \\))**\n- **Equation (1b):**  \n  The relation module computes pairwise interactions:\n  \\[\n  \\{c_{ij}\\} = F_{IRM}(G^{(0)} R + t, H^{(0)}).\n  \\]\n  - Since \\( F_{IRM} \\) takes \\( G^{(0)} R + t \\) (equivariant) and \\( H^{(0)} \\) (invariant) as inputs, and outputs \\(\\{c_{ij}\\}\\), these relations must be **invariant** to \\( R \\) and \\( t \\).  \n  - This is because \\( c_{ij} \\) typically depends on distances or angles, which are preserved under rigid transformations.\n\n#### **3. Inductive Step: Equivariant Graph Filter Layer (\\( F_{EGFL}^{(l)} \\))**\n- **Equation (1c):**  \n  The graph filter updates the equivariant features:\n  \\[\n  G^{(l+1)} R + t = F_{EGFL}^{(l)}(G^{(l)} R + t, H^{(l)}, \\{c_{ij}\\}).\n  \\]\n  - By the induction hypothesis, assume \\( G^{(l)} R + t \\) is equivariant and \\( H^{(l)} \\), \\(\\{c_{ij}\\}\\) are invariant.  \n  - If \\( F_{EGFL}^{(l)} \\) is designed to preserve equivariance (e.g., using linear transformations or message passing that respects \\( R \\) and \\( t \\)), then \\( G^{(l+1)} R + t \\) remains equivariant.\n\n#### **4. Invariant Pointwise Filter Layer (\\( F_{IPFL}^{(l)} \\))**\n- **Equation (1d):**  \n  The pointwise filter updates the invariant features:\n  \\[\n  H^{(l+1)} = F_{IPFL}^{(l)}(G^{(l)} R + t, H^{(l)}).\n  \\]\n  - Since \\( G^{(l)} R + t \\) is equivariant and \\( H^{(l)} \\) is invariant, \\( F_{IPFL}^{(l)} \\) must process them in a way that \\( H^{(l+1)} \\) remains **invariant**.  \n  - This can be achieved by only using invariant operations (e.g., norms, inner products, or MLPs on invariant inputs).\n\n#### **5. Equivariant Output Layer (\\( F_{EOL} \\))**\n- **Equation (1e):**  \n  The final output is computed as:\n  \\[\n  \\hat{Y} R + t = F_{EOL}(G^{(L)} R + t).\n  \\]\n  - Since \\( G^{(L)} R + t \\) is equivariant by the induction, \\( F_{EOL} \\) must preserve this property.  \n  - If \\( F_{EOL} \\) applies an equivariant transformation (e.g., a linear layer), then \\( \\hat{Y} R + t \\) is correctly transformed.\n\n---\n\n### **Conclusion**\nBy induction:\n1. The **equivariant features** (\\( G^{(l)} \\)) transform as \\( G^{(l)} R + t \\) at every layer.  \n2. The **invariant features** (\\( H^{(l)} \\)) and relations (\\( \\{c_{ij}\\} \\)) remain unchanged under \\( R \\) and \\( t \\).  \n3. The **final output** \\( \\hat{Y} \\) transforms equivariantly as \\( \\hat{Y} R + t \\).\n\nThus, the network satisfies the claimed equivariance and invariance properties under arbitrary translations \\( t \\) and rotations/reflections \\( R \\). \\(\\square\\)", "ground-truth": "C"}
{"paper": "arxiv:2303.10876", "question": "Starting from the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features, how do we obtain $G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})R + t$? Show how the transformation $R$ and translation $t$ commute with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$ to yield $G_i^{(l+1)} R + t$ again.", "A": "To understand how $G_i^{(l+1)} R + t$ is obtained from the given equation, let's break down the process step by step, focusing on how the transformation $R$ and translation $t$ interact with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$. Here's the reasoning:\n\n1. Initial Definition: We start with the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features:\n   $$G_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})$$\n\n2. Apply Transformation and Translation: We then apply the transformation $R$ and translation $t$ to $G_i^{(l+1)}$:\n   $$G_i^{(l+1)} R + t = \\left(G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\\right) R + t$$\n\n3. Distribute $R$ and $t$: Distribute the transformation $R$ and the translation $t$ inside the parentheses:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R + t$$\n   Here, we see that $R$ is distributed over the difference $(G_j^{(l)} - G_i^{(l)})$, and $t$ is added to each term.\n\n4. Simplify the Expression: Notice that the translation $t$ appears twice in the equation. However, since $t$ is a constant translation vector, adding it once is sufficient. Thus, the equation simplifies to:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This simplification shows that the transformation $R$ commutes with the difference operation, and the translation $t$ is applied once to the entire expression.\n\n5. Final Form: The final form demonstrates that the transformation and translation preserve the structure of the neighbor aggregation, leading to the equivariance property:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This confirms that $G_i^{(l+1)} R + t$ is indeed obtained as described, with $R$ and $t$ commuting appropriately with the operations involved.", "B": "To understand how $G_i^{(l+1)} R + t$ is obtained from the given equation, let's break down the process step by step, focusing on how the transformation $R$ and translation $t$ interact with the summation and the difference $(G_j^{(l)} - G_i^{(l)})$. Here's the reasoning:\n\n1. Initial Definition: We start with the definition of $G_i^{(l+1)}$ as an aggregation of neighbor features:\n   $$G_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})$$\n\n2. Apply Transformation and Translation: We then apply the transformation $R$ and translation $t$ to $G_i^{(l+1)}$:\n   $$G_i^{(l+1)} R + t = \\left(G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\\right) R + t$$\n\n3. Distribute $R$ and $t$: Distribute the transformation $R$ and the translation $t$ inside the parentheses:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R$$\n   Here, we see that $R$ is distributed over the difference $(G_j^{(l)} - G_i^{(l)})$, but the translation $t$ is only added to the first term.\n\n4. Simplify the Expression: Notice that the translation $t$ appears only once in the equation. However, since $t$ is a constant translation vector, it should be added to the entire expression. Thus, the equation should be:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This simplification shows that the transformation $R$ commutes with the difference operation, but the translation $t$ is not properly distributed.\n\n5. Final Form: The final form demonstrates that the transformation and translation preserve the structure of the neighbor aggregation, leading to the equivariance property:\n   $$G_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)$$\n   This confirms that $G_i^{(l+1)} R + t$ is indeed obtained as described, with $R$ commuting appropriately but $t$ not being fully distributed.", "C": "Starting from the definition of the updated node feature $G_i^{(l+1)}$ as a neighborhood aggregation:\n\n$$\nG_i^{(l+1)} = G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)\n$$\n\nWe aim to show how applying the transformation $R$ (rotation) and translation $t$ preserves the structure when commuting with the aggregation. Specifically, we derive:\n\n$$\nG_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R + t\n$$\n\n**Step-by-Step Derivation:**\n\n1. **Apply $R$ and $t$ to $G_i^{(l+1)}$:**\n   $$\n   G_i^{(l+1)} R + t = \\left[G_i^{(l)} + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)\\right] R + t\n   $$\n\n2. **Distribute $R$ over the summation:**\n   $$\n   = G_i^{(l)} R + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R + t\n   $$\n\n3. **Handle the translation $t$:**\n   - The translation $t$ is additive and commutes with linear operations. Since $t$ is applied *after* the rotation $R$, it remains outside the summation:\n   $$\n   = G_i^{(l)} R + t + \\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R\n   $$\n\n4. **Final alignment with the target expression:**\n   - The last term $\\sum_{j\\in N_i} e_{ij}^{(l)} \\cdot \\left(G_j^{(l)} - G_i^{(l)}\\right)R$ inherently excludes $t$ because:\n     - The difference $\\left(G_j^{(l)} - G_i^{(l)}\\right)$ cancels any shared translation between neighbors.\n     - The rotation $R$ acts linearly on the difference, preserving relative geometric relationships.\n\n**Key Observations:**\n- **Linear transformation:** $R$ distributes over the summation and differences because it is a linear operator.\n- **Translation invariance:** The subtraction $\\left(G_j^{(l)} - G_i^{(l)}\\right)$ removes absolute positional information (invariant to $t$), leaving only relative differences to be scaled by $e_{ij}^{(l)}$ and rotated by $R$.\n- **Final composition:** The term $G_i^{(l)} R + t$ represents the transformed \"base\" feature, while the summation term accounts for transformed neighborhood differences.\n\nThus, the equality holds as the transformation $R$ and translation $t$ commute with the aggregation mechanics. The apparent extra $t$ in the original statement arises from emphasizing the composition of transformations, but it is absorbed into the final additive structure consistently.", "D": "Let's carefully derive the given expression step by step, showing how the transformation \\( R \\) and translation \\( t \\) commute with the aggregation operation.\n\n### Starting Definition\nThe feature \\( G_i^{(l+1)} \\) is defined as an aggregation of neighbor features:\n\\[\nG_i^{(l+1)} = G_i^{(l)} + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)})\n\\]\nwhere:\n- \\( G_i^{(l)} \\) is the feature of node \\( i \\) at layer \\( l \\),\n- \\( N_i \\) is the set of neighbors of node \\( i \\),\n- \\( e_{ij}^{(l)} \\) is the attention coefficient or edge weight between nodes \\( i \\) and \\( j \\) at layer \\( l \\).\n\n### Applying Transformation \\( R \\) and Translation \\( t \\)\nWe want to compute \\( G_i^{(l+1)} R + t \\). Substituting the definition of \\( G_i^{(l+1)} \\):\n\\[\nG_i^{(l+1)} R + t = \\left( G_i^{(l)} + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) \\right) R + t\n\\]\n\n### Distributing \\( R \\) and \\( t \\)\nDistribute the transformation \\( R \\) over the summation:\n\\[\n= G_i^{(l)} R + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R + t\n\\]\n\nNow, observe that the translation \\( t \\) is added only once at the end, not inside the summation. This is because translations are additive and commute with linear transformations (like \\( R \\)) in affine spaces. Thus:\n\\[\n= G_i^{(l)} R + t + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R\n\\]\n\n### Simplifying the Difference Inside the Sum\nThe term \\( (G_j^{(l)} - G_i^{(l)}) R \\) can be expanded using the linearity of \\( R \\):\n\\[\n(G_j^{(l)} - G_i^{(l)}) R = G_j^{(l)} R - G_i^{(l)} R\n\\]\nThus, the summation becomes:\n\\[\n\\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)\n\\]\n\n### Final Expression\nSubstituting back, we get:\n\\[\nG_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} R - G_i^{(l)} R)\n\\]\nThis matches the given expression:\n\\[\nG_i^{(l+1)} R + t = G_i^{(l)} R + t + \\sum_{j \\in N_i} e_{ij}^{(l)} \\cdot (G_j^{(l)} - G_i^{(l)}) R\n\\]\n\n### Key Observations\n1. **Linearity of \\( R \\)**: The transformation \\( R \\) distributes over the difference \\( (G_j^{(l)} - G_i^{(l)}) \\), allowing us to write \\( (G_j^{(l)} - G_i^{(l)}) R = G_j^{(l)} R - G_i^{(l)} R \\).\n2. **Translation \\( t \\)**: The translation is added only once at the end because it is a global shift applied after the linear transformation and aggregation. The difference \\( (G_j^{(l)} - G_i^{(l)}) \\) cancels out any translations, as \\( t - t = 0 \\).\n\nThus, the transformation \\( R \\) and translation \\( t \\) commute with the aggregation and difference operations, preserving the structure of the update rule.", "ground-truth": "A"}
{"paper": "A Connection Between Score Matching and Denoising Autoencoders", "question": "Starting from the expression:\n\\[ S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right], \\]\nhow can we rewrite it using the definition \n\\[ q_\\sigma(x) = \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}? \\]", "A": "To rewrite the expression \\( S(\\theta) \\) using the definition of \\( q_\\sigma(x) \\), we proceed as follows:\n\n1. **Given Definitions:**\n   \\[\n   S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right],\n   \\]\n   \\[\n   q_\\sigma(x) = \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}.\n   \\]\n\n2. **Compute the Gradient \\( \\partial_x q_\\sigma(x) \\):**\n   Since \\( q_\\sigma(x) \\) is defined as an integral over \\( \\tilde{x} \\), we can bring the gradient inside the integral:\n   \\[\n   \\partial_x q_\\sigma(x) = \\partial_x \\left( \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right) = \\int_{\\tilde{x}} \\partial_x q_\\sigma(x|\\tilde{x}) \\, q(\\tilde{x}) \\, d\\tilde{x}.\n   \\]\n\n3. **Substitute into \\( S(\\theta) \\):**\n   Replace \\( \\partial_x q_\\sigma(x) \\) in \\( S(\\theta) \\):\n   \\[\n   S(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\int_{\\tilde{x}} \\partial_x q_\\sigma(x|\\tilde{x}) \\, q(\\tilde{x}) \\, d\\tilde{x} \\rangle \\right].\n   \\]\n\n4. **Interchange Expectation and Integral:**\n   The expectation \\( \\mathbb{E}_{q_\\sigma(x)} \\) is an integral over \\( x \\), so we can interchange the order of integration (assuming Fubini's theorem applies):\n   \\[\n   S(\\theta) = \\int_x \\langle \\psi(x;\\theta), \\int_{\\tilde{x}} \\partial_x q_\\sigma(x|\\tilde{x}) \\, q(\\tilde{x}) \\, d\\tilde{x} \\rangle \\, q_\\sigma(x) \\, dx.\n   \\]\n   This can be written as:\n   \\[\n   S(\\theta) = \\int_{\\tilde{x}} \\left( \\int_x \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle \\, q_\\sigma(x) \\, dx \\right) q(\\tilde{x}) \\, d\\tilde{x}.\n   \\]\n\n5. **Final Expression:**\n   The inner integral is an expectation over \\( q_\\sigma(x) \\), so we can write:\n   \\[\n   S(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right].\n   \\]\n\n   Alternatively, if we recognize that \\( q_\\sigma(x) \\) is the marginal distribution, we can also express it as:\n   \\[\n   S(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\log q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right],\n   \\]\n   where we used the identity \\( \\partial_x q_\\sigma(x|\\tilde{x}) = q_\\sigma(x|\\tilde{x}) \\partial_x \\log q_\\sigma(x|\\tilde{x}) \\).\n\n**Final Answer:**\n\\[\nS(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\log q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right].\n\\]", "B": "To rewrite the expression for \\( S(\\theta) \\), we start by substituting the definition of \\( q_\\sigma(x) \\) into the original equation. This gives us:\n\\[ S(\\theta) = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} q_\\sigma(x) \\right\\rangle dx = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} \\int_{\\tilde{x}} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx. \\]\nHere, we have interchanged the derivative and integral operations, assuming the conditions for doing so are met. This step is crucial for moving forward with the derivation, as it allows us to work directly with the conditional probability density function \\( q_\\sigma(x|\\tilde{x}) \\) and the marginal density \\( q(\\tilde{x}) \\).", "C": "Starting from the original expression:\n\\[\nS(\\theta) = \\mathbb{E}_{q_\\sigma(x)} \\left[ \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x) \\rangle \\right],\n\\]\nwe substitute the definition \\( q_\\sigma(x) = \\int q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\). The gradient term becomes:\n\\[\n\\partial_x q_\\sigma(x) = \\int \\partial_x q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x}.\n\\]\nSubstituting this into \\( S(\\theta) \\), we have:\n\\[\nS(\\theta) = \\int \\left\\langle \\psi(x;\\theta), \\int \\partial_x q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx.\n\\]\nBy Fubini's theorem, we interchange the order of integration:\n\\[\nS(\\theta) = \\int q(\\tilde{x}) \\left( \\int \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle dx \\right) d\\tilde{x}.\n\\]\nRecognizing that \\( \\partial_x q_\\sigma(x|\\tilde{x}) = q_\\sigma(x|\\tilde{x}) \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\), the inner integral becomes an expectation over \\( q_\\sigma(x|\\tilde{x}) \\):\n\\[\n\\int \\langle \\psi(x;\\theta), \\partial_x q_\\sigma(x|\\tilde{x}) \\rangle dx = \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\rangle \\right].\n\\]\nThus, the final expression for \\( S(\\theta) \\) is:\n\\[\nS(\\theta) = \\mathbb{E}_{q(\\tilde{x})} \\left[ \\mathbb{E}_{q_\\sigma(x|\\tilde{x})} \\left[ \\langle \\psi(x;\\theta), \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\rangle \\right] \\right].\n\\]\nThis represents a nested expectation over \\( \\tilde{x} \\) and \\( x \\), utilizing the score function \\( \\partial_x \\ln q_\\sigma(x|\\tilde{x}) \\).", "D": "To rewrite the expression for \\( S(\\theta) \\), we start by substituting the definition of \\( q_\\sigma(x) \\) into the original equation. This gives us:\n\\[ S(\\theta) = \\int_x \\left\\langle \\psi(x;\\theta), \\frac{\\partial}{\\partial x} q_\\sigma(x) \\right\\rangle dx = \\int_x \\left\\langle \\psi(x;\\theta), \\int_{\\tilde{x}} \\frac{\\partial}{\\partial x} q_\\sigma(x|\\tilde{x}) q(\\tilde{x}) \\, d\\tilde{x} \\right\\rangle dx. \\]\nHere, we have interchanged the derivative and integral operations, assuming the conditions for doing so are met. This step is crucial for moving forward with the derivation, as it allows us to work directly with the conditional probability density function \\( q_\\sigma(x|\\tilde{x}) \\) and the marginal density \\( q(\\tilde{x}) \\).", "ground-truth": "B"}
{"paper": "arxiv:2405.16577", "question": "Prove the following theorem:\n\\begin{theorem}[Wasserstein Bound]\nAssume $\\Omega$ is convex and the velocity model $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$ (uniformly on $t$). \nLet $p_{\\bm{\\theta},t}(\\bm{x})$ be the probability path of the reflected CNF induced by $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ starting from the same prior distribution $p_0(\\bm{x})$.\nThen the squared Wasserstein-2 distance between $p_{1}(\\bm{x})$ and $p_{\\bm{\\theta},1}(\\bm{x})$ is bounded by\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\n\\end{theorem}", "A": "We first give an alternative expression of the reflected ODE\n\\begin{subequations}\n\\arraycolsep=1.8pt\n\\def\\arraystretch{1.5}\n\\begin{align}\n\\mathrm{d}\\bm{\\phi}_t(\\bm{x}) &= \\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t,\\\\\n\\bm{\\phi}_0(\\bm{x}) &= \\bm{x},\n\\end{align}\n\\end{subequations}\nwhere the initial point $\\bm{x}\\sim p_0(\\bm{x})$, $\\bm{n}(\\bm{x})$ is the inward unit normal vector at $\\bm{x}$ on $\\partial\\Omega$, and $l_t$ is non-decreasing in $t$ with $l_0=0$.\nMoreover, $l_t$ satisfies $\\int_{0}^t \\mathbb{I}(\\bm{\\phi}_s(\\bm{x})\\notin\\partial\\Omega)\\dif l_s=0$ for $t>0$, i.e., $\\dif l_t$ vanishes in the interior and will push the velocity along $\\bm{n}(\\bm{x})$ when the trajectory hits the boundary.\n\nLet $\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})$ (with a reflection term $\\bar{l}_t$) be the solution to the reflected ODE with velocity field $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$.\nBy the definition of Wasserstein-2 distance, we have\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq \\int_{\\Omega}\\|\\bm{\\phi}_1(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},1}(\\bm{x})\\|^2 p_0(\\bm{x})\\dif \\bm{x}:= \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=1}.\n\\end{equation}\nTake the derivative of $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)$ w.r.t. $t$ gives\n\\begin{equation}\n\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\n=2\\int_{\\Omega}\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right) p_0(\\bm{x})\\dif \\bm{x}\n\\end{equation}\nNote that\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left[(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t-\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\right]\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t \\\\\n&\\quad\\quad\\quad\\quad\\quad+ \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t - \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t,\n\\end{align*}\nThe first term can be bounded by\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n=&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t))+\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n\\leq & \\frac{1}{2}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2+\\frac{1}{2}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2 + M\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\n\\end{align*}\nwhere in the last inequality we use the mean inequality and the fact that $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$.\nTo handle the second term, note that $dl_t>0$ only if $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$. When $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$, we know\n$\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\leq 0$ due to the convexity of $\\Omega$.\nTherefore, the second term satisfies\n\\[\n\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t\\leq 0.\n\\]\nUsing the same argument, we know that the third term satisfies\n\\[\n-\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\leq 0\n\\]\nTherefore, we have the following bound for $\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t$\n\\begin{align*}\n&\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t\\\\\n\\leq & \\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2p_0(\\bm{x})\\dif \\bm{x}+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x} + 2M\\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\np_0(\\bm{x})\\dif \\bm{x}\\\\\n=&(1+2M)\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}.\n\\end{align*}\nBy Gronwall's inequality and $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=0}=0$, we have\n\\begin{equation}\n\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right) \\leq e^{1+2M}\\int_0^1\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}\\dif t=e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\nBy combining the above equations, we finally conclude\n\\[\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{1+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nThis finishes the proof.", "B": "To prove the Wasserstein Bound theorem, we analyze the squared Wasserstein-2 distance between the true distribution \\(p_1(\\bm{x})\\) and the model distribution \\(p_{\\bm{\\theta},1}(\\bm{x})\\) induced by the velocity field \\(\\bm{v}_{\\bm{\\theta}}\\). The key steps involve comparing the particle trajectories governed by the true and model velocity fields, leveraging Gronwall's inequality, and bounding the accumulated error via the RFM loss.\n\n**Step 1: Trajectory Difference Dynamics**\n\nLet \\(\\bm{\\phi}_t(\\bm{x}_0)\\) and \\(\\bm{\\psi}_t(\\bm{x}_0)\\) denote the true and model trajectories starting from \\(\\bm{x}_0 \\sim p_0\\), satisfying:\n\\[\n\\frac{d\\bm{\\phi}_t}{dt} = \\bm{v}^*(\\bm{\\phi}_t,t), \\quad \\frac{d\\bm{\\psi}_t}{dt} = \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t),\n\\]\nwith \\(\\bm{\\phi}_0 = \\bm{\\psi}_0 = \\bm{x}_0\\). Define the displacement \\(\\bm{\\delta}_t = \\bm{\\phi}_t - \\bm{\\psi}_t\\). Its dynamics are:\n\\[\n\\frac{d\\bm{\\delta}_t}{dt} = \\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t).\n\\]\n\n**Step 2: Differential Inequality for \\(\\|\\bm{\\delta}_t\\|^2\\)**\n\nCompute the time derivative of \\(\\|\\bm{\\delta}_t\\|^2\\):\n\\[\n\\frac{d}{dt}\\|\\bm{\\delta}_t\\|^2 = 2\\bm{\\delta}_t \\cdot \\left[\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t)\\right].\n\\]\nDecompose the velocity difference:\n\\[\n\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t) = \\underbrace{\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t)}_{\\text{Model error}} + \\underbrace{\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t)}_{\\text{Lipschitz term}}.\n\\]\nUsing the \\(M\\)-Lipschitz condition of \\(\\bm{v}_{\\bm{\\theta}}\\) and Cauchy-Schwarz:\n\\[\n\\left\\|\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\psi}_t,t)\\right\\| \\leq M\\|\\bm{\\delta}_t\\|, \\quad \\bm{\\delta}_t \\cdot \\left(\\bm{v}^* - \\bm{v}_{\\bm{\\theta}}\\right) \\leq \\|\\bm{\\delta}_t\\|\\|\\bm{v}^* - \\bm{v}_{\\bm{\\theta}}\\|.\n\\]\nThus:\n\\[\n\\frac{d}{dt}\\|\\bm{\\delta}_t\\|^2 \\leq 2\\|\\bm{\\delta}_t\\|\\|\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t)\\| + 2M\\|\\bm{\\delta}_t\\|^2.\n\\]\nApply the inequality \\(2ab \\leq a^2 + b^2\\) (with \\(a = \\|\\bm{\\delta}_t\\|\\), \\(b = \\|\\bm{v}^* - \\bm{v}_{\\bm{\\theta}}\\|\\)):\n\\[\n\\frac{d}{dt}\\|\\bm{\\delta}_t\\|^2 \\leq (1 + 2M)\\|\\bm{\\delta}_t\\|^2 + \\|\\bm{v}^*(\\bm{\\phi}_t,t) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t,t)\\|^2.\n\\]\n\n**Step 3: Gronwall's Inequality**\n\nLet \\(y(t) = \\mathbb{E}[\\|\\bm{\\delta}_t\\|^2]\\). Taking expectations and applying Gronwall's inequality:\n\\[\ny(t) \\leq e^{(1 + 2M)t} \\int_0^t e^{-(1 + 2M)s} \\mathbb{E}\\left[\\|\\bm{v}^*(\\bm{\\phi}_s,s) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_s,s)\\|^2\\right] ds.\n\\]\nAt \\(t = 1\\):\n\\[\ny(1) \\leq e^{1 + 2M} \\int_0^1 \\mathbb{E}\\left[\\|\\bm{v}^*(\\bm{\\phi}_s,s) - \\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_s,s)\\|^2\\right] ds = e^{1 + 2M} \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}).\n\\]\n\n**Step 4: Relating to Wasserstein Distance**\n\nThe squared Wasserstein-2 distance is bounded by the expectation of the squared displacement:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq \\mathbb{E}[\\|\\bm{\\phi}_1 - \\bm{\\psi}_1\\|^2] = y(1).\n\\]\nCombining results:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq e^{1 + 2M} \\mathcal{L}_{\\text{RFM}}(\\bm{\\theta}).\n\\]\nThis completes the proof.", "C": "We first give an alternative expression of the reflected ODE\n\\begin{subequations}\n\\arraycolsep=1.8pt\n\\def\\arraystretch{1.5}\n\\begin{align}\n\\mathrm{d}\\bm{\\phi}_t(\\bm{x}) &= \\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t,\\\\\n\\bm{\\phi}_0(\\bm{x}) &= \\bm{x},\n\\end{align}\n\\end{subequations}\nwhere the initial point $\\bm{x}\\sim p_0(\\bm{x})$, $\\bm{n}(\\bm{x})$ is the inward unit normal vector at $\\bm{x}$ on $\\partial\\Omega$, and $l_t$ is non-decreasing in $t$ with $l_0=0$.\nMoreover, $l_t$ satisfies $\\int_{0}^t \\mathbb{I}(\\bm{\\phi}_s(\\bm{x})\\notin\\partial\\Omega)\\dif l_s=0$ for $t>0$, i.e., $\\dif l_t$ vanishes in the interior and will push the velocity along $\\bm{n}(\\bm{x})$ when the trajectory hits the boundary.\n\nLet $\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})$ (with a reflection term $\\bar{l}_t$) be the solution to the reflected ODE with velocity field $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$.\nBy the definition of Wasserstein-2 distance, we have\n\\begin{equation}\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq \\int_{\\Omega}\\|\\bm{\\phi}_1(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},1}(\\bm{x})\\|^2 p_0(\\bm{x})\\dif \\bm{x}:= \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=1}.\n\\end{equation}\nTake the derivative of $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)$ w.r.t. $t$ gives\n\\begin{equation}\n\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\n=2\\int_{\\Omega}\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right) p_0(\\bm{x})\\dif \\bm{x}\n\\end{equation}\nNote that\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left(\\dif\\bm{\\phi}_t(\\bm{x})-\\dif\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\left[(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t + \\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t-\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\right]\\\\\n=& \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\mathrm{d}t \\\\\n&\\quad\\quad\\quad\\quad\\quad+ \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t - \\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t,\n\\end{align*}\nThe first term can be bounded by\n\\begin{align*}\n&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n=&\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t(\\bm{x})\\right)(\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t))+\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)(\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}),t))\\\\\n\\leq & \\frac{1}{2}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2+\\frac{1}{2}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2 + M\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\n\\end{align*}\nwhere in the last inequality we use the mean inequality and the fact that $\\bm{v}_{\\bm{\\theta}}(\\bm{x},t)$ is $M$-Lipschitz in $\\bm{x}$.\nTo handle the second term, note that $dl_t>0$ only if $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$. When $\\bm{\\phi}_t(\\bm{x})\\in\\partial\\Omega$, we know\n$\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\leq 0$ due to the convexity of $\\Omega$.\nTherefore, the second term satisfies\n\\[\n\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_t(\\bm{x}))\\dif l_t\\leq 0.\n\\]\nUsing the same argument, we know that the third term satisfies\n\\[\n-\\left(\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\right)\\bm{n}(\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x}))\\dif \\bar{l}_t\\leq 0\n\\]\nTherefore, we have the following bound for $\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t$\n\\begin{align*}\n&\\dif \\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)/\\dif t\\\\\n\\leq & \\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2p_0(\\bm{x})\\dif \\bm{x}+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x} + 2M\\int_{\\Omega}\\|\\bm{\\phi}_t(\\bm{x})-\\bm{\\phi}_{\\bm{\\theta},t}(\\bm{x})\\|^2\np_0(\\bm{x})\\dif \\bm{x}\\\\\n=&(1+2M)\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)+\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}.\n\\end{align*}\nBy Gronwall's inequality and $\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right)\\big|_{t=0}=0$, we have\n\\begin{equation}\n\\hat{W}_2^2\\left(p_{t}(\\bm{x}),p_{\\bm{\\theta},t}(\\bm{x})\\right) \\leq e^{2+2M}\\int_0^1\\int_{\\Omega}\\|\\bm{v}_t(\\bm{\\phi}_t(\\bm{x}))-\\bm{v}_{\\bm{\\theta}}(\\bm{\\phi}_t(\\bm{x}),t)\\|^2p_0(\\bm{x})\\dif \\bm{x}\\dif t=e^{2+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\end{equation}\nBy combining the above equations, we finally conclude\n\\[\nW_2^2\\left(p_{1}(\\bm{x}),p_{\\bm{\\theta},1}(\\bm{x})\\right)\\leq e^{2+2M}\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}).\n\\]\nThis finishes the proof.", "D": "We will prove the Wasserstein bound theorem step-by-step. The key idea is to relate the Wasserstein distance between the true and learned distributions at time $t=1$ to the RFM (Reflected Flow Matching) loss $\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})$.\n\n### Step 1: Flow Representation\nLet $\\phi_t(\\bm{x})$ and $\\phi_{\\bm{\\theta},t}(\\bm{x})$ be the true and learned flow maps respectively, which satisfy:\n\\[\n\\frac{d}{dt}\\phi_t(\\bm{x}) = \\bm{v}_t(\\phi_t(\\bm{x})), \\quad \\phi_0(\\bm{x}) = \\bm{x}\n\\]\n\\[\n\\frac{d}{dt}\\phi_{\\bm{\\theta},t}(\\bm{x}) = \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t), \\quad \\phi_{\\bm{\\theta},0}(\\bm{x}) = \\bm{x}\n\\]\n\n### Step 2: Difference Between Flows\nDefine the difference between trajectories:\n\\[\n\\bm{u}_t(\\bm{x}) = \\phi_t(\\bm{x}) - \\phi_{\\bm{\\theta},t}(\\bm{x})\n\\]\nIts time derivative is:\n\\[\n\\frac{d}{dt}\\bm{u}_t(\\bm{x}) = \\bm{v}_t(\\phi_t(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\n\\]\n\n### Step 3: Decompose the Velocity Difference\nAdd and subtract $\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x}))$:\n\\[\n\\frac{d}{dt}\\bm{u}_t(\\bm{x}) = [\\bm{v}_t(\\phi_t(\\bm{x})) - \\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x}))] + [\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)]\n\\]\n\n### Step 4: Apply Lipschitz Condition\nSince $\\bm{v}_{\\bm{\\theta}}$ is $M$-Lipschitz in $\\bm{x}$ and $\\Omega$ is convex, we have:\n\\[\n\\|\\bm{v}_t(\\phi_t(\\bm{x})) - \\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x}))\\| \\leq M\\|\\bm{u}_t(\\bm{x})\\|\n\\]\nThus:\n\\[\n\\frac{d}{dt}\\|\\bm{u}_t(\\bm{x})\\| \\leq M\\|\\bm{u}_t(\\bm{x})\\| + \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\|\n\\]\n\n### Step 5: Apply Grnwall's Inequality\nThis differential inequality implies:\n\\[\n\\|\\bm{u}_1(\\bm{x})\\| \\leq e^M \\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\| dt\n\\]\n\n### Step 6: Relate to Wasserstein Distance\nThe squared Wasserstein-2 distance is bounded by:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq \\mathbb{E}_{p_0(\\bm{x})}[\\|\\phi_1(\\bm{x}) - \\phi_{\\bm{\\theta},1}(\\bm{x})\\|^2] = \\mathbb{E}_{p_0(\\bm{x})}[\\|\\bm{u}_1(\\bm{x})\\|^2]\n\\]\nUsing the inequality from Step 5:\n\\[\n\\|\\bm{u}_1(\\bm{x})\\|^2 \\leq e^{2M} \\left(\\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\| dt\\right)^2\n\\]\nBy Jensen's inequality:\n\\[\n\\|\\bm{u}_1(\\bm{x})\\|^2 \\leq e^{2M} \\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\|^2 dt\n\\]\n\n### Step 7: Connect to RFM Loss\nThe RFM loss is defined as:\n\\[\n\\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) = \\mathbb{E}_{t, p_t(\\bm{x})}[\\|\\bm{v}_t(\\bm{x}) - \\bm{v}_{\\bm{\\theta}}(\\bm{x}, t)\\|^2]\n\\]\nUsing the change of variables $\\bm{x} = \\phi_{\\bm{\\theta},t}(\\bm{x}_0)$:\n\\[\n\\mathbb{E}_{p_0(\\bm{x})}\\left[\\int_0^1 \\|\\bm{v}_t(\\phi_{\\bm{\\theta},t}(\\bm{x})) - \\bm{v}_{\\bm{\\theta}}(\\phi_{\\bm{\\theta},t}(\\bm{x}), t)\\|^2 dt\\right] = \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\n\\]\n\n### Step 8: Final Bound\nCombining all results:\n\\[\nW_2^2(p_1, p_{\\bm{\\theta},1}) \\leq e^{2M} \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta}) \\int_0^1 dt = e^{2M} \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\n\\]\nThe constant can be tightened to $e^{1+2M}$ by more careful analysis of the integration constant, giving the final bound:\n\\[\nW_2^2(p_1(\\bm{x}), p_{\\bm{\\theta},1}(\\bm{x})) \\leq e^{1+2M} \\mathcal{L}_{\\textrm{RFM}}(\\bm{\\theta})\n\\]\n\nThis completes the proof. The key was tracking the difference between flow trajectories and relating it to the velocity matching loss through Grnwall's inequality.", "ground-truth": "A"}
{"paper": "arxiv:2407.10490", "question": "Consider a classification model with V classes. For a given input x, assume that a fixed feature extractor \\(\\phi(x) \\in \\mathbb{R}^d\\) produces a feature vector, and the logits are computed as \\(z_t = w_t^\\top \\phi(x)\\). The predicted probability vector at time step t is given by \\(p_t = \\text{Softmax}(z_t)\\). Suppose that the model parameters are updated via stochastic gradient descent with learning rate \\(\\eta\\) (with an effective learning rate defined as \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\)) based on a single training example with ground truth label \\(y\\). This update results in the following change in the logits:\n\n\\[\nz_{t+1} = z_t - \\eta' (p_t - e_y),\n\\]\n\nwhere \\(e_y\\) is the one-hot vector corresponding to label \\(y\\). Define the ratio of the confidence change for each class \\(i\\) as\n\n\\[\n\\alpha_i \\equiv \\frac{p_{t+1,i}}{p_{t,i}},\n\\]\n\nLemma 1 asserts that\n\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^{V} e^{z_t^j}}{\\sum_{j=1}^{V} \\beta_j e^{z_t^j}},\n\\]\n\nwith the coefficients \\(\\beta_j\\) specified according to two cases:\n\n1. When \\(i = y\\) (Case 1):\n\n\\[\n\\beta_j = \\begin{cases} e^{-\\eta' (1 + p_t^j - p_t^y)} & \\text{if } j \\neq y, \\\\ 1 & \\text{if } j = y, \\end{cases}\n\\]\n\n2. When \\(i \\neq y\\) (Case 2):\n\n\\[\n\\beta_j = \\begin{cases} e^{-\\eta' (p_t^j - p_t^i)} & \\text{if } j \\neq y, \\\\ e^{-\\eta' (p_t^y - p_t^i - 1)} & \\text{if } j = y. \\end{cases}\n\\]\n\nCould you please provide a complete derivation of Lemma 1 using the above definitions and assumptions, ensuring that the meaning of each symbol and condition is clear?", "A": "Certainly! Below is a complete derivation of **Lemma 1**, which describes the ratio of confidence change \\(\\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}}\\) for each class \\(i\\) after a single SGD update. We will carefully analyze both cases: \\(i = y\\) (Case 1) and \\(i \\neq y\\) (Case 2).\n\n---\n\n### **Notation Recap**\n- **Input**: \\(x\\) with feature vector \\(\\phi(x) \\in \\mathbb{R}^d\\).\n- **Logits at time \\(t\\)**: \\(z_t = (z_t^1, \\dots, z_t^V)\\), where \\(z_t^i = w_t^i \\top \\phi(x)\\).\n- **Predicted probability (Softmax)**:  \n  \\[\n  p_{t,i} = \\frac{e^{z_t^i}}{\\sum_{j=1}^V e^{z_t^j}}.\n  \\]\n- **Ground truth label**: \\(y\\), represented as a one-hot vector \\(e_y\\).\n- **SGD update rule for logits**:  \n  \\[\n  z_{t+1} = z_t - \\eta' (p_t - e_y), \\quad \\text{where } \\eta' = \\eta \\|\\phi(x)\\|_2^2.\n  \\]\n  This implies:\n  \\[\n  z_{t+1}^i = z_t^i - \\eta' (p_{t,i} - \\delta_{i,y}),\n  \\]\n  where \\(\\delta_{i,y}\\) is the Kronecker delta (\\(\\delta_{i,y} = 1\\) if \\(i = y\\), else \\(0\\)).\n\n- **Ratio of confidence change**:  \n  \\[\n  \\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}}.\n  \\]\n\n---\n\n### **Derivation of \\(\\alpha_i\\)**\nWe compute \\(\\alpha_i\\) by expressing \\(p_{t+1,i}\\) in terms of \\(p_{t,i}\\) and the updated logits \\(z_{t+1}\\).\n\n#### **Step 1: Express \\(p_{t+1,i}\\)**\nThe updated probability is:\n\\[\np_{t+1,i} = \\frac{e^{z_{t+1}^i}}{\\sum_{j=1}^V e^{z_{t+1}^j}}.\n\\]\n\nSubstitute \\(z_{t+1}^j = z_t^j - \\eta' (p_{t,j} - \\delta_{j,y})\\):\n\\[\np_{t+1,i} = \\frac{e^{z_t^i - \\eta' (p_{t,i} - \\delta_{i,y})}}{\\sum_{j=1}^V e^{z_t^j - \\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\nFactor out \\(e^{z_t^i}\\) in the numerator and \\(e^{z_t^j}\\) in the denominator:\n\\[\np_{t+1,i} = \\frac{e^{z_t^i} e^{-\\eta' (p_{t,i} - \\delta_{i,y})}}{\\sum_{j=1}^V e^{z_t^j} e^{-\\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\n#### **Step 2: Relate to \\(p_{t,i}\\)**\nRecall that \\(p_{t,i} = \\frac{e^{z_t^i}}{\\sum_{j=1}^V e^{z_t^j}}\\). Thus:\n\\[\np_{t+1,i} = p_{t,i} \\cdot \\frac{e^{-\\eta' (p_{t,i} - \\delta_{i,y})} \\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V e^{z_t^j} e^{-\\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\nThe ratio \\(\\alpha_i\\) is:\n\\[\n\\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}} = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V e^{z_t^j} e^{-\\eta' (p_{t,j} - \\delta_{j,y})}}.\n\\]\n\nLet \\(\\beta_j = e^{-\\eta' (p_{t,j} - \\delta_{j,y})}\\). Then:\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V \\beta_j e^{z_t^j}}.\n\\]\n\nNow, we analyze \\(\\beta_j\\) for the two cases.\n\n---\n\n### **Case 1: \\(i = y\\) (True Class)**\nHere, \\(\\delta_{i,y} = 1\\), so:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - \\delta_{j,y})}.\n\\]\n\n- **If \\(j = y\\)**: \\(\\delta_{j,y} = 1\\), so \\(\\beta_y = e^{-\\eta' (p_{t,y} - 1)} = 1\\) (since \\(e^{\\eta' (1 - p_{t,y})}\\) cancels out in the ratio).\n- **If \\(j \\neq y\\)**: \\(\\delta_{j,y} = 0\\), so \\(\\beta_j = e^{-\\eta' p_{t,j}}\\).\n\nHowever, to match the given Lemma 1, we adjust the exponent by noting:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - p_{t,y} + p_{t,y} - \\delta_{j,y})}.\n\\]\nFor \\(j \\neq y\\), this simplifies to:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - p_{t,y} + p_{t,y})} = e^{-\\eta' (1 + p_{t,j} - p_{t,y})},\n\\]\nwhere we use the fact that \\(p_{t,y}\\) is subtracted and added back. For \\(j = y\\), \\(\\beta_y = 1\\).\n\nThus:\n\\[\n\\beta_j = \\begin{cases} \ne^{-\\eta' (1 + p_{t,j} - p_{t,y})} & \\text{if } j \\neq y, \\\\\n1 & \\text{if } j = y.\n\\end{cases}\n\\]\n\n---\n\n### **Case 2: \\(i \\neq y\\) (Other Classes)**\nHere, \\(\\delta_{i,y} = 0\\), so:\n\\[\n\\beta_j = e^{-\\eta' (p_{t,j} - \\delta_{j,y})}.\n\\]\n\nWe rewrite \\(\\beta_j\\) in terms of \\(p_{t,i}\\):\n- **If \\(j = y\\)**: \\(\\delta_{j,y} = 1\\), so:\n  \\[\n  \\beta_y = e^{-\\eta' (p_{t,y} - 1)} = e^{-\\eta' (p_{t,y} - p_{t,i} - 1 + p_{t,i})}.\n  \\]\n  To match Lemma 1, we express it as:\n  \\[\n  \\beta_y = e^{-\\eta' (p_{t,y} - p_{t,i} - 1)}.\n  \\]\n- **If \\(j \\neq y\\)**: \\(\\delta_{j,y} = 0\\), so:\n  \\[\n  \\beta_j = e^{-\\eta' p_{t,j}} = e^{-\\eta' (p_{t,j} - p_{t,i})}.\n  \\]\n\nThus:\n\\[\n\\beta_j = \\begin{cases} \ne^{-\\eta' (p_{t,j} - p_{t,i})} & \\text{if } j \\neq y, \\\\\ne^{-\\eta' (p_{t,y} - p_{t,i} - 1)} & \\text{if } j = y.\n\\end{cases}\n\\]\n\n---\n\n### **Final Expression for \\(\\alpha_i\\)**\nCombining both cases, we have:\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V \\beta_j e^{z_t^j}},\n\\]\nwhere \\(\\beta_j\\) depends on whether \\(i = y\\) or \\(i \\neq y\\), as specified in Lemma 1.\n\n---\n\n### **Conclusion**\nThe derivation confirms **Lemma 1**, showing how the confidence ratio \\(\\alpha_i\\) depends on the logits \\(z_t\\), the probabilities \\(p_t\\), and the effective learning rate \\(\\eta'\\). The two cases for \\(\\beta_j\\) arise from the different effects of the SGD update on the true class \\(y\\) versus other classes.", "B": "To derive Equation (34), we need to have the analytical expression of each \\(p_{t+1,i}\\) and \\(p_{t,i}\\). As \\(p = \\text{Softmax}(z)\\), we need to link \\(z_{t+1}\\) and \\(z_t\\) first. With Equations (32) and (33), \\(z_{t+1}\\) can be recursively written down as:\n\n\\[\nz_{t+1} = w_{t+1}^\\top \\phi(x) = \\Bigl( w_t - \\eta\\, \\phi(x)(p_t - e_y)^\\top \\Bigr)^\\top \\phi(x) = z_t - \\eta \\|\\phi(x)\\|_2^2 (p_t - e_y) = z_t - \\eta'(p_t - e_y),\n\\] \\(36\\)\n\nwhere \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\) is the equivalent learning rate that depends on the norm of the feature representation. Note that \\(z\\), \\(p\\), and \\(e_y\\) are all length-\\(V\\) vectors and \\(y\\) is an integer ranging from 1 to \\(V\\). Then we can write down each \\(z_{t+1,i}\\) as:\n\n\\[\nz_{t+1,i} = \\begin{cases}\n z_{t,i} - \\eta' p_{t,i} + \\eta', & \\text{if } i = y, \\\\\n z_{t,i} - \\eta' p_{t,i}, & \\text{if } i \\neq y.\n\\end{cases}\n\\] \\(37\\)\n\nThen, we can combine the definition of the Softmax function and write down different \\(p_{t+1,i}\\) case-by-case. For Case 1 where \\(i = y\\), we have:\n\n\\[\np_{t+1,y} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j=1}^{V} e^{z_{t+1,j}}} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (38) \n\\]\n\nCombining the fact that \\(p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^{V} e^{z_{t,j}}}\\), we can derive \\(\\alpha_i\\) and \\(\\beta_j\\) as in Equation (35). Similarly, when \\(i \\neq y\\), we have:\n\n\\[\np_{t+1,i} = \\frac{e^{z_{t,i} - \\eta' p_{t,i}}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (39)\n\\]\n\nwhich leads to the right part of Equation (35).", "C": "\n\nTo derive Lemma 1, we analyze the ratio of confidence changes \\(\\alpha_i = \\frac{p_{t+1,i}}{p_{t,i}}\\) under the given update rule for logits. Lets proceed step-by-step for both cases:\n\n---\n\n### **Case 1: \\(i = y\\) (Correct Class)**\n1. **Logit Update**:  \n   For the true class \\(y\\):  \n   \\[\n   z_{t+1}^y = z_t^y - \\eta'\\left(p_t^y - 1\\right)\n   \\]\n   For other classes \\(j \\neq y\\):  \n   \\[\n   z_{t+1}^j = z_t^j - \\eta' p_t^j\n   \\]\n\n2. **Exponential Ratio**:  \n   Compute \\(e^{z_{t+1}^i}/e^{z_t^i}\\) for \\(i = y\\):  \n   \\[\n   \\frac{e^{z_{t+1}^y}}{e^{z_t^y}} = e^{-\\eta'\\left(p_t^y - 1\\right)}\n   \\]\n\n3. **Softmax Denominators**:  \n   Let \\(S_t = \\sum_{j=1}^V e^{z_t^j}\\). The updated denominator becomes:  \n   \\[\n   S_{t+1} = \\sum_{j=1}^V e^{z_{t+1}^j} = \\sum_{j \\neq y} e^{z_t^j - \\eta' p_t^j} + e^{z_t^y - \\eta'\\left(p_t^y - 1\\right)}\n   \\]\n   Factor out \\(e^{z_t^j}\\) and apply \\(\\beta_j\\):  \n   \\[\n   S_{t+1} = \\sum_{j=1}^V e^{z_t^j} \\beta_j, \\quad \\text{where } \\beta_j = \\begin{cases} \n   e^{-\\eta'\\left(p_t^j - \\delta_{jy}\\right)} & \\text{(original)} \n   \\end{cases}\n   \\]\n\n4. **Adjust \\(\\beta_j\\) for Lemma 1**:  \n   To match Lemma 1s \\(\\beta_j\\) for \\(i = y\\):  \n   - For \\(j \\neq y\\):  \n     \\[\n     \\beta_j = e^{-\\eta'\\left(1 + p_t^j - p_t^y\\right)} \\quad \\text{(derived by re-scaling with \\(e^{\\eta'\\left(p_t^y - 1\\right)}\\))}\n     \\]\n   - For \\(j = y\\):  \n     \\[\n     \\beta_j = 1 \\quad \\text{(direct substitution)}\n     \\]\n\n5. **Final Expression**:  \n   Substitute into \\(\\alpha_i\\):  \n   \\[\n   \\alpha_y = \\frac{e^{-\\eta'\\left(p_t^y - 1\\right)} S_t}{S_{t+1}} = \\frac{S_t}{\\sum_{j=1}^V \\beta_j e^{z_t^j}}\n   \\]\n\n---\n\n### **Case 2: \\(i \\neq y\\) (Incorrect Class)**\n1. **Logit Update**:  \n   For class \\(i \\neq y\\):  \n   \\[\n   z_{t+1}^i = z_t^i - \\eta' p_t^i\n   \\]\n   For \\(j = y\\):  \n   \\[\n   z_{t+1}^y = z_t^y - \\eta'\\left(p_t^y - 1\\right)\n   \\]\n\n2. **Exponential Ratio**:  \n   Compute \\(e^{z_{t+1}^i}/e^{z_t^i}\\):  \n   \\[\n   \\frac{e^{z_{t+1}^i}}{e^{z_t^i}} = e^{-\\eta' p_t^i}\n   \\]\n\n3. **Softmax Denominators**:  \n   The updated denominator \\(S_{t+1}\\) remains:  \n   \\[\n   S_{t+1} = \\sum_{j=1}^V e^{z_t^j} \\beta_j, \\quad \\text{with original } \\beta_j = e^{-\\eta'\\left(p_t^j - \\delta_{jy}\\right)}\n   \\]\n\n4. **Adjust \\(\\beta_j\\) for Lemma 2**:  \n   To match Lemma 1s \\(\\beta_j\\) for \\(i \\neq y\\):  \n   - For \\(j \\neq y\\):  \n     \\[\n     \\beta_j = e^{-\\eta'\\left(p_t^j - p_t^i\\right)} \\quad \\text{(absorb \\(e^{\\eta' p_t^i}\\) into \\(\\beta_j\\))}\n     \\]\n   - For \\(j = y\\):  \n     \\[\n     \\beta_j = e^{-\\eta'\\left(p_t^y - p_t^i - 1\\right)} \\quad \\text{(absorb \\(e^{\\eta' p_t^i}\\) and re-scale)}\n     \\]\n\n5. **Final Expression**:  \n   Substitute into \\(\\alpha_i\\):  \n   \\[\n   \\alpha_i = \\frac{e^{-\\eta' p_t^i} S_t}{S_{t+1}} = \\frac{S_t}{\\sum_{j=1}^V \\beta_j e^{z_t^j}}\n   \\]\n\n---\n\n### **Lemma 1 Summary**\nFor both cases, the ratio \\(\\alpha_i\\) simplifies to:\n\\[\n\\alpha_i = \\frac{\\sum_{j=1}^V e^{z_t^j}}{\\sum_{j=1}^V \\beta_j e^{z_t^j}},\n\\]\nwhere \\(\\beta_j\\) depends on whether \\(i = y\\) (Case 1) or \\(i \\neq y\\) (Case 2), as defined in the lemma. This derivation confirms the correctness of the coefficients \\(\\beta_j\\) under the given update rule.", "D": "To derive Equation (34), we need to have the analytical expression of each \\(p_{t+1,i}\\) and \\(p_{t,i}\\). As \\(p = \\text{Softmax}(z)\\), we need to link \\(z_{t+1}\\) and \\(z_t\\) first. With Equations (32) and (33), \\(z_{t+1}\\) can be recursively written down as:\n\n\\[\nz_{t+1} = w_{t+1}^\\top \\phi(x) = \\Bigl( w_t - \\eta\\, \\phi(x)(p_t - e_y)^\\top \\Bigr)^\\top \\phi(x) = z_t - \\eta \\|\\phi(x)\\|_2^2 (p_t - e_y) = z_t - \\eta'(p_t - e_y),\n\\] \\(36\\)\n\nwhere \\(\\eta' \\equiv \\eta \\|\\phi(x)\\|_2^2\\) is the equivalent learning rate that depends on the norm of the feature representation. Note that \\(z\\), \\(p\\), and \\(e_y\\) are all length-\\(V\\) vectors and \\(y\\) is an integer ranging from 1 to \\(V\\). Then we can write down each \\(z_{t+1,i}\\) as:\n\n\\[\nz_{t+1,i} = \\begin{cases}\n z_{t,i} - \\eta' p_{t,i} + \\eta', & \\text{if } i = y, \\\\\n z_{t,i} - \\eta' p_{t,i}, & \\text{if } i \\neq y.\n\\end{cases}\n\\] \\(37\\)\n\nThen, we can combine the definition of the Softmax function and write down different \\(p_{t+1,i}\\) case-by-case. For Case 1 where \\(i = y\\), we have:\n\n\\[\np_{t+1,y} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j=1}^{V} e^{z_{t+1,j}}} = \\frac{e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (38) \n\\]\n\nCombining the fact that \\(p_{t,i} = \\frac{e^{z_{t,i}}}{\\sum_{j=1}^{V} e^{z_{t,j}}}\\), we can derive \\(\\alpha_i\\) and \\(\\beta_j\\) as in Equation (35). Similarly, when \\(i \\neq y\\), we have:\n\n\\[\np_{t+1,i} = \\frac{e^{z_{t,i} - \\eta' p_{t,i}}}{\\sum_{j \\neq y} e^{z_{t,j} - \\eta' p_{t,j}} + e^{z_{t,y} - \\eta' p_{t,y} + \\eta'}}, \\quad (39)\n\\]\n\nwhich leads to the right part of Equation (35). However, the coefficients \\(\\beta_j\\) in Case 2 should instead be:\n\n\\[\n\\beta_j = \\begin{cases} \ne^{-\\eta' (p_t^j - p_t^i)} & \\text{if } j \\neq y, \\\\ \ne^{-\\eta' (p_t^y - p_t^i + 1)} & \\text{if } j = y. \n\\end{cases}\n\\]", "ground-truth": "B"}
{"paper": "arxiv:2312.11456", "question": "Let \\(\\Pi\\) be a policy class and let \\(\\pi, \\hat{\\pi} \\in \\Pi\\) be two policies that, along with a reference policy \\(\\pi_0\\), share the same support. Let \\(d_0\\) denote the distribution over states (or prompts), and let \\(\\hat{r}: X \\times A \\to \\mathbb{R}\\) be an estimated reward function. Suppose that \\(\\hat{\\pi}\\) is generated by applying Oracle 1 with \\(\\hat{r}\\). Under these conditions, Lemma 2 states that\n\n\\[\n\\mathbb{E}_{x\\sim d_0}\\Big[\\mathbb{E}_{a\\sim \\pi}\\big[\\hat{r}(x,a)\\big] - \\mathbb{E}_{a\\sim \\hat{\\pi}}\\big[\\hat{r}(x,a)\\big] + \\eta\\, D_{KL}\\big(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big) - \\eta\\, D_{KL}\\big(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x)\\big)\\Big] = -\\eta\\, \\mathbb{E}_{x\\sim d_0}\\Big[ D_{KL}\\big(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x)\\big)\\Big].\n\\]\n\nCould you provide a complete derivation or proof of this lemma?", "A": "Since $\\hat{\\pi}$ is induced by calling Oracle 1 with $\\hat{r}$, we know that for any $x \\in X$,\n\n\\[\n\\hat{\\pi}(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\, \\hat{r}(x,a)\\Big),\n\\]\n\nwhere $Z(x)=\\sum_{a \\in A}\\pi_0(a|x)\\exp\\Big(\\frac{1}{\\eta}\\hat{r}(x,a)\\Big)$ is the normalization constant. We can rewrite the reward function as\n\n\\[\n\\hat{r}(x,a)=\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}+\\eta\\log Z(x).\n\\]\n\nPlugging this reward reparameterization into the policy optimization error under $\\hat{r}$, we have\n\n\\[\n\\mathbb{E}_{\\pi}[\\hat{r}(x,a)]-\\mathbb{E}_{\\hat{\\pi}}[\\hat{r}(x,a)] = \\mathbb{E}_{\\pi}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big] - \\mathbb{E}_{\\hat{\\pi}}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big]\n\\]\n\n\\[\n= \\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x))+\\eta\\,D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)).\n\\]\n\nPlugging the above equality into the left-hand side of Lemma 2 completes the proof.", "B": "Certainly! Below is the detailed derivation of Lemma 2, which connects the expected reward difference between policies \\(\\pi\\) and \\(\\hat{\\pi}\\) with their KL divergences relative to \\(\\pi_0\\) and each other.\n\n---\n\n### **Derivation of Lemma 2**\n\nGiven:\n- Policies \\(\\pi, \\hat{\\pi}, \\pi_0\\) share the same support.\n- \\(\\hat{\\pi}\\) is obtained by applying Oracle 1 with the estimated reward \\(\\hat{r}\\).\n- Oracle 1 solves the following optimization problem:\n  \\[\n  \\hat{\\pi} = \\arg\\max_{\\pi' \\in \\Pi} \\mathbb{E}_{x \\sim d_0, a \\sim \\pi'}\\big[\\hat{r}(x,a)\\big] - \\eta D_{KL}\\big(\\pi'(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big).\n  \\]\n  The solution to this is known to be:\n  \\[\n  \\hat{\\pi}(a|x) = \\frac{\\pi_0(a|x) e^{\\hat{r}(x,a)/\\eta}}{Z(x)},\n  \\]\n  where \\(Z(x) = \\mathbb{E}_{a \\sim \\pi_0}\\big[e^{\\hat{r}(x,a)/\\eta}\\big]\\) is the partition function.\n\n---\n\n#### **Step 1: Express \\(\\hat{r}(x,a)\\) in terms of \\(\\hat{\\pi}\\) and \\(\\pi_0\\)**\nFrom the optimal form of \\(\\hat{\\pi}\\), we have:\n\\[\n\\hat{r}(x,a) = \\eta \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) + \\eta \\log Z(x).\n\\]\n\n---\n\n#### **Step 2: Compute \\(\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)]\\) and \\(\\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)]\\)**\nUsing the expression for \\(\\hat{r}(x,a)\\):\n\\[\n\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] = \\eta \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] + \\eta \\log Z(x),\n\\]\n\\[\n\\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] = \\eta \\mathbb{E}_{a \\sim \\hat{\\pi}}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] + \\eta \\log Z(x).\n\\]\n\nSubtracting the two expectations:\n\\[\n\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] = \\eta \\left( \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] - \\mathbb{E}_{a \\sim \\hat{\\pi}}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right] \\right).\n\\]\n\n---\n\n#### **Step 3: Relate to KL Divergences**\nRecall that the KL divergence is defined as:\n\\[\nD_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) = \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\pi(a|x)}{\\hat{\\pi}(a|x)} \\right) \\right],\n\\]\n\\[\nD_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) = \\mathbb{E}_{a \\sim \\pi}\\left[ \\log \\left( \\frac{\\pi(a|x)}{\\pi_0(a|x)} \\right) \\right],\n\\]\n\\[\nD_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) = \\mathbb{E}_{a \\sim \\hat{\\pi}}\\left[ \\log \\left( \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} \\right) \\right].\n\\]\n\nThus, we can rewrite the difference as:\n\\[\n\\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] = \\eta \\left( D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) - D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\right).\n\\]\n\n---\n\n#### **Step 4: Rearrange Terms**\nSubstituting back into the original expression:\n\\[\n\\mathbb{E}_{x \\sim d_0} \\Big[ \\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] + \\eta D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - \\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\Big]\n\\]\n\\[\n= \\mathbb{E}_{x \\sim d_0} \\Big[ \\eta \\left( D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) - D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\right) + \\eta D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - \\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\Big]\n\\]\n\\[\n= \\mathbb{E}_{x \\sim d_0} \\Big[ -\\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) \\Big].\n\\]\n\n---\n\n### **Final Result**\nThus, we have:\n\\[\n\\mathbb{E}_{x \\sim d_0} \\Big[ \\mathbb{E}_{a \\sim \\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a \\sim \\hat{\\pi}}[\\hat{r}(x,a)] + \\eta D_{KL}\\big(\\hat{\\pi}(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) - \\eta D_{KL}\\big(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)\\big) \\Big] = -\\eta \\mathbb{E}_{x \\sim d_0} \\Big[ D_{KL}\\big(\\pi(\\cdot|x) \\| \\hat{\\pi}(\\cdot|x)\\big) \\Big].\n\\]\n\nThis completes the proof. The key steps involve:\n1. Expressing \\(\\hat{r}(x,a)\\) in terms of \\(\\hat{\\pi}\\) and \\(\\pi_0\\),\n2. Relating expectations to KL divergences,\n3. Simplifying the expression by cancellation.\n\nThe result shows that the difference in expected rewards, adjusted by KL penalties, reduces to the negative KL divergence between \\(\\pi\\) and \\(\\hat{\\pi}\\).", "C": "Since \\(\\hat{\\pi}\\) is induced by calling Oracle 1 with \\(\\hat{r}\\), we know that for any \\(x \\in X\\),\n\n\\[\n\\hat{\\pi}(a|x) = \\frac{1}{Z(x)} \\pi_0(a|x) \\cdot \\exp\\Big( \\frac{1}{\\eta}\\, \\hat{r}(x,a)\\Big),\n\\]\n\nwhere \\(Z(x)=\\sum_{a \\in A}\\pi_0(a|x)\\exp\\Big(\\frac{1}{\\eta}\\hat{r}(x,a)\\Big)\\) is the normalization constant. We can rewrite the reward function as\n\n\\[\n\\hat{r}(x,a)=\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}+\\eta\\log Z(x).\n\\]\n\nPlugging this reward reparameterization into the policy optimization error under \\(\\hat{r}\\), we have\n\n\\[\n\\mathbb{E}_{\\pi}[\\hat{r}(x,a)]-\\mathbb{E}_{\\hat{\\pi}}[\\hat{r}(x,a)] = \\mathbb{E}_{\\pi}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big] - \\mathbb{E}_{\\hat{\\pi}}\\Big[\\eta\\log\\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)}\\Big]\n\\]\n\n\\[\n= \\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\pi_0(\\cdot|x))-\\eta\\,D_{KL}(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x))-\\eta\\,D_{KL}(\\hat{\\pi}(\\cdot|x)\\|\\pi_0(\\cdot|x)).\n\\]\n\nPlugging the above equality into the left-hand side of Lemma 2 completes the proof.", "D": "To prove Lemma 2, we leverage the optimality condition of \\(\\hat{\\pi}\\) derived from Oracle 1 and simplify the given expression using properties of KL divergence. Here's the complete derivation:\n\n---\n\n**Step 1: Optimal Policy \\(\\hat{\\pi}\\) via Oracle 1**  \nOracle 1 generates \\(\\hat{\\pi}\\) by solving the regularized optimization problem for each \\(x\\):  \n\\[\n\\hat{\\pi}(\\cdot|x) = \\arg\\max_{\\pi'} \\Big[ \\mathbb{E}_{a\\sim\\pi'}\\big[\\hat{r}(x,a)\\big] - \\eta D_{KL}\\big(\\pi'(\\cdot|x) \\,\\|\\, \\pi_0(\\cdot|x)\\big) \\Big].\n\\]\nThe solution takes the form:  \n\\[\n\\hat{\\pi}(a|x) = \\frac{\\pi_0(a|x) \\exp\\left(\\frac{\\hat{r}(x,a)}{\\eta}\\right)}{Z(x)}, \\quad Z(x) = \\mathbb{E}_{a\\sim\\pi_0}\\left[\\exp\\left(\\frac{\\hat{r}(x,a)}{\\eta}\\right)\\right].\n\\]\n\n**Step 2: Express \\(\\hat{r}(x,a)\\) via Optimality Condition**  \nTaking logarithms of \\(\\hat{\\pi}(a|x)\\):  \n\\[\n\\log \\hat{\\pi}(a|x) = \\log \\pi_0(a|x) + \\frac{\\hat{r}(x,a)}{\\eta} - \\log Z(x).\n\\]\nRearranging yields:  \n\\[\n\\hat{r}(x,a) = \\eta \\left( \\log \\hat{\\pi}(a|x) - \\log \\pi_0(a|x) + \\log Z(x) \\right).\n\\]\n\n**Step 3: Substitute \\(\\hat{r}(x,a)\\) into the LHS**  \nThe LHS of the lemma becomes:  \n\\[\n\\mathbb{E}_{x\\sim d_0} \\Bigg[ \\underbrace{\\mathbb{E}_{a\\sim\\pi}[\\hat{r}(x,a)] - \\mathbb{E}_{a\\sim\\hat{\\pi}}[\\hat{r}(x,a)]}_{\\text{Term 1}} + \\eta \\underbrace{D_{KL}\\big(\\hat{\\pi} \\,\\|\\, \\pi_0\\big)}_{\\text{Term 2}} - \\eta \\underbrace{D_{KL}\\big(\\pi \\,\\|\\, \\pi_0\\big)}_{\\text{Term 3}} \\Bigg].\n\\]\n\n**Step 4: Expand Term 1**  \nSubstitute \\(\\hat{r}(x,a)\\) into Term 1:  \n\\[\n\\text{Term 1} = \\eta \\mathbb{E}_{a\\sim\\pi}\\left[ \\log \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} + \\log Z(x) \\right] - \\eta \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[ \\log \\frac{\\hat{\\pi}(a|x)}{\\pi_0(a|x)} + \\log Z(x) \\right].\n\\]\nSimplify using \\(\\mathbb{E}_{a\\sim p}[\\log Z(x)] = \\log Z(x)\\):  \n\\[\n\\text{Term 1} = \\eta \\left( \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] \\right).\n\\]\n\n**Step 5: Expand Terms 2 and 3**  \nTerm 2:  \n\\[\nD_{KL}\\big(\\hat{\\pi} \\,\\|\\, \\pi_0\\big) = \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[ \\log \\frac{\\hat{\\pi}}{\\pi_0} \\right].\n\\]\nTerm 3:  \n\\[\nD_{KL}\\big(\\pi \\,\\|\\, \\pi_0\\big) = \\mathbb{E}_{a\\sim\\pi}\\left[ \\log \\frac{\\pi}{\\pi_0} \\right].\n\\]\n\n**Step 6: Combine All Terms**  \nSubstitute Terms 1-3 into the LHS:  \n\\[\n\\eta \\mathbb{E}_{x\\sim d_0} \\Bigg[ \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] + \\mathbb{E}_{a\\sim\\hat{\\pi}}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\pi}{\\pi_0}\\right] \\Bigg].\n\\]\nSimplify by cancellation:  \n\\[\n\\eta \\mathbb{E}_{x\\sim d_0} \\Bigg[ \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\hat{\\pi}}{\\pi_0}\\right] - \\mathbb{E}_{a\\sim\\pi}\\left[\\log \\frac{\\pi}{\\pi_0}\\right] \\Bigg].\n\\]\nFactor out \\(\\mathbb{E}_{a\\sim\\pi}\\):  \n\\[\n\\eta \\mathbb{E}_{x\\sim d_0} \\mathbb{E}_{a\\sim\\pi} \\left[ \\log \\frac{\\hat{\\pi}}{\\pi_0} - \\log \\frac{\\pi}{\\pi_0} \\right] = \\eta \\mathbb{E}_{x\\sim d_0} \\mathbb{E}_{a\\sim\\pi} \\left[ \\log \\frac{\\hat{\\pi}}{\\pi} \\right].\n\\]\n\n**Step 7: Relate to \\(D_{KL}(\\pi \\,\\|\\, \\hat{\\pi})\\)**  \nRecognize that:  \n\\[\n\\mathbb{E}_{a\\sim\\pi} \\left[ \\log \\frac{\\hat{\\pi}}{\\pi} \\right] = -D_{KL}\\big(\\pi \\,\\|\\, \\hat{\\pi}\\big).\n\\]\nThus:  \n\\[\n\\text{LHS} = -\\eta \\mathbb{E}_{x\\sim d_0} \\Big[ D_{KL}\\big(\\pi(\\cdot|x) \\,\\|\\, \\hat{\\pi}(\\cdot|x)\\big) \\Big].\n\\]\n\n---\n\nThis completes the proof of Lemma 2. The key steps involve substituting the optimal policy's form, leveraging the relationship between \\(\\hat{r}\\) and \\(\\hat{\\pi}\\), and simplifying using KL divergence properties.", "ground-truth": "C"}
{"paper": "arxiv:2402.05749", "question": "Consider a discrete set of outcomes $Y$ and a parametric distribution (policy) $\\pi_{\\theta}(y)$ with parameters $\\theta\\in\\mathbb{R}^d$, and a fixed reference distribution $\\pi_{\\mathrm{ref}}(y)$ on $Y$. Define the Kullback-Leibler divergence\n\n$$\n\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\n$$\n\nIntroduce the log-density ratio for each $y\\in Y$:\n\n$$\n\\rho_{\\theta}(y) := \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}.\n$$\n\nShow how to derive the following gradient identity:\n\n$$\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n$$\n\nYou may assume that you can interchange gradient and expectation and that\n$\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)]=0$ under the shared support of the two distributions.", "A": "By definition we have\\n\\n$$\\n\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\\n$$\\n\\nIts gradient contains two terms:\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr]\n+\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\bigr].\\n$$\\n\\nThe second term vanishes because it is the expectation of the score function under its own distribution. Meanwhile, rewriting the \\mu-weighted squared loss with \\mu=\\pi_{\\theta} gives\\n\\n$$\\n\\frac12\\,\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[\\rho_{\\theta}^2]\n=\\frac12\\,\\mathbb{E}_{(y_1,y_2)\\sim\\mu}\\Bigl[\\Bigl(\\log\\frac{\\pi_{\\theta}(y_1)}{\\pi_{\\mathrm{ref}}(y_1)}+\\log\\frac{\\pi_{\\theta}(y_2)}{\\pi_{\\mathrm{ref}}(y_2)}\\Bigr)^2\\Bigr],\\n$$\\n\\nand taking its gradient under \\mu=\\pi_{\\theta} yields\\n\\n$$\\n\\mathbb{E}_{(y_1,y_2)\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}^2)\\bigr]\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr],\\n$$\\n\\nwhere the cross term and exchange of expectations use independence and identical distribution of y_1,y_2. Equating the two expressions proves\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}(y)^2)\\bigr].\\n$$", "B": "To derive the gradient identity, we start with the definition of the Kullback-Leibler (KL) divergence and proceed step-by-step.\n\n**Given**:\n\\[\n\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)], \\quad \\text{where } \\rho_{\\theta}(y) = \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}.\n\\]\n\n**Goal**: Show that\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr)\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n\\]\n\n**Derivation**:\n\n1. **Express the gradient of KL divergence**:\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)].\n   \\]\n\n2. **Interchange gradient and expectation** (as permitted by assumption):\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\rho_{\\theta}(y)] + \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)].\n   \\]\n   *Explanation*: The gradient operator acts on both \\(\\rho_{\\theta}(y)\\) and the distribution \\(\\pi_{\\theta}(y)\\), leading to two terms via the product rule:\n   \\[\n   \\nabla_{\\theta}(\\pi_{\\theta}(y)\\rho_{\\theta}(y)) = \\pi_{\\theta}(y)\\nabla_{\\theta}\\rho_{\\theta}(y) + \\rho_{\\theta}(y)\\nabla_{\\theta}\\pi_{\\theta}(y).\n   \\]\n   The second term uses the identity \\(\\nabla_{\\theta}\\pi_{\\theta}(y) = \\pi_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\).\n\n3. **Simplify the second term**:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)].\n   \\]\n\n4. **Analyze the first term**:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)] = 0.\n   \\]\n   *Explanation*: Since \\(\\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\\), its gradient is \\(\\nabla_{\\theta}\\rho_{\\theta}(y) = \\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\). The expectation of the score function \\(\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\) over \\(y \\sim \\pi_{\\theta}\\) is zero (given assumption).\n\n5. **Combine results**:\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y)].\n   \\]\n\n6. **Relate to \\(\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\)**:\n   Compute the gradient inside the expectation:\n   \\[\n   \\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr) = \\rho_{\\theta}(y)\\nabla_{\\theta}\\rho_{\\theta}(y) = \\rho_{\\theta}(y)\\nabla_{\\theta}\\log\\pi_{\\theta}(y).\n   \\]\n   Substituting this into the expectation gives:\n   \\[\n   \\nabla_{\\theta}\\,\\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n   \\]\n\n**Conclusion**: The gradient of the KL divergence is equivalent to the expectation of \\(\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\) under \\(\\pi_{\\theta}\\), as required.", "C": "Alright, let's tackle this problem step by step. I'm going to derive the gradient of the KL divergence with respect to the parameters \\(\\theta\\) and show how it relates to the given expectation involving \\(\\rho_{\\theta}(y)\\).\n\n### 1. Understanding the Definitions\n\nFirst, let's recall the definitions:\n\n- **KL Divergence**:\n  \\[\n  \\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n  \\]\n  \n- **Log-density ratio**:\n  \\[\n  \\rho_{\\theta}(y) = \\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)} = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n  \\]\n\n### 2. Computing the Gradient of KL Divergence\n\nWe want to find:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n\\]\n\nAssuming we can interchange the gradient and expectation (which is given), we have:\n\\[\n\\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n\\]\n\n### 3. Expanding \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\)\n\nNow, let's compute \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\):\n\\[\n\\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n\\]\n\\[\n\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y) - \\nabla_{\\theta} \\log\\pi_{\\mathrm{ref}}(y)\n\\]\n\nBut \\(\\pi_{\\mathrm{ref}}(y)\\) does not depend on \\(\\theta\\), so \\(\\nabla_{\\theta} \\log\\pi_{\\mathrm{ref}}(y) = 0\\). Therefore:\n\\[\n\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\n### 4. Relating to \\(\\rho_{\\theta}(y)\\)\n\nNow, let's look at \\(\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\):\n\\[\n\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\n### 5. Expectation of \\(\\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\)\n\nWe are given that:\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\n\\]\n\nThis is a key property because it tells us that the expectation of the score function is zero.\n\n### 6. Combining the Pieces\n\nNow, let's express \\(\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\\) in terms of \\(\\rho_{\\theta}(y)\\):\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\n\\]\n\nBut this seems to give us zero, which doesn't match the desired identity. Wait, this suggests that perhaps there's more to it. Maybe we need to consider that \\(\\rho_{\\theta}(y)\\) itself is involved in the expectation.\n\nLet me think differently. The desired identity is:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\right]\n\\]\n\nWe know:\n\\[\n\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\nSo:\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nNow, recall that:\n\\[\n\\nabla_{\\theta} \\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nBut this is zero, which doesn't match. Hmm, perhaps the trick is to express \\(\\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\) in terms of \\(\\rho_{\\theta}(y)\\).\n\nWait, let's think about the relationship between \\(\\rho_{\\theta}(y)\\) and \\(\\log\\pi_{\\theta}(y)\\):\n\\[\n\\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n\\]\n\\[\n\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n\\]\n\nBut \\(\\mathbb{E}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), so how does \\(\\rho_{\\theta}(y)\\) come into play?\n\nPerhaps the key is to consider that:\n\\[\n\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[(\\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\\[\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\log\\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] - \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\log\\pi_{\\mathrm{ref}}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nThe first term is:\n\\[\n\\mathbb{E}[\\log\\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = \\mathbb{E}[\\nabla_{\\theta} (\\pi_{\\theta}(y) \\log\\pi_{\\theta}(y)) - \\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] \n\\]\nWait, this seems convoluted. Maybe another approach.\n\n### 7. Alternative Approach: Express KL in Terms of \\(\\rho_{\\theta}(y)\\)\n\nLet's write the KL divergence explicitly:\n\\[\n\\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)] = \\sum_{y} \\pi_{\\theta}(y) \\rho_{\\theta}(y)\n\\]\n\nTaking the gradient:\n\\[\n\\nabla_{\\theta} \\mathrm{KL} = \\sum_{y} \\nabla_{\\theta} (\\pi_{\\theta}(y) \\rho_{\\theta}(y)) = \\sum_{y} \\left( \\rho_{\\theta}(y) \\nabla_{\\theta} \\pi_{\\theta}(y) + \\pi_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) \\right)\n\\]\n\nUsing \\(\\nabla_{\\theta} \\pi_{\\theta}(y) = \\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\):\n\\[\n= \\sum_{y} \\left( \\rho_{\\theta}(y) \\pi_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y) + \\pi_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) \\right)\n\\]\n\\[\n= \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] + \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n\\]\n\nBut \\(\\mathbb{E}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), so:\n\\[\n\\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n\\]\n\nAnd since \\(\\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\), we have:\n\\[\n\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\n\\]\n\nTherefore:\n\\[\n\\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right]\n\\]\n\n### 8. Final Derivation\n\nPutting it all together:\n\n1. Start with the KL divergence:\n   \\[\n   \\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n   \\]\n\n2. Take the gradient:\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\nabla_{\\theta} \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n   \\]\n   (by interchanging gradient and expectation)\n\n3. Compute \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\):\n   \\[\n   \\rho_{\\theta}(y) = \\log\\pi_{\\theta}(y) - \\log\\pi_{\\mathrm{ref}}(y)\n   \\]\n   \\[\n   \\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n   (since \\(\\pi_{\\mathrm{ref}}(y)\\) does not depend on \\(\\theta\\))\n\n4. Now, consider \\(\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\):\n   \\[\n   \\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n\n5. Take the expectation:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n   \\]\n\n6. But from step 2, we have:\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n   \\]\n   However, \\(\\mathbb{E}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), which seems to suggest \\(\\nabla_{\\theta} \\mathrm{KL} = 0\\), but this contradicts unless we consider that the gradient of KL is actually captured by the term involving \\(\\rho_{\\theta}(y)\\).\n\n   The correct interpretation is that the gradient of KL is equal to the expectation of \\(\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\), because the direct expectation of \\(\\nabla_{\\theta} \\log\\pi_{\\theta}(y)\\) is zero, but when weighted by \\(\\rho_{\\theta}(y)\\), it gives the gradient.\n\n### 9. Conclusion\n\nThus, we've shown that:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\left(\\frac{1}{2} \\rho_{\\theta}(y)^2\\right)\\right]\n\\]\n\n### Final Answer\n\nWe derive the gradient identity as follows:\n\n1. Start with the KL divergence:\n   \\[\n   \\mathrm{KL}(\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y)]\n   \\]\n\n2. Take the gradient with respect to \\(\\theta\\):\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\rho_{\\theta}(y)]\n   \\]\n   (by interchanging gradient and expectation)\n\n3. Compute \\(\\nabla_{\\theta} \\rho_{\\theta}(y)\\):\n   \\[\n   \\nabla_{\\theta} \\rho_{\\theta}(y) = \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n   (since \\(\\pi_{\\mathrm{ref}}(y)\\) is independent of \\(\\theta\\))\n\n4. Observe that:\n   \\[\n   \\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\rho_{\\theta}(y) = \\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)\n   \\]\n\n5. Therefore:\n   \\[\n   \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)]\n   \\]\n\n6. Since \\(\\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = 0\\), the gradient of KL is captured by the weighted expectation:\n   \\[\n   \\nabla_{\\theta} \\mathrm{KL} = \\mathbb{E}_{y\\sim\\pi_{\\theta}}[\\rho_{\\theta}(y) \\nabla_{\\theta} \\log\\pi_{\\theta}(y)] = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\left( \\frac{1}{2} \\rho_{\\theta}(y)^2 \\right)\\right]\n   \\]\n\nThus, we have derived the desired identity:\n\\[\n\\nabla_{\\theta}\\,\\mathrm{KL}\\bigl(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathrm{ref}}\\bigr) = \\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\bigl(\\tfrac{1}{2}\\,\\rho_{\\theta}(y)^2\\bigr)\\Bigr].\n\\]", "D": "By definition we have\\n\\n$$\\n\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\Bigr].\\n$$\\n\\nIts gradient contains two terms:\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr]\n+\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\bigr].\\n$$\\n\\nThe second term vanishes because it is the expectation of the score function under its own distribution. Meanwhile, rewriting the \\mu-weighted squared loss with \\mu=\\pi_{\\theta} gives\\n\\n$$\\n\\frac12\\,\\mathbb{E}_{(y_w,y_l)\\sim\\mu}[\\rho_{\\theta}^2]\n=\\frac12\\,\\mathbb{E}_{(y_1,y_2)\\sim\\mu}\\Bigl[\\Bigl(\\log\\frac{\\pi_{\\theta}(y_1)}{\\pi_{\\mathrm{ref}}(y_1)}-\\log\\frac{\\pi_{\\theta}(y_2)}{\\pi_{\\mathrm{ref}}(y_2)}\\Bigr)^2\\Bigr],\\n$$\\n\\nand taking its gradient under \\mu=\\pi_{\\theta} yields\\n\\n$$\\n\\mathbb{E}_{(y_1,y_2)\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}^2)\\bigr]\n=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\Bigl[\\log\\frac{\\pi_{\\theta}(y)}{\\pi_{\\mathrm{ref}}(y)}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(y)\\Bigr],\\n$$\\n\\nwhere the cross term and exchange of expectations use independence and identical distribution of y_1,y_2. Equating the two expressions proves\\n\\n$$\\n\\nabla_{\\theta}\\,\\mathrm{KL}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=\\mathbb{E}_{y\\sim\\pi_{\\theta}}\\bigl[\\nabla_{\\theta}(\\tfrac12\\rho_{\\theta}(y)^2)\\bigr].\\n$$", "ground-truth": "D"}
{"paper": "arxiv:2403.03419", "question": "Could you please provide a complete derivation and proof of the following lemma, making use only of the accompanying definitions and assumptions? \n\nDefinitions and assumptions\n  \\(\\pi_r(y)\\): the probability distribution over responses y produced by the original (pre-trained) language model.\n  \\(\\pi_{\\theta}(y)\\): a trainable policy over responses.\n  \\(r^{*}(y)\\): the ground-truth scalar reward assigned to an individual response.\n  \\(\\beta>0\\): a fixed temperature (scaling) constant; \\(Z>0\\) is a finite normalising factor.\n  For any distribution \\(\\pi\\), let \\(\\varphi(\\pi)\\) be its distributional preference value.\n  Let \\(\\mu\\) denote a fixed distribution of dispreferred responses.\n  A Generation-with-Distributional-Control (GDC) problem is solved under the moment constraint \\(\\varphi(\\pi) > \\varphi(\\mu)\\).\n\nLemma 1\nIf the policy \\(\\pi_{\\theta}(y)\\) obtained from the GDC optimisation satisfies the constraint above, then\n\\[\n r^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\qquad\n \\varphi^{*}(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\Big[\\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)}\\Big].\n\\]\n\nPlease present a rigorous, step-by-step derivation and proof of Lemma 1.", "A": "Certainly! Below is a rigorous, step-by-step derivation and proof of **Lemma 1**, based on the given definitions and assumptions.\n\n---\n\n### **Step 1: Formulating the GDC Optimization Problem**\n\nThe **Generation-with-Distributional-Control (GDC)** problem involves optimizing the policy \\(\\pi_{\\theta}(y)\\) to maximize the expected reward while satisfying the moment constraint \\(\\varphi(\\pi) > \\varphi(\\mu)\\).  \n\nThe constrained optimization problem can be written as:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}}[r^{*}(y)] \\quad \\text{subject to} \\quad \\varphi(\\pi_{\\theta}) > \\varphi(\\mu).\n\\]\n\nUsing the **method of Lagrange multipliers**, we introduce a dual variable \\(\\beta > 0\\) (the temperature constant) and write the Lagrangian:\n\\[\n\\mathcal{L}(\\pi_{\\theta}, \\beta) = \\mathbb{E}_{y \\sim \\pi_{\\theta}}[r^{*}(y)] - \\beta \\left( \\varphi(\\pi_{\\theta}) - \\varphi(\\mu) \\right).\n\\]\n\nSince \\(\\varphi(\\mu)\\) is fixed, we can ignore it in the optimization (as it does not depend on \\(\\pi_{\\theta}\\)), simplifying the problem to:\n\\[\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}}[r^{*}(y)] - \\beta \\varphi(\\pi_{\\theta}).\n\\]\n\n---\n\n### **Step 2: Solving for the Optimal Policy \\(\\pi^{*}(y)\\)**\n\nTo find the optimal policy \\(\\pi^{*}(y)\\), we take the **variational derivative** of \\(\\mathcal{L}\\) with respect to \\(\\pi_{\\theta}\\) and set it to zero:\n\\[\n\\frac{\\delta \\mathcal{L}}{\\delta \\pi_{\\theta}} = r^{*}(y) - \\beta \\frac{\\delta \\varphi(\\pi_{\\theta})}{\\delta \\pi_{\\theta}} = 0.\n\\]\n\nFrom the definition of \\(\\varphi(\\pi)\\), we assume it is a **linear expectation functional**:\n\\[\n\\varphi(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi(y)}{\\pi_r(y)} \\right].\n\\]\nThus, its variational derivative is:\n\\[\n\\frac{\\delta \\varphi(\\pi)}{\\delta \\pi} = \\log \\frac{\\pi(y)}{\\pi_r(y)} + 1.\n\\]\n\nSubstituting back into the optimality condition:\n\\[\nr^{*}(y) = \\beta \\left( \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + 1 \\right).\n\\]\n\nTo ensure \\(\\pi^{*}(y)\\) is normalized, we introduce a **partition function** \\(Z\\):\n\\[\n\\pi^{*}(y) = \\frac{1}{Z} \\pi_r(y) \\exp \\left( \\frac{r^{*}(y)}{\\beta} \\right), \\quad \\text{where} \\quad Z = \\sum_y \\pi_r(y) \\exp \\left( \\frac{r^{*}(y)}{\\beta} \\right).\n\\]\n\nTaking the logarithm and rearranging:\n\\[\n\\log \\pi^{*}(y) = \\log \\pi_r(y) + \\frac{r^{*}(y)}{\\beta} - \\log Z.\n\\]\nSolving for \\(r^{*}(y)\\):\n\\[\nr^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z.\n\\]\nThis proves the **first part of Lemma 1**.\n\n---\n\n### **Step 3: Deriving \\(\\varphi^{*}(\\pi)\\)**\n\nGiven the optimal policy \\(\\pi^{*}(y)\\), the **distributional preference value** \\(\\varphi^{*}(\\pi)\\) is:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\n\nSubstituting the expression for \\(\\pi^{*}(y)\\) from Step 2:\n\\[\n\\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} = \\frac{r^{*}(y)}{\\beta} - \\log Z.\n\\]\nThus:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\frac{r^{*}(y)}{\\beta} - \\log Z \\right].\n\\]\nSince \\(\\log Z\\) is a constant, it factors out of the expectation:\n\\[\n\\varphi^{*}(\\pi) = \\frac{1}{\\beta} \\mathbb{E}_{y \\sim \\pi} [r^{*}(y)] - \\log Z.\n\\]\n\nHowever, from the **first part of Lemma 1**, we have:\n\\[\n\\mathbb{E}_{y \\sim \\pi} [r^{*}(y)] = \\beta \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right] + \\beta \\log Z.\n\\]\nSubstituting back:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right] + \\log Z - \\log Z = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\n\nBut this is circular. Instead, let us directly compute \\(\\varphi^{*}(\\pi)\\) using the optimal policy:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\nFrom Step 2, we have:\n\\[\n\\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} = \\frac{r^{*}(y)}{\\beta} - \\log Z.\n\\]\nThus:\n\\[\n\\varphi^{*}(\\pi) = \\frac{1}{\\beta} \\mathbb{E}_{y \\sim \\pi} [r^{*}(y)] - \\log Z.\n\\]\n\nBut since \\(\\mathbb{E}_{y \\sim \\pi} [r^{*}(y)]\\) is not directly given, we instead use the **definition of \\(\\varphi\\)** and the **optimal policy** to conclude:\n\\[\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right],\n\\]\nwhere the \\(\\beta\\) scaling is introduced to match the **first part of Lemma 1**.\n\nThis proves the **second part of Lemma 1**.\n\n---\n\n### **Final Result**\n\nCombining the results from Steps 2 and 3, we obtain **Lemma 1**:\n\\[\nr^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\qquad\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y \\sim \\pi} \\left[ \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} \\right].\n\\]\n\nThis completes the proof. \\(\\square\\)", "B": "The Generation-with-Distributional-Control (GDC) can be formalized as\n\\[\\theta^{*}=\\arg\\min_{\\theta} \\mathrm{KL}\\big[p(x)\\,\\|\\,\\pi_{\\theta}(x)\\big]-\\big[\\varphi(p)-\\varphi(\\mu)\\big].\\tag{5}\\]\nThe KL term is minimized when \\(p(x)=\\pi_{\\theta}(x)\\), that is, \\(\\pi_r(x)e^{r^{*}(x)/\\beta}/Z=\\pi_{\\theta}(x)\\). Hence we obtain \\(r^{*}(x)=\\beta\\,\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}+\\beta\\log Z\\). We then need only maximize \\(\\varphi(p)-\\varphi(\\mu)\\). \\(\\varphi\\) is defined as the expectation of reward over the given distribution, yielding\n\\[\\begin{aligned}\n\\arg\\max_{\\pi} \\varphi(p)-\\varphi(\\mu)\n&=\\mathbb{E}_{p}[r(x)]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{p(x)}{\\pi_{\\theta}(x)}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{\\pi_r(x)}{\\pi_{\\theta}(x)}\\,\\frac{\\pi_r(x)e^{r^{*}(x)/\\beta}}{Z}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\beta\\,\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big]+\\beta\\,\\mathbb{E}_{\\mu}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big].\\tag{6}\n\\end{aligned}\\]\nThus the GDC problem can be solved by maximizing Eq. (6). Consequently, we may directly define the distributional reward (a constant for a fixed \\(\\pi\\)) as \\(\\varphi^{*}(\\pi)=\\mathbb{E}_{\\pi}\\big[\\beta\\,\\log\\tfrac{\\pi^{*}(x)}{\\pi_r(x)}\\big]\\). Therefore, learning a distributionally controlled LLM is equivalent to maximizing the gap \\(\\varphi(\\pi)-\\varphi(\\mu)\\) when the constraint reflects a human preference relation.", "C": "The Generation-with-Distributional-Control (GDC) can be formalized as\n\\[\\theta^{*}=\\arg\\min_{\\theta} \\mathrm{KL}\\big[p(x)\\,\\|\\,\\pi_{\\theta}(x)\\big]-\\big[\\varphi(p)-\\varphi(\\mu)\\big].\\tag{5}\\]\nThe KL term is minimized when \\(p(x)=\\pi_{\\theta}(x)\\), that is, \\(\\pi_r(x)e^{r^{*}(x)/\\beta}/Z=\\pi_{\\theta}(x)\\). Hence we obtain \\(r^{*}(x)=\\beta\\,\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}+\\beta\\log Z\\). We then need only maximize \\(\\varphi(p)-\\varphi(\\mu)\\). \\(\\varphi\\) is defined as the expectation of reward over the given distribution, yielding\n\\[\\begin{aligned}\n\\arg\\max_{\\pi} \\varphi(p)-\\varphi(\\mu)\n&=\\mathbb{E}_{p}[r(x)]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{p(x)}{\\pi_{\\theta}(x)}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta}}\\bigg[\\frac{\\pi_r(x)}{\\pi_{\\theta}(x)}\\,\\frac{\\pi_r(x)e^{r^{*}(x)/\\beta}}{Z}\\,r(x)\\bigg]-\\mathbb{E}_{\\mu}[r(x)]\\\\\n&=\\beta\\,\\mathbb{E}_{\\pi_{\\theta}}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big]-\\beta\\,\\mathbb{E}_{\\mu}\\Big[\\log\\tfrac{\\pi_{\\theta}(x)}{\\pi_r(x)}\\Big].\\tag{6}\n\\end{aligned}\\]\nThus the GDC problem can be solved by maximizing Eq. (6). Consequently, we may directly define the distributional reward (a constant for a fixed \\(\\pi\\)) as \\(\\varphi^{*}(\\pi)=\\mathbb{E}_{\\pi}\\big[\\beta\\,\\log\\tfrac{\\pi^{*}(x)}{\\pi_r(x)}\\big]\\). Therefore, learning a distributionally controlled LLM is equivalent to maximizing the gap \\(\\varphi(\\pi)-\\varphi(\\mu)\\) when the constraint reflects a human preference relation.", "D": "Here is the step-by-step derivation and proof of Lemma 1:\n\n**Step 1: Formulate the GDC Optimization Problem**\n\nWe aim to find the optimal policy $\\pi^*(y)$ that maximizes the expected reward while maintaining proximity to the original model $\\pi_r(y)$, subject to the constraint $\\varphi(\\pi) > \\varphi(\\mu)$. This is equivalent to solving:\n\n$$\n\\max_{\\pi} \\mathbb{E}_{y\\sim\\pi}[r^*(y)] - \\beta \\cdot \\text{KL}(\\pi \\parallel \\pi_r)\n$$\n\nwhere $\\text{KL}(\\pi \\parallel \\pi_r) = \\mathbb{E}_{y\\sim\\pi}\\left[\\log\\frac{\\pi(y)}{\\pi_r(y)}\\right]$ ensures policy proximity, and $\\beta > 0$ is the temperature parameter.\n\n---\n\n**Step 2: Solve the Unconstrained Optimization**\n\nIntroduce the Lagrangian for normalization constraint $\\sum_y \\pi(y) = 1$:\n\n$$\n\\mathcal{L} = \\mathbb{E}_{y\\sim\\pi}[r^*(y)] - \\beta \\cdot \\mathbb{E}_{y\\sim\\pi}\\left[\\log\\frac{\\pi(y)}{\\pi_r(y)}\\right] + \\lambda\\left(1 - \\sum_y \\pi(y)\\right)\n$$\n\nTake functional derivative w.r.t. $\\pi(y)$ and set to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi(y)} = r^*(y) - \\beta\\left(\\log\\frac{\\pi(y)}{\\pi_r(y)} + 1\\right) - \\lambda = 0\n$$\n\nSolve for $\\pi(y)$:\n$$\n\\log\\frac{\\pi^*(y)}{\\pi_r(y)} = \\frac{r^*(y)}{\\beta} - \\frac{\\lambda + \\beta}{\\beta}\n$$\nExponentiate both sides:\n$$\n\\pi^*(y) = \\pi_r(y) \\exp\\left(\\frac{r^*(y)}{\\beta}\\right) \\cdot \\exp\\left(-\\frac{\\lambda + \\beta}{\\beta}\\right)\n$$\n\n---\n\n**Step 3: Identify Normalization Constant**\n\nLet $Z = \\exp\\left(\\frac{\\lambda + \\beta}{\\beta}\\right)$, which ensures $\\sum_y \\pi^*(y) = 1$:\n$$\n\\pi^*(y) = \\frac{\\pi_r(y) \\exp\\left(r^*(y)/\\beta\\right)}{Z}\n$$\n\nSolve for $r^*(y)$ by rearranging:\n$$\nr^*(y) = \\beta \\log\\frac{\\pi^*(y)}{\\pi_r(y)} + \\beta \\log Z\n$$\n\n---\n\n**Step 4: Derive Distributional Preference Value**\n\nBy definition, $\\varphi^*(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[\\beta \\log\\frac{\\pi^*(y)}{\\pi_r(y)}\\right]$. Substitute $r^*(y)$ from above:\n$$\n\\varphi^*(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[r^*(y) - \\beta \\log Z\\right] = \\mathbb{E}_{y\\sim\\pi}\\left[r^*(y)\\right] - \\beta \\log Z\n$$\n\nHowever, for the **optimal policy** $\\pi^*$, the normalization term $\\beta \\log Z$ equals the maximum achievable reward:\n$$\n\\beta \\log Z = \\mathbb{E}_{y\\sim\\pi^*}\\left[r^*(y)\\right] - \\beta \\cdot \\text{KL}(\\pi^* \\parallel \\pi_r)\n$$\n\nThus, under the GDC constraint $\\varphi(\\pi) > \\varphi(\\mu)$, the preference value reduces to:\n$$\n\\varphi^*(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[\\beta \\log\\frac{\\pi^*(y)}{\\pi_r(y)}\\right]\n$$\n\n---\n\n**Conclusion**\n\nThe optimal reward function and preference value under GDC constraints are:\n$$\n\\boxed{\nr^{*}(y) = \\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)} + \\beta \\log Z, \\quad\n\\varphi^{*}(\\pi) = \\mathbb{E}_{y\\sim\\pi}\\left[\\beta \\log \\frac{\\pi^{*}(y)}{\\pi_r(y)}\\right]\n}\n$$\n\nThis completes the proof of Lemma 1.", "ground-truth": "C"}
{"paper": "arxiv:2308.02585", "question": "How can the norm of the second-order mixed Hessian term of the lower-level value function, $\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|$, be bounded?\n\n**Definitions:**\n\\begin{itemize}\n    \\item Lower-level value function: $V_s(\\nu, \\theta) = \\mathbb{E}_{\\pi_\\theta, P}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h r_\\nu(s_h, a_h) \\mid s_0=s \\right]$. Here $H_\\ell$ is the lower-level horizon length and $\\gamma \\in (0,1)$ is the discount factor.\n    \\item Approximate policy parameter at iteration $t$: $\\theta^K(\\nu_t)$.\n    \\item The mixed Hessian term $\\nabla^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))$ (representing second derivatives w.r.t. $\\nu$ and $\\theta$) is defined via expectation over trajectories $\\tau$ generated by the policy $\\pi_{\\theta^K(\\nu_t)}$ with distribution $\\rho(\\tau;\\theta^K(\\nu_t))$:\n    \\[\n        {\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t)) = \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg]\n    \\]\n\\end{itemize}\n\n**Assumptions:**\n\\begin{itemize}\n    \\item \\textbf{Reward Gradient Bound:} The reward function $r_\\nu(s,a)$ has a bounded gradient w.r.t. $\\nu$: $\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r$ for all $s, a$.\n    \\item \\textbf{Policy Score Bound:} The score function (log-policy gradient) is bounded: $\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B$ for all $s, a$.\n\\end{itemize}\n\n**Goal:** Using these definitions and assumptions, derive the upper bound:\n\\[\n    \\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq H_\\ell^2 L_r B\n\\]", "A": "To bound the norm of the second-order mixed Hessian term \\(\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|\\), we proceed as follows:\n\n1. **Apply Norm Properties:**  \n   Using the triangle inequality and Jensens inequality for the expectation over trajectories:  \n   \\[\n   \\|{\\nabla}^2_{\\nu, \\theta}V_s\\| \\leq \\mathbb{E}_{\\rho}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h \\|\\nabla_{\\nu} r_{\\nu_t}\\| \\left\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right\\|\\right]\n   \\]\n\n2. **Substitute Bounds from Assumptions:**  \n   - By the **Reward Gradient Bound**: \\(\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r\\) for all \\(s, a\\).  \n   - By the **Policy Score Bound**: \\(\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B\\) for all \\(s, a\\).  \n   Thus, the inner sum is bounded as:  \n   \\[\n   \\left\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}\\right\\| \\leq \\sum_{j=0}^{h} B = (h+1)B\n   \\]\n\n3. **Bound the Sum Over Horizon Steps:**  \n   Substitute these into the inequality:  \n   \\[\n   \\|{\\nabla}^2_{\\nu, \\theta}V_s\\| \\leq \\mathbb{E}_{\\rho}\\left[\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h L_r (h+1)B \\right] = L_r B \\sum_{h=0}^{H_{\\ell}-1} \\gamma^h (h+1)\n   \\]  \n   Since \\(\\gamma \\in (0,1)\\), the sum \\(\\sum_{h=0}^{H_{\\ell}-1} \\gamma^h (h+1)\\) is maximized when \\(\\gamma = 1\\), yielding:  \n   \\[\n   \\sum_{h=0}^{H_{\\ell}-1} (h+1) = \\frac{H_{\\ell}(H_{\\ell}+1)}{2} \\leq H_{\\ell}^2 \\quad \\text{(for \\(H_{\\ell} \\geq 1\\))}\n   \\]\n\n4. **Final Upper Bound:**  \n   Combining these results:  \n   \\[\n   \\|{\\nabla}^2_{\\nu, \\theta}V_s\\| \\leq H_{\\ell}^2 L_r B\n   \\]\n\n**Conclusion:** The norm of the mixed Hessian is bounded by \\(\\boxed{H_{\\ell}^2 L_r B}\\).", "B": "We start with the definition:\n\\begin{align}\n    \\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t)) &=  \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right)^T \\Bigg] \\nonumber \\\\\n    & \\leq \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\nonumber \\\\\n    & = \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \n\\end{align}\nwhere $\\tau' = [(s_0', a_0') \\cdot (s_h', a_h') \\cdots]$ represents the trajectory for which the term inside the expectation is maximum and thereby upper-bounding with that leads to the expression above. Next we upper-bound the norm of the $\\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\|$ as\n\\begin{align}\n    \\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\| &\\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T\\Bigg\\| \\nonumber \\\\\n    & \\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\Bigg\\|\\cdot\\Bigg\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\Bigg\\| \\nonumber\\\\\n    & \\leq H_\\ell^2 L_r B \n\\end{align}\nwhere we apply the Cauchy-Schwarz inequality to get the inequality in the second line. Subsequently, we apply triangle inequality with the reward gradient bound assumption ($L_r$) and the bounded score function assumption ($B$) to get the final bound for the mixed hessian term.", "C": "We start with the definition:\n\\begin{align}\n    \\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t)) &=  \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\right)^T \\Bigg] \\nonumber \\\\\n    & \\leq \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \\sum_{\\tau} \\rho(\\tau; \\theta^K(\\nu_t)) \\nonumber \\\\\n    & = \\Bigg[ \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T \\Bigg] \n\\end{align}\nwhere $\\tau' = [(s_0', a_0') \\cdot (s_h', a_h') \\cdots]$ represents the trajectory for which the term inside the expectation is maximum and thereby upper-bounding with that leads to the expression above. Next we upper-bound the norm of the $\\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\|$ as\n\\begin{align}\n    \\|\\nabla^2_{\\nu,\\theta}V_s(\\nu_t, \\theta^K(\\nu_t))\\| &\\leq \\Bigg\\| \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\cdot\\left(\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\right)^T\\Bigg\\| \\nonumber \\\\\n    & \\leq \\sum_{h =0}^{H_\\ell-1}\\gamma^{h} \\cdot \\Bigg\\| \\nabla_{\\nu} r_{\\nu_t}(s_h',a_h')\\Bigg\\|\\cdot\\Bigg\\|\\sum_{j=0}^{h}\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j'|s_j')\\Bigg\\| \\nonumber\\\\\n    & \\leq H_\\ell^2 L_r B \n\\end{align}\nwhere we apply the triangle inequality to get the inequality in the second line. Subsequently, we apply the reward gradient bound assumption ($L_r$) and the bounded score function assumption ($B$) to get the final bound for the mixed hessian term.", "D": "To derive the upper bound for the norm of the second-order mixed Hessian term \\(\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\|\\), we will proceed step-by-step using the given definitions and assumptions.\n\n### Step 1: Express the Mixed Hessian Term\nThe mixed Hessian term is given by:\n\\[\n{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t)) = \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg]\n\\]\n\n### Step 2: Take the Norm Inside the Expectation\nUsing the linearity of expectation and the property that the norm of an expectation is less than or equal to the expectation of the norm (Jensen's inequality), we have:\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq \\mathbb{E}_{\\rho(\\tau;\\theta^K(\\nu_t))} \\Bigg[\\Bigg\\|\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\Bigg]\n\\]\n\n### Step 3: Apply the Triangle Inequality\nThe triangle inequality allows us to bound the norm of the sum by the sum of the norms:\n\\[\n\\Bigg\\|\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\leq \\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} \\Bigg\\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\|\n\\]\n\n### Step 4: Bound the Inner Terms\nUsing the submultiplicative property of norms, we have:\n\\[\n\\Bigg\\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\leq \\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\| \\cdot \\Bigg\\|\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\Bigg\\|\n\\]\n\nFrom the **Reward Gradient Bound** assumption, \\(\\|\\nabla_{\\nu} r_{\\nu_t}(s,a)\\| \\leq L_r\\). From the **Policy Score Bound** assumption, \\(\\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a|s)\\| \\leq B\\). Therefore:\n\\[\n\\Bigg\\|\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\Bigg\\| \\leq \\sum_{j=0}^{h} \\|\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)\\| \\leq (h+1)B\n\\]\n\nThus:\n\\[\n\\Bigg\\|\\nabla_{\\nu} r_{\\nu_t}(s_h,a_h)\\bigg(\\sum_{j=0}^{h}[\\nabla_{\\theta} \\log \\pi_{\\theta^K(\\nu_t)}(a_j|s_j)]^T\\bigg) \\Bigg\\| \\leq L_r \\cdot (h+1)B\n\\]\n\n### Step 5: Sum Over the Horizon\nSubstituting back into the sum over \\(h\\):\n\\[\n\\sum_{h =0}^{H_{\\ell}-1}\\gamma^{h} L_r (h+1)B \\leq L_r B \\sum_{h =0}^{H_{\\ell}-1} (h+1)\n\\]\nsince \\(\\gamma \\in (0,1)\\) and \\(\\gamma^h \\leq 1\\).\n\nThe sum \\(\\sum_{h=0}^{H_{\\ell}-1} (h+1)\\) is a simple arithmetic series:\n\\[\n\\sum_{h=0}^{H_{\\ell}-1} (h+1) = \\frac{H_{\\ell}(H_{\\ell}+1)}{2} \\leq H_{\\ell}^2\n\\]\nfor \\(H_{\\ell} \\geq 1\\).\n\n### Step 6: Final Bound\nCombining all the steps, we obtain:\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq L_r B \\cdot H_{\\ell}^2\n\\]\n\n### Conclusion\nThus, under the given assumptions, the norm of the second-order mixed Hessian term is bounded by:\n\\[\n\\|{\\nabla}^2_{\\nu, \\theta}V_s(\\nu_t,\\theta^K(\\nu_t))\\| \\leq H_\\ell^2 L_r B\n\\]\n\nThis completes the derivation. The key steps involved applying the triangle inequality, submultiplicative property of norms, and the given bounds on the reward gradient and policy score function. The horizon length \\(H_\\ell\\) appears quadratically due to the nested summation over the trajectory length.", "ground-truth": "B"}
{"paper": "arxiv:2410.11443", "question": "Let $x_{is}, x_{jt}$ be difference between node positions and $Y$ be the harmonics. Why there exists a bijection between the set of inner products $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ and angles $\\{\\arccos \\langle x_{is}, x_{jt}\\rangle\\}$?", "A": "To establish the bijection between the set of inner products \\(\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle\\) and the angles \\(\\{\\arccos \\langle x_{is}, x_{jt} \\rangle\\}\\), we need to carefully analyze the properties of spherical harmonics and the geometric interpretation of the inner products.\n\n### 1. **Spherical Harmonics and Their Properties**\nSpherical harmonics \\(Y^{(l)}\\) are eigenfunctions of the Laplacian on the sphere \\(S^{d-1}\\) and form an orthonormal basis for the space of square-integrable functions on the sphere. A key property is that the inner product of two spherical harmonics of the same degree \\(l\\) depends only on the angle between the points \\(x\\) and \\(y\\) on the sphere. Specifically, for unit vectors \\(x, y \\in S^{d-1}\\), we have:\n\n\\[\n\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = P_l(\\langle x, y \\rangle),\n\\]\n\nwhere \\(P_l\\) is the Legendre polynomial of degree \\(l\\) (in the case of \\(S^2\\)) or more generally, the Gegenbauer polynomial for higher-dimensional spheres. The Legendre/Gegenbauer polynomials are functions that depend solely on the inner product \\(\\langle x, y \\rangle\\).\n\n### 2. **Geometric Interpretation**\nThe inner product \\(\\langle x, y \\rangle\\) between two unit vectors \\(x\\) and \\(y\\) is equal to the cosine of the angle \\(\\theta\\) between them:\n\n\\[\n\\langle x, y \\rangle = \\cos \\theta, \\quad \\theta = \\arccos \\langle x, y \\rangle.\n\\]\n\nThus, the angle \\(\\theta\\) is uniquely determined by \\(\\langle x, y \\rangle\\), and vice versa, since \\(\\cos\\) is bijective on \\([0, \\pi]\\).\n\n### 3. **Bijection Between Inner Products and Angles**\nGiven the above, the inner product \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = P_l(\\langle x, y \\rangle)\\) is a function of \\(\\langle x, y \\rangle\\), which in turn is a function of \\(\\theta = \\arccos \\langle x, y \\rangle\\). \n\nThe Legendre/Gegenbauer polynomial \\(P_l\\) is a polynomial of degree \\(l\\), and for fixed \\(l\\), it is a strictly monotonic function on \\([-1, 1]\\) for certain ranges (e.g., \\(P_l\\) is strictly increasing on \\([0, 1]\\) for even \\(l\\) and strictly decreasing for odd \\(l\\) on some intervals). This monotonicity ensures that \\(P_l\\) is injective (one-to-one) on the relevant domain, meaning that each value of \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle\\) corresponds to exactly one value of \\(\\langle x, y \\rangle\\), and hence to exactly one angle \\(\\theta = \\arccos \\langle x, y \\rangle\\).\n\n### 4. **Conclusion**\nTherefore, there exists a bijection between:\n- The set of inner products \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle\\), and \n- The set of angles \\(\\{\\arccos \\langle x, y \\rangle\\}\\), \n\nbecause:\n1. \\(\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle\\) is a function of \\(\\langle x, y \\rangle\\) via \\(P_l\\).\n2. \\(\\langle x, y \\rangle\\) is a bijective function of \\(\\theta = \\arccos \\langle x, y \\rangle\\).\n3. The composition of these relationships preserves the bijection, as \\(P_l\\) is injective on the relevant domain.\n\nThis bijection is mathematically expressed as:\n\n\\[\n\\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = P_l(\\cos \\theta), \\quad \\theta = \\arccos \\langle x, y \\rangle,\n\\]\n\nwhere \\(P_l\\) is invertible on the appropriate interval, ensuring the bijection.", "B": "First, the output of inner product $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ is Legendre polynomials, which span a space isomorphic to power polynomials. Moreover, through Newton's identities and Vieta's theorem, we can get the set of all angle cosines. Also note that trigonometric functions directly form bijections to angles, so this bijection exists. However, this bijection is only valid for angles in the interval $[0, \\pi/2]$.", "C": "The existence of a bijection between the set of inner products $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ and angles $\\{\\arccos \\langle x_{is}, x_{jt} \\rangle\\}$ arises from the properties of spherical harmonics and the relationship between their inner products and angular geometry. Here's the key derivation:\n\n1. **Spherical Harmonics and Addition Theorem**:  \n   For unit vectors $x, y \\in \\mathbb{S}^2$, the inner product of spherical harmonics $Y^{(l)}(x)$ and $Y^{(l)}(y)$ (treated as vectors over $m = -l, \\dots, l$) satisfies:  \n   \\begin{align*}\n   \\langle Y^{(l)}(x), Y^{(l)}(y) \\rangle = \\sum_{m=-l}^l Y_{l,m}(x) Y_{l,m}(y) = \\frac{2l + 1}{4\\pi} P_l(\\cos\\theta),\n   \\end{align*}\n   where $\\theta = \\arccos \\langle x, y \\rangle$ is the angle between $x$ and $y$, and $P_l$ is the Legendre polynomial of degree $l$.\n\n2. **Bijection Condition**:  \n   The bijection exists if and only if the function $P_l(\\cos\\theta)$ is injective over $\\theta \\in [0, \\pi]$. This holds **specifically for $l = 1$**:  \n   - For $l = 1$, $P_1(t) = t$, so $\\frac{2(1) + 1}{4\\pi} P_1(\\cos\\theta) = \\frac{3}{4\\pi} \\cos\\theta$. Since $\\cos\\theta$ is strictly decreasing and injective on $[0, \\pi]$, there is a one-to-one correspondence between $\\langle Y^{(1)}(x), Y^{(1)}(y)\\rangle$ and $\\theta$.  \n   - For $l \\geq 2$, $P_l(\\cos\\theta)$ is not injective. For example, $P_2(\\cos\\theta) = \\frac{3\\cos^2\\theta - 1}{2}$ is symmetric about $\\theta = \\pi/2$, leading to multiple $\\theta$ values mapping to the same inner product.\n\n3. **Conclusion**:  \n   The bijection exists **only for $l = 1$** because $P_1(\\cos\\theta)$ linearly and uniquely encodes the angle $\\theta$. For higher $l$, $P_l(\\cos\\theta)$ loses injectivity due to polynomial oscillations, breaking the bijection. Thus, the bijection is fundamentally tied to the linearity of the dipole term ($l = 1$) in spherical harmonics.", "D": "First, the output of inner product $\\langle Y^{(l)}(x), Y^{(l)}(y)\\rangle$ is Legendre polynomials, which span a space isomorphic to power polynomials. Moreover, through Newton's identities and Vieta's theorem, we can get the set of all angle cosines. Also note that trigonometric functions directly form bijections to angles, so this bijection exists.", "ground-truth": "D"}
{"paper": "Real Analysis: Modern Techniques and Their Applications", "question": "For any measurable functions $f$ and $g$ on a measure space $(X, \\mu)$, let $p, q > 1$ be conjugate exponents, meaning they satisfy \n \\[\\frac{1}{p} + \\frac{1}{q} = 1.\\] \n Holder's inequality states:\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\left( \\int_X |g(x)|^q d\\mu(x) \\right)^{\\frac{1}{q}}.\n\\]", "A": "Define the function\n\\[\n    h(x) = \\frac{|f(x)|}{\\|f\\|_p} \\quad \\text{and} \\quad k(x) = \\frac{|g(x)|}{\\|g\\|_q}.\n\\]\n\nBy Young's inequality, which states that for nonnegative $a, b$,\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]\n\nwe apply this to $h(x)^p$ and $k(x)^q$:\n\\[\n    |f(x) g(x)| \\leq \\frac{|f(x)|^p}{p \\|f\\|_p^p} + \\frac{|g(x)|^q}{q \\|g\\|_q^q}.\n\\]\n\nIntegrating both sides over $X$ gives\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\nMultiplying both sides by $\\|f\\|_p \\|g\\|_q$ completes the proof.\n", "B": "**Proof of Hlder's Inequality:**\n\nWe will prove Hlder's inequality for measurable functions \\( f \\) and \\( g \\) on a measure space \\( (X, \\mu) \\), where \\( p, q > 1 \\) are conjugate exponents, i.e., \\( \\frac{1}{p} + \\frac{1}{q} = 1 \\).\n\n### Step 1: Reduction to the Case Where \\( \\|f\\|_p = \\|g\\|_q = 1 \\)\n\nFirst, observe that if either \\( \\|f\\|_p = 0 \\) or \\( \\|g\\|_q = 0 \\), then \\( f \\) or \\( g \\) is zero almost everywhere, and the inequality holds trivially. Thus, assume \\( \\|f\\|_p > 0 \\) and \\( \\|g\\|_q > 0 \\). Define the normalized functions:\n\\[\n\\tilde{f} = \\frac{f}{\\|f\\|_p}, \\quad \\tilde{g} = \\frac{g}{\\|g\\|_q}.\n\\]\nThen \\( \\|\\tilde{f}\\|_p = \\|\\tilde{g}\\|_q = 1 \\). If we can show that\n\\[\n\\int_X |\\tilde{f}(x) \\tilde{g}(x)| d\\mu(x) \\leq 1,\n\\]\nthen multiplying both sides by \\( \\|f\\|_p \\|g\\|_q \\) yields the desired inequality:\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\|f\\|_p \\|g\\|_q.\n\\]\nThus, it suffices to prove the inequality under the assumption \\( \\|f\\|_p = \\|g\\|_q = 1 \\).\n\n### Step 2: Application of Young's Inequality\n\nFor any \\( a, b \\geq 0 \\), Young's inequality states:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]\nApply this to \\( a = |f(x)| \\) and \\( b = |g(x)| \\):\n\\[\n|f(x) g(x)| \\leq \\frac{|f(x)|^p}{p} + \\frac{|g(x)|^q}{q}.\n\\]\nIntegrate both sides over \\( X \\):\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} \\int_X |f(x)|^p d\\mu(x) + \\frac{1}{q} \\int_X |g(x)|^q d\\mu(x).\n\\]\nBy our assumption \\( \\|f\\|_p = \\|g\\|_q = 1 \\), this simplifies to:\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\nThis completes the proof under the normalization \\( \\|f\\|_p = \\|g\\|_q = 1 \\).\n\n### Step 3: General Case\n\nFor general \\( f \\) and \\( g \\), as shown in Step 1, we can normalize and then multiply by \\( \\|f\\|_p \\|g\\|_q \\) to obtain:\n\\[\n\\int_X |f(x) g(x)| d\\mu(x) \\leq \\left( \\int_X |f(x)|^p d\\mu(x) \\right)^{\\frac{1}{p}} \\left( \\int_X |g(x)|^q d\\mu(x) \\right)^{\\frac{1}{q}}.\n\\]\nThis is Hlder's inequality. \\(\\square\\)", "C": "Define the function\n\\[\n    h(x) = \\frac{|f(x)|}{\\|f\\|_p} \\quad \\text{and} \\quad k(x) = \\frac{|g(x)|}{\\|g\\|_q}.\n\\]\n\nBy Young's inequality, which states that for nonnegative $a, b$,\n\\[\n    ab \\leq \\frac{a^p}{p} + \\frac{b^q}{q},\n\\]\n\nwe apply this to $h(x)$ and $k(x)$:\n\\[\n    |f(x) g(x)| \\leq \\frac{|f(x)|^p}{p \\|f\\|_p^p} + \\frac{|g(x)|^q}{q \\|g\\|_q^q}.\n\\]\n\nIntegrating both sides over $X$ gives\n\\[\n    \\int_X |f(x) g(x)| d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\nMultiplying both sides by $\\|f\\|_p \\|g\\|_q$ completes the proof.", "D": "To prove Hlder's inequality, we proceed as follows:\n\n**Step 1: Handle Trivial Cases**  \nIf \\(\\|f\\|_p = 0\\) or \\(\\|g\\|_q = 0\\), then \\(f\\) or \\(g\\) vanishes \\(\\mu\\)-almost everywhere, making the left-hand side zero. The inequality holds trivially. Hence, assume \\(\\|f\\|_p > 0\\) and \\(\\|g\\|_q > 0\\).\n\n**Step 2: Normalize \\(f\\) and \\(g\\)**  \nDefine normalized functions:  \n\\[\nF(x) = \\frac{f(x)}{\\|f\\|_p}, \\quad G(x) = \\frac{g(x)}{\\|g\\|_q},\n\\]  \nso that \\(\\|F\\|_p = 1\\) and \\(\\|G\\|_q = 1\\). The inequality reduces to:  \n\\[\n\\int_X |F(x)G(x)| \\, d\\mu(x) \\leq 1.\n\\]\n\n**Step 3: Apply Young's Inequality**  \nFor \\(a, b \\geq 0\\), Young's inequality states:  \n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}.\n\\]  \nSubstitute \\(a = |F(x)|\\) and \\(b = |G(x)|\\):  \n\\[\n|F(x)G(x)| \\leq \\frac{|F(x)|^p}{p} + \\frac{|G(x)|^q}{q}.\n\\]\n\n**Step 4: Integrate Both Sides**  \nIntegrate over \\(X\\):  \n\\[\n\\int_X |F(x)G(x)| \\, d\\mu(x) \\leq \\frac{1}{p} \\int_X |F(x)|^p \\, d\\mu(x) + \\frac{1}{q} \\int_X |G(x)|^q \\, d\\mu(x).\n\\]  \nSince \\(\\|F\\|_p = 1\\) and \\(\\|G\\|_q = 1\\), we have:  \n\\[\n\\int_X |F(x)|^p \\, d\\mu(x) = 1, \\quad \\int_X |G(x)|^q \\, d\\mu(x) = 1.\n\\]  \nThus:  \n\\[\n\\int_X |F(x)G(x)| \\, d\\mu(x) \\leq \\frac{1}{p} + \\frac{1}{q} = 1.\n\\]\n\n**Step 5: Recover Original Inequality**  \nMultiplying back by \\(\\|f\\|_p \\|g\\|_q\\) gives:  \n\\[\n\\int_X |f(x)g(x)| \\, d\\mu(x) \\leq \\|f\\|_p \\|g\\|_q.\n\\]  \nThis completes the proof of Hlder's inequality. \n\n**Key Points:**  \n- The normalization ensures the application of Young's inequality.  \n- Equality holds in Youngs inequality if and only if \\(a^p = b^q\\), corresponding to \\(|f(x)|^p\\) and \\(|g(x)|^q\\) being proportional \\(\\mu\\)-a.e. in the original inequality.", "ground-truth": "A"}
{"paper": "Numerical Optimization", "question": "Let the function \\( f(x) \\) be twice continuously differentiable in a neighborhood of the minimum point \\( x^* \\). Suppose there exist \\(\\varepsilon > 0\\) and \\( M > m > 0 \\) such that when \\(\\|x - x^*\\| < \\varepsilon\\),\n\\[\nm \\|y\\|^2 \\leq y^T G(x) y \\leq M \\|y\\|^2, \\quad \\forall y \\in \\mathbb{R}^n \n\\]\nholds. \n\nAdditionally, given the expansion:\n\\[\nf(x + \\alpha d) = f(x) + \\alpha g^T d + \\alpha^2 \\int_{0}^{1} (1 - t) \\left[ d^T G(x + t\\alpha d) \\right] dt,\n\\]\n\nwe aim to prove:\n\\begin{align*}\n   \\|g(x)\\| \\geq m \\|x - x^*\\|. \n\\end{align*}", "A": "To prove the inequality \\(\\|g(x)\\| \\geq m \\|x - x^*\\|\\) under the given conditions, we proceed with the following steps:\n\n### 1. Taylor Expansion Around the Minimum Point \\(x^*\\)\nSince \\(x^*\\) is a minimum point, the gradient \\(g(x^*) = 0\\). Using the second-order Taylor expansion of \\(f(x)\\) around \\(x^*\\), we have:\n\\[\nf(x) = f(x^*) + \\frac{1}{2} (x - x^*)^T G(\\xi) (x - x^*),\n\\]\nwhere \\(\\xi\\) lies on the line segment between \\(x\\) and \\(x^*\\). Taking the gradient of both sides with respect to \\(x\\) yields:\n\\[\ng(x) = G(\\xi)(x - x^*).\n\\]\n\n### 2. Bounding the Hessian \\(G(\\xi)\\)\nGiven the condition on the Hessian \\(G(x)\\) in the neighborhood \\(\\|x - x^*\\| < \\varepsilon\\):\n\\[\nm \\|y\\|^2 \\leq y^T G(x) y \\leq M \\|y\\|^2, \\quad \\forall y \\in \\mathbb{R}^n,\n\\]\nit follows that the eigenvalues of \\(G(\\xi)\\) are bounded below by \\(m\\) and above by \\(M\\). Therefore, the operator norm of \\(G(\\xi)\\) satisfies:\n\\[\n\\|G(\\xi)\\| \\geq m.\n\\]\n\n### 3. Estimating the Norm of the Gradient\nFrom the expression \\(g(x) = G(\\xi)(x - x^*)\\), we take norms on both sides:\n\\[\n\\|g(x)\\| = \\|G(\\xi)(x - x^*)\\|.\n\\]\nUsing the property of the operator norm:\n\\[\n\\|G(\\xi)(x - x^*)\\| \\geq \\|G(\\xi)\\| \\cdot \\|x - x^*\\| \\geq m \\|x - x^*\\|.\n\\]\n\n### 4. Conclusion\nThus, we have established the desired inequality:\n\\[\n\\|g(x)\\| \\geq m \\|x - x^*\\|.\n\\]\n\nThis completes the proof. \\(\\square\\)", "B": "By the expansion,\n\\begin{align*}\nf(x) - f(x^*) &= g(x^*)^T (x - x^*) +\\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\mathrm{d}t \\\\ \n&= \\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\, \\mathrm{d}t,\n\\end{align*}\n\nPerforming a Taylor expansion on \\( g(x) \\):\n\\[\ng(x) = g(x) - g(x^*) = \\int_0^1 G(tx + (1-t)x^*) (x - x^*) dt.\n\\]\n\nThus, we have:\n\\begin{align*}\n\\|g(x)\\|\\|x - x^*\\| &\\geqslant (x - x^*)^\\top g(x) \\\\\n&= \\int_0^1 (x - x^*)^\\top G(tx + (1 - t)x^*)(x - x^*) \\, \\mathrm{d}t \\\\\n&\\geqslant m \\|x - x^*\\|^2\n\\end{align*}", "C": "By the expansion,\n\\begin{align*}\nf(x) - f(x^*) &= g(x^*)^T (x - x^*) +\\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\mathrm{d}t \\\\ \n&= \\int_0^1 (1-t)(x-x^*)^{\\top}G(tx+(1-t)x^*)(x-x^*) \\, \\mathrm{d}t,\n\\end{align*}\n\nPerforming a Taylor expansion on \\( g(x) \\):\n\\[\ng(x) = g(x) - g(x^*) = \\int_0^1 G(tx + (1-t)x^*) (x - x^*) dt.\n\\]\n\nThus, we have:\n\\begin{align*}\n\\|g(x)\\|\\|x - x^*\\| &\\geqslant (x - x^*)^\\top g(x) \\\\\n&= \\int_0^1 (x - x^*)^\\top G(tx + (1 - t)x^*)(x - x^*) \\, \\mathrm{d}t \\\\\n&\\geqslant M \\|x - x^*\\|^2\n\\end{align*}", "D": "To prove \\(\\|g(x)\\| \\geq m \\|x - x^*\\|\\), we proceed as follows:\n\nSince \\(x^*\\) is a minimum point, \\(g(x^*) = 0\\). For \\(x\\) near \\(x^*\\), consider the integral representation of the gradient:\n\\[\ng(x) = \\int_{0}^{1} G(x^* + t(x - x^*)) (x - x^*) \\, dt = H (x - x^*),\n\\]\nwhere \\(H = \\int_{0}^{1} G(x^* + t(x - x^*)) \\, dt\\). \n\nUnder the given Hessian condition, for any \\(y \\in \\mathbb{R}^n\\) and \\(t \\in [0,1]\\):\n\\[\ny^T G(x^* + t(x - x^*)) y \\geq m \\|y\\|^2.\n\\]\nIntegrating over \\(t\\), we have:\n\\[\ny^T H y = \\int_{0}^{1} y^T G(x^* + t(x - x^*)) y \\, dt \\geq m \\|y\\|^2.\n\\]\nThus, \\(H \\succeq m I\\) (positive semi-definite with eigenvalues \\(\\geq m\\)).\n\nTaking \\(y = x - x^*\\), the quadratic form gives:\n\\[\n(x - x^*)^T H (x - x^*) \\geq m \\|x - x^*\\|^2.\n\\]\nBut \\((x - x^*)^T H (x - x^*) = (x - x^*)^T g(x)\\). By the Cauchy-Schwarz inequality:\n\\[\n(x - x^*)^T g(x) \\leq \\|x - x^*\\| \\|g(x)\\|.\n\\]\nCombining these:\n\\[\nm \\|x - x^*\\|^2 \\leq \\|x - x^*\\| \\|g(x)\\| \\implies \\|g(x)\\| \\geq m \\|x - x^*\\|.\n\\]\nHence, the inequality is proven.", "ground-truth": "B"}
{"paper": "arxiv:2501.18922", "question": "Let the model's initial performance on the annotated dataset $D_{a}$ be $E_{\\mathrm{base}}$, and let its performance after incremental fine-tuning be $E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^{*})$, where\n$$\\Delta E(\\gamma^{*}) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}\\bigl(D_{i}(\\gamma^{*})\\bigr).$$\nHere:\n- $\\lambda>0$ is a scaling factor,\n- $D_{i}(\\gamma^{*})$ is the set of logical-form-answer pairs $(\\widehat F_{Q},\\widehat A_{Q})$ drawn from the unannotated dataset $D_{n}$ of size $N_{n}=|D_{n}|$, filtered by the criteria $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_{Q})>\\gamma^{*}$ and $\\widehat A_{Q}\\neq\\emptyset$, \n- $N_{\\mathrm{high}}(\\gamma^{*}) = N_{n}\\cdot P_{\\mathrm{high}}(\\gamma^{*})$ with\n$$P_{\\mathrm{high}}(\\gamma^{*}) = \\int_{\\gamma^{*}}^{\\beta} P(R)\\,dR,\\quad\\text{where }\\beta\\text{ is the maximum reward score,}$$\n- $Q_{\\mathrm{avg}}\\bigl(D_{i}(\\gamma^{*})\\bigr)$ is the average quality of the incremental data.\n\nPlease state and derive the result in Proposition 4.3 showing that there exists some threshold $\\gamma^{*}<\\beta$ for which $\\Delta E(\\gamma^{*})$ grows significantly large.", "A": "First, we define the performance improvement of incremental fine-tuning, $\\Delta E(\\gamma^*)$. The model's initial performance on the annotated dataset $D_a$ is denoted as $E_{\\mathrm{base}}$, and its performance after incremental fine-tuning is\n> \\[E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^*).\\]\n> Here, $\\Delta E(\\gamma^*)$ represents the performance gain contributed by incremental data, which depends on the quantity and quality of the incremental data. \n\n> The incremental data originates from the unannotated dataset $D_n$, where the logical forms $\\widehat F_Q$ and answers $\\widehat A_Q$ are generated through a filtering mechanism. The filtering criteria are: 1) the reward score of the logical form $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_Q)>\\gamma^*$; 2) the answer set is non-empty, $\\widehat A_Q\\neq\\emptyset$. The logical forms that satisfy these criteria form the incremental dataset $D_i(\\gamma^*)$. The size and quality of this dataset are both dependent on the reward threshold $\\gamma^*$.\n\n> The performance improvement can be expressed as:\n> \\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^*) \\cdot Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr),\\]\n> where $\\lambda>0$ is a scaling factor; \n> \\[N_{\\mathrm{high}}(\\gamma^*) = N_n \\cdot P_{\\mathrm{high}}(\\gamma^*),\\quad P_{\\mathrm{high}}(\\gamma^*) = \\int_{\\gamma^*}^{\\beta} P(R)\\,dR;\\]\n> \\[Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr) = \\frac{\\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR}{\\int_{\\gamma^*}^{\\beta} P(R)\\,dR}.\\]\n\n> Substituting these into $\\Delta E(\\gamma^*)$ and simplifying, we get:\n> \\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_n \\cdot \\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR.\\]\n\n> To analyze the critical point, we take the derivative with respect to $\\gamma^*$:\n> \\[\\frac{\\partial \\Delta E(\\gamma^*)}{\\partial \\gamma^*} = -\\lambda \\cdot N_n \\cdot \\gamma^* \\cdot P(\\gamma^*).\\]\n> As $\\gamma^*\\to -\\infty$, this derivative is positive, indicating $\\Delta E(\\gamma^*)$ increases with $\\gamma^*$; as $\\gamma^*\\to \\beta$, the derivative is negative, indicating $\\Delta E(\\gamma^*)$ decreases. Thus, $\\Delta E(\\gamma^*)$ is a unimodal function with a maximum at some $\\gamma^*<\\beta$.\n\n**Conclusion:** There exists a reward threshold $\\gamma^*<\\beta$ such that the trade-off between the quantity and quality of incremental data is optimized and the performance improvement $\\Delta E(\\gamma^*)$ reaches its maximum.", "B": "First, we define the performance improvement of incremental fine-tuning, $\\Delta E(\\gamma^*)$. The model's initial performance on the annotated dataset $D_a$ is denoted as $E_{\\mathrm{base}}$, and its performance after incremental fine-tuning is\n\\[E_{\\mathrm{new}} = E_{\\mathrm{base}} + \\Delta E(\\gamma^*).\\]\nHere, $\\Delta E(\\gamma^*)$ represents the performance gain contributed by incremental data, which depends on the quantity and quality of the incremental data. \n\nThe incremental data originates from the unannotated dataset $D_n$, where the logical forms $\\widehat F_Q$ and answers $\\widehat A_Q$ are generated through a filtering mechanism. The filtering criteria are: 1) the reward score of the logical form $R^{\\pi_{\\mathrm{reward}}}(\\widehat F_Q)>\\gamma^*$; 2) the answer set is non-empty, $\\widehat A_Q\\neq\\emptyset$. The logical forms that satisfy these criteria form the incremental dataset $D_i(\\gamma^*)$. The size and quality of this dataset are both dependent on the reward threshold $\\gamma^*$.\n\nThe performance improvement can be expressed as:\n\\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^*) \\cdot Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr),\\]\nwhere $\\lambda>0$ is a scaling factor; \n\\[N_{\\mathrm{high}}(\\gamma^*) = N_n \\cdot P_{\\mathrm{high}}(\\gamma^*),\\quad P_{\\mathrm{high}}(\\gamma^*) = \\int_{\\gamma^*}^{\\beta} P(R)\\,dR;\\]\n\\[Q_{\\mathrm{avg}}\\bigl(D_i(\\gamma^*)\\bigr) = \\frac{\\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR}{\\int_{\\gamma^*}^{\\beta} P(R)\\,dR}.\\]\n\nSubstituting these into $\\Delta E(\\gamma^*)$ and simplifying, we get:\n\\[\\Delta E(\\gamma^*) = \\lambda \\cdot N_n \\cdot \\int_{\\gamma^*}^{\\beta} R\\,P(R)\\,dR.\\]\n\nTo analyze the critical point, we take the derivative with respect to $\\gamma^*$:\n\\[\\frac{\\partial \\Delta E(\\gamma^*)}{\\partial \\gamma^*} = -\\lambda \\cdot N_n \\cdot \\gamma^* \\cdot P(\\gamma^*).\\]\nAs $\\gamma^*\\to -\\infty$, this derivative is positive, indicating $\\Delta E(\\gamma^*)$ increases with $\\gamma^*$; as $\\gamma^*\\to \\beta$, the derivative is negative, indicating $\\Delta E(\\gamma^*)$ decreases. Thus, $\\Delta E(\\gamma^*)$ is a unimodal function with a maximum at some $\\gamma^*<\\beta$.\n\n**Conclusion:** There exists a reward threshold $\\gamma^*<\\beta$ such that the trade-off between the quantity and quality of incremental data is optimized and the performance improvement $\\Delta E(\\gamma^*)$ reaches its maximum. However, this maximum is only guaranteed if $P(\\gamma^*)>0$ for all $\\gamma^*<\\beta$, which may not hold if the reward distribution $P(R)$ has gaps.", "C": "To derive Proposition 4.3, we analyze the behavior of \\(\\Delta E(\\gamma^{*})\\) as a function of \\(\\gamma^{*}\\) and identify conditions under which it attains significant growth. \n\n### Proposition 4.3  \nThere exists a threshold \\(\\gamma^{*} < \\beta\\) such that the performance improvement \\(\\Delta E(\\gamma^{*})\\) grows significantly large, provided:  \n1. The reward distribution \\(P(R)\\) has sufficient density in the interval \\([\\gamma^{*}, \\beta]\\).  \n2. The average quality \\(Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*}))\\) does not decay faster than \\(P_{\\mathrm{high}}(\\gamma^{*})\\) as \\(\\gamma^{*}\\) increases.  \n\n### Derivation  \n\n1. **Expression for \\(\\Delta E(\\gamma^{*})\\):**  \n   \\[\n   \\Delta E(\\gamma^{*}) = \\lambda \\cdot N_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*})).\n   \\]\n   Substituting \\(N_{\\mathrm{high}}(\\gamma^{*}) = N_{n} \\cdot P_{\\mathrm{high}}(\\gamma^{*})\\), we get:  \n   \\[\n   \\Delta E(\\gamma^{*}) = \\lambda N_{n} \\cdot P_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*})).\n   \\]\n\n2. **Behavior of \\(P_{\\mathrm{high}}(\\gamma^{*})\\):**  \n   \\[\n   P_{\\mathrm{high}}(\\gamma^{*}) = \\int_{\\gamma^{*}}^{\\beta} P(R) \\, dR.\n   \\]\n   - As \\(\\gamma^{*} \\to \\beta\\), \\(P_{\\mathrm{high}}(\\gamma^{*}) \\to 0\\).  \n   - For \\(\\gamma^{*} < \\beta\\), if \\(P(R)\\) is non-negligible in \\([\\gamma^{*}, \\beta]\\), then \\(P_{\\mathrm{high}}(\\gamma^{*})\\) remains substantial.  \n\n3. **Assumption on \\(Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*}))\\):**  \n   - Let \\(Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*}))\\) be lower-bounded by a function \\(Q_{\\mathrm{min}}(\\gamma^{*})\\) such that:  \n     \\[\n     Q_{\\mathrm{avg}}(D_{i}(\\gamma^{*})) \\geq Q_{\\mathrm{min}}(\\gamma^{*}).\n     \\]\n   - Assume \\(Q_{\\mathrm{min}}(\\gamma^{*})\\) does not decay faster than \\(P_{\\mathrm{high}}(\\gamma^{*})\\), i.e.,  \n     \\[\n     \\lim_{\\gamma^{*} \\to \\beta} \\frac{Q_{\\mathrm{min}}(\\gamma^{*})}{P_{\\mathrm{high}}(\\gamma^{*})} \\geq c > 0.\n     \\]\n\n4. **Lower Bound for \\(\\Delta E(\\gamma^{*})\\):**  \n   \\[\n   \\Delta E(\\gamma^{*}) \\geq \\lambda N_{n} \\cdot P_{\\mathrm{high}}(\\gamma^{*}) \\cdot Q_{\\mathrm{min}}(\\gamma^{*}).\n   \\]\n   If \\(Q_{\\mathrm{min}}(\\gamma^{*}) = \\kappa \\cdot P_{\\mathrm{high}}(\\gamma^{*})\\) for some \\(\\kappa > 0\\), then:  \n   \\[\n   \\Delta E(\\gamma^{*}) \\geq \\lambda N_{n} \\kappa \\cdot [P_{\\mathrm{high}}(\\gamma^{*})]^{2}.\n   \\]\n   Thus, \\(\\Delta E(\\gamma^{*})\\) grows quadratically with \\(P_{\\mathrm{high}}(\\gamma^{*})\\).\n\n5. **Existence of \\(\\gamma^{*}\\):**  \n   - Since \\(P_{\\mathrm{high}}(\\gamma^{*})\\) is continuous and monotonically decreasing in \\(\\gamma^{*}\\), by the intermediate value theorem, there exists \\(\\gamma^{*} < \\beta\\) such that \\(P_{\\mathrm{high}}(\\gamma^{*})\\) is sufficiently large to make \\(\\Delta E(\\gamma^{*})\\) significant.  \n   - The optimal \\(\\gamma^{*}\\) balances the trade-off between selectivity (high \\(\\gamma^{*}\\)) and data volume (high \\(P_{\\mathrm{high}}(\\gamma^{*})\\)).  \n\n### Conclusion  \nUnder reasonable assumptions on \\(P(R)\\) and \\(Q_{\\mathrm{avg}}\\), there exists a threshold \\(\\gamma^{*} < \\beta\\) where \\(\\Delta E(\\gamma^{*})\\) attains significant growth, justifying the proposition.  \n\n\\[\n\\boxed{\\exists \\gamma^{*} < \\beta \\text{ such that } \\Delta E(\\gamma^{*}) \\text{ grows significantly large.}}\n\\]", "D": "To establish the existence of a threshold $\\gamma^* < \\beta$ where $\\Delta E(\\gamma^*)$ grows significantly large, we analyze the trade-off between $P_{\\mathrm{high}}(\\gamma)$ (the quantity of filtered data) and $Q_{\\mathrm{avg}}(D_i(\\gamma))$ (the quality of filtered data). The key steps are as follows:\n\n**Proposition 4.3:**  \n*Under the assumptions that:*\n1. $Q_{\\mathrm{avg}}(\\gamma)$ *is non-decreasing and differentiable in* $\\gamma$,\n2. $P(R)$ *is a continuous probability density function over* $[\\alpha, \\beta]$,\n\n*there exists a threshold* $\\gamma^* \\in (\\alpha, \\beta)$ *at which* $\\Delta E(\\gamma^*)$ *attains a local maximum, making it significantly large compared to endpoints.*\n\n**Proof:**  \nDefine the unnormalized performance gain component:  \n$$\nf(\\gamma) = P_{\\mathrm{high}}(\\gamma) \\cdot Q_{\\mathrm{avg}}(\\gamma), \\quad \\gamma \\in [\\alpha, \\beta].\n$$  \nSince $\\Delta E(\\gamma) = \\lambda N_n f(\\gamma)$, maximizing $f(\\gamma)$ suffices.\n\n1. **Boundary Behavior:**  \n   - At $\\gamma = \\beta$:  \n     $P_{\\mathrm{high}}(\\beta) = 0$ (integral over zero measure) $\\implies f(\\beta) = 0$.  \n   - At $\\gamma = \\alpha$:  \n     $P_{\\mathrm{high}}(\\alpha) = \\int_{\\alpha}^\\beta P(R)dR = 1$ (total probability). Assuming $Q_{\\mathrm{avg}}(\\alpha) = Q_{\\mathrm{min}} > 0$, $f(\\alpha) = Q_{\\mathrm{min}}$.  \n\n2. **Intermediate Value Analysis:**  \n   - Since $Q_{\\mathrm{avg}}(\\gamma)$ is non-decreasing, $Q_{\\mathrm{avg}}(\\gamma) \\geq Q_{\\mathrm{min}}$ for $\\gamma > \\alpha$.  \n   - $P_{\\mathrm{high}}(\\gamma)$ decreases continuously from $1$ to $0$ as $\\gamma$ increases from $\\alpha$ to $\\beta$.  \n\n3. **Differentiability and Critical Points:**  \n   Compute the derivative:  \n   $$\n   f'(\\gamma) = \\underbrace{\\frac{dP_{\\mathrm{high}}}{d\\gamma}}_{-P(\\gamma)} Q_{\\mathrm{avg}}(\\gamma) + P_{\\mathrm{high}}(\\gamma) \\underbrace{\\frac{dQ_{\\mathrm{avg}}}{d\\gamma}}_{Q'(\\gamma)}.\n   $$  \n   Setting $f'(\\gamma) = 0$ yields:  \n   $$\n   Q_{\\mathrm{avg}}(\\gamma) \\cdot P(\\gamma) = P_{\\mathrm{high}}(\\gamma) \\cdot Q'(\\gamma).\n   $$  \n\n4. **Existence of $\\gamma^*$:**  \n   - At $\\gamma = \\alpha$:  \n     $Q'(\\alpha) \\geq 0$ (non-decreasing), so $f'(\\alpha) = -P(\\alpha)Q_{\\mathrm{min}} + 1 \\cdot Q'(\\alpha)$.  \n     If $Q'(\\alpha) > P(\\alpha)Q_{\\mathrm{min}}$, then $f'(\\alpha) > 0$.  \n   - At $\\gamma \\to \\beta^-$:  \n     $P_{\\mathrm{high}}(\\gamma) \\to 0$, $Q_{\\mathrm{avg}}(\\gamma) \\to Q_{\\mathrm{max}}$, so $f'(\\gamma) \\approx -P(\\gamma)Q_{\\mathrm{max}} < 0$.  \n\n   By the Intermediate Value Theorem, if $f'(\\gamma)$ transitions from positive to negative and is continuous, there exists $\\gamma^* \\in (\\alpha, \\beta)$ where $f'(\\gamma^*) = 0$. At this point, $f(\\gamma)$ attains a local maximum, making $\\Delta E(\\gamma^*)$ significantly large compared to $\\gamma = \\alpha$ or $\\beta$.  \n\n5. **Conditions for Significant Growth:**  \n   - If $Q_{\\mathrm{avg}}(\\gamma)$ increases sufficiently rapidly (e.g., high-quality data concentrates above moderate $\\gamma$), the product $P_{\\mathrm{high}}(\\gamma)Q_{\\mathrm{avg}}(\\gamma)$ peaks at $\\gamma^*$.  \n   - The scaling factor $\\lambda N_n$ amplifies this peak, ensuring $\\Delta E(\\gamma^*)$ is non-trivial.  \n\nThus, under mild assumptions on $Q_{\\mathrm{avg}}(\\gamma)$ and $P(R)$, such a threshold $\\gamma^* < \\beta$ exists.  \n$\\blacksquare$", "ground-truth": "A"}
